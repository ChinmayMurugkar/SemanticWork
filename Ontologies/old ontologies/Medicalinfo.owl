<?xml version="1.0"?>


<!DOCTYPE rdf:RDF [
    <!ENTITY owl "http://www.w3.org/2002/07/owl#" >
    <!ENTITY xsd "http://www.w3.org/2001/XMLSchema#" >
    <!ENTITY owl2xml "http://www.w3.org/2006/12/owl2-xml#" >
    <!ENTITY rdfs "http://www.w3.org/2000/01/rdf-schema#" >
    <!ENTITY rdf "http://www.w3.org/1999/02/22-rdf-syntax-ns#" >
]>


<rdf:RDF xmlns="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#"
     xml:base="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
     xmlns:owl2xml="http://www.w3.org/2006/12/owl2-xml#"
     xmlns:owl="http://www.w3.org/2002/07/owl#"
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    <owl:Ontology rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Datatypes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Object Properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a -->

    <owl:ObjectProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Data properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Classes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Brussels -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Brussels">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Lausanne -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Lausanne">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Patient_Record -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Patient_Record"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Vancouver -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Vancouver">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#active_networks -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#active_networks">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ifip"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#indianapolis"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#algorithmic_languages_and_calculi -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#algorithmic_languages_and_calculi">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ifip"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#athens_greece -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#athens_greece">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#beijing_china -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#beijing_china">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#cacm -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#cacm">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#computaional_collective_intelligence -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#computaional_collective_intelligence">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#lncs"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#athens_greece"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_data_mining -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_data_mining">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#indianapolis"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_consistency_and_parallel_DB -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_consistency_and_parallel_DB">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_source_selection_and_integration -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_source_selection_and_integration">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#beijing_china"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dke -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dke">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Lausanne"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_data_in_cloud -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_data_in_cloud">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_data_stream -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_data_stream">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_distributed_databases -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_distributed_databases">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_energy_and_performance -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_energy_and_performance">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_mining_and_complex_event -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_mining_and_complex_event">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_olap_and_decision_support -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_olap_and_decision_support">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt_upassala">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#entity_resolution -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#entity_resolution">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#providence_rhode_island"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Vancouver"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Brussels"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_bpm -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_bpm">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_data_evolution_and_adaption -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_data_evolution_and_adaption">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_data_model_theory -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_data_model_theory">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_integration_and_composition -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_integration_and_composition">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_ontologies -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_ontologies">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ifip -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ifip">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#indianapolis -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#indianapolis">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_datamanagement -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_datamanagement">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#indianapolis"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#inofrmation_extraction -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#inofrmation_extraction">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#providence_rhode_island"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#istambul_turkey -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#istambul_turkey">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journal_on_data_semantic -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journal_on_data_semantic">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#lncs"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#lncs -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#lncs">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p2p_based_data_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p2p_based_data_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#beijing_china"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#providence_rhode_island -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#providence_rhode_island">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#query_processing_and_optimization -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#query_processing_and_optimization">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ranking -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ranking">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vancouver"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#seattle_wa -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#seattle_wa">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#security -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#security">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#providence_rhode_island"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig1 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig1">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Patient_Record"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Patient_Record"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#singapore -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#singapore">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#skylines -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#skylines">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vancouver"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#socio_technical_factors -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#socio_technical_factors">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er_2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tods -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tods">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tois -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tois">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#toplas -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#toplas">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tracking_data_in_space -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tracking_data_in_space">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vancouver"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vancouver -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vancouver">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb_j. -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb_j.">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume3 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume3">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#singapore"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume34_2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume34_2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume35_2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume35_2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume36_2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume36_2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#tods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume4 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume4">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#seattle_wa"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume5 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume5">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#istambul_turkey"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume53_2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume53_2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#cacm"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume54_2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume54_2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#cacm"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume54_2012 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume54_2012">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#cacm"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume69 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume69">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dke"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume70 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume70">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dke"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume71 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#volume71">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dke"/>
    </owl:Class>
    


    <!-- http://www.w3.org/2002/07/owl#Thing -->

    <owl:Class rdf:about="&owl;Thing"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Individuals
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p10 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p10">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title : BPO: The Precursor of Digital Firms ###
author : Kakul Agha
Co-ordinator
Department of Business Studies
Middle East College of Information Technology (MECIT)
Knowledge Oasis Muscat
P.O.Box: 79; Al Rusayl; P. C 124
Sultanate of Oman
Tel. no.: +968 99762 634; +968-24446698 Ext – 202
Fax: +968 24446028
E mail : kakul_p@yahoo.com
Registration No. : T 026
Abstract :
The concept of BPO is explained by analyzing primarily the Asian developing countries’
economy and the market situation. Outsourcing has been recently clubbed together with
the advanced use of information technology to invent a tool called BPO, which is the act
of giving a third-party the responsibility of running what would otherwise be an internal
system or service. Typically, companies that are looking at business process outsourcing
are hoping  to  achieve cost savings by  handing  the work  to  a third-party  that can  take
advantage of economies of scale by  doing  the same work  for many  companies. Or
perhaps the cost savings can be achieved because labor costs are lower due to different
costs of living in different countries.
The question is where are we heading to? Are the businesses utilizing the n
th
advantage of
BPO also or are they leaving something to be done in the ‘brick and mortar’ rooms of an
organization?
My  research  paper probes into  the possibilities and  the likelihood  of BPO being  the
precursor of a totally  digital firm that adopts a new business model, the functions and
activities of the management that are spread  globally, communicate, share and 
disseminate information required to operate successfully, on a habitual basis. The tasks
are completed regionally but outputs and profits are generated globally.
Key words: Virtualization, Core competencies2 ###
Introduction : 
The growth  of internet coupled with outsourcing has given  a new tool to the business
world named as Business Process Outsourcing (BPO). It capitalizes on the extensive and
heightened  use of internet and its related technology  to  yield  profit maximization  and
reduction of costs to the organizations. BPO can prove to be more effective in the future
with use of allied  and  sophisticated  tools but is there a possibility  that the human 
resources can be completely taken away? Scientists prove that human resources shall be
an inevitable part of organizations for decision making situation, but surely the ‘brick and
mortar’ offices shall get replaced  with  high  tech  outsourced  jobs. The products and
services that get generated  by  organizations shall be produced  globally  and  marketed
globally too.
Evolution of Internet
The internet is vast in its scope as well as application. The concept of internet came into 
being as a result of an experiment by the US Department of Defense in the 1960s. They
wanted to create a computer network that would continue to function even at the event of
a disaster. It was supposed to be flexible so that if a part of the network was damaged or
destroyed, the rest of the system would still work. And from there started a series of
inventions that resulted in high-tech business processes of today.
Earlier the internet was mainly used for information access and communication. Today
the use of internet has been extended to collaborating with business partners, researching
competitors, providing  customer support and  buying  and selling  products and services.
This implies that the business use of internet is shifting  from that of an  electronic
information exchange to a broad platform for strategic business applications.
The internet itself can be viewed as having six strategic capabilities that support a variety 
of key  applications, which can  provide value to  the organization. These capabilities
include global dissemination, interaction, customization, collaboration, electronic
commerce and integration. Each of these capabilities serves as building blocks for a mix
of internet based applications that can give competitive advantage and strategic business
value to an organization.
With the advent of E-Commerce or electronic commerce, monetary transactions need to
be done online, without the help of physical notes. Experts studying trends in technology
have already gone to the extent of predicting the existence of a society without physical
currency. All a person would actually have to do is transfer the currency from his little
digital wallet to  someone else’s. E-commerce ensures minimum inventory  control,
improved customer service and global reach
1
.
Another widely  used aspect of the internet is databases. Analysts, who  foresee more
complex database in the future view today’s databases as being rather primitive. On the
internet complex  databases are already  being  developed  so much that experts have
predicted that the day is not so far off when practically any information about anything
and anybody will be available on the net.3
These are just some predictions about where internet could take us in the near future. The
scope of the internet is increasing  at such  a pace that it is impossible today to foresee
what it could include tomorrow.
Nolan  has presented  a framework  for understanding  the evolution  of Information
Technology within organizations which describes three eras – the Data Processing or DP
era, the Information Technology or IT era and the Network era
2
.
A Brief History of Outsourcing 
Outsourcing (or contracting out) is often defined as the delegation of non-core operations
or jobs from internal production  to  an external entity  (such  as sub-contractor) that
specializes in  that operation. Outsourcing  is a business decision  that is often  made to
focus on core competence. A subset of the term ‘Offshoring’ also implies transferring
jobs to another country, either by hiring local subcontractors or building a facility in an
area where labour is cheap. It became a popular buzzword in business and management
in the 1990s.
Back in the early years of US History, the making of America&apos;s covered wagon covers
and clipper ships&apos; sails was a job outsourced to workers in Scotland, with raw material
imported  from India. England&apos;s textile industry  became so  efficient in  the 1830s that
eventually  Indian  manufacturers couldn&apos;t compete, and  that work  was outsourced  to
England.
More recently, in the US in the 1970s, it was common for computer companies to export
their payrolls to outside service providers for processing. This continued into the 1980s,
where accounting services, payroll, billing, and word processing all became outsourced
work. But most of this work  was outsourced  to service providers only  as far away  as
another state, not overseas, and the reasons for outsourcing had more to do with small
efficiencies than reshaping the economy.
It wasn&apos;t until the late 1980s that outsourcing began to emerge as a potentially powerful
force in transforming global economies. Meanwhile, in technology circles, the focus on
outsourcing turned from its efficiency to its economy and productivity. Early outsourcing 
to  overseas providers by  corporations like Kodak  and  American  Standard  began  to
capture the public&apos;s attention. Kathleen Hudson, then Kodak’s CIO, said, her goal was to 
&quot;plug into the wall and have data come out”. That type of thinking helped put outsourcing
on the map
3
.
Outsourcing defined…
Outsourcing  is defined  as the management and  / or day-to-day execution  of an  entire
business function by a third party service provider.
A related term is out-tasking – turning a narrowly defined segment of business to another
business, typically on  an annual contract, or sometimes a shorter one. This usually
involves continued direct or indirect management and decision making by the client of
the out-tasking business.4
Outsourcing  and  out-tasking  involve transferring  a significant amount of management
control to the supplier. Buying  products from another entity is not outsourcing  or outtasking, but merely a vendor relationship. Likewise, buying services from a provider is
not necessarily  outsourcing  or out-tasking. Outsourcing  always involves a considerable
degree of two way information exchange, coordination and trust.
The term “outsourcing” became a household  name perhaps because many  high-tech
companies in the early 1990s were not large enough to hire their own customer service
departments. These companies later hired technical writers to completely simplify their
usage instructions for their products, index the key points of information and contracted 
with temporary employment agencies to find, train and hire ( for low wages) low skilled 
workers to  answer their telephone technical support and  customer service calls. These
agents generally  worked  in  ‘Call Centers’ where the information  needed  to assist the
calling customer was/is indexed in a computer system. The “agents” were/are generally 
not allowed to tell the customer whether or not he/she actually works for the company;
thereby creating the illusion that the customer was/is calling the company.
The amount of investment required for customer service through outsourcing, and many
companies, from utilities to  manufacturers, closed  their in-house customer relations
departments and outsourced their customer service to call centers across and outside the
country. The latest in outsourcing customer service is the offshoring of customer service
that can only be enabled due to heightened use of Information Technology.
For some, globalization  is about opening up free trade between  countries – increasing
globalization helps to create opportunities for nations and benefits workers in both rich
and poor countries. For others, globalization is yet another way for the rich to line their
pockets at the expense of the poor – a non-sustainable system that excludes developing
nations.
Organizations that deliver such services feel that outsourcing requires the turning over of
management responsibility for running  a segment of business. In  theory, this business
segment should  be mission-critical, but practice often  dictates otherwise. Many
companies look  to  employ  expert organizations in  the areas targeted  for outsourcing
business segments typically  outsourced  include Information  Technology, Human 
Resources, Facilities and  Asset Management and  Accounting. Many  companies also
outsource customer support and  call center functions, manufacturing  and engineering.
Outsourcing business is characterized by expertise not inherent to the core of the client
organization.
The worldwide business process outsourcing  (BPO) market is undergoing  rapid
transformation. The drive to leverage technology, the arrival of web services and a more
cost conscious customer are in  combination, causing  major shifts in  BPO service
provision and adoption. According to a new special study published by the International
Data Corporation (IDC), worldwide spending  on BPO services totaled $712  billion  in
2001. IDC projects the worldwide BPO market to grow to $1.2 trillion by 2006
4
.5
‘Digital Firm’ or ‘Virtualization’
The internet supports collaborative processes on  a global scale. Business Process
Outsourcing  (BPO) enables new forms of cooperation  amongst organizations. It is
believed that with the help of internet, organizations can be established without the use of
‘brick and mortar’ buildings and business functions could be carried out by outsourcing
the latter. This form of an organization is called a digital firm, where most of the business
functions have been digitized. Virtualization is another term for a digital firm. Even 
though the business is going on, still there is no existence of a corporate office, branch 
offices and factories in the vicinity to keep rolling in the work
5
.
There are new ways of conducting  business electronically  both  inside and outside the
firm that can  ultimately  result in  the creation  of digital firms. Increasingly  internet is
providing the underlying technology for these changes. The internet can link thousands of
organizations into a single network, creating the foundation for a vast digital marketplace.
A digital market is an information system that links many buyers and sellers to exchange
information, products, services and  payments. Through  computers and  networks these
systems function like electronic intermediaries with lowered costs for typical marketplace
transactions. Buyers and sellers can  complete purchase and sale transactions digitally,
regardless of their location
6
.
‘Virtualization’ means that every company  that wants to  transform its business to  ebusiness needs to find ways to virtualize its business. For large monolithic enterprises this
means that they have to break themselves up into pieces and operate as a virtual network 
of companies. For example a manufacturing company  can  split its supply  chain  into 
smaller independent companies and then outsource the old functions to these new and
nimble companies while only the core competencies remain with the original enterprise.
To react quickly to change a new business process must be creatable instantaneously that
is much easier to achieve for a network of small companies than for one large monolithic
organization. In short, virtualization provides for organic form, improvisation, individual
responsibility and for fast reactions to change.
BPO over the Web
Once the organization has redesigned the business processes to fit the e – business
environment they  need  to  be implemented. Ideally  this implementation  should  be as
painless for the company  as possible. It should  be automatic, rapid  and  inexpensive.
Companies increasingly try to avoid this hassle of installing new softwares, by moving
the operations of whole business processes to external outsourcing companies. This fits
well with the current tendency to focus on the core competencies and have all the noncrucial services performed and managed by outside service providers.
BPO is long term contracting  of a company’s business processes to  an  outside service
provider in the hope of getting the same service at a lower cost and better quality. This
should  allow the company’s management to  focus its attention  full time on  the
company’s core business. In  the 1950s there were the fully  integrated  companies who 
were doing everything internally. Since then more and more business functions have been
moved to external service providers. This started with peripheral functions like security 6
services, janitorial and  house cleaning  services. Currently  even  the core business
functions like HR, finance and project management are being outsourced. What does this
indicate? Is this a new trend in the business world? The answer to this question is simple
– organizations view all the functions in terms of competency  indicators. Hence it is
essential for them to strive for the best output in all of them. Globalization puts a lot of
pressure on these organizations and hence it is merely ‘survival of the fittest’
7
.
The following list of outsourcing areas is by no means conclusive:
1. Finance and  accounting: management of financial and  accounting  department
functions to streamline planning, controls and processing.
2. Internal audit – management of business, operating  and financial risks and  ongoing evaluation and strengthening on internal accounting controls.
3. Tax  compliance – management of home country  and  international tax  return 
planning, and preparations to reduce taxes and processing costs.
4. Procurement – management of strategic procurement and  global sourcing  to 
optimize savings and profitability.
5. Human resources – management of cost competitive HR programs designed  to
attract, retain and motivate workers.
6. Real estate management – management of facilities to  improve real estate
operations, property usage, asset management and administration.
7. Applications process – management of enterprise resource planning systems and
individual software applications that support business processes.
BPO means moving the entire business functions to outsourcing provider who assumes
full responsibility  for processing  these functions. To successfully  outsource parts of a
business, the business needs to be split into disjoined vertical pieces where each piece is
made up of the entire business process.
The internet has enabled a new brand of outsourcing where the outsourcing provider is
linked  directly  to  core processes of the company. Rather than  vertically  splitting  the
business by  moving  whole processes such  as recruiting pieces to  the outsourcing
provider, business processes can be cut into  horizontal pieces where different parties
operate different parts of the process.7
Example of this sort of outsourcing  is operations of business travel, handling  of the
logistics process, procurement or customer billing.
A close look  at the four generic process groups reveals that the groups have reached
different levels of outsourcing maturity. Well structured processes such as “buying and
selling’ and  “design  and  production” are well suited  to  BPO while like “learning and 
change” or “management and  decision  making” are candidates for horizontal web
outsourcing.
Buying and selling Structured  Customer Relationship
Management
Vertical and 
horizontal BPO
Design and
production
Structured Enterprise resource
planning 
Vertical and 
horizontal BPO
Learning and
change
Unstructured  Knowledge navigation  Horizontal Web 
Management and
decision making
Unstructured  MIS – Data warehousing  Horizontal Web
Organization
structure
IT Technology Outsourcing
Four main generic process categories – Figure 2
Identify open
position
Search 
candidates
Conduct
interviews
Select new
hire
Receive
payment
Deliver
product
Obtain order
Identify 
customers
Send 
payment
Select
suppliers
Obtain product
catalogue
Identify 
supplier
Activity 
1
Activity 
4
Activity 
3
Activity 
2
Recruitment Sales
Vertical
outsourcing
Horizontal
outsourcing
Vertical and Horizontal BPO – Figure 1
Procurement Business Process8
Enabling technologies for outsourcing 
Internet technologies offer the ideal environment for business process outsourcing. The
Web provides open standardized information access, seamlessly linking a company and
its outsourcing provider. Virtual collaboration tools like groupware and teleconferencing
allow teams with  members from different companies to  communicate easily. The
standardization  of general business process automation  on  a few systems forces
companies to adapt their processes to work with their packages. This makes life easier for
outsourcing providers, because they can set up large competing centres running multiple
versions of their packages for multiple customers.
Trends
Outsourcing  is big  business, generating  global revenues of $298.5  billion  in  2003,
according  to  Gartner Inc. Many  now relate outsourcing  with  call centers in  India.
However, Forrester Research estimates that by 2015, as many as 3.3 million US jobs and
$136 billion in wages will move not only to India, but also to China, Russia, Pakistan,
and Vietnam. Europe has taken a central role in outsourcing as well, and is soon expected
to reach nearly 25 percent of total global outsource spending.
Carlos Watson, political analyst for CNN, observes that globalization  affects where
businesses set up shop. He notes that while media attention has focused on the corporate
exodus to India, &quot;three to four times [as] many jobs have moved to China in the last 15
years.&quot; There can be many reasons to it like - the difference may be in the kinds of jobs –
the majority of jobs outsourced to China are in manufacturing. Chinese goods are cheaper
for American companies to buy because of currency rate differences – the US had a $120
billion trade deficit with China in  2003, and it is expected to  be even higher in  2004.
International monetary  policy  inequities may  be the next important item in the
globalization discussion, according to Watson
8
.
Outsourcing  is clearly  not just about payrolls and  call centers. It could  be that the
research  and  development of the product may  be outsourced to  one country  and  the
production to another. There is a possibility that the billing could be done in some other
country and the customer’s queries could be answered in another.
Concluding remarks
At first glance it would seem that outsourcing has the potential to make jobs at home less
secure, and  so global trade organizations ought to  be responsible for resolving
employment problems where and when they occur. But on the other hand in terms of
information technology developments it appears as if the growth of outsourcing will give
rise to  complete digital firms. The brick  and  mortar buildings where employees are
confined to the four walls get replaced with  people spread  out geographically working 
towards achieving the mission of any one organization.
Innovations in outsourcing have yet to be exhausted – outsourcing can boost developing
world economies. But the world is a finite space with limited natural resources, including
human resources. Ironically, it is humankind’s ability to innovate that may account for
the eventual demise of outsourcing, as we mechanize more and more work – technology
is changing manufacturing and retail jobs radically. Hands-on work is being outsourced 9
now, but new innovations may eventually  make the workplace as we currently
understand it obsolete. We may one day be able to punch out and go home – forever.
But today it is clear that Business Process Outsourcing can lead to a situation where the
organizations are totally  digitized  in  nature and  their complete virtualization happens.
BPO can lead to a world where the parent company is at one place and the work is being
outsourced to various other countries while the output product or service is being sent to 
distant lands for delivery. Virtualization is not far from today but definitely the presence
of human resources cannot be ruled out.
Bibliographical References
                                               
1. Internet and E-Commerce, (2001), Information  Technology  and  Systems,
Icfaian Center for Management Research, Hyderabad, Chapter 13, p - 118.
2. Sumner, M. Schultheis, R. Management Information Systems: The Manager’s
view, 4
th
ed. Tata McGraw Hill Edition, 1999, ISBN 0-07-463879-3.
3. Outsourcing, http://en.wikipedia.org/wiki/Outsourcing.
4. Agha, K. (2005) A Study on the motivational levels of workforce in the BPO
industry in India – PhD Thesis in progress.
5. Gloor, P. (2003) Making the e-business Transformation, Gromwell Press,
England, ISBN 1-85233-265-4, Chapter 1, p 48 – 52.
6. Laudon, C. K. and  Laudon, P J. (2004) Management Information Systems:
Managing the Digital Firm, 8
th
ed. Prentice Hall of India, ISBN 81-203-2353-X.
7. Agha, K. E-Business: Challenges and Opportunities in the GCC, in  the
Proceeding of the International Conference in ‘E-Business, E-Business in GCC –
Challenges and  Prospects’ (EGCC’04), (19th May  2004, Majan  College
(University College), Muscat, Sultanate of Oman).
8. Kelly,T. A Brief History  of outsourcing, Portland, Oregon. Available from
&lt;http://www.globalenvision.org/library/3/702/&gt;, [Accessed 24 September 2005].
9. Watson, C. (2004) The inside edge: The rise of online citizen: Not India but
China, CNN.com, Available from
&lt;http://www.cnn.com/2004/ALLPOLITICS/03/17/polls/&gt;.
10. Wilson, S. Marsh, I. and Breusch, T. (2002) Job insecurity and globalization:
what factors shape public opinion in Asia and Europe? Technical Report,
Second  Semester Seminars, Political Science Program, RSSS, ANU. Available
from &lt;http://eprints.anu.edu.au/archive/00002014/&gt;.10
                   
11. Rajaraman, V. (2004), Introduction to Information Technology, Prentice Hall
of India, ISBN 81-203-2402-1.
12. Curtin, D. P. Foley, K. Sen, K. Morin, C. (1998) Information Technology: The
Breaking Wave, Tata McGraw Hill Publishing Company Limited, ISBN 0-07-
463558-1.</doc>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p15 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p15">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Patient_Record"/>
        <doc>Organizing the Biomedical Paper ###
Marianne Mallia
The preparation of a scientific paper has less to
do with literary skill than with organization (Lang,
1987). Authors of biomedical manuscripts want to organize each manuscript so that readers will be able
to follow a sequence of events and understand the
message. The editor (or the peer reviewer) of a biomedical manuscript reads it to discern (among other
things) whether the author has organized the manuscript successfully and, if not, whether a specific
rearrangement might make the manuscript more
understandable to readers. Readers of a biomedical
paper are usually physicians or scientists; they read
a paper because they are interested in the message. If
a paper is difficult to follow, such readers probably
will not be interested in trying to understand it. In
some cases, they will read the abstract, but seldom
will that be enough to present the author’s conclusions effectively. For the manuscript to be effective, it
must be written with a specific plan in mind.
The organization of a medical or scientific paper
mirrors the sequence of events detailed and discussed
in the paper. The author-researcher begins by asking
a question (in the Introduction), then undertakes the
activities required to find an answer (described in the
Materials and Methods), obtains and compiles the
data (described in the Results), and answers the question (in the Discussion). Other important elements of
the biomedical paper that require specific organization include the title and references. This chapter will
briefly discuss each of these elements. Writing the
abstract is covered in this book (see “Writing Abstracts,” p. 92), and determining authorship is discussed in a chapter in Essays for Biomedical Communicators: Volume 2 of Selected AMWA Workshops (Witte,
1997).
Update of Iles RL. Organizing the scientific journal paper. In
Minick P, ed. Biomedical Communication: Selected AMWA Workshops. Bethesda, Md: American Medical Writers Association;
1994:133-138. ###
Begin at the Beginning
Do not overlook the obvious. Begin by thinking
about the journal to which the paper will be submitted. Get a copy of the journal, read it, and familiarize
yourself with its style and format. Make sure that your
article is suitable for the intended journal. In the
journal’s “Instructions to Authors,” the editor should
describe acceptable types of manuscripts and give
guidelines for submitting manuscripts, including the
format for references. For example, some journals no
longer accept case reports, and most journals have
space limitations. Knowing the desired format before
you start will make your paper easier to write and
keep you from having to reformat it later. When you
are reporting a clinical trial, remember to use the
CONSORT guidelines (Moher et al, 2001) for structuring your manuscript. These guidelines are discussed in the AMWA workshop Reporting the Results
of Randomized Controlled Clinical Trials.
When writing an investigative biomedical paper,
you can begin with any section, but it is often easiest
to start with the Methods section, which details the
steps taken to prove the hypothesis; then move to the
Results section. For the purposes of this chapter, however, the sections of the paper will be discussed in
sequential order.
The Title
Most authors do not realize the importance of the
title. They concentrate on the text, completely overlooking the title until the paper is finished, and then
quickly write a title as an afterthought. However, your
title is your first chance to hook your reader or reviewer. Your title will be read by far more people than
will your paper, and, often, the title will determine
whether your paper is read at all. Reviewers will use
your title and key words to index your work.102   Biomedical Communication: Selected AMWA Workshops
Your title should accurately, specifically, and completely identify the central topic of the paper. The title
should be structured like other titles in the journal
but should also be creative. If you use a subtitle, the
title should state the general topic of the manuscript;
and the subtitle, the specific topic: eg,
Local Paclitaxel Delivery for the Prevention of Restenosis:
Biological Effects and Efficacy In Vivo [Herdeg et al, 2000,
p. 1969]
Stating the specific topic creatively in the subtitle
might pique your readers’ curiosity:
Device-Supported Myocardial Revascularization: Safe
Help for Sick Hearts [Sweeney &amp; Frazier, 1992, p. 1065]
Often, however, subtitles just add unnecessary words.
For instance,
Third Coronary Artery Bypass Operations: Risks and Costs
[Lytle et al, 1997, p. 1287]
could easily have been
Risks and Costs of Third Coronary Artery Bypass
Operations
which is simpler and more specific.
Main Title
Most journals prefer titles with 100 or fewer character spaces. Begin the title with an important word
to attract your intended readers, and remember to
use the same key terms as in your hypothesis (the
question at the end of the Introduction) and in your
Conclusions (in the Discussion). Often, you can just
insert an adjective before the dependent variable to
make your point.
Impaired Chronotropic Response to Exercise Stress
Testing as a Predictor of Mortality  [Lauer et al, 1999, p.
524; italics mine]
If your study includes an independent and a dependent variable, list both. If the study was not conducted in humans, name the species at the end of the
title.
Adenovirus-Mediated Insulin Gene Transfer Improves
Nutritional and Post-Hepatectomized Conditions in Diabetic Rats [Yamaguchi et al, 2000, p. 670]
If the study is a randomized trial, identify it as
such. Other examples of good titles include these:
Coronary Flow Reserve as a Physiologic Measure of
Stenosis Severity [Gould et al, 1990, p. 459]
Effect of Hemodialysis on Plasma Nitric Oxide Levels
[Hon et al, 2000, p. 387]
Acute Myocardial Infarction after the Use of Sildenafil
[Arora et al, 1999, p. 700]
In titles, avoid
• Noun clusters or too many nouns
Academic Health Systems Management: The Rationale
Behind Capitated Contracts [Taheri et al, 2000, p. 849]
Subacute Stent Thrombosis in the Era of Intravascular
Ultrasound-guided Coronary Stenting without Anticoagulation: Frequency, Predictors, and Clinical Outcome
[Moussa et al, 1997, p. 6]
• Words ending in -tion and -ment, which should
be verbs
Surgical  Management of Anatomical  Variations  of
the Right Lobe in Living Donor Liver Transplantation
[Marcos et al, 2000, p. 824; italics mine]
• Nonstandard abbreviations
• The at the beginning of and within the title; just
delete the
The Use of Subcutaneous Erythropoietin and Intravenous Iron for  the Treatment of  the Anemia Caused
by Severe, Resistant Congestive Heart Failure Improves
Cardiac and Renal Function and Functional Cardiac
Class, and Markedly Reduces Hospitalization [Silverberg et al, 2000, p. 1737; italics mine]
Complete sentences can be used as titles when
your point is especially strong or when the journal
routinely uses complete sentences.
Gender and Carotid Endarterectomy: Does it Matter?
[Akbari et al, 2000, p. 1103]
Growth Hormone Enhances Amino Acid Uptake by the
Human Small Intestine [Inoue et al, 1994, p. 715]
And finally, proofread for misplaced or poorly
placed modifiers. Can you find them in the following titles?
Ceftriaxone-Resistant Salmonella Infection Acquired by
a Child from Cattle [Fey et al, 2000, 1242]Biomedical Communication: Selected AMWA Workshops   103
Factors Influencing HLA Sensitization in Implantable
LVAD Recipients [Massad et al, 1997, 1120]
Running Title
The journal will usually ask for a shorter version
of the title, to be used as a “running title” or “running head” on subsequent pages of the manuscript.
The running title is typed on the manuscript’s title
page and is generally limited to 40 character spaces.
Remember to include all of the main words from the
title, specifying the independent and dependent variables whenever possible. For the running title, you
can omit the species. For example, shorten
Dexamethasone Alone or in Combination with Ondansetron for the Prevention of Delayed Nausea and Vomiting Induced by Chemotherapy
to
Prevention of Delayed Nausea and Vomiting Induced
by Chemotherapy [The Italian Group for Antiemetic Research, 2000, p. 1555]
Or shorten
Association Between Method of Delivery and Maternal
Rehospitalization
to
Delivery Method and Postpartum Rehospitalization
[Lydon-Rochelle et al, 2000, p. 1574]
IMRAD
IMRAD stands for Introduction, Methods (and
Materials), Results, and Discussion. Together, these
sections constitute a scientific manuscript. The
IMRAD system for writing a scientific paper originated with Pasteur, although he did not use the nowstandard headings (Day, 1988). In 1972, the American Standards Institute decided to standardize all the
headings used in investigative scientific papers. Thus,
the IMRAD system was born. Although IMRAD is a
useful organizational format, there is no absolute formula for writing; every paper is different and has, as
Zeiger (2000, p. 8) says, “its own story to tell and its
own organizational challenge.” And, you want the
story to be complete. Although each section should
focus on its specific part of the process, keep the main
question and answer of the study in mind as you
write.
Introduction
What question was studied? The answer is in the Introduction.
The Introduction creates the expectation that is
fulfilled by the rest of the paper. Structure the Introduction like a funnel; ie, begin with what is known
about the subject, move to what is unknown, and end
with the question your study answers. You can also
think about this as progressing from the general to
the specific. Begin with a background statement or
two describing the nature and scope of the study. A
good Introduction gives readers enough background
to understand the problem but not so much as to overwhelm them and detract from the research question.
Also, make sure that the background statement relates only to the specific subject of the paper. For example, if you are writing about the effectiveness of
the different immunosuppressants used to treat patients who have undergone transplantation, start your
Introduction with background on immunosuppressants, not with information about the first transplantation procedures.
Next, explain why the study is necessary: what
“gap”does it fill? In this section, preliminary reports
or abstracts can be cited, as can closely related, previously published work. However, avoid using the
names of investigators in the Introduction; remember, the Introduction is intended to hook your reader
into reading the paper. Mentioning others by name
(rather than by contributions) takes the spotlight away
from your work.
Discussion of the gap should lead directly into
the specific research question. If your work added
nothing to the known literature (even a different interpretation), it would be of no value to the field, so
make sure that the unknown element is obvious to
the reader. At the end of the Introduction, clearly state
the research question; precede it with a phrase that
signals that the answer is coming. Examples of signal phrases include “To determine whether . . . ,” “The
purpose of this study was . . . ,” “Therefore, in this
study, we asked whether . . . ,” and “The current study
was, therefore, designed to determine whether . . . .”
The question should repeat the key terms of the
title and the Introduction as well as the objectives,
the independent and dependent variables, the species, and, when necessary, the groups. You can include a short statement of results in the Introduction,
but this is unnecessary if the journal requires an abstract, which would state your conclusions. The importance of the study may also be briefly stated in
the final sentence of the Introduction.
Keep the Introduction short: one or two typed
pages. You want to catch and hold your readers’ attention, not overwhelm them. Write verbs in present104   Biomedical Communication: Selected AMWA Workshops
tense for the question and for what is known and in
past tense for previous findings. And remember to
use transitions.
The following brief Introduction follows the format nicely. I have italicized repeated key words and
transitions, all of which make the paragraph flow
well.
General Area
Restenosis after an initially successful percutaneous transluminal coronary angioplasty remains an important unsolved problem with this promising revascularization technique. Retrospective studies have found
that several clinical, angiographic, and procedural
variables are important predictors of restenosis.
xx-xx
Gap or General Problem
There is considerable variation among the studies, however, and the results are often difficult to interpret. Prospective trials are clearly needed to confirm the  observations made in retrospective studies
and to assess whether the risk of restenosis can be
predicted accurately in specific patients.
Previous Findings
Seve r a l   s tudi e s have   repor t ed high  rat e s  of
restenosis among patients with  coronary vasospasm,
such as Prinzmetal’s angina,
 xx-xx
 as well as among
those with coronary lesions susceptible to abnormal
vasoconstriction during provocative testing.
 xx
Hypothesis or Research Question
We designed a prospective trial to test whether
abnormal coronary vasoconstriction, detected by hyperventilation testing before angioplasty, increases
the likelihood of restenosis. A test that could accurately
identify patients at high risk of restenosis might influence management.
This Introduction reads well. Note the repetition
of key words and phrases, which also serve as transitions for the reader. With the first word, you know
the subject of the paper, “restenosis,” which is followed almost immediately by the narrowed subject,
“predictors of restenosis.” The author then discusses
the shortcomings of the available retrospective studies and addresses the need for a prospective study.
His hypothesis, “We designed a prospective trial . . .”
follows obviously from the gap in knowledge. The
author’s final statement informs the reader of the
importance of this work to patient care.
Materials and Methods
How was the problem studied? The answer is given in
Materials and Methods.
The Materials and Methods section should read
like a cookbook recipe and is usually arranged chronologically. The Methods section should be thorough
enough for someone else to be able to reproduce the
experiment. Because this section is usually the easiest to write, many authors begin with it. In Methods,
describe what was done to answer the research question, clearly stating the study design and detailing
the chosen methodology (materials, subjects, populations).
Study Design
Begin with a brief statement of the study design
(use a header), which should include a sentence about
Institutional Review Board approval, the process of
informed consent, and compliance with the Animal
Welfare Act and Good Laboratory Practices. For example,
The EXCITE study was a double-blind, randomized,
paral lel  design,  p lacebo-cont rol led,   i nternat ional
multicenter trial designed to compare the efficacy and
safety of xemilofiban to placebo when administered to
patients prior to and for up to 6 months after PTCR [percutaneous coronary revascularization] [O’Neill et al,
1999, p. 110)].
The protocol was approved by the institutional review
board of each participating hospital, and all patients gave
written informed consent before they were involved in
the study [O’Neill et al, 2000, p. 1317].
Study Protocol
The detail of the protocol comes next. Start by
repeating your research question:
We tested the efficacy of xemilofiban administered orally
in a dose of 10 or 20 mg given three times daily for up to
six months [O’Neill et al, 2000, p. 1317].
If the purpose of the procedure is not clear, explain it to the reader. Repeat the description of the
study population, as well as the inclusion and exclusion criteria, unless these elements have been previously detailed in a readily obtainable journal. (If this
is the case, refer to the previous study: “The protocol
for the trial has been explained elsewhere.
xx
”)Biomedical Communication: Selected AMWA Workshops   105
Patients with angiographic evidence of clinically significant coronary artery disease necessitating PTCR
were eligible for the study. Patients at high risk for ischemic events were sought in order to maximize the
event rate and thus increase the opportunity to demonstrate a therapeutic effect. Patients who had received
abciximab before PTCR were not eligible for enrollment
[O’Neill et al, 2000, pp. 1316-1317].
At this point, you should explain how the study
was randomized, if it was a randomized trial (eg, geographically or individually); how the allocation
schedule was generated and how and when the allocation was done; and how the person generating the
assignment was separated from the person executing the assignment. After describing the randomization, list the precautions taken to mask (or blind) the
trial: eg, capsules and tablets; the location of the code
during the trial and when the code was broken; and
evidence of successful blinding among the participants, interventionalists, the outcome assessors, and
the data analysts (Moher et al, 2001).
After the diagnostic angiogram had been obtained
and before PTCR was performed, patients were randomly assigned to one of three regimens: a single oral
dose of 20 mg xemilofiban administered before PTCR
. . . or placebo administered both before and after the
procedure. The random assignments were made by telephone with the use of an interactive voice-response
computer system and were stratified according to the
study center. . . . Patients were evaluated 10 to 21 days
and 60 days after PTCR. Subsequent monitoring for
cardiac events, safety, laboratory values, concurrent
medications, and compliance was performed monthly
by telephone or by site visits. . . . [O’Neill et al, 2000, p.
1317].
Explain how you projected the target sample size,
and include primary and secondary outcome measures, such as study end points:
There were two primary end points. The first was
event-free survival at 182 days, with an event defined
as death or nonfatal myocardial infarction. . . . Secondary end points included . . . [O’Neill et al, 2000, p. 1317].
As you complete the description of the protocol,
make sure that you have accounted for all of the
materials used, including drugs, culture media,
buffers, gases, and subjects (human or animal) with
inclusion and exclusion criteria for the study. Give
exact names, manufacturers, and manufacturers’ addresses for the materials you used (some journals require only the city and state for the address).
After the assessment of blood pressure and habitus,
technicians affixed polysomnography leads to each participant and performed calibrations. An 18-channel
polysomnographic recording system (model 78, Grass
Instruments, Quincy, Mass.) was used to assess sleep
state and respiratory and cardiac variables. . . . Arterial
oxyhemoglobin saturation, oral and nasal airflow, nasal
air pressure, and ribcage and abdominal respiratory
motion were used to assess episodes of sleep-disordered breathing. Oxyhemoglobin saturation was continuously recorded with a pulse oximeter (model 3740,
Ohmeda, Englewood, Colo.) [Peppard et al, 2000, p.
1379].
Give all of the details necessary for understanding the study and all of the details that affected the
study. If a questionnaire was administered, make sure
that you have told the reader how it was administered and by whom (see above). If a method is not
well established and has not been published or is
fairly complex, do not refer to the reference without
describing the method completely in the paper. Methods that failed to lead to your study’s desired conclusion must be included. To avoid interrupting the flow
of the manuscript, place details in parentheses.
The growth hormone group received a loading dose
of 5 mg of growth hormone (somatropin, Humatrope,
Eli Lilly, Indianapolis) per day subcutaneously for the
first week (for example, a 70-kg patient received 0.5 mg
per kilogram of body weight per week), followed by a
maintenance dose of 1.5 mg per day for the remaining
16 weeks of the study (for example, a 70-kg patient received 0.15 mg per kilogram per week) [Slonim et al,
2000, p. 1633].
Results should be included in the Methods section ONLY if they are specifically pertinent to the
protocol.
For simplicity, and because so few women in this cohort drank heavily (1.2 percent reported drinking more
than 45 g of alcohol per day), we did not define an upper limit for alcohol consumption, although clearly this
would be necessary in establishing public health guidelines [Stampfer et al, 2000, p. 17].
Visual elements work well in Methods. In fact,
using an illustration is the best way to help the reader
understand your protocol (Figure 1). In an illustration, you can easily define participant flow numbers
and timing of randomization; assignment, interventions, and measurements for each randomized group;
time lengths; arms of the study; and patient designations. Patient characteristics can often be best presented in a table (Table 1 [The Italian Group for Antiemetic Research, 2000, p. 1556]). In a paper with a106   Biomedical Communication: Selected AMWA Workshops
complicated Methods section, like a surgical paper,
always include an illustration that shows exactly how
the procedure was done (Figure 2).
Explain anything that would make your reader
ask “why?”—including dead-end methods and study
limitations. Explain the limitations of the study methods in a matter-of-fact way. The limitations need to
be addressed, but keep the statements short and
simple. You do not want to overwhelm the reader
with possibly negative implications.
Dead-end method
To reduce concern about observer error and the ability to validate temperature measurements, the analysis
included only the initial temperature determinations for
children evaluated at Children’s Hospital and Regional
Medical Center in Seattle. The laboratory data that were
analyzed consisted of white-cell counts and serum urea
nitrogen and creatinine concentrations. Only the initial
laboratory test result for each child was analyzed as a
potential risk factor for the development of the hemolyticuremic syndrome [Wong et al, 2000, p. 1931].
Limitation of Study Methods
We excluded years before 1939 because the cause-ofdeath portion of the death certificate was substantially
different in earlier years. Data on the cause of death
were available for more than 99 percent of all deaths in
the United States, except for 1972, when a 50 percent
sample was used to estimate the number of deaths
[Dowell et al, 2000, p. 1399].
Statistical Analysis
The last paragraph(s) of Methods should state the
analytical procedures that you used to determine the
significance of your Conclusions. State the procedures
used to analyze each set of data and the software used
for analysis. Include your rationale, detailing the main
comparative analyses used. Explain whether the
analyses were completed on an intention-to-treat basis. The following excerpt is a small portion of a threeparagraph description of statistical analyses for a randomized trial.
The trial was designed to have 90 percent power
to detect a 25 percent reduction in the composite
end point of death, nonfatal myocardial infarction, or
urgent revascularization in pairwise comparisons of each
xemilofiban treatment group with placebo, with a twosided type I error of 0.025, assuming an event rate of
17.6 percent in the placebo group. . . . For the final analysis, the level of significance was 0.02 for the first primary end point and 0.01 for the second primary end
point. . . . Cumulative event rates for each end point
Figure 1. Study protocol (Hallstrom, p. 1548). An outline of
this type makes understanding the protocol simple. [Editors’
note: All figures are reprinted with permission.]
were estimated with the use of the Kaplan-Meier method;
. . . . Analyses of cardiac end points were performed on
an intention-to-treat basis and included all patients according to the assigned treatment and all adjudicated
cardiac end points during the designated follow-up period [O’Neill et al, 2000, p. 1317].
In the Methods section, subheadings should be
used whenever possible, especially when the section
is long and complicated, and always for clinical trials. Sample subheadings include “Study Design,”
“Enrollment of Patients,” “Study Protocol,” “Study
End Points,” and “Statistical Analyses.”
Because the Methods section describes work already completed, write it in past tense, in either
passive or active voice. Although the active voice is
more interesting to read, frequent use of “I” may seemBiomedical Communication: Selected AMWA Workshops   107
egotistical, so passive is often used in this section.
Remember, however, that you can mix the voice and
tense in scientific manuscripts, so you can change
from active to passive, using active when you want
more emphasis.
Results
What were the findings? The answer is in the Results
section.
The Results section, which logically answers the
research question, should correlate directly with the
Methods section. For every method, there should be
a result. When possible, use the same order and subheadings that you used in Methods so that the correlations will be easy for the reader to follow. For example, in a manuscript called “Administration of
Wine and Grape Juice Inhibits In Vivo Platelet Activity and Thrombosis in Stenosed Canine Coronary
Arteries” (Demrow et al, 1995), the subheadings used
by the authors in the Methods section are
• Group 1: Red Wine
• Group 2: White Wine
• Group 3: Grape Juice
• High-Performance Liquid Chromatography
Analysis
Likewise, the Results section has the same headers.
Although not all headers from the two sections
must mirror each other, try to keep them as closely
related as possible. Another example comes from
“Hemodynamic Effects of Sildenafil in Men with Severe Coronary Artery Disease” (Herrmann et al,
2000). Headers used in Methods are
• Study Subjects
• Study Protocol
• Calculations
• Statistical Analysis
whereas headers in Results are
• Clinical Characteristics
• Systemic and Pulmonary Hemodynamic Effects
• Coronary Hemodynamic Effects
• Adverse Effects
Begin each paragraph by stating a result. Do not
begin by restating your methods. Cite data that establish the similarities between the treatment groups
first, and then present the results of the treatment.
State the effect of the intervention on the primary and
secondary outcome measures in the trial and include
the confidence level. Remember to use data from only
Figur e  2. A  s t ep-by - s t ep proc edure   i s  be s t   shown  in an
Table 1. illustration.108   Biomedical Communication: Selected AMWA Workshops
the study being reported. If necessary, describe a previous study in the Introduction and discuss its relevance in the Discussion, but do not include any previous work in Results. Only in certain scientific fields
(eg, biochemistry) would the methods and results be
reported together. In some scientific studies, in which
multiple experiments lead to a final result, each experiment may be reported with its result, the paper
being organized chronologically by experiment.
Also remember that the Results section is another
appropriate place for tables and figures, which
are perfect for presenting detailed data (Table 2
[Moynihan et al, 2000, p. 1648], Figure 3). No one
wants to read strings of data written into sentences.
By using illustrations, you can keep written data to a
minimum. Charts make protocol results easier to understand (Figures 4 and 5). A response to treatment
can be shown graphically with a line drawing (Figure 6). Bar graphs show changes better than tables
(Figure 7) and can be used for more complex data to
show comparisons (Figures 8 and 9). Diagrammatic
illustrations can also be used to enhance figures that
show the results of diagnostic tests and surgical proc e d u r e s   ( F i g u r e   1 0 )   a n d   t o   s i m p l i f y   c o m p l e x
scientific concepts (Figure 11). Never repeat textual
information in the tables or graphs. The text should
supplement or highlight, rather than repeat, the
graphical data. And, remember, always make sure
that the numbers in the Results match the numbers
in the Abstract and Discussion.
When results are expressed in words, put data in
parentheses after the result:
When data on all 893 follow-up studies were analyzed, there was a decrease in mean blood pressure
from base line to follow-up (from 125/82 mmHg to
123/79 mmHg) and an increase in the prevalence of
stage 1 or worse hypertension (from 28 to 31 percent)
[Peppard et al, 2000, p. 1380].
Figure 3. Hematocrit, hemoglobin, and calculated HBOC-201
levels, presented graphically (Mullon et al, 2000, p. 1641). These
data would be impossible to present in the text.
Table 2.Biomedical Communication: Selected AMWA Workshops   109
Figure 7. Adjusted births by season, as a percentage of adjusted
total births in all seasons (Levine et al, 1990, p. 15), shown by a
bar graph.
Figure 6. Kaplan-Meier estimates of the time to relapse in
patients given methotrexate and placebo (Feagan et al, 2000, p.
1630), shown in a line drawing.
Figure 5. Algorithm to predict response or unresponsiveness to
rHuEPO therapy in chronic anemia of cancer (Ludwig et al,
1994, p. 1059).
Figure 4. Trial profile (Sherman et al, 2000, p. 2398) that shows
patient assignment to the different arms of the study.
In addition to reporting percentages, include absolute numbers in parentheses when feasible:
There was no significant difference in the incidence
of hospitalization for congestive heart failure between
the two groups; the annual rates were 3.5 percent among
the patients with a ventricular pacemaker and 3.1 percent among those with a physiologic pacemaker (reduction in relative risk, 7.9 percent; 95 percent confidence interval, -18.5 to 28.3 percent; P = 0.52) [Connolly
et al, 2000, p. 1389].110   Biomedical Communication: Selected AMWA Workshops
Remember that good writing keeps the reader
from having to guess the author’s meaning. Whenever you use the word significant, report confidence
intervals, standard deviations, and P values. Many
journals now require exact P values, even for studied
data sets for which the results are not significant.
Always state your data clearly and simply, and
write in the past tense because you are describing
what you have already done. If there were any deviations from the study as stated in the protocol, describe them, along with the reasons for the deviations.
Discussion
What do your findings mean? The answer is provided
by the Discussion.
The purpose of the Discussion is to explain the
principles, relationships, and generalizations implied
by the Results. You should discuss—not recapitulate—the results, and you need to be persuasive. Write
in the present tense, except when describing results;
then write in the past tense.
Every Discussion should have a beginning,
middle, and end. The first sentence of the Discussion
should clearly answer the research question by using the same key terms that were used in the statement of the question at the end of the Introduction.
Readers should not have to guess at your answer. In
the following example, note the repetition of key
words and phrases.
Ending of the Introduction
. . . to test whether abnormal coronary vasoconstriction
detected by hyperventilation testing before angioplasty
increases the likelihood of restenosis.
Beginning of the Discussion
. . . The presence of abnormal coronary vasoconstrict ion,  detected on hypervent i lat ion  test ing before
angioplasty, was associated with an increased likelihood
of restenosis in patients with unstable angina and singlevessel coronary disease.
Follow this statement with your Conclusions,
based on the Results, presenting your strongest evidence first.
Another example follows:
The EXCITE trial tested the hypothesis that in patients treated with PTCR, long-term oral administration
of xemilofiban, after an initial dose given before the procedure, would extend the clinical benefit of short-term
glycoprotein IIb/IIIa receptor blockade previously demonstrated with abciximab, tirofiban, and eptifibatide. Our
finding that treatment with xemilofiban did not improve
the long-term outcome after PTCR has two possible explanations. First, this short-acting oral agent may not
have had sufficient efficacy in the short term. Second,
long-term use of the agent may not have had sufficient
. . . [O’Neill et al, 2000, pp. 1320-1321].
Figure 8. Myocardial infarctions occurring within one day after
randomization of the index revascularization, according to
creatine kinase level (O’Neill et al, 2000), shown by a bar graph.
Figure 9. Secondary clinical end points in the study groups
(Crawford et al, 1991), shown by comparative bar graphs.Biomedical Communication: Selected AMWA Workshops   111
Figure 10. Aortic arch reconstruction with endovascular branched stent graft (Inoue et al, 1999, p. II-317). The side-by-side photo
and illustration allow the reader to see the device as well as the technique used to insert it.
Figure 11. Mechanisms important in the resolution of acute lung injury and acute respiratory distress syndrome (Ware and Matthay,
2000, p. 1342). Use of a drawing makes this scientific concept easier to understand.112   Biomedical Communication: Selected AMWA Workshops
Never begin the Discussion with background information, and never repeat information stated in the
Introduction. Background material should be found
only in the Introduction.
In the middle of the Discussion, interpret your
results and show how they support your answer.
Topics should be discussed in descending order of
their importance to the answer. Use comparisons to
other studies to explain how the results fit in with
existing knowledge. You can do this in several ways:
Introduce Points with Your Own
Findings
Our data show that oral sildenafil does not adversely
affect coronary blood flow, coronary vascular resistance,
or coronary flow reserve. On the basis of the decrease
in the heart rate—systolic blood pressure double product (a surrogate measure of myocardial oxygen demand), we might have expected a parallel decrease in
coronary blood flow due to autoregulation. The absence
of such a finding in our study may reflect the inaccuracy
of the double product as a true measure of myocardial
demand, variations in the calculated values for coronary blood flow and resistance, or a vasodilatory effect
of sildenafil that blunts the expected reduction in coronary blood flow [Herrmann et al, 2000, p. 1625].
Our study did not address the mechanism for the previously reported adverse cardiovascular events after the
use of sildenafil, but our results do suggest that this
mechanism is not the result of an adverse effect on coronary hemodynamics. Others have speculated that cardiac events may be due to interactions with other drugs
. . . [Herrmann et al, 2000, p. 1625].
Comparison with Earlier Work (Use Your
Work to Support Previous Studies)
The fact that our study was prospective lends support to the evidence of a causal role of sleep-disordered
breathing in hypertension. We found that the presence
of sleep-disordered breathing was predictive of hypertension four years later [Peppard et al, 2000, p. 1382].
It is noteworthy that high percentages of the patients
at low risk who were given placebo did not have delayed vomiting (87.2 percent) or moderate-to-severe
nausea (81.8 percent). These percentages are similar
to those we found in a previous study of patients who
received neither prophylaxis nor placebo against
delayed emesis, in which the same regimen for prophylaxis against emesis during the first 24 hours was used
[The Italian Group for Antiemetic Research, 2000, p.
1559].
Comparison with Earlier Work
(Use Others’ Work to Support Your Study)
Previous studies of the hemodynamic effects of intravenous and oral sildenafil in normal men and men
with stable ischemic heart disease have demonstrated
a small but consistent decrease in systemic and pulmonary blood pressure after administration of the drug.
xx
The results of the present study confirm these findings
in men with anatomically severe coronary disease. In
addition, we investigated the effects of sildenafil on coronary hemodynamics [Herrmann et al, 2000, p. 1625].
The increase in insulin-like growth factor in patients
in the growth hormone group was consistent with that
seen in adults with other diseases that are treated with
growth hormone.
xx
 However, our findings do not support the possibility that the beneficial effect of growth
hormone is due to the action of insulin-like growth factor I on the bowel, since the degree of clinical improvement in individual patients was not correlated with their
levels of insulin-like growth factor I [Slonim et al, 2000,
p. 1637].
Ambiguous results and any discrepancies between your work and that of others should also be
presented in the middle of the Discussion. These are
your least impressive results, so present them objectively and bury them. The middle of the discussion
is the place to explain any limitations of the study
(methods, validity of assumptions, study design, and
bias) or unexpected findings. The following paragraph is the sixth in a 10-paragraph Discussion:
We did not have data that could be used to model the
dynamic relation between sleep-disordered breathing,
habitus, and hypertension. For example, although there
have been few relevant studies, there has been speculation that sleep-disordered breathing has a causal role in
obesity.
xx
 If this is the case, then our efforts to control for
confounding by including measures of obesity in our
models may have led to a partial overadjustment of the
association between sleep-disordered breathing and
hypertension and thus to an underestimate of the association [Peppard et al, 2000, pp. 1382-1383].
The following paragraph was also buried:
Our study has several limitations. The Doppler
guidewire was carefully placed to optimize signal
strength and to ensure an accurate measurement of
peak velocity. Nevertheless, this method assumes a
time-averaged parabolic flow velocity, negates the effects of vessel tortuosity on alterations in pulsatility . . .
[Herrmann et al, 2000, pp. 1625-1626].Biomedical Communication: Selected AMWA Workshops   113
The Ending
Make the ending of your Conclusion section
strong. The concluding paragraph should restate the
answer to the research question. Begin with a signal,
such as “In conclusion” or “In summary,” so your
readers will know that this is the answer. After stating the Conclusion, you can briefly mention possible
applications, implications, or speculations.
Application
Our findings support the statement of the American
College of Cardiology and the American Heart Association that “primary PTCA should be used as an alternative to thrombolytic therapy only if performed in a timely
fashion . . .” [Canto et al, 2000, p. 1579].
Implication
Because sleep-disordered breathing is highly prevalent, afflicting as many as 9 percent of women and 24
percent of men in the United States,
xx
 a causal association could be responsible for a substantial number of
cases of hypertension and its sequelae, such as cardiovascular and cerebrovascular morbidity and mortality [Peppard et al, 2000, p. 1383].
Speculation
The presence of elevated cardiac troponin I levels
immediately after transplantation in cardiac transplant
recipients suggests the need for intervention before
transplantation to protect the microvasculature within
the donor hearts, perhaps by improving the preservation of the donor organs or preparing the recipient in
advance to prevent damage during the reperfusion period [Labarrere et al, 2000, p. 463].
Suggest future work, if necessary.
A larger multicenter study should be conducted to
confirm these results and to address many issues, including the best dose of growth hormone and the length
and frequency of therapy that are necessary to produce
and maintain clinical remission [Slonim et al, 2000, p.
1637].
In summary, keep the Discussion as short as possible, so that your reader grasps the take-home message. Authors most often err by including too much
information in the Discussion without including transitions between paragraphs. Do, however, thoroughly
discuss the answer to your research question, beginning with the strongest result from your study. Minor points should be presented in the middle of the
section and treated briefly. Any conflicting data
should be presented objectively, and speculations and
opinions must be clearly distinguished from facts.
In General
As in anything you write,
• Include only one thought per sentence; one idea
per paragraph.
• Use the active voice whenever possible.
• Use simple words. Scientific words are complicated enough. Don’t subject your readers (even if
they are brilliant) to every four- or five-syllable
word you know. Long words make a paper very
hard to read.
• Keep the sections as short and simple as possible.
• Use transitions and key words throughout your
manuscript.
• Write an outline for each section before you begin writing.
• Consult a statistician before you plan a study.
• Make sure that every word you write relates directly to your thesis. Keep your question in mind
as you write.
Acknowledgments
Not everyone should be listed as an author. Use
the guidelines suggested in the Uniform Requirements for Authors of Biomedical Journals (International Committee of Medical Journal Editors, 1997)
to determine the criteria for authorship and the
order in which to list authors. Names of those who
participated in the study but who do not meet the
c r i t e r ia  for  author ship  should be   l i s t ed  in  the
Acknowledgments if the journal permits this section.
Acknowledge intellectual assistance, technical help,
gifts of special equipment or materials, and outside
financial assistance (grants, contracts, fellowships,
cash support). Remember, everyone whom you thank
must give signed permission to be acknowledged
and should see a copy of the final draft. A sample
Acknowledgment follows:
The STAT investigators and coordinators acknowledge with thanks the cooperation of the patients and
their families who participated in this study. Contributions to this report from the following individuals at Knoll
Pharmaceutical Co. are gratefully acknowledged: Gerry
Fava, Shi-Yun Shen, members of the Biostatistics and
Data Management Department, for guidance with statistical analysis; and Thomas Zimmerman and Kenneth
Kashkin of the Clinical Development Department for suggestions in manuscript preparation [Sherman et al, 2000,
p. 2403].114   Biomedical Communication: Selected AMWA Workshops
References
Before beginning to format your references, make
sure that you have checked the target journal’s Instructions for Authors, because reference style varies
from journal to journal. Always get a copy of the journal to check for reference style also; occasionally, the
samples given in the Instructions for Authors will
differ from the style actually printed in the journal.
In that case, use the printed style.
Include among your references only published
works. Whenever possible, cite experts and the most
important works in the field—those that are readily
available to the reader. Reference numbers are placed
according to how the citation is written. If an author
is mentioned, place the reference number after the
author’s name. If you cite more than one author, write
“Cooley and colleagues
xx
” or “Cooley et al,
xx
” depending on journal style. If you cite only two authors, write
“Cooley and Brown.
xx
” If ideas are referenced, place
reference numbers at the end of the statement of each
idea, unless all of the references refer to all of the
ideas. For example,
Lower mortality rates have been associated with higher
volumes of elective procedures in studies of percutaneous transluminal coronary angioplasty (PTCA),
xx-xx 
coronary stenting,
xx
 and coronary-artery bypass grafting.
xx,xx
”
 But
Although ancrod does not directly affect any other coagulation factors or hematological components, rapid
defibrinogenation does
xx-xx
” [Sherman et al, 2000, p.
2395].
 If more than one reference is needed for an idea,
cite the references in chronological order within the
group. In a table or figure, number the reference according to where the table or figure is first cited in
the text.
Unpublished Data
References to unpublished data are listed in the
text within parentheses: eg, “(unpublished data, with
permission, D. Cooley).” In these cases, a signed statement that the reference is correct should be obtained
from the person giving the reference; this statement
must be enclosed with your manuscript when it is
submitted. Articles in press can be listed in the references, but a copy of the acceptance letter should be
enclosed with the submitted manuscript.
Make sure that there is a reference for every citation, and always check the original source. Although
it may seem easier to copy a reference from another
manuscript’s reference list, that author might have
placed or typed the reference incorrectly. Reference
citations are easy to check on the Internet, and the
full text of many articles can be found online. Until
the manuscript is published, it is wise to keep a copy
of each article cited.
Styling Your Manuscript
When you think you have finished writing or
editing, go back and verify that you have carefully
followed the journal’s Instructions to Authors. If there
is a word limit, adhere to it. Look at the target journal to make sure that your manuscript is typed exactly as it would be typeset in the journal. If the journal uses a flush-left, bold title, type yours in the same
fashion. Use the same wording for the “Address reprint requests to . . . ” line. Believe me, whatever you
can do to make less work for the journal will be appreciated. Make sure that your title page includes all
the information requested by the journal and that
everything is packaged and labeled neatly. If you have
any questions, call the managing editor.
Your manuscript should make a good visual impression. The originality and significance of the work
are certainly most important, but reviewers will also
note poor organization, formatting, and writing. Reviewers in our institution often tell me that when
papers they review are poorly written, they decrease
the score they give the manuscript or ask for editorial revisions.
And Now, Get Organized
If you want to write (or edit) a good biomedical
paper, get organized. Organization is the key to success. The best research in the world can be hidden in
a poorly written paper. Even if the paper is published,
a reader may not take the time to ferret out the message. And in some cases, the paper won’t get published. Some editors do not even review poorly written manuscripts because they believe that a sloppy
manuscript signals sloppy science.
Knowing the formula for writing a biomedical
paper will make the writing and editing process much
easier. Just as solving an algebraic problem becomes
easier with a formula, writing a biomedical paper
requires an understanding of the IMRAD system and
a willingness to take the time to apply that system to
your work.
References
Akbar i  CM,  Pul l ing MC,  Pompos e l l i  FB  J r,  Gibbons  GW,
Campbell DR, Logerfo FW. Gender and carotid endarterectomy: does it matter? J Vasc Surg. 2000;31:1103-1108.
Arora RR, Timoney M, Melilli L. Acute myocardial infarction
after the use of sildenafil. N Engl J Med. 1999;341:700.Biomedical Communication: Selected AMWA Workshops   115
Canto JG, Every NR, Magid DJ, et al. The volume of primary
angioplasty procedures and survival after acute myocardial
infarction. N Engl J Med. 2000;342:1573-1580.
Crawford J, Ozer H, Stoller R, et al. Reduction by granulite
colony-stimulating factor of fever and neutropenia induced
by chemotherapy in patients with small-cell lung cancer. N
Engl J Med. 1991;325:164-170.
Connolly SJ, Kerr CR, Gent M, et al. Effects of physiologic pacing versus ventricular pacing on the risk of stroke and death
due to cardiovascular causes. N Engl J Med. 2000;342:1385-
1391.
Day RA. How to Write &amp; Publish a Scientific Paper. Phoenix, Ariz:
Oryx Press; 1988.
Demrow HS, Slane PR, Folts JD. Administration of wine and
grape juice inhibits in vivo platelet activity and thrombosis in
stenosed canine coronary arteries. Circulation. 1995;91:1182-
1188.
Dowell SF, Kupronis BA, Zell ER, Shay DK. Mortality from pneumonia in children in the United States, 1939 through 1996. N
Engl J Med. 2000;342:1399-1407.
Feagan BG, Fedorak RN, Irvine EJ, et al. A comparison of methotrexate with placebo for the maintenance of remission in
Crohn’s disease. N Engl J Med. 2000;342:1627-1632.
Fey PD, Safranek TJ, Rupp ME. Ceftriaxone-resistant salmonella
infection acquired by a child from cattle. N Engl J Med.
2000;27:1242-1249.
Gould KL, Kirkeeide RL, Buchi M. Coronary flow reserve as a
physiologic measure of stenosis severity. J Am Coll Cardiol.
1990;15:459-474.
Hallstrom A, Cobb L, Johnson E, Copass M. Cardiopulmonary
resuscitation by chest compression alone or with mouth-tomouth ventilation. N Engl J Med. 2000;342:1546-1553.
Herdeg C, Oberhoff M, Baumbach A. Local paclitaxel delivery
for the prevention of restenosis: biological effects and efficacy
in vivo. J Am Coll Cardiol. 2000;35:1969-1976.
Herrmann HC, Chang C, Klugherz BD, Mahoney PD. Hemodynamic effects of sildenafil in men with severe coronary artery
disease. N Engl J Med. 2000;342:1622-1626.
Hon WM, Lee JC, Lee KH. Effect of hemodialysis on plasma nitric oxide levels. Artif Organs. 2000;24:387-390.
Inoue K, Hosokawa H, Iwase T, et al. Aortic arch reconstruction
by transluminally placed endovascular branched stent graft.
Circulation. 1999;100:II-316-321.
Inoue Y, Copeland EM, Souba WW. Growth hormone enhances
amino acid uptake by the human small intestine. Ann Surg.
1994;219:715-722.
International Committee of Medical Journal Editors. Uniform
Requirements for Manuscripts Submitted to Biomedical Journals. JAMA. 1997;277:927-934.
The Italian Group for Antiemetic Research. Dexamethasone alone
or in combination with ondansetron for the prevention of delayed nausea and vomiting induced by chemotherapy. N Engl
J Med. 2000;342:1554-1559.
Labarrere CA, Nelson DR, Cox CJ, Pitts D, Kirlin P, Halbrook H.
Cardiac-specific troponin I levels and risk of coronary artery
disease and graft failure following heart transplantation.
JAMA. 2000;284:457-464.
Lang T. Technical writing is not one of the humanities. AMWA J.
1987; 2:3-8.
Lauer MS, Francis GS, Okin PM, Pashkow FJ, Snader CE,
Marwick TH. Impaired chronotropic response to exercise
stress testing as a predictor of mortality. JAMA. 1999;281:524-
529.
Levine RJ, Mathew RM, Chenault CB, et al. Differences in quality of semen in outdoor workers. N Engl J Med. 1990;323:12-
16.
Ludwig H, Fritz E, Leitgeb C, Pecherstorfer M, Samonigg H,
Schuster J. Prediction of response to erythropoietin treatment
in chronic anemia of cancer. Blood. 1994;84:1056-1063.
Lydon-Rochelle M, Holt VL, Martin DP, Easterling TR. Association between method of delivery and maternal rehospitalization. JAMA. 2000;342:1573-1580.
Lytle BW, Navia JL, Taylor PC. Third coronary artery bypass operations: risks and costs. Ann Thorac Surg. 1997;64:1287-1295.
Marcos A, Ham JM, Fisher RA, Olzinki AT, Posner MP. Surgical
management of anatomical variations of the right lobe in living donor liver transplantation. Ann Surg. 2000;231:824-831.
Massad MG, Cook DJ, Schmitt SK, et al. Factors influencing HLA
sensitization in implantable LVAD recipients. Ann Thorac Surg.
1997;64:1120-1125.
Moher D, Schulz KF, Altman DG, et al. The CONSORT statement: revised recommendations for improving the quality of
reports of parallel-group randomized trials. JAMA. 2001;285:
1987-1991.
Moussa I, DiMario C, Reimers B, Akiyama T, Tobis J, Columbo
A. Subacute stent thrombosis in the era of intravascular ultrasound-guided coronary stenting without anticoagulation:
frequency, predictors, and clinical outcome. J Am Coll Cardiol.
1997;29:6-12.
Moynihan R, Bero L, Ross-Degnan D, et al. Coverage by the news
media of the benefits and risks of medications. N Engl J Med.
2000;342:1645-1650.
Mullon J, Giacoppe G, Clagett C, McCune D, Dillard T. Transfusions of polymerized bovine hemoglobin in a patient with
s e v e re   a u t o immu n e   h emo l y t i c   a n emi a .   N  E n g l   J  Me d .
2000;342:1638-1643.
O’Neill WW, Serruys P, Knudtson M, et al. Design and objectives
of the evaluation of oral xemilofiban in controlling thrombotic events (EXCITE) study. J Interven Cardiol. 1999;12:109-
115.
O’Neill WW, Serruys P, Knudtson M, et al. Long-term treatment
with a platelet glycoprotein-receptor antagonist after percutaneous coronary revascularization. N Engl J Med. 2000;342:
1316-1324.
Peppard PE, Young T, Palta M, Skatrud J. Prospective study of
the association between sleep-disordered breathing and hypertension. N Engl J Med. 2000;342:1378-1384.
Sherman DG, Atkinson RP, Chippendale T, et al. Intravenous
ancrod for treatment of acute ischemic stroke. The STAT Study:
a randomized controlled trial. JAMA. 2000;283:2395-2403.
Silverberg DS, Wexler D, Blum M. The use of subcutaneous
erythropoietin and intravenous iron for the treatment of the
anemia of severe, resistant congestive heart failure improves
cardiac and renal function and functional cardiac class, and116   Biomedical Communication: Selected AMWA Workshops
ma r k e d l y   re d u c e s   h o s p i t a l i z a t i o n s .   J  Am  C o l l   C a r d i o l .
2000;35:1737-1744.
Slonim AE, Bulone L, Damores MB, Goldberg T, Wingertzahn
MA, McKinley MJ. A preliminary study of growth hormone
therapy for Crohn’s disease. N Engl J Med. 2000;342:1633-1637.
Stampfer MJ, Hu FB, Manson JE, Rimm EB, Willett WC. Primary
prevention of coronary heart disease in women through diet
and lifestyle. N Engl J Med. 2000;343:16-22.
Swe e n e y  MS ,   F r a z i e r  OH.  De v i c e - s u p p o r t e d  my o c a rd i a l
revascularization: safe help for sick hearts. Ann Thorac Surg.
1992;54:1065-1070.
Taheri PA, Butz DA, Greenfield LJ. Academic health systems
management: the rationale behind capitated contracts. Ann
Surg. 2000;231:849-859.
Ware LB, Matthay MA. The acute respiratory distress syndrome.
N Engl J Med. 2000;342:1334-1349.
Witte FM. Authorship ethics: A clash of cultures. In Witte FM,
Taylor ND, eds. Essays for Biomedical Communicators: Volume 2
of Selected AMWA Workshops. Bethesda, Md: American Medical Writers Association; 1997:93-102.
Wong CS, Jelacic S, Habeeb RL, Watkins SL, Tarr PI. The risk of
the hemolytic-uremic syndrome after antibiotic treatment of
escherichia coli 0157:H7 infections. N Engl J Med. 2000;342:
1930-1936.
Yamaguchi M, Kuzume M, Matsumoto T. Adenovirus-mediated
insulin gene transfer improves nutritional and post-hepatectomized conditions in diabetic rats. Surgery. 2000;127:670-678.
Zeiger M. Essentials of Writing Biomedical Research Papers. 2nd ed.
New York, NY: McGraw-Hill; 2000.
About the Author
Marianne Mallia has worked as a medical editor
and writer for 25 years. She is currently senior medical writer and manager in the Section of Scientific
Publications at the Texas Heart Institute in Houston,
Texas. Mallia has edited and written more than 1600
scientific articles, speeches, and books, including Surgical Treatment of Aortic Aneurysms, by Denton A.
Cooley, MD; Reflections and Observations: Essays of
Denton A. Cooley; A History of the Texas Heart Institute;
and the Heart Owner’s Handbook. For many years she
served as a consultant to the Texas Heart Institute Journal. Mallia has served on various committees of the
American Medical Writers Association (AMWA), both
in the Southwest Chapter and at the national level.
For more than 15 years, she has led workshops
throughout the Texas Medical Center and Houston,
at other institutions around the country, and at chapter and Annual Conferences for AMWA. Mallia’s
AMWA workshops include Organizing the Biomedical
Paper, Medical Manuscripts Other Than the Biomedical
Paper, How to Organize and Run Medical Writing Internships, and Advanced Writing. In 1996 she was
named an AMWA Fellow, and in 1998 she received
the Golden Apple Award for outstanding workshop
leadership. Mallia was workshop coordinator for the
1997 and 1998 Annual Conferences, was Annual Conference Chair in 1999, and currently serves as Administrator of Education.</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p16 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p16">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2"/>
        <doc>RISK OF HAEMORRHAGIC COMPLICATIONS OF RETROPUBIC SURGERY 
IN FEMALES: ANATOMIC REMARKS ###
Ladislav Jaburek
a
, Jana Jaburkova
b
, Marek Lubusky
a
, Martin Prochazka
a
*
a
 Department of Obstetrics and Gynaecology, University Hospital Olomouc, Czech Republic
b  
Department of Anatomy, Faculty of Medicine and Dentistry, Palacky University Olomouc
E-mail: martin.prochazka@fnol.cz
Received: September 28, 2010; Accepted with revision: January 1, 2011
Key words: Urogynaecology/Retropubic surgery/Bleeding/Corona mortis/TVT
Background. An anatomic study.
Objective. To point out the risk of bleeding during retropubic surgery in females.
Methods. A pelvic dissection, preparation of vessels and photodocumentation in colour. 
Results. A detailed representation of topographic vessel relations in pelvic and retropubic regions is presented. This 
could be used as an authentic visual aid for postgraduate training in urogynaecological surgery. 
Conclusion. This study highlights the risk of vascular lesions common to all suspensory surgical procedures for female 
stress urinary incontinence. Apart from paraurethral vessels, the vessels of the urinary bladder, the paravesical plexuses, 
the retropubic anastomosis and the external iliac vessles can be injured in surgery. Preceeded by training at an accredited urogynaecologic centre, TVT can be considered a safe method. Introduction of other modifications such as the 
transobturator system (namely the + “inside-out” method) makes all urogynaecological surgical procedures much safer. ###
INTRODUCTION
Retropubic surgical procedures in females such as the 
TVT (Tension-free Vaginal Tape) in their modifications 
is a needle suspension retropubic operation for stress 
urinary incontinence. The technique was introduced by 
Ulmsten et al.
1
 in 1996. As a one-day surgery performed 
in local or regional anaesthesia it has some advantages 
over the Burch colposuspension. The operation requires 
only minimal excisions and tissue dissection and consists 
in a loose application of a prolene tape, without any fixation, under the middle part of the urethra. In this way 
the tape creates a support, exerting neither traction nor 
tension. The operation results in complete elimination 
or a significant improvement of incontinence in about 
90% of patients. At present, this method is widely used 
all over the world. Operations are performed with a minimum of complications and long-lasting good results
2,3
. 
Nevertheless, there is some risk of vascular injury during the needle penetration in the retropubic part of the 
procedure which cannot be done under visual control. 
Not only paraurethral vessels but also the vessels of the 
urinary bladder, the paravesical plexuses, the retropubic 
anastomosis and the external iliac vessels may be injured.
SUBJECT MATTER
Paraurethral vessels
The blood supply to the urethra is provided by the 
branches of the cervicovaginal artery (from the uterine 
artery) and the branches of the inferior vesical artery 
(from the internal iliac artery). In the perineal part, the 
urethra is supplied by the branches of the artery of the 
bulb of the vestibule (from the internal pudendal artery). 
The venous blood is drained into interconnected plexuses, 
the vesicovaginal plexus situated paraurethrally and the 
pudendal plexus Santorini which lies under the symphysis. 
Bleeding can be prevented by a minimal paraurethral dissection just sufficient to create an initial channel for the 
needle insertion. With preparation scissors it is then possible to gently perforate the urogenital diaphragm. If there 
is no contraindication for anaesthesia, a local anaesthetic 
with a vasoconstrictory additive (Supracaine 4%) can be 
infiltrated paraurethrally (a hydrodissection).
Urinary bladder vessels and paravesical plexuses
The arteries for the urinary bladder come in pairs from 
both sides. These are: the superior vesical artery (from 
the umbilical artery) and the inferior vesical artery (from 
the internal iliac artery). Tiny branches for the urinary 
bladder, the anterior vesical arteries, divert from the internal pudendal artery and the obturator artery. Other 
small branches, the posterior vesical arteries, divert from 
the medial rectal artery. Venous blood from the urinary 
bladder is collected from three regions – the venous submucosal plexus, the muscular plexus and the perivesical 
plexus, which have mutual anastomoses. They lead into 
the interconnected plexuses in the tenuous tissues of the 
cavum Retzii. It is the vesicovaginal plexus, from which 
the vesical veins lead into the internal iliac veins and the 
pudendal plexus Santorini, which drains blood into the 
internal pudendal vein. The important vessel stems are 
situated paravesically.
The risk of bleeding can be markedly decreased if 
contact of the introduced needle with the posterior wall 76 L. Jaburek, J. Jaburkova, M. Lubusky, M. Prochazka
Fig. 1. Contralateral retropubic anastomoses and the 
 corona mortis on the right side.
1 right external iliac artery
2 right corona mortis
3 right pubic bone
4 pubic symphysis
5 interepigastric anastomosis
6 interobturator anastomosis
7 interpudendal anastomosis
8 female urethra
Fig. 2. Corona mortis on the left side.
1 left external iliac artery
2 left corona mortis
3 left pubic bone
of the pubic bone is maintained. The flexion in the hip 
joints should not be greater than 60° so that with the manipulation with the rigid catheter guide the bladder neck 
can be easily moved in the opposite direction. Moderate 
flexion of hip joints also contributes to lesser congestion 
of pelvic vessels.
Retropubic anastomoses
The retropubic anastomoses pass along the posterior 
wall of the pubic bone and function as a part of the supplementary collateral pelvic circulation in some types of 
aortoiliac obstructions. There are contra- and the homolateral anastomoses.
The contralateral anastomoses are formed by transversal junctions on three levels (Fig. 1) right next to the upper margin of the pubic bone the anastomoses are formed 
by the inferior epigastric vessels, in the middle by the 
obturator vessels and at the lower margin by the internal 
pudendal vessels
4
.
The atraumatic construction of the needle tip also 
helps to minimize the risk of injury to these vessels.
The homolateral anastomoses, in contrast, represent an 
actual risk of vessels lesions during the lateral penetration 
of needles. Bleeding can occur from the known anastomosis between the obturator artery and the inferior epigastric 
artery (Fig. 2). This ramus pubicus anastomoticus became 
sadly famous as the corona mortis 
Hesselbachi in the past, when the strangulated inguinal hernia was treated by the percutaneous dissection 
of the hernial hilum. Bleeding in this situation requires 
surgical revision of the cavum Retzii. The corona mortis 
is present in 31% in our population
5
. The venous anastomoses only develop in 50%, the arterial ones in 14%, 
and anastomoses of both kinds develop in the rest. The 
average distance of the medial border of the anastomosing branches from the middle of the symphysis is 6.2 cm, 
ranging from 3 to 9 cm6
. In cases dissected at Department 
of Anatomy we found the average distance about 3.6 cm7
. 
Injury to the obturator artery can cause a life-threathening 
haemorrhage. The risk of bleeding can be decreased by 
the exact targeting of needle tips towards the abdominal 
incisions at the upper border of the pubic bones. The 
incisions should not be more than 4 to 5 cm apart (the 
width of three fingers, with the middle finger placed over 
the symphysis).
External iliac vessels
The external iliac artery, as well as the vein, can be 
injured when the needle tip deviates too laterally, similar 
to injury of the corona mortis. Such bleeding requires the 
urgent intervention of a specialist in vascular surgery. It 
is essential to keep in mind that even a small deviation 
of the handle medially leads to a greater excursion of the 
needle laterally. The risk of bleeding can be lowered by a 
controlled insertion of the needle with the handle resting 
against the open hand. Direct pressure should be avoided. 
As soon as the needle tip is introduced into the cavum 
Retzii, the contact of the needle with the posterior wall 
of the pubic bone is achieved by lowering the handle. 
The pressure can be applied only after the needle tip has 
reached the skin incision.
SUMMARY
This contribution highlights the possible risk of vascular lesions common to all suspensory surgical procedures 
for female stress urinary incontinence. Apart from the 
paraurethral vessels, the vessels of the urinary bladder, the Risk of haemorrhagic complications of retropubic surgery in females: anatomic remarks 77
paravesical plexuses, the retropubic anastomosis and the 
external iliac vessles can also be injured. The risk of bleeding can be decreased to a minimum by strict adherence 
to the guidelines for this surgical procedure suggested by 
Ulmsten et al.
1
 and in all awareness of the anatomic situation in the cavum Retzii. The area of safe needle insertion 
is not more than 3 cm to both sides from the middle of the 
symphysis. On condition that the implementation of the 
procedure is preceeded by training at an accredited urogynaecologic centre, TVT can be considered a safe method 
of surgical treatment which conforms to all the criteria of 
a one-day procedure. Introduction of other modifications 
such as the transobturator system (namely the + “insideout” method) makes all the urogynaecological surgical 
procedures much safer. A thorough understanding of the 
vascular anatomy in this space should help avoid serious 
operative complications
8-10
.
REFERENCES
 1. Ulmsten U, Henriksson L, Johnson P, Varhos G. An ambulatory 
surgical procedure under local anesthesia for treatment of female 
urinary incontinence. Int Urogynecol J 1996;7:81-86.
 2. Ulmsten U, Johnson P, Rezapour M. A three-year follow up of tension free vaginal tape for surgical treatment of female stress urinary 
incontinence. Brit J Obstet Gynaecol 1999;106:345-350.
 3. Wang AC, Lo TS. Tension-free Vaginal Tape - A minimally Invasive 
Solution to Stress Urinary Incontinence in women. J Reprod Med 
1998;43:429-434.
 4. Weigner K. Topograficka anatomie IV; 2nd ed. Prague: Vesmir; 
1936.
 5. Dylevsky I, Ticha J. Variability of the Branches of the Arteria iliaca 
interna. Folia Morphol 1987;35:425-435.
 6. Tornetta P 3rd, Hochwald N, Levine R. Corona mortis. Incidence 
et location. Clin Orthop 1996. p. 97-101.
 7. Jaburkova J. Collaterals and variations of human female pelvic 
vessels related to gynaecological surgery. Thesis; Olomouc; 2001. 
 8. Pathi SD, Castellanos ME, Corton MM. Variability of the retropubic space anatomy in female cadavers. Am J Obstet Gynecol 
2009;201(5):524.e1-5.
 9. Kachlik D, Pechacek V, Musil V, Baca V. The venous system of the 
pelvis: new nomenclature. Phlebology 2010;25(4):162-73.
 10. Darmanis S, Lewis A, Mansoor A, Bircher M. Corona mortis: an 
anatomical study with clinical implications in approaches to the 
pelvis and acetabulum. Clin Anat 2007;20(4):433-9.</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p7 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p7">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title : Classifying Unknown Proper Noun Phrases Without Context 
Joseph SMARR 
Symbolic Systems Program ###
Stanford University 
Stanford, CA 94305-2181 
jsmarr@stanford.edu 
Christopher D. MANNING 
Computer Science Department 
Stanford University 
Stanford, CA 94305-9040 
manning@cs.stanford.edu 
Abstract  
We present a probabilistic generative model 
used to classify unknown Proper Noun 
Phrases into semantic categories.  The core 
of the classifier is an n-gram character 
model, which is enhanced with an n-gram 
word-length model and a common word 
model.  While most work has depended 
largely on context or domain-specific rules 
for semantic disambiguation of unknown 
names, we demonstrate that there is 
surprisingly reliable statistical information 
available in the composition of the names 
themselves.  Using the context-independent 
probabilities assigned by our domain 
independent classifier is sufficient to 
achieve greater than 90% classification 
accuracy on typical tasks. 
Keywords: named-entity classification, unknown 
words, probabilistic modeling, n-grams ###
Introduction 
Unknown words and phrases are a continual 
source of trouble for statistical NLP techniques, 
because no statistics can be generated for an 
item that you have never seen before (Mikheev, 
1997).  This problem is particularly acute in 
domains where there are large numbers of 
specific names, or where new names are 
constantly being created.  For this study, we 
looked at five such categories of Proper Noun 
Phrases (PNPs): drug names, company names, 
movie titles, place names, and people’s names.   
There has been a great deal of interest in the 
named entity extraction task of extracting PNPs 
from text and classifying them.  For example, 
the MUC tests require identifying PNPs and 
classifying them as company names, place 
names, and people names (Mikheev et al, 1998, 
Collins et al, 1999).  Similarly, several studies 
have tried to identify novel terms in medical 
abstracts (e.g. Campbell, et al, 1999, Bodenreider et al, 2000).  Here we focus solely on the 
latter task of classifying PNPs. 
Traditional approaches to PNP classification 
rely primarily on large, manually constructed 
lists of known names or “gazetteers” (Wacholder et al, 1997, Charoenpornsawat et al, 1998, 
Mikheev et al, 1998, Bodenreider et al, 2000), 
hand-built, domain-specific rules based on patterns in the syntactic context (Appelt et al, 1995, 
Mikheev et al, 1998, Bodenreider et al, 2000), 
and/or gross word-level features such as capitalization, punctuation, or presence of numbers and 
other “special characters” (Bikel et al 1997, 
Wacholder et al, 1997, Baluja et al, 1999, Bikel 
et al, 1999, Bodenreider et al, 2000).  Many of 
these systems use some form of machine 
learning in conjunction with these features. 
These methods achieve relatively high levels of 
performance, but suffer from the problem that 
building lists of words or heuristic rules is slow, 
expensive, and domain-specific.  These shortcomings have been widely acknowledged (e.g., 
Wacholder et al, 1997, Mihkeev et al, 1999), yet 
few alternative strategies for unknown PNP 
classification have been proposed. 
In this paper we show that the internal 
composition of PNPs provides surprisingly 
strong evidence for classification, independent 
of any context. People often find it easy to 
classify PNPs that they have never seen before: 
a PNP like Novo-Doxylin just looks like a drug 
name.  In an application of named entity classification, the context of use of a PNP often 
provides powerful clues to classification, and 
these are exploited by most systems. Our central contention is that current systems insufficiently 
exploit information in the word shape of a PNP 
(i.e., essentially phonaesthetic considerations), 
which can also be effectively used for classification. By word shape, we mean features like 
common letter sequences and word lengths, 
along with the presence of key words within the 
PNP (Inc., Jr., etc.).  
In a complete system, our classifier would 
serve as an informed prior probability over PNP 
classes, which could be combined with 
additional clues from context, for instance using 
Bayesian evidence combination.  However, we 
focus here solely on the problem of computing 
context-independent priors, which has so far 
been under explored. 
The value of word-level features has been 
exploited in part-of-speech tagging, where suffix 
morphology is used to decide the class of 
unknown words (e.g., Mikheev, 1997). This 
technique would not work for semantic classification, but we propose a method of building a 
probabilistic model of the generation of PNPs, 
which can exploit latent regularities in word 
sequencing and shape. The method is similar to 
the use of character n-grams for language identification (Dunning 1994, Cavnar and Trenkle 
1994), and essentially reuses known techniques, 
though tuned to the features of the case at hand.   
The contribution of this paper is thus not so 
much in the novelty of the methods used, but in 
showing how much value can be gotten from an 
information source that has been ignored.  
Moreover, by automatically learning the 
statistics of a given category of PNPs, rather 
than manually constructing domain-specific 
heuristics, we also realize the ability to quickly 
reach high levels of classification accuracy in 
novel domains, a challenge that few existing 
methods have met.  
1 Formalization of Problem 
1.1 Task 
The performance task is to take a string 
representing a Proper Noun Phrase (e.g. Aureomycin or Keyspan Corp) and classify it into one 
of a predefined set of categories (e.g. drug name
or company name).  A Proper Noun Phrase is a 
sequence of one or more words that represents 
the name of a person, place, or thing.  As mentioned above, our goal is to assess the ability to 
classify an already segmented PNP independent 
of context. Segmentation of PNPs from text is a 
separate and prior problem, which has been well 
studied (Abney, 1991, Ramshaw et al, 1995, 
Bikel et al, 1997). 
1.2 Training 
We use a standard supervised learning paradigm.   
A training example consists of a PNP labeled 
with its semantic category.  A portion of the 
training examples (20% in the reported results) 
are held-out for learning various parameters 
described in the next section, and the remainder 
are counted to derive various statistics.  After 
cross-validation parameters have been set, the 
held-out data is also trained on before testing. 
1.3 Testing 
After training is completed, the classifier is 
presented with another list of PNPs, none of 
which appeared in the training data.  Some PNPs 
are inherently ambiguous and thus could be 
judged as correctly belonging to multiple classes 
(e.g., Washington is both a place and a name).  
However, we follow the stringent evaluation 
standard of only accepting the category from 
which the example was originally taken, 
regardless of its ambiguity. 
1.4 Evaluation 
Each result presented is the average of 10 
separate train/test runs.  In each case, a 
randomly selected 90% of the supervised data is 
used for training, and the remaining 10% is used 
for testing.  The evaluation metric we use is raw 
classification accuracy, defined as the number of 
test examples for which the correct category was 
provided by the classifier, divided by the total 
number of test examples presented.   
2 Model Used for Classification 
Classification of PNPs is performed using a 
probabilistic generative model for each category.  
Classification is determined as follows: 
Predicted-Category(pnp) = argmaxc
 P(c|pnp)  
           = argmaxc
 P(c)
α
×P(pnp|c) 
P(c) is estimated empirically from the training 
data, and α is a prior-boost set with a line search on held-out data.
1
    P(pnp|c) is a model of each 
category of PNPs, which is based principally on 
two types of  features: the number and length of 
words used, and the composition of each word. 
Formally, P(pnp|c) is a generative model of 
the following form (each term is implicitly 
conditioned on the category): 
P(pnp|c) = Pn-gram(word-lengths(pnp)) ×
Πword i∈pnp P(wi
|word-length(wi
)) 
P(wi
|len) = λlen× Pn-gram(wi
|len)
k/len
 + (1-λlen
) × Pword
(wi
|len) 
The probability for each word is computed 
as the weighted average of the character n-gram 
estimate and the known-word estimate.  
Interpolation weights are learned for each 
distinct word length using a line search on heldout data.
2
    P(w|len) is estimated empirically as 
the number of times word  w  was seen in the 
training data divided by the total number of 
words of length len characters seen. 
The function  word-lengths(pnp) returns a 
list of integers, representing the number of 
characters in each word of the PNP.  The list is 
prepended with (n–1) boundary symbols for 
conditioning, and a final “0-lenth word” 
indicating the end of the PNP is also added so 
that the termination of PNPs becomes a 
statistically observable event.  For example, 
word-lengths(Curtis E. Lemay) = [6, 2, 5, 0].   
Two n-gram (Markov) models are used, one 
for word lengths, and one for characters.  We set 
n=4 for lengths, and n=6 for characters, and used 
deleted interpolation to estimate probabilities, 
using the following recursive function: 
P0-gram(symbol|history) = uniform-distribution 
Pn-gram(s|h) =  λC(h)Pempirical
(s|h) + (1- λC(h)
)P(n-1)-gram(s|h)
Thus, the 2-gram estimate is a mix of the 
empirical 2-gram distribution and the combined 
1/0-gram distribution, the 3-gram estimate is a 
mix of the empirical 3-gram and the combined 
2/1/0-gram, etc.  Interpolation parameters  λC(h) 
are estimated via EM on held-out data, and are 
                                                     
                                                     
1
 Setting a prior boost usually made little difference 
as it was often set to 1.0 during cross-validation. 
2
If a novel word length is seen during testing, the 
interpolation parameter is automatically set to 1 for 
the character n-gram and 0 for the word model, since 
no words of that length have been seen before.   
learned separately based on the binned count of 
the conditioning context  C(h).  We fixed the 
following bins in our experiments: {0, ≤5, ≤50, 
≤500,  ≤5000, &gt;5000}, though we found only 
minor deviation in our results by changing the 
specific bins used.   
The character n-gram estimate for the entire 
word is conditioned on word-length by dividing 
by the fraction of words in the training data with 
the given number of characters.
3
  The first word 
in the PNP is prepended with n-1 spaces (starter 
symbols, with the effect that the 2-gram and 
lower estimates treat the first word identically to 
a middle word) and the subsequent words are 
prepended with the preceding n-1 chars in the 
PNP (including the preceding space).  Each 
estimate is run up through the following space, 
and for the last word, a unique termination 
symbol is appended. 
Working with character level models means 
that longer words have more influence on 
classification than short words. To mitigate the 
effects of this bias, we normalize the n-gram 
estimate for length by taking the (k/length)’th 
root, where  k is a global constant learned on 
held-out data through a line search.  The 
motivation for this process and its results are 
discussed in more detail in Section 5.3.2. 
3 Data Sets Used in Experiments 
We assembled five categories of PNPs for our 
experiments, each containing several thousand 
examples (see the appendix for a complete 
information on counts and sources).  The categories were pharmaceutical drugs (drug), companies listed on the New York Stock Exchange 
(nyse), movies produced in 2000 (movie), cities 
and countries from around the world (place), 
and famous people’s names (person).  These 
collections were selected because they represented major sources of unknown PNPs of interest, and because of their diverse composition.   
These data sets were intentionally left in the 
rather “noisy” state in which they were found, to 
breed robustness, and to accurately measure performance on “real world” examples.  There is in-
3
 We condition the probability for each word on its 
length in order to keep it independent from the 
probability of seeing any word of that length, which 
is already computed by the length n-gram model. Figure 1. Classification accuracy on pairwise, 1-rest, and n-way tests
98.93%
98.70%
64%
41%
16%
98.
98.
98.
97.76%
96.81%
95.77%
95.47%
95.24%
94.34%
92.70%
91.86%
90.90%
89.94%
88.11%
93.25%
94.57%
82% 84% 86% 88% 90% 92% 94% 96% 98% 100%
drug-nyse
nyse-drug_movie_place_person
nyse-place
nyse-person
drug-person
nyse-movie
drug-nyse_movie_place_person
drug-movie
person-drug_nyse_movie_place
drug-place
nyse-place-person
place-person
drug-nyse-place-person
movie-person
place-drug_nyse_movie_person
movie-drug_nyse_place_person
movie-place
drug-nyse-movie-place-person
consistent use of capitalization, punctuation, and 
canonical formatting.  Many of the PNPs within 
a given category come from different languages 
(e.g., foreign films).  Some categories contain a 
number of frequently occurring words (e.g., Inc.
and Corporation in nyse); others do not.
4
   
4 Experimental Results 
To assess the accuracy of our classifier, we ran
three types of tests:  pairwise tests of a single
category against another single category,  1-rest
tests of a single category against the union of the 
other categories, and  n-way tests,  where all 
categories are against each other.  The results of 
these tests are presented in Figure  1, sorted by 
classification accuracy and shown with standard 
deviations (computed from the ten separate 
train/test runs carried out for each result). 
It has been pointed out in the  MUC
competitions that PNPs often appear abbreviated 
in text, especially when mentioned earlier in full
form.  In such cases, not  all of the information 
contained in these data sets would be available.
Previous work in Named Entity Extraction  has 
addressed this problem by first trying to find full 
PNPs, then looking for their abbreviated 
versions (Mikheev et al, 1998).  However, we 
note that the current system can also recognize 
single-word unknown PNPs directly.  Over 30% 
of the total PNPs used in these data sets  are
single-word PNPs, and in some categories, the
number is much higher (e.g. 84% of place names
are  single  words).  Thus the ability of the
classifier to handle both the presence and 
absence of common peripheral words in PNPs is
being directly measured in our results. 
As expected, the n-way test was the  most
difficult.  The ranking of results also reflects the
inherent difficulty of the different categories. 
Overall, company names were most easily
recognized, followed by drug names, person
names, and place names, with  movie titles 
proving the most difficult.   
5 Discussion 
In this section, we account for the experimental 
results above, analyze the contribution of each
piece of our model to overall performance, and 
examine the various parameters learned during
cross validation.  We also present novel PNPs 
stochastically generated from our  model, and
conclude with demonstration of the ease with
which this model can achieve proficiency with
new categories. 
                                                     
4
 One thing we did manually correct was names that
appear with their words in a non-standard order used 
for indexing, such as movie titles like Ghost, The and 
names like Adams, John Quincy.  Each of these cases
was restored to their “natural” word order.62.35%
72.18%
89.66%
74.45%
89.59%
92.09%
91.94%
0% 20% 40% 60% 80% 100%
length n-gram only
word model only
char n-gram only
length+word
char+word
char+length
full model
Figure 3. Classification accuracy for individual 
model components and combinations (4-way test).
5.1 Analysis of Experimental Results 
Figure 2 shows the  confusion matrix for the 5-
way classification task.  The area of each circle
is proportional to the number of examples in that 
cell.  Movies, places, and people are most often 
confused for one another, and  drugs are often
misclassified as places (they both contain many
odd-looking one-word names). As an indication 
of the difficulty of dealing with movie titles, if 
the n-way tests are rerun as a four-way test 
without movie titles,  average classification
accuracy jumps from 88.1% to 93.2%.  3-way
classification between companies, places, and 
people (similar to the ENAMEX classification 
task in MUC) is performed with an average
classification accuracy of 94.6%. 
A similar source of errors stems from words 
(and common intra-word letter sequences) that
appear in one category and drive classification in 
other categories when there is insufficient 
information to the contrary.  For example, in one
run  Delaware is erroneously classified as a 
company, because it was never seen  as a place 
name, but it was seen in several company names 
(such as  GTE Delaware LP).  Cases like these
appear to be an inherent limitation of this 
classifier.   However  we are being unusually
restrictive by forcing our test set to be 
completely disjoint with our training set.  In  a 
real application, common place names like 
Delaware would have been trained  on and
would be readily recognizable as place names.  
Predicted Category
drug nyse movie place person
             
Figure 2. Confusion matrix for 5-way test.
drug  nyse  movie  place 
Correct Category 
person 
5.2  Contribution of Model Features 
To assess the relative contribution of the three
major components of our model (length n-gram, 
character n-gram, common words), we present
accuracy results using each model component in 
isolation and each possible pair of components 
(Figure 3).
5
  We use the 4-way drug-nyse-placeperson test  as a representative indicator of 
The ease of  identifying company names is performance.
largely attributable to the plethora of  common
words available, such as International, Capital, 
Inc., Corporation, and so on.  The difficulty of
place names and movie titles is partly due to the
fact that they contain words from many different 
languages (and thus the estimates learned blur 
together what should really be  separate 
distributions).  Movie titles are also the most
inherently ambiguous, since they are often 
named after  people (e.g.  John Henry) or places
(e.g.  Nuremberg), and  often contain words 
normally associated with  another category (e.g. 
Prozac Nation and  Love, Inc.).  Mikheev et  al
(1998) report instances of similar ambiguity as a 
source of error in their work.   
Each feature gives a classification accuracy
significantly above a most-frequent-class 
baseline (34%), with the character n-gram being
by far the most powerful single feature.   
Combining features reveals that the character and length n-grams provide complementary
information, but adding the word model to the
character n-gram does not improve performance. 
The common word model by itself is quite 
effective, but it is largely subsumed by our high
order character n-gram model, because common
                                                     
5
 Note that the full model in Figure 3 is identical to
the 4-way test in Figure 1.  The slight difference in 
performance is merely due to data set differences. 91.0%
91.5%
92.0%
92.5%
93.0%
93.5%
94.0%
0 1 2 3 4 5 6 7 8 9 10 11
Figure 4. Classification accuracy vs. word length 
normalization constant (4-way test).  
short words  are  memorized as single n-gram
entries, and common long words contain many
common n-grams.  The  word  model  could be 
eliminated without hurting performance. 
The word model could also be regarded as a
reasonable  baseline, since it is basically
equivalent to the performance that  could be
expected from a (multinomial) Naïve Bayes
word model, a  model that is often used as a 
baseline in  text classification tasks. As one
further indicative baseline, we ran a publicly
available variable n-gram language identifier on 
our data (Beeferman 1996). It achieves a performance of 76.54%. This is not a fair comparison: Beeferman explicitly notes that his system
is unlikely to be reliable on very short inputs of
the sort present in our data, but this nevertheless
again shows that our system is sufficiently well 
tuned to the task at hand to achieve performance 
well above obvious baseline levels.  
5.3  Impact of Other Model Parameters 
In addition to the three major model components 
described above, performance is affected by the
length of the n-gram models used, the  use of a 
word length normalization constant for the
character n-gram,  and the amount of  available 
training data.
5.3.1  Increasing N-Gram Length
The only important model parameters not set on 
held-out data are the  sizes of the length and
character n-gram models.  In principle, the use
of deleted interpolation with weights set on heldout data means that very large n-gram models 
could be  used, and  once  data sparseness was  a
larger factor than predictive accuracy, the
higher-order n-gram factors would be downweighted.   However in practice training and 
testing is exponentially slow in the length of the 
n-gram, and the largest useful n-gram size, once 
empirically determined, is relatively stable. 
Table 1 shows classification accuracy of the 
character n-gram model alone for increasing 
values of n.  Accuracy increases and plateaus, at
which point  increasing n further has no effect.
The same analysis holds for increasing n for the 
word-length n-gram, also shown in Table 1. 
5.3.2  Word Length Normalization 
As mentioned above, modeling words with a 
character n-gram model means treating 
characters as the unit of  evidence, so that long 
words have more of an impact on classification 
than short  words. But  in many instances, it
seems intuitively that words are a better unit of 
evidence (indeed, many telling common words
like Inc. or Lake are very short).  To compensate
for this effect we introduce a parameter to
normalize the probability assigned to each word 
in a PNP by taking the (k/length)’th root, where 
length is the number of characters in the word, 
and k is a global constant set with a line search
on held-out data.  
Figure 4 shows how varying the value of  k
affects performance on  a typical run.  The
optimal value of  k varies by data set, but is 
usually around 2 to 3.  Probability judgments for 
words of length &lt;k  are  magnified, while those 
for words of length &gt;k are diminished.  The 
result is that compelling short words can 
effectively compete with less compelling longer 
words,  thus shifting the unit of evidence from 
the character to the word. 
5.3.3  Training Data Size 
n  1 2 3 4 5 6 7
char  60.0 81.4 87.7 89.2 89.5 89.7 89.8
length 46.4 60.1 62.2 62.4 62.4 - -
Table 1. Classification accuracy of char and 
word-length n-gram models alone (4-way test). 
Obtaining a large number of examples of each 
category to use as training data was not difficult. 
Nevertheless, it is still worth examining 
classifier performance as a function of the
amount of training data provided.   Figure 5 illustrates that  while performance
continues to improve as more training examples 
are provided, the classifier only requires a small
subset of the training data to approach its full
potential.  This  means that when faced with
novel categories, for which large collections of 
examples are not immediately available, 
acquiring proficiency should still be possible.   
Figure 5 also indicates that increasing the 
amount of training data would not significantly
boost performance.  This hypothesis is supported
by the observation that the majority of misclassified examples are either inherently ambiguous, 
or contain words that  appeared in another 
category, but that are not strongly indicative of 
any one category (as mentioned in Section 5.1). 
5.4 Generation of Novel PNPs 
Generative  models are common for classification, but can also be used to perform generation. 
We stochastically generated a collection of
novel PNPs as an alternative means for getting a
sense of the  quality of the learned  models.  A
favorable selection of generated examples for 
each category is presented in Table 2. 
Not all generated examples are this coherent, but we  are encouraged by the surprisingly 
natural look of a large fraction of what is 
generated.  Sometimes  entire training examples 
are generated, but usually the result is a mix of 
existing and novel words mixed together. 
0%
20%
40%
60%
80%
100%
0 5000 10000 15000 20000
Figure 5. Classification accuracy vs. number of 
training examples (4-way test). 
5.5 Generalization to New Categories 
The fact that our classification technique is 
domain independent means that we can quickly
attain high levels of  performance on categories 
that have not previously been studied.  We 
illustrate the versatility of our approach here, by 
applying the classifier to a novel domain, without making any changes to the classifier design. 
Inspired by the game “Cheese or Disease?”, 
featured on the MTV show  Idiot Savants  (see
Holman, 1997), as our novel categories, we 
chose discriminating names of cheeses, and
names of diseases.  The basic idea is that there 
are  many odd-sounding cheese and disease 
names, some of which are quite difficult to tell
apart.  This seemed like an ideal test for the PNP
classifier.   
Finding existing lists of cheeses and diseases
on the web proved trivial (see  Appendix for 
details), and since the classifier is domain
independent, we were  able to start training
immediately. With 10 minutes of work, we had 
a classifier that achieved 93.5% classification
accuracy.   While recognizing references to 
cheese  may not  be high on the Defense
Department’s priority list, we feel that the ability 
to quickly reach high levels of proficiency in 
novel domains is a key benefit of our approach. 
6 Conclusion 
We have demonstrated that there are reliable 
regularities in the way names are constructed, 
which can be exploited for the purposes of
named-entity classification.  Specifically, there 
are common sequences of letters in the words
used, there  are common words that appear
alongside uncommon words, and there are 
regularities in the number and length  of words
used in the name.  These clues are sufficient for 
highly accurate classification, even in the absence of further context,  and can be effectively 
used to complement existing context-based 
techniques for named-entity recognition by
supplying an informed prior probability over
categories. 
drug: Esidrine Plus Base with Moisturalent •
Ambenylin • Carbosil DM 49 
nyse: Downe Financial Grp PR • Intermedia Inc. 
• Host Manage U.S.B. Householding Ltd.  
movie: Dragons: The Ever Harlane •  
Alien in Oz • El Tombre 
place: Archfield • Lee-Newcastleridge • Qatad 
person: Benedict W. Suthberg • Hugh Grob II •
Elias Lindbert Atkinson
Table 2.  Sample of generated PNPs. It should not come as too much of a surprise 
that categories like drugs, companies, and 
movies have similarly constructed names.  As 
Krauskfopf (2002) points out, coming up with 
new drug names is usually a multi-million dollar 
process in which special consultants are hired to 
find names that are different enough to be 
legally protected, but that have a “product of 
several powerful sounds” (Prozac is touted as 
one of the best invented drug names).  It may be 
the case that our classifier is learning the 
regularities in drug names that these consultants 
have buried subconsciously in their heads.  
Generation of novel PNPs  with this type of 
model could prove to be a compelling 
application on its own. 
Acknowledgements 
Special thanks to Stephen Patel, who worked on 
an earlier version of this project. 
References  
Abney, S. (1991). Parsing By Chunks. In Berwick, 
R., Abney, S., and Tenny, C., editors,  PrincipleBased Parsing. Kluwer Academic Publishers. 
Appelt, D., Hobbs, J., Bear, J., Israel, D., Kameyama, 
M., Kehler, A., Martin, D., Myers, K., &amp; Tyson, 
M. (1995). SRI International FASTUS system: 
MUC-6 test results and analysis. In Proc. of the 6th 
Message Understanding Conference, pp. 237-248.  
Baluja, S., Mittal, V., &amp; Sukthankar, R. (1999). 
Applying machine learning for high performance 
named-entity extraction.  Proceedings of the 
Conference of the Pacific Association for 
Computational Linguistics (pp. 365-378). 
Beeferman, D. (1996). Stochastic language identifier. 
Unpublished, Carnegie Mellon University. 
http://www.dougb.com/ident.html 
Bikel, D., R. Schwartz, and R. Weischedel. (1999). 
An Algorithm that Learns What&apos;s in a Name. 
Machine Learning 34: 211–231. 
Bikel, D.M., Miller, S., Schwartz, R. &amp; Weischedel, 
R. (1997). Nymble: a High-Performance Learning 
Name-finder. Proc. ANLP-97, pp. 194-201. 
Bodenreider, O., &amp; Zweigenbaum, P. (2000). Identifying proper names in parallel medical terminologies. In Medical Infobahn for Europe – Proceedings of MIE2000 and GMDS2000, pp. 443-447. 
Campbell, D. A., &amp; Johnson, S. B. (1999). A 
Technique for Semantic Classification of Unknown 
Words Using UMLS Resources. In  AMIA ’99 
Annual Symposium (American Medical Informatics 
Association), session on Innovations in NLP.
Cavnar, W. B. and Trenkle, J. M. (1994). Ngram 
Based Text Categorization.  Proceedings of the 
Third Annual Symposium on Document Analysis 
and Information Retrieval, pp 161-169. 
Charoenpornsawat, P., Kijsirikul, B., &amp; Meknavin, S. 
(1998).  Feature-Based Proper Name Identification 
in Thai. In  NCSEC-98 (The National Computer 
Science and Engineering Conference &apos;98). 
Collins M. and Singer Y. (1999). Unsupervised 
models for named entity classification. In 
Proceedings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language 
Processing and Very Large Corpora, pp. 189-196. 
Dunning, T. (1994). Statistical identification of 
language. Computing Research Laboratory 
technical memo MCCS 94-273, New Mexico State 
University, Las Cruces, New Mexico. 
Holman, C. (1997). TV on the edge: Idiot Box. 
Creative Loafing, Atlanta, February 08, 1997.  
&lt;http://www.creativeloafing.com/archives/atlanta/ 
newsstand/020897/b_edge.htm&gt; (Visited: 3/20/02). 
Krauskopf, L. (2002). Naming new drugs: Costly, 
complex.   The New Jersey Record, January 15, 
2002. &lt;http://home.cwru.edu/activism/READ/ 
Bergen011502.html&gt; (Visited: 03/20/02). 
Mikheev A., Moens M. and Grover C. (1999) Named 
Entity recognition without gazetteers. In 
Proceedings of the Annual Meeting of the 
European Association for Computational 
Linguistics (EACL&apos;99), Bergen, Norway, pp. 1-8. 
Mikheev, A. (1997). Automatic Rule Induction for 
Unknown Word Guessing.  Computational 
Linguistics vol 23(3), ACL 1997. pp. 405-423. 
Mikheev, A., Grover, C., &amp; Moens, M. (1998) 
Description of the LTG System Used for MUC-7. 
MUC-7. Fairfax, Virginia. 
Ramshaw, L. A. and Marcus, M. P. (1995). Text 
chunking using transformation-based learning. In 
Yarowsky, D. and Church, K., editors, Proceedings 
of the Third Workshop on Very Large Corpora.
Wacholder, N., Ravin, Y., &amp; Choi, M. (1997). 
Disambiguation of Proper Names in Text. In: 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing, pp. 202-208. 
Appendix: Size and Source of Each Data Set 
Category:   drug (6871 examples) 
Description:  Micromedex 2000 USP Drug Index 
Source:  my.webmd.com/drugs Category:   nyse (3403 examples) 
Description: Companies on the NY Stock Exchange 
Source:  www.nyse.com/listed 
Category:   movie (8619 examples) 
Description: Internet Movie Database (IMDB) listing 
of 2000 movies and videos 
Source:  us.imdb.com/Sections/Years/2000 
Category:   place (4701 examples) 
Description: Collection of country, state/province, 
and city names from around the world 
Source: dir.yahoo.com/Regional/Countries  
Category:   person (5282 examples) 
Description:  List of people with online biographies 
Source: www.biography-center.com
Category:   cheese (599 examples) 
Description: Global database of cheese information 
Source:   www.cheese.com 
Category:   disease (1362 examples) 
Description: MeSH List of Diseases and Disorders 
Source: www.mic.ki.se/Diseases/alphalist.html</doc>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p8 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p8">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title : The Fastest Fourier Transform in the West 
(MIT-LCS-TR-728)
Matteo Frigo
1
Steven G. Johnson
2
Massachusetts Institute of Technology
September 11, 1997
Matteo Frigo was supported in part by the Defense Advanced Research Projects Agency (DARPA) under
Grant N00014-94-1-0985. Steven G. Johnson was supported in part by a DoD NDSEG Fellowship, an MIT
Karl Taylor Compton Fellowship, and by the Materials Research Science and Engineering Center program of
the National Science Foundation under award DMR-9400334.
This paper is a technical report of the MIT Labroratory for Computer Science MIT-LCS-TR-728
1
MIT Laboratory for Computer Science, 545 Technology Square NE43-203, Cambridge, MA
02139. athena@theory.lcs.mit.edu
2
Massachusetts Institute of Technology, 77 Massachusetts Avenue, 12-104, Cambridge, MA
02139. stevenj@mit.edu ###
Abstract
This paper describes FFTW, a portable C package for computing the one- and multidimensional complex discrete Fourier transform (DFT). FFTW is typically faster than all other
publicly available DFT software, including the well-known FFTPACK and the code from
Numerical Recipes. More interestingly, FFTW is competitive with or better than proprietary, highly-tuned codes such as Sun’s Performance Library and IBM’s ESSL library. FFTW
implements the Cooley-Tukey fast Fourier transform, and is freely available on the Web at
http://theory.lcs.mit.edu/˜fftw.
Three main ideas are the keys to FFTW’s performance. First, the computation of the
transform is performed by an executor consisting of highly-optimized, composable blocks
of C code called codelets. Second, at runtime, a planner ﬁnds an efﬁcient way (called a
‘plan’) to compose the codelets. Through the planner, FFTW adapts itself to the architecture
of the machine it is running on. Third, the codelets are automatically generated by a codelet
generator written in the Caml Light dialect of ML. The codelet generator produces long,
optimized, unreadable code, which is nevertheless easy to modify via simple changes to the
generator.
Keywords: Fast Fourier transform, high performance, ML, code generation. ###
1 Introduction
This paper describes FFTW, a portable C package for computing the one- and multidimensional complex discrete Fourier transform (DFT). Extensive benchmarking demonstrates
that FFTW is typically faster than all other publicly available DFT software, including the
well-known FFTPACK [1] and the code from Numerical Recipes [2]. More interestingly,
FFTW is competitive with or better than proprietary, highly-tuned codes such as Sun’s Performance Library and IBM’s ESSL library. FFTW is an implementation of the CooleyTukey [3] fast Fourier transform (FFT), and is freely available on the World Wide Web at
the URL http://theory.lcs.mit.edu/˜fftw.
Three main ideas are the keys to FFTW’s performance. First, the computation of the
transform is performed by an executor consisting of highly-optimized, composable blocks
of C code called codelets. Second, at runtime, a planner ﬁnds an efﬁcient way (called a plan) 
to compose the codelets. Through the planner, FFTW adapts itself to the architecture of the
machine it is running on. In this way, FFTW is a single program that performs efﬁciently
on a variety of architectures. Third, the codelets are automatically generated by a codelet
generator written in the Caml Light dialect of ML [4]. The codelet generator produces long,
optimized, unreadable code, which is nevertheless easy to modify via simple changes to the
generator.
Despite its internal complexity, FFTW is easy to use. (See Figure 1.) The user interacts
with FFTW only through the planner and the executor; the codelet generator is not used after
compile-time. FFTW provides a function that creates a plan for a transform of a certain size.
Once the user has created a plan, she can use the plan as many times as needed. FFTW is
not restricted to transforms whose size is a power of2. A parallel version of the executor,
written in Cilk [5], also exists.
The executor implements the well-known Cooley-Tukey algorithm [3], which works by
factoring the sizeN of the transform intoN = N1N2. The algorithm then recursively computesN1 transforms of sizeN2 andN2 transforms of sizeN1. The base case of the recursion
is handled by the codelets, which are hard-coded transforms for various small sizes. We emphasize that the executor works by explicit recursion, in sharp contrast with the traditional
loop-based implementations [6, page 608]. The recursive divide-and-conquer approach is
superior on modern machines, because it exploits all levels of the memory hierarchy: as
soon as a subproblem ﬁts into cache, no further cache misses are needed in order to solve
that subproblem. Our results contradict the folk theorem that recursion is slow. Moreover,
the divide-and-conquer approach parallelizes naturally in Cilk.
The Cooley-Tukey algorithm allows arbitrary choices for the factorsN1 andN2 ofN.
The best choice depends upon hardware details like the number of registers, latency and
throughput of instructions, size and associativity of caches, structure of the processor pipeline,
1fftw_plan plan;
int n = 1024;
COMPLEX A[n], B[n];
/* plan the computation */
plan = fftw_create_plan(n);
/* execute the plan */
fftw(plan, A);
/* the plan can be reused for other inputs
of size n */
fftw(plan, B);
Figure 1: Simpliﬁed example of FFTW’s use. The user must ﬁrst create a plan, which can
be then used at will. In the actual code, there are a few other parameters that specify the
direction, dimensionality, and other details of the transform.
etc. Most high-performance codes are tuned for a particular set of these parameters. In contrast, FFTW is capable of optimizing itself at runtime through the planner, and therefore the
same code can achieve good performance on many architectures. We can imagine the planner as trying all factorizations ofN supported by the available codelets, measuring their execution times, and selecting the best. In practice, the number of possible plans is too large for
an exhaustive search. In order to prune the search, we assume that the optimal solution for
a problem of sizeN is still optimal when used as a subroutine of a larger problem. With this
assumption, the planner can use a dynamic-programming [7, chapter 16] algorithm to ﬁnd
a solution that, while not optimal, is sufﬁciently good for practical purposes. The solution
is expressed in the form of byte-code that can be interpreted by the executor with negligible
overhead. Our results contradict the folk theorem that byte-code is slow.
The codelet generator produces C subroutines (codelets) that compute the transform of
a given (small) size. Internally, the generator itself implements the Cooley-Tukey algorithm
in symbolic arithmetic, the result of which is then simpliﬁed and unparsed to C. The simpli-
ﬁcation phase applies to the code many transformations that an experienced hacker would
perform by hand. The advantage of generating codelets in this way is twofold. First, we can
use much higher radices than are practical to implement manually (for example, radix-32
tends to be faster than smaller radices on RISC processors). Second, we can easily experiment with diverse optimizations and algorithmic variations. For example, it was easy to
add support for prime factor FFT algorithms (see [8] and [6, page 619]) within the codelets.
2A huge body of previous work on the Fourier transform exists, which we do not have
space to reference properly. We limit ourselves to mention some references that are important to the present paper. A good tutorial on the FFT can be found in [9] or in classical textbooks such as [6]. Previous work exists on automatic generation of FFT programs: [10] describes the generation of FFT programs for prime sizes, and [11] presents a generator of
Pascal programs implementing a Prime Factor algorithm. Johnson and Burrus [12] ﬁrst applied dynamic programming to the design of optimal DFT modules. Although these papers
all deal with the arithmetic complexity of the FFT, we are not aware of previous work where
these techniques are used to maximize the actual performance of a program. The behavior
of the FFT in the presence of nonuniform memory was ﬁrst studied by [13]. Savage [14]
gives an asymptotically optimal strategy for minimizing the memory trafﬁc of the FFT under very general conditions. Our divide-and-conquer strategy is similiar in spirit to Savage’s approach. The details of our implementation are asymptotically suboptimal but faster
in practice. Some other theoretical evidence in support of recursive divide-and-conquer algorithms for improving locality can be found in [15].
In this short paper we do not have space to give more details about the planner and the
executor. Instead, in Section 2 we present performance comparisons between FFTW and
various other programs. In Section 3, we discuss the codelet generator and its optimization
strategy. Finally, in Section 4 we give some concluding remarks.
2 Performance results
In this section, we present the result of benchmarking FFTW against many public-domain
and a few proprietary codes. From the results of our benchmark, FFTW appears to be the
fastest portable FFT implementation for almost all transform sizes. Indeed, its performance
is competitive with that of the vendor-optimized Sun Performance and ESSL libraries on
the UltraSPARC and the RS/6000, respectively. At the end of the section we describe our
benchmark methodology.
It is impossible to include the benchmark results for all machines here. Instead, we
present the data from two machines: a 167-MHz Sun UltraSPARC-I and an IBM RS/6000
Model 3BT (Figures 2 through 6). Data from other machines are similar and can be found
on our web site, as well as results for transforms whose size is not a power of2. The performance results are given as a graph of the speed of the transform in MFLOPS versus array
size for both one and three dimensional transforms. The MFLOPS count is computed by
postulating the number of ﬂoating point operations to be5N log2 N, whereN is the number
of complex values being transformed (see [16, page 23] and [17, page 45]). This metric is
imprecise because it refers only to radix-2 Cooley-Tukey algorithms. Nonetheless, it allows
3our numbers to be compared with other results in the literature [1]. Except where otherwise
noted, all benchmarks were performed in double precision. A complete listing of the FFT
implementations included in the benchmark is given in Table 1.
Figure 2 shows the results on a 167MHz UltraSPARC-I. FFTW outperforms the Sun
Performance Library for large transforms in double precision, although Sun’s software is
faster for sizes between128 and2048. In single precision (Figure 4) FFTW is superior over
the entire range. On the RS/6000 FFTW is always comparable or faster than IBM’s ESSL
library, as shown in Figures 3 and 6. The high priority that was given to memory locality in
FFTW’s design is evident in the benchmark results for large, one-dimensional transforms,
for which the cache size is exceeded. Especially dramatic is the factor of three contrast on
the RS/6000 (Figure 3) between FFTW and most of the other codes (with the exception of
CWP, discussed below, and ESSL, which is optimized for this machine). This trend also
appeared on most of the other hardware that we benchmarked.
A notable program is the one labelled ‘CWP’ in the graphs, which sometimes surpasses
the speed of FFTW for large transforms. Unlike all other programs we tried, CWP uses a
prime-factor algorithm [18, 19] instead of the Cooley-Tukey FFT. CWP works only on a
restricted set of transform sizes. Consequently, the benchmark actually times it for a transform whose size (chosen by CWP) is slightly larger than that used by the rest of the codes.
We chose to include it on the graph since, for many applications, the exact size of the transform is unimportant. The reader should be aware that the point-to-point comparison of CWP
with other codes may be meaningless: CWP is solving a bigger problem and, on the other
hand, it is choosing a problem size it can solve efﬁciently.
The results of a particular benchmark run were never entirely reproducible. Usually, the
differences from run to run were 5% or less, but small changes in the benchmark could produce much larger variations in performance, which proved to be very sensitive to the alignment of code and data in memory. We were able to produce changes of up to 10% in the
benchmark results by playing with the data alignment (e.g. by adding small integers to the
array sizes). More alarmingly, changes to a single line of code of one FFT could occasionally affect the performance of another FFT by more than 10%. The most egregious offender
in this respect was one of our Pentium Pro machines running Linux 2.0.17 and the gcc 2.7.2
compiler. On this machine, the insertion of a single line of code into FFTW slowed down
a completely unrelated FFT (CWP) by almost a factor of twenty. Consequently, we do not
dare to publish any data from this machine. We do not completely understand why the performance Pentium Pro varies so dramatically. Nonetheless, on the other machines we tried,
the overall trends are consistent enough to give us conﬁdence in the qualitative results of
the benchmarks.
Our benchmark program works as follows. The benchmark uses each FFT subroutine
to compute the DFT many times, measures the elapsed time, and divides by the number</doc>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p9 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p9">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title : A Stochastic Iterative Amplitude Adjusted Fourier Transform
algorithm with improved accuracy 
V. Venema, F. Ament, and C. Simmer
Meteorologisches Institut, Universitat Bonn, Germany ¨
Received: 16 January 2006 – Revised: 20 April 2006 – Accepted: 20 April 2006 – Published: 26 July 2006 ###
Abstract. A stochastic version of the Iterative Amplitude
Adjusted Fourier Transform (IAAFT) algorithm is presented.
This algorithm is able to generate so-called surrogate time
series, which have the amplitude distribution and the power
spectrum of measured time series or ﬁelds. The key difference between the new algorithm and the original IAAFT
method is the treatment of the amplitude adjustment: it is not
performed for all values in each iterative step, but only for
a fraction of the values. This new algorithm achieves a better accuracy, i.e. the power spectra of the measurement and
its surrogate are more similar. We demonstrate the improvement by applying the IAAFT algorithm and the new one to
13 different test signals ranging from rain time series and 3-
dimensional clouds to fractal time series and theoretical input. The improved accuracy can be important for generating
high-quality geophysical time series and ﬁelds. The traditional application of the IAAFT algorithm is statistical nonlinearity testing. Reassuringly, we found that in most cases
the accuracy of the original IAAFT algorithm is sufﬁcient for
this application. ###
1 Introduction
The Iterative Amplitude Adjusted Fourier Transform
(IAAFT) algorithm was developed by Schreiber and Schmitz
(1996, 2000) to generate surrogate time series for statistical nonlinearity testing (Theiler et al., 1992; Theiler and
Prichard, 1996; Kugiumtzis, 1999). Surrogates are time series which share certain statistical properties with the original
time series. In case of the IAAFT algorithm, the surrogates
share their distribution and power spectrum with the measurement. To stress that the surrogate is a permutation of the
original, i.e. that the values of the original are reproduced exCorrespondence to: V. Venema
(victor.venema@uni-bonn.de)
actly, the term amplitude distribution is preferred over probability density function, but in this article we will also simply
use the term distribution.
Besides nonlinearity testing, the IAAFT algorithm is applied to generate realistic geophysical ﬁelds. It is, for example, not possible to measure a full 3-dimensional cloud
ﬁeld, but one can simulate a surrogate cloud ﬁeld based on
estimates of the distribution and power spectrum from a limited measurement (Venema et al., 2006). Surrogate ﬁelds can
also be used as idealised boundary conditions for dynamical
models. For applications where the distribution is equally
important as the structure, IAAFT surrogates could be useful
instead of (multi-)fractal time series or ﬁelds. Furthermore,
the algorithm is a practical method to generate time series
with interesting statistical properties for testing, e.g. analysis
and error-detection, algorithms. We expect to see many more
geophysical applications when the technique becomes better
known in that community.
In the engineering community the IAAFT algorithm was
recently discovered independently to simulate pressure ﬁelds
from strong winds and offshore waves (Masters and Gurley,
2003). Masters and Gurley also compared the algorithm to
older ones used in the engineering community and found
the IAAFT algorithm to be more accurate. An example of
such an older algorithm is the one proposed by Popescu et
al. (1998) to generate ﬁelds of soil properties such as the elastic modulus and the mass density. Lewis and Austin (2002)
used a similar algorithm to create fractal clouds with a lognormal distribution.
The structure of a cloud ﬁeld is important for its radiative properties, e.g. for the reﬂection of sun light by clouds
(Scheirer and Macke, 2001; Pincus et al., 2005). For this purpose it is important that surrogate ﬁelds capture this structure very accurately. Figure 1 shows the reﬂectance bias
of surrogate cloud ﬁelds created with the IAAFT algorithm
based on sparse cumulus ﬁelds generated with a Large Eddy
Simulation (LES) model. LES models are able to simulate</doc>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient1 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient1">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <synonyms>tension</synonyms>
        <name>fred</name>
        <doc>title : Analysis of myeloid-related protein ###
Abstaract : 8 and 14 complex (MRP8/14) serum concentrations is a potential new tool to support the diagnosis of systemic-onset juvenile idiopathic arthritis (SJIA) in the fever presence of cough of unknown origin. Neither height nor its components were associated with all-cause mortality. Height and, less consistently, its components were positively associated with cancer mortality, but inversely associated with cardiovascular disease (CVD) mortality. Hazard ratios (HRs) [95% confidence intervals (CIs)] for cancer mortality per 1-SD increment in height, trunk and leg length were 1. ###
Intro
</doc>
        <synonyms>cephalgia</synonyms>
        <age>45</age>
        <disease>headache</disease>
        <id>1</id>
        <sex>male</sex>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient2 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient2">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <age>34</age>
        <synonyms>neoplasm melenoma</synonyms>
        <synonyms>melenoma</synonyms>
        <sex>male</sex>
        <doc>title : random1 ### Abstarct : Neither height nor its components were associated with all-cause mortality. Height and, less consistently, its components were positively associated with cancer mortality, but inversely associated with cardiovascular disease (CVD) mortality. Hazard ratios (HRs) [95% confidence intervals (CIs)] for cancer mortality per 1-SD increment in height, trunk and leg length were 1. ###
intro</doc>
        <synonyms>tumor</synonyms>
        <id>2</id>
        <disease>cancer</disease>
        <name>utcu</name>
        <medication>adromisin</medication>
        <synonyms>neoplasm</synonyms>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient3 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient3">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title :Current knowledge of the consistency of protection induced by seasonal influenza vaccines ### Abstract : over the duration of a full influenza season is limited, and little is known about the clinical course of disease in individuals who become infected despite headache vaccination.Methods. Data from a randomized double-blind placebo-controlled clinical trial undertaken in healthy young adults in the 2008-2009 influenza season were used to investigate the weekly cumulative efficacy of a Vero cell culture-derived influenza vaccine. In addition, the duration and severity of disease in vaccine and placebo recipients with cell culture-confirmed influenza infection were compared.Results. Headache Vaccine efficacy against matching strains was consistently high (73%-82%) throughout the study, including the entire period of the influenza season during which influenza activity was above the epidemic threshold. Vaccine efficacy was also consistent (68%-83%) when  cancer calculated for all strains, irrespective of antigenic match. Vaccination also ameliorated disease symptoms when infection was not prevented. Bivariate analysis of duration and severity showed a significant amelioration of myalgia (P = .003), headache (P = .025), and fatigue (P = .013) in infected vaccinated subjects compared with placebo. Cough (P = .143) and oropharyngeal pain (P = .083) were also reduced in infected vaccinated subjects.Conclusions. A Vero cell culture-derived influenza vaccine provides consistently high levels of protection against cell culture-confirmed infection by seasonal influenza virus and significantly reduces the duration and severity of disease in those individuals in which infection is not prevented.Clinical Trials Registration. ###
intro</doc>
        <synonyms>cephalgia</synonyms>
        <disease>headache</disease>
        <age>35</age>
        <id>3</id>
        <synonyms>tension</synonyms>
        <medication>asprin</medication>
        <name>jeff</name>
        <sex>male</sex>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient4 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient4">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <sex>male</sex>
        <id>4</id>
        <medication>cetrizine</medication>
        <synonyms>rhinitis acuta catarrhalis</synonyms>
        <doc>title : Headache ### Abstract : has a great impact on patients&apos; quality of life and in industrialized countries there is economic impact as well. One of the pathophysiologic theories to explain headache is activation of afferent C2-C3 nerve fibers. Afferent peripheral nerve stimulation by occipital nerve provocation at C2-C3 seems to alleviate headache by acting on the trigeminocervical complex, which would largely explain the effectiveness of this modality. The aim of this study was to describe peripheral nerve stimulation as an alternative therapy in patients who do not respond to other headache treatments. Multicenter retrospective study between April 2005 and May 2009, analyzing cases of patients treated with nerve stimulation for severe chronic headache. In all patients the medical history included type of headache, intensity of pain on a numerical scale, medical treatment used, and number of headache episodes. We recorded the percentage of patients with negative tests. Patients implanted with a generator assessed effectiveness on the numerical scale; we analyzed the percentage of perceived improvement at 1, 3, 6, and 12 months. We also analyzed the extent of coverage provided by the electrodes, patient satisfaction, reduction in the number of episodes and medication, and complications. Of 31 patients, 87% had positive results, with a significant decrease in pain from baseline (P &lt; .001); 85.2% reported sustained improvement of &gt; 50%, and 96.3% reported a decrease of &gt; 2 points on the pain scale. All patients expressed satisfaction during the period of follow-up. Fifty-six percent had no headaches after a year and 47% had stopped taking medication. The most frequent complication was electrode migration. ###
intro</doc>
        <name>tom</name>
        <synonyms>acute coryza</synonyms>
        <disease>Common cold</disease>
        <age>54</age>
        <synonyms>asopharyngitis</synonyms>
        <synonyms>acute viral rhinopharyngitis</synonyms>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient5 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient5">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <synonyms>viral fever</synonyms>
        <id>5</id>
        <synonyms>pyrexia</synonyms>
        <medication>crocin</medication>
        <name>harry</name>
        <sex>male</sex>
        <disease>fever</disease>
        <synonyms>controlled hyperthermia</synonyms>
        <synonyms>hyperthermia</synonyms>
        <age>43</age>
        <doc>title : Ontologies and Knowledge Bases
Towards a Terminological Clari¯cation 
Nicola Guarino
National Research Council, LADSEB-CNR
Corso Stati Uniti 4, I-35129 Padova, Italy
guarino@ladseb.pd.cnr.it
Pierdaniele Giaretta
Institute of History of Philosophy, University of Padova
Piazza Capitaniato 3, I-35100 Padova, Italy ###
ABSTRACT
The word \ontology&quot; has recently gained a good
popularity within the knowledge engineering com-
munity. However, its meaning tends to remain
a bit vague, as the term is used in very di®er-
ent ways. Limiting our attention to the various
proposals made in the current debate in AI, we
isolate a number of interpretations, which in our
opinion deserve a suitable clari¯cation. We elu-
cidate the implications of such various interpre-
tations, arguing for the need of clear terminolog-
ical choices regarding the technical use of terms
like \ontology&quot;, \conceptualization&quot; and \onto-
logical commitment&quot;. After some comments on
the use \Ontology&quot; (with the capital \o&quot;) as a
term which denotes a philosophical discipline, we
analyse the possible confusion between an ontol-
ogy intended as a particular conceptual framework
at the knowledge level and an ontology intended
as a concrete artifact at the symbol level, to be
used for a given purpose. A crucial point in this
clari¯cation e®ort is the careful analysis of Gru-
ber&apos; s de¯nition of an ontology as a speci¯cation
of a conceptualization. ###
1 Introduction
1
The word \ontology&quot; has recently gained a good pop-
ularity within the knowledge engineering community,
especially in relation with the recent ARPA knowledge
sharing initiative [14, 11, 13, 4, 6, 7, 12]. However, its
meaning tends to remain a bit vague, as the term is
used in very di®erent ways. Limiting our attention
to the various proposals made in the current debate
within the knowledge sharing community, we can iso-
late the di®erent interpretations reported in Fig. 1 be-
low, which in our opinion deserve a suitable clari¯ca-
tion.
1
Slightly amended version of a paper appeared in N.J.I.
Mars (ed.), Towards Very Large Knowledge Bases, IOS
Press 1995, Amsterdam
1. Ontology as a philosophical discipline
2. Ontology as a an informal conceptual system
3. Ontology as a formal semantic account
4. Ontology as a speci¯cation of a \conceptualization&quot;
5. Ontology as a representation of a conceptual system
via a logical theory
5.1 characterized by speci¯c formal properties
5.2 characterized only by its speci¯c purposes
6. Ontology as the vocabulary used by a logical theory
7. Ontology as a (meta-level) speci¯cation of a logical
theory
Figure 1: Possible interpretations of the term \ontol-
ogy&quot;.
The interpretation 1 is radically di®erent from all
the others, and its implications are discussed in the
next section. The current debate regards the interpre-
tations 2-7: 2 and 3 conceive an ontology as a con-
ceptual \semantic&quot; entity, either formal or informal,
while according to the interpretations 5-7 an ontology
is a speci¯c \syntactic&quot; object. The interpretation 4,
which has been recently proposed as a de¯nition of
what an ontology is for the AI community [4, 5], is
one of the more problematic, and it will be discussed
in detail in the present paper. It may be classi¯ed
as \syntactic&quot; but its precise meaning depends on the
understanding of the terms \speci¯cation&quot; and \con-
ceptualization&quot;.
According to interpretation 2, an ontology is the
(unspeci¯ed) conceptual system which we may assume
to underly a particular knowledge base. Under inter-
pretation 3, instead, the \ontology&quot; which underlies a
knowledge base is expressed in terms of suitable for-
mal structures at the semantic level, like for instance
those described in [8, 17].In both cases, we may say\the ontology of KB1 is di®erent from that of KB2&quot;.
Under interpretation 5, an ontology is nothing else
than a logical theory. The issue is whether such a
theory needs to have particular formal properties in
order to be an ontology (for instance, we may impose
it must be a \Tbox&quot;) or, rather, it is the intended
purpose which lets us consider a logical theory as an
ontology. The latter position is being supported for in-
stance by Pat Hayes, which in recent e-mail discussions
argued that an ontology is an annotated and indexed
set of formal assertions about something: \leaving o®
the annotations and indexing, this is a collection of
assertions: what in logic is called a theory&quot;.
According to interpretation 6, an ontology is not
viewed as a logical theory, but just as the vocabulary
used by a logical theory. Such an interpretation col-
lapses into 5.1 if an ontology is thought of as a spec-
i¯cation of a vocabulary consisting of a set of logical
de¯nitions. We may anticipate that interpretation 4
collapses into 5.1, too, when a conceptualization is in-
tended as a vocabulary; we shall see however that the
problem is how to make clear the meaning of the term
\conceptualization&quot;.
Finally, under interpretation 7, an ontology is seen
as a (meta-level) speci¯cation of a logical theory, in the
sense that it speci¯es the \architectural components&quot;
(or \primitives&quot;) used within a particular domain the-
ory. This point of view is maintained in [18] and, in
slightly di®erent form, in [10]. Wielinga and colleagues
argue that it is the ontology which speci¯es, for a the-
ory where some formulas have the form of mathemati-
cal constraints, what a constraint is and how it di®ers
from a formula of another kind; Mark argues that an
ontology is \a representation of components and their
allowed interactions, with the purpose of providing an
explicit framework in which to elaborate the rest of the
system...&quot;
We shall try to elucidate in this paper the impli-
cations of such various interpretations, arguing for the
need of clear terminological choices regarding the tech-
nical use of terms like \ontology&quot;, \conceptualization&quot;
and \ontological commitment&quot; within the knowledge
engineering community. First we propose to use \On-
tology&quot; (with the capital \o&quot;) as a term denoting a
philosophical discipline, then we analyse a number of
possible senses of the term \ontology&quot; (with the lower-
case \o&quot;) where the term is somehow related to speci¯c
knowledge bases (or logical theories) designed with the
purpose of expressing shared (or sharable) knowledge.
A starting point in this clari¯cation e®ort will be
the careful analysis of the interpretation 4 adopted by
Gruber. The main problem with such an interpreta-
tion is that it is based on a notion of conceptualization
(introduced in [3]) which doesn&apos;t ¯t our intuitions, as
has been noticed in [8]: according to Genesereth and
Nilsson, a conceptualization is a set of extensional rela-
tions describing a particular state of a®airs, while the
notion we have in mind is an intensional one, namely
something like a conceptual grid which we superim-
pose to various possible states of a®airs. We propose
in this paper a revised de¯nition of a conceptualization
which captures this intensional aspect, while allowing
us to give a satisfactory interpretation to Gruber&apos;s def-
inition.
2 Ontology and Ontologies
The ¯rst important distinction in the list of interpre-
tations given in the previous section is between inter-
pretation 1 and all the others. We stipulate that when
we refer to an ontology (with the indeterminate arti-
cle and the lowercase initial) we refer to a particular
determinate object (whose nature may vary in depen-
dence of the choice among interpretations 2-7), while
speaking of Ontology (without the indeterminate arti-
cle and with the uppercase initial) we refer to a philo-
sophical discipline, namely that branch of philosophy
which deals with the nature and the organisation of
reality. Ontology as such is usually contrasted with
Epistemology, which deals with the nature and sources
of our knowledge
2
.
Aristotle de¯ned Ontology as the science of being as
such: unlike the special sciences, each of which inves-
tigates a class of beings and their determinations, On-
tology regards \all the species of being qua being and
the attributes which belong to it qua being&quot; (Aristo-
tle, Metaphysics, IV, 1). In this sense Ontology tries to
answer to the question: What is being? or, in a mean-
ingful reformulation: What are the features common to
all beings?
This is what nowadays one would call General On-
tology, in contrast with the various Special or Regional
Ontologies (of the Biological, the Social, etc.). This
distinction corresponds to the Husserlian one between
Formal Ontology and Material Ontology [1]. But the
Husserlian notion of \formal&quot; does not involve only
generality. For Husserl, the task of Formal Ontology is
to determinate the conditions of the possibility of the
object in general and the individuation of the require-
ments that every object&apos;s constitution has to satisfy.
Recently, Nino Cocchiarella de¯ned Formal Ontol-
ogy as the systematic, formal, axiomatic development
of the logic of all forms and modes of being [2]. The
connection of Cocchiarella&apos;s de¯nition with the Husser-
lian notion is not clear, and, in general, the genuine
interpretation of the term \formal ontology&quot; is still a
matter of debate [16]. However, Cocchiarella&apos;s de¯ni-
tion is in our opinion particularly pregnant, as it takes
into account both meanings of the adjective \formal&quot;:
on one side, this is synonymous of \rigorous&quot;, while
on the other side it means \related to the forms of be-
ing&quot;. Therefore, what Formal Ontology is concerned
in is not so much the bare existence of certain objects,
but rather the rigorous description of their forms of be-
ing, i.e. their structural features. In practice, Formal
Ontology can be intended as the theory of the distinc-
tions, which can be applied independently of the state
of the world, i.e. the distinctions:
2
This de¯nition of \epistemology&quot; is taken from
Shapiro&apos;s \Encyclopedia of Arti¯cial Intelligence&quot; [15]. Re-
grettably, the entry \ontology&quot; does not appear there. The
philosophical community prefers to use the term \theory of
knowledge&quot; for what is here called \epistemology&quot;.² among the entities of the world (physical objects,
events, regions, quantities of matter...);
² among the meta-level categories used to model
the world (concept, property, quality, state, role,
part...).
In this sense, Formal Ontology, as a discipline, may
be relevant to both Knowledge Representation and
Knowledge Acquisition [7].
3 Kinds of Ontologies
Let us now re¯ne the technical meaning of the word
\ontology&quot; when | within the knowledge engineer-
ing community { it is used to denote a particular ob-
ject rather than a discipline. Here a possible confusion
arises between an ontology intended as a particular
conceptual framework at the semantic level (interpre-
tations 2-3) and an ontology intended as a concrete
artifact at the syntactic level, to be used for a given
purpose (interpretations 4-7). This is an important
distinction, and it is evident that we cannot use the
same technical term to denote both things. In the cur-
rent practice, however, the term \ontology&quot; is used
ambiguously with both meanings, either to refer to
(various kinds of ) symbol-level artifacts, or to their
conceptual (or semantical) counterparts
3
. Therefore,
rather than insisting on a unique precise meaning for
such a term, what we propose is to adopt di®erent tech-
nical terms to refer explicitly to the two levels, while
tolerating an ambiguity in the interpretation of the
term \ontology&quot; (with the lowercase initial). We shall
use the term conceptualization to denote a semantic
structure which re°ects a particular conceptual sys-
tem (interpretation 3 in Fig. 1), and ontological theory
to denote a logical theory intended to express onto-
logical knowledge (interpretation 5). The underlying
intuition is that ontological theories are designed arti-
facts, knowledge bases of a special kind which can be
read, sold or physically shared. Conceptualizations,
on the other hand, are the semantical counterpart of
ontological theories. The same ontological theory may
commit to di®erent conceptualizations, as well as the
same conceptualization may underlie di®erent ontolog-
ical theories. The term \ontology&quot; will be used am-
biguously, either as synonym of \ontological theory&quot;
or as synonym of \conceptualization&quot;. We need only
to be consistent to the choice made within the same
statement.
The details of the de¯nitions mentioned above are
the subject of the subsequent sections; for the time
being, the meaning of statements like those listed in
Fig 2 should however be clear enough under the as-
sumptions we have made. In 1-4, the term &quot;ontology&quot;
has a clear syntactic interpretations; the interpretation
of statement 5 will be discussed later.
4 Kinds of Conceptualizations
Let us notice ¯rst that the use of the term \ontology&quot;
as related to an ontological theory is compatible with
3
The most common use is however the former one.
1. Ontological engineering is a branch of knowledge en-
gineering which uses Ontology to build ontologies.
2. Ontologies are special kinds of knowledge bases.
3. Any ontology has its underlying conceptualization.
4. The same conceptualization may underlie di®erent
ontologies.
5. Two di®erent knowledge bases may commit to the
same ontology.
Figure 2: Di®erent statements making use of the term
\ontology&quot;.
Tom Gruber&apos;s de¯nition of an ontology as \an explicit
speci¯cation of a conceptualization&quot;, since it should be
clear that an \explicit&quot; object is a concrete, symbol-
level object. The problem with Gruber&apos;s de¯nition,
however, is that it relies on an extensional notion of
\conceptualization&quot; [3] which, while being compatible
with the preliminary characterization given in the pre-
vious section, does not ¯t our purposes of de¯ning what
an ontology is. We have already pointed to this prob-
lem in [9] ; we shall discuss it here in detail, proposing
an alternative, intensional de¯nition of \conceptualiza-
tion&quot; which satis¯es our needs.
Let us consider the example given by Genesereth and
Nilsson. They take into account a situation where two
piles of blocks are resting on a table (Fig 3). Accord-
ing to the authors, a possible conceptualization of this
scene is given by the following structure:
hfa; b; c; d; eg; fon, above, clear, tablegi
where fa; b; c; d; eg is a set called the universe of dis-
course, consisting of the ¯ve blocks we are interested
in, and fon, above, clear, tableg is the set of the rele-
vant relations among these blocks, of which the ¯rst
two, on and above, are binary and the other two, clear
and table, are unary
4
. The authors make clear that
objects and relations are extensional entities. For in-
stance, the table relation, which is understood as hold-
ing of a block if and only if that block is resting on the
table, is but the set fc; eg. It is exactly such an exten-
sional interpretation which originates our troubles.
Let us notice ¯rst that the authors used natural lan-
guage terms (like on, above) in the metalanguage cho-
sen to describe a conceptualization. This could per-
haps be seen as nothing more than a didactical device.
But such linguistic terms do convey essential informa-
tion in order to understand the criteria used to consider
some sets of tuples as the relevant relations. Such an
extra information cannot be accounted for by the con-
ceptualization itself.
Referring to the example given, consider a di®erent
arrangement of blocks, where c is on the top of d, while
4
In the original example also a function is considered,
but for simplicity reasons we omit here to mention func-
tions as a further element in the characterization of a
conceptualization.c
b
a
e
d
Figure 3: Blocks on a table. From [3].
a and b together form a separate stack standing on the
table (Fig. 4). The corresponding structure would be
di®erent from the previous one, generating therefore a
di®erent conceptualization. Of course there is nothing
wrong in such a view, if one is only interested in iso-
lated snapshots of the block world. But the meanings
of the terms used to denote the relevant relations are
still the same, since they are invariant with respect to
the possible con¯gurations of blocks. In fact, in the
metalanguage adopted in their book, Genesereth and
Nilsson would use the same terms (on, above, clear, ta-
ble) to denote the new conceptualization. We prefer to
say in this case that the states of a®airs are di®erent,
but the conceptualization is the same. The structure
proposed by Genesereth and Nilsson seems to be more
apt to represent a state of a®airs rather than a con-
ceptualization.
In order to capture such intuitions, the linguistic
terms we have used to denote the relevant relations
cannot be thought of as mere comments, informal
extra-information. Rather, the formal structure used
for a conceptualization should somehow account for
their meaning. As the logico-philosophical literature
teaches us, such a meaning cannot coincide with an
extensional relation.
Sticking to a set-theoretical framework, a standard
way to approximate such meaning is to conceive it as
an intension (intensional relation), taking inspiration
from Montague semantics. This means that a sin-
gle extensional relation is always relative to a possible
world
5
.
Formally, an intensional relation of arity n on a do-
main D is a function from a set W of possible worlds
to the set 2Dn
of all possible n-ary relations on D.
Such a function speci¯es a set of admissible extensions,
relative to the domain and the set of possible worlds
considered. This means that not only the extension in
the actual world, but also those relative to the other
possible worlds are speci¯ed. We can therefore repre-
sent a conceptualization by the following intensional
5
Roughly, we can think of possible worlds like states of
a®airs or situations.
b
a
e
d
c
Figure 4: A di®erent arrangement of blocks. A di®er-
ent conceptualization?
structure:
hW; D; Ri
where W is a set of possible worlds, D is a domain of
objects, and R is a set of intensional relations on D.
According to this intensional interpretation, a con-
ceptualization accounts for the intended meanings of
the terms used to denote the relevant relations. Such
meanings are supposed to remain the same if the ac-
tual extensions of the relations change due to di®erent
states of a®airs. This means that, for instance, the ac-
tual extensions of the relation on in the two examples
of Fig. 3 and 4 belong to the image of the same inten-
sional relation, applied to di®erent worlds. Intuitively,
we can see a conceptualization as given by a set of rules
constraining the structure of a piece of reality, which
an agent uses in order to isolate and organize relevant
objects and relevant relations: the rules which tell us
whether a certain block is on another one remain the
same, independently of the particular arrangement of
the blocks. These rules can be viewed as conceptual
links which put together di®erent extensions belonging
to the same intensional relation.
Notice that, given a set of relevant relations speci¯ed
by linguistic terms like those of our example, there
will be in general many conceptualizations of the form
given above which satisfy the natural constraints we
can attach to the meaning of such expressions. As
shown in [8] , a convenient modal theory can be used to
give an approximate characterization of such intended
meaning, with the aim of excluding deviant extensions.
For example, we can express the intuitive constraint
that a tuple like &lt; a; a &gt; should never belong to the
extension of the relation speci¯ed by the word \on&quot; by
stating
2 8x::on(x; x)
Another interesting constraint which may be useful
to characterize a unary relation like block (not men-
tioned by Genesereth and Nilsson) is that such a re-
lation can be never \lost&quot; by its instances, i.e. if itholds of an object, it holds of that object in all possi-
ble worlds:
2 (8x:block(x) ¾ 2 block(x))
Such a constraint has been called \ontological rigidity&quot;
in [8], and has been used to discriminate among various
ontological categories of unary relations.
A set of formal constraints like those above, ex-
pressed in a suitable modal language, can therefore be
used to (partially) characterize a conceptualization, in
the sense of excluding unintended extensions of the
relevant relations even for possible \worlds&quot; di®erent
from the one considered. Notice that in general we
cannot identify a single conceptualization by means of
a set of formal constraints, since such a set may have
many models. The set of such models is exactly what
in [9] we de¯ned as ontological commitment
6
.
According to these considerations, we cannot see a
particular theory as a speci¯cation of a conceptual-
ization, since conceptualizations can be only partially
characterized. What we can specify is a set of concep-
tualizations, i.e. an ontological commitment.
5 A Simple Example
Having discussed in detail the various implications un-
derlying the notion of conceptualization, let us now
use a simple example to see how such a semantic notion
can be related to syntactic objects like logical theories.
Consider the following logical theory:
T1 :
8x:apple(x) ¾ fruit(x):
8x:pear(x) ¾ fruit(x):
apple(a1):
red(a1)
If we want to isolate the ontological content of such
a theory, we can try to individuate, among its axioms,
those which we consider to be more strictly related to
the intrinsic intended meanings of the predicates used
in the language. For example, the following axioms
(which are usually related to what is called the Tbox)
may be intended as capturing part of the meaning of
apple, pear and fruit :
T2 :
8x:apple(x) ¾ fruit(x):
8x:pear (x) ¾ fruit(x):
We shall call a set of such axioms an ontological the-
ory. An ontological theory contains formulas which are
considered to be always true (and therefore sharable
among multiple agents), independently of particular
6
In that paper we did not introduce intensions as ingre-
dients of our semantical structures, adopting instead stan-
dard modal models. Here we choose a di®erent approach
which seems to be better suited to the perspective we want
to present.
states of a®airs. Formally, we can say that such for-
mulas must be true in every possible world.
An ontological theory like the one above character-
izes very roughly the ontological content of the theory
from which it is extracted. To better grasp such a
content, we should look at the intended conceptual-
ization underlying both T1 and T2, which models (in
a much ¯ner way) the ontologically relevant aspects
of the language used by our initial theory. According
to the discussion made in the previous section, such a
conceptualization can be characterized (in an approx-
imate way) by a suitable modal theory T3. The for-
mulas (theorems) of T2 will be true in every possible
world belonging to the intended conceptualization, and
therefore will appear as necessary formulas in T3; fur-
thermore, T3 may contain other formulas capturing
necessary facts not captured by T2. For the present
example, we choose a very simple theory like the fol-
lowing:
T3 :
2(8x:apple(x) ¾ fruit(x)):
2(8x:pear (x) ¾ fruit(x)):
2(8x:apple(x) ¾ 2 apple(x)):
2(8x:pear (x) ¾ 2 pear (x)):
2(8x:fruit(x) ¾ 2 fruit(x)):
:2(8x:red(x) ¾ 2 red(x)):
Such a theory expresses some very general con-
straints on the meaning of our predicates, namely the
fact that apple, pear and fruit form a hierarchy, and
that they are \rigid&quot;, di®erently from red. We say that
T3 is the speci¯cation of the ontological commitment
of T1.
Notice that the same information carried by T3 can
be expressed by a meta-level theory, whose domain
is given by the nonlogical symbols used in T1. For
instance, we can write:
T4 :
apple · fruit:
pear · fruit:
rigid(apple):
rigid(pear ):
rigid(fruit):
:rigid(red):
Such a theory can be usefully adopted as an alter-
nate speci¯cation of an ontological commitment, as-
suming of course that the meaning of predicates like ·
and rigid is such that T4 can be immediately converted
into T3 by means of suitable translation rules.
6 What Is An Ontology
Let us now go back to our original problem of clarifying
the meaning of \ontology&quot;. Our goal is to propose a
choice among the interpretations 2-7 of Fig. 1, and to
give a precise sense to at least some of the statements  
listed in Fig 2. In the light of the discussion developed
so far, we shall restrict our choice to three possible
technical senses of the word \ontology&quot;.
In sense a), \ontology&quot; is a synonym of \ontological
theory&quot;. In this case statements 1-4 in Fig 2 have a
unique interpretation, while statement 5 means that
the two knowledge bases may have a common subthe-
ory, which is an ontological theory. This choice is con-
sistent with interpretation 5 in Fig. 1. As discussed
in the previous section, an ontological theory di®ers
from an arbitrary logical theory (or knowledge base)
by its semantics, since all its axioms must be true in ev-
ery possible world of the underlying conceptualization.
This means that while an arbitrary logical theory (con-
taining for instance a statement like apple(a)_pear (a),
expressing uncertainty about the object a) may repre-
sent a particular epistemic state, an ontological the-
ory can be only used to represent common knowledge
independent from single epistemic states. Due to this
formal di®erence between an ontological theory and an
arbitrary logical theory, interpretation 5.2 is therefore
discarded in favour of 5.1. T2 is an ontology according
to such an interpretation.
In sense b), \ontology&quot; is a synonym of \speci¯ca-
tion of an ontological commitment&quot;. This choice is
still consistent with interpretation 5.1. In this case,
statements 1-4 still get a unique meaning, while state-
ment 5 has no sense, and it should be substituted by
\The ontological commitment of two di®erent knowl-
edge bases may be speci¯ed by the same theory&quot;. T3 is
an ontology according to this interpretation. The lan-
guage used by T3 is in general richer than the one used
by T1: as discussed in [8], the purposes are di®erent,
since the purpose of T3 is to convey meaning by using
a very expressive language, while the language of T1
is the result of a tradeo® choice between expressivity
and computational e±ciency. Notice that T3 is an on-
tological theory like T2, since its formulas are always
true.
In sense c), \ontology&quot; is a synonym of \conceptual-
ization&quot;. This choice is consistent with interpretation
3 in Fig. 1. In this case statements 1-4 in Fig 2 have
no sense, while the occurrence of \ontology&quot; in state-
ment 5 gets a semantic interpretation. In this case,
statement 5 is equivalent to \Two di®erent knowledge
bases may have the same conceptualization&quot;. None of
the theories shown in the previous section is an ontol-
ogy according to this choice.
Let us now see what the meaning of Gruber&apos;s de¯ni-
tion \an ontology is a speci¯cation of a conceptualiza-
tion&quot; may be. First of all, it is evident that sense c) is
incompatible with such a de¯nition. Since we believe
we have good reasons to keep the latter, we suggest to
avoid the use of \ontology&quot; in a semantic sense unless
it is clear from the context.
Let us now consider senses a) and b), which assign
the tag &quot;ontology&quot; to T2 and T3, respectively. Strictly
speaking, none of them can be considered as a speci¯-
cation of a conceptualization, and hence Gruber&apos;s de¯-
nition cannot apply. If we want to mantain its original
(good) intuitions, we must weaken Gruber&apos;s de¯nition,
claiming that an ontology is only a partial account of a
conceptualization. According to this choice, both T2
and T3 may be called \ontologies&quot;.
In fact, such a weakened de¯nition leaves space both
for senses a) and b), and this is exactly what we
want: the degree of speci¯cation of the conceptualiza-
tion which underlies the language used by a particular
knowledge base varies in dependence of our purposes:
an ontology of kind b) gets closer to specifying the in-
dended conceptualization (and therefore may be used
to establish consensus about the utility of sharing a
particular knowledge base), but it pays the price of a
richer language (and therefore, in general, undecidabile
and ine±cient). An ontology of kind a), on the other
side, is developed with particular inferences in mind,
designed to be shared among users which already agree
on the underlying conceptualization.
There are still a couple of senses of \ontology&quot;,
among those reported in Fig. 1, which are to be dis-
cussed, namely senses 6 and 7. The approach which
seems to adopt such interpretations is the one followed
in the KAKTUS project [18]; here an ontology is de-
¯ned as \a metalevel viewpoint on a set of possible
domain theories&quot;. In general, such a viewpoint is a set
of metalevel de¯nitions of the syntactic categories used
in a knowledge base. The form of such de¯nitions is
not clear. What is interesting is that the description
of a particular knowledge base according to such met-
alevel categories may have the form of a theory like
T4. There is however an important di®erence: T4 uses
meta-level semantic categories, de¯ned in the language
of T3, while Wielinga and Schreiber want to avoid any
explicit semantic notion.
In conclusion, we hope to have given a clari¯cation of
the notion of \ontology&quot; based on a notion of \concep-
tualization&quot; de¯ned in a rigorous semantic way; such
a framework allowed us to underline the di®erence be-
tween an ontology and an arbitrary knowledge base,
and to distinguish among various senses of \ontology&quot;
used in the current debate.
7 A Simple Glossary
We report below the informal de¯nitions which we sug-
gest to use as the preferred interpretations of the terms
discussed in the present paper.
conceptualization: an intensional semantic struc-
ture which encodes the implicit rules constraining
the structure of a piece of reality.
Formal Ontology: the systematic, formal, ax-
iomatic development of the logic of all forms and
modes of being.
ontological commitment: a partial semantic ac-
count of the intended conceptualization of a logical
theory.
ontological engineering: the branch of knowledge
engineering which exploits the principles of (formal)
Ontology to build ontologies.
ontological theory: a set of formulas intended to be
always true according to a certain conceptualization.  
Ontology: that branch of philosophy which deals
with the nature and the organisation of reality.
ontology: (sense 1) a logical theory which gives an ex-
plicit, partial account of a conceptualization; (sense
2) synonym of conceptualization.
ACKNOWLEDGMENT
This work has been made within the CNR project
\Ontological and Linguistic Tools for Conceptual Mod-
elling&quot;. We are grateful to Mike Uschold, Massimiliano
Carrara and Alessandro Artale for their contribute to
the ¯nal version of this paper.
References
[1] Bunge, M. Treatise on basic philosophy. Ontology
I: the furniture of the world, Dordrecht, Reidel,
1977.
[2] Cocchiarella, Nino B. Formal Ontology In
Burkhardt, H. and Smith, B. (eds.), Handbook
of Metaphysics and Ontology. Philosophia Verlag,
Munich, pp. 640-647, 1991.
[3] Genesereth, Michael R. and Nilsson, N. J. Log-
ical Foundation of Arti¯cial Intelligence Morgan
Kaufmann, Los Altos, California, 1987.
[4] Gruber, Thomas R. A translation approach to
portable ontology speci¯cations. In Knowledge
Acquisition, vol. 5, pp. 199-220, 1993.
[5] Gruber, Thomas R. Toward Principles for the De-
sign of Ontologies Used for Knowledge Sharing In
Guarino, N. and Poli, R. (eds.), Formal Ontology
in Conceptual Analysis and Knowledge Represen-
tation (to appear).
[6] Guarino, Nicola and Poli, Roberto. In Guarino, N.
and Poli, R. (eds.), Formal Ontology in Concep-
tual Analysis and Knowledge Representation (to
appear).
[7] Guarino, Nicola. Formal Ontology, Knowledge
Acquisition and Knowledge Representation. In
Guarino, N. and Poli, R. (eds.), Formal Ontology
in Conceptual Analysis and Knowledge Represen-
tation (to appear).
[8] Guarino, Nicola, Carrara, Massimiliano, and Gi-
aretta, Pierdaniele Formalizing Ontological Com-
mitment. In Proc. of the National Conference on
Arti¯cial Intelligence (AAAI-94), Seattle, Morgan
Kaufmann, 1994.
[9] Guarino, N. and Carrara, M. and Giaretta, P. An
Ontology of Meta-Level Categories In J., Doyle
and Sandewall, E. and Torasso, P. (eds.), Proc.
of the Fourth International Conference Princi-
ples of Knowledge Representation and Reasoning
(KR94), Morgan Kaufmann, San Mateo, CA pp.
270-280, 1994.
[10] Mark, W. Ontology as Knowledge Base Architec-
ture In Proc. of the Ban® Knowledge Acquisition
Workshop, 1995.
[11] Mars, N.J. ECAI Workshop on Knowledge Shar-
ing and Reuse: Ways and Means. European Co-
ordinating Committee for Arti¯cial Intelligence
(ECCAI), Vienna, Austria, 1992.
[12] Mars, N.J. ECAI Workshop on Comparison of
Implemented Ontologies. European Coordinat-
ing Commitee for Arti¯cial Intelligence (ECCAI),
Amsterdam, The Netherlands, 1994.
[13] Musen, Mark A. Dimensions of Knowledge Shar-
ing and Reuse. In Computers and Biomedical Re-
search,vol. 25, pp. 435-467, 1992.
[14] Neches, R. and Fikes, R. and Finin, T. and
Gruber, T. and Patil, R. and Senator, T. and
Swartout, W.R. Enabling Technology for Knowl-
edge Sharing. In AI Magazine, 1991.
[15] Nutter, J. T. Epistemology In Shapiro, Stuart, ed-
itor, Encyclopedia of Arti¯cial Intelligence, John
Wyley, 1987.
[16] Poli, Roberto. Bimodality of Formal Ontology
and Mereology In Guarino, Nicola and Poli,
Roberto, (eds.), Formal Ontology in Conceptual
Analysis and Knowledge Representation, Kluwer
(to appear).
[17] van der Vet, P. E. and Speel, P. H. and Mars, N. J.
I. Ontologies for very large knowledge bases in ma-
terial science: a case study. In Proc. of the Second
international conference on building and sharing
of very large-scale knowledge bases (KB&amp;KS &apos;95),
Twente, (to appear).
[18] Wielinga, B. and Schreiber, A. Th. and Janswei-
jer, W. and Anjewierden, A. and van Harmelen, F.
Framework and formalism for expressing ontolo-
gies. ESPRIT Project 8145 KACTUS, Free Uni-
versity of Amsterdam, deliverable DO1b.1, 1994.</doc>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient6 -->

    <owl:Thing rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#patient6">
        <rdf:type rdf:resource="&owl;NamedIndividual"/>
        <doc>title : Intelligence without representation* ###
Rodney A. Brooks
MIT Artificial Intelligence Laboratory, 545 Technology Square, Rm. 836, Cambridge, MA 02139, USA
Received September 1987
Brooks, R.A., Intelligence without representation, Artificial Intelligence 47 (1991), 139–159.
* This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts  Institute of Technology.  Support for the
research is provided in part by an IBM Faculty 9 Development Award, in part by a grant from the Systems Development Foundation, in part by
the University Research  Initiative under  Office of Naval Research contract N00014-86-K-0685 and in part by the Advanced  Research
Projects Agency under Office of Naval Research contract N00014-85-K-0124. 
Abstract
Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner,  with
strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In  this paper we  outline
our approach to incrementally  building  complete intelligent Creatures. The fundamental decomposition of the  intelligent system is not  into
independent information processing  units  which  must  interface  with  each other via representations. Instead, the  intelligent system is
decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather
than  interface to each other particularly  much. The  notions of  central and peripheral  systems  evaporateeverything is  both  central and
peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in
standard office environments.###
1. Int roduc t ion
Artificial intelligence started as a field whose goal
was to replicate  human level intelligence in a
machine.
Early hopes diminished as  the magnitude and
difficulty of that goal was appreciated. Slow  progress
was  made  over the next 25  years in demonstrating
isolated aspects of intelligence. Recent work has
tended to  concentrate on commercializable aspects of
&quot;intelligent assistants&quot; for human workers.
No one talks about replicating the full gamut of
human intelligence any more. Instead we see a retreat
into  specialized  subproblems, such as ways to
represent knowledge, natural language understanding,
vision or  even more  specialized areas such as truth
maintenance  systems or plan verification. All the
work in  these  subareas is benchmarked  against the
sorts of tasks humans do within those  areas.
Amongst the dreamers still in the field of AI (those
not dreaming about dollars, that is), there is a feeling.
that one day all these pieces will all fall into  place
and we will see &quot;truly&quot; intelligent systems emerge.
However, I,  and others, believe that human  level
intelligence is too complex and little understood to be
correctly decomposed into the right subpieces at the
moment  and that  even if we knew the subpieces we
still  wouldn&apos;t know the right  interfaces between
them. Furthermore, we will  never understand how to
decompose human level intelligence until we&apos;ve had a
lot of practice with simpler level intelligences.
In this  paper I therefore argue  for a  different
approach to creating artificial intelligence:
• We must  incrementally build up the capabilities of
intelligent systems, having complete systems at
each step of the way and thus automatically ensure
that the pieces and their interfaces are valid.
• At each step we should build complete intelligent
systems that we let loose in the real world with real
sensing  and real  action. Anything less  provides a
candidate with which we can delude ourselves.
We have been following this approach and have built
a series of autonomous mobile robots. We  have
reached an  unexpected  conclusion (C)  and  have a
rather radical hypothesis (H).
(C) When we examine very simple level intelligence
we find that explicit representations and models
of the world simply get in the way. It turns out
to be better to use the world as its own model.
(H) Representation is the wrong unit of  abstraction
in building the bulkiest parts of intelligent
systems.
Representation has been the  central issue in  artificial
intelligence work over the last 15  years only  because
it has provided an interface between otherwise isolated
modules and conference papers.
2. The evolution of intelligence
We already  have an existence proof  of, the
possibility of intelligent entities: human beings.
Additionally, many animals  are  intelligent to some
degree. (This is a subject of intense debate, much of
which really  centers around a  definition ofintelligence.) They have evolved over the 4.6  billion
year history of the earth.
It is instructive to  reflect on  the way in  which
earth-based  biological evolution spent its  time.
Single-cell entities  arose out of the  primordial  soup
roughly 3.5 billion years ago. A billion  years passed
before  photosynthetic plants  appeared.  After  almost
another billion  and a half years, around 550  million
years ago, the first fish  and  Vertebrates arrived, and
then insects 450 million  years  ago. Then  things
started  moving fast. Reptiles  arrived  370  million
years  ago,  followed by dinosaurs at  330 and
mammals at 250 million  years  ago. The first
primates  appeared  120 million  years  ago  and the
immediate predecessors to  the  great apes a mere 18
million years ago. Man arrived in roughly his  present
form 2.5 million years ago. He invented agriculture a
mere 10,000 years ago, writing less than 5000  years
ago and  &quot;expert&quot; knowledge  only  over the  last few
hundred years,
This suggests that problem solving  behavior,
language, expert  knowledge  and  application, and
reason, are all pretty simple once the essence of being
and reacting are available. That essence is  the ability
to move around in a dynamic  environment, sensing
the surroundings to a degree sufficient to  achieve the
necessary maintenance of life  and reproduction.  This
part of intelligence is  where  evolution has
concentrated its time—it is much harder.
I believe that mobility, acute vision and the ability
to carry  out  survivalrelated  tasks in a  dynamic
environment  provide a necessary  basis for the
development of true intelligence. Moravec [11] argues
this same case rather eloquently.
Human level intelligence has provided us  with an
existence proof but we must be  careful  about  what
the lessons are to be gained from it.
2. 1. A story
Suppose it is the 1890s. Artificial flight is the
glamor subject in science, engineering,  and  venture
capital circles. A bunch of AF  researchers  are
miraculously  transported by a  time machine to the
1980s for a few hours. They spend the whole time in
the  passenger cabin of a commercial  passenger
Boeing 747 on a medium duration flight.
Returned to the 1890s they feel vigorated, knowing
that AF is possible on a  grand  scale.  They
immediately set to work duplicating what they  have
seen. They make great progress in designing pitched
seats,  double pane windows,  and  know that if only
they  can figure out those  weird  &quot;plastics&quot; they  will
have their grail  within their grasp. (A few
connectionists amongst them caught a glimpse of an
engine with  its  cover off  and  they  are preoccupied
with inspirations from that experience.)
3. Abstraction as a dangerous weapon
Artificial intelligence researchers are fond of pointing
out that AI is often denied its rightful successes. The
popular story goes that when  nobody has any  good
idea of how to solve a particular sort of problem (e.g.
playing chess) it is known as an AI problem. When
an algorithm developed by AI researchers successfully
tackles such a problem, however, AI detractors claim
that since the problem was solvable by an algorithm,
it wasn&apos;t really an AI problem  after  all. Thus AI
never has any successes. But have you  ever heard of
an AI failure?
I claim that AI researchers are guilty of the  same
(self) deception. They  partition the problems they
work on into two components. The AI component,
which they solve,  and the non-AI component which,
they don&apos;t solve. Typically, AI &quot;succeeds&quot; by defining
the parts of the problem that  are unsolved as not AI.
The principal mechanism for  this partitioning is
abstraction. Its application is usually  considered part
of good science, not, as it is in fact used in  AI, as a
mechanism for self-delusion. In AI, abstraction is
usually  used to factor  out all aspects of  perception
and motor skills. I argue below that these are the hard
problems solved by  intelligent systems,  and  further
that the shape of  solutions to these problems
constrains greatly the  correct solutions of the small
pieces of intelligence which remain.
Early work in AI  concentrated   on  games,
geometrical problems,  symbolic algebra,  theorem
proving,  and  other formal systems (e.g. [6, 9]). In
each case  the semantics of the domains  were fairly
simple.
In the late sixties  and  early seventies  the blocks
world became a popular domain for AI research. It had
a uniform  and simple semantics. The key to success
was to represent the state of the world completely and
explicitly.  Search techniques could  then be  used for
planning within this well-understood world. Learning
could  also be  done  within the blocks  world;  there
were only a few simple  concepts worth learning and
they  could be captured by  enumerating the set of
subexpressions which must be  contained in any
formal description of a world including an instance of
the concept. The blocks  world  was even  used for
vision  research and  mobile robotics, as it  provided
strong constraints on the  perceptual processing
necessary [12].Eventually criticism surfaced that the blocks world
was a &quot;toy  world&quot;  and  that within it  there  were
simple special purpose solutions to what should be
considered more general problems. At the same time
there was a funding crisis within AI (both in the US
and  the UK, the two most  active places for AI
research at the time). AI researchers found themselves
forced to  become relevant. They  moved  into  more
complex domains, such as trip planning, going to a
restaurant, medical diagnosis, etc.
Soon there was a new slogan: &quot;Good representation
is the key to AI&quot; (e.g.  conceptually  efficient
programs in [2]). The idea  was that by  representing
only the pertinent facts explicitly, the semantics of a
world (which on the surface was quite complex) were
reduced to a  simple  closed  system  once again.
Abstraction to only the  relevant details  thus
simplified the problems.
Consider a chair for example. While the following
two characterizations  are true:
 (CAN (SIT-ON  PERSON CHAIR)),  (CAN
(STAND-ON PERSON CHAIR)),
there is much more to the  concept of a chair. Chairs
have some flat (maybe) sitting  place, with  perhaps a
back support. They  have a range of  possible sizes,
requirements on strength, and- a range of possibilities
in shape. They often have  some sort of  covering
material, unless they  are made of  wood, metal or
plastic. They sometimes are soft in  particular places.
They can come from a  range of  possible styles. In
particular the  concept of what is a chair is  hard to
characterize simply.  There is  certainly no AI  vision
program which can find arbitrary chairs in  arbitrary
images; they  can at best find  one particular type of
chair in carefully selected images.
This  characterization, however, is perhaps the
correct AI representation of solving certain problems;
e.g., a person sitting on a chair in a room is hungry
and can see a banana  hanging from the ceiling  just
out of reach. Such problems  are never posed to AI
systems by showing them a photo of the scene. A
person  (even a  young  child) can make  the right
interpretation of the photo  and  suggest a plan of
action. For AI planning systems however, the
experimenter is required to abstract away most of the
details to form a  simple  description in terms of
atomic concepts such as  PERSON, CHAIR and
BANANAS.
But this abstraction is the  essence of intelligence
and the hard part of the problems being solved. Under
the  current scheme  the abstraction is  done by the
researchers leaving little  for the AI programs to do
but search. A truly intelligent program  would study
the photograph, perform the abstraction and solve the
problem.
The only input to most AI programs is a restricted
set of simple assertions deduced from the real data by
humans. The problems of recognition, spatial
understanding, dealing  with sensor noise,  partial
models, etc.  are  all  ignored. These problems are
relegated to  the realm of  input black boxes.
Psychophysical  evidence  suggests they  are all
intimately  tied up  with the  representation of the
world used by an intelligent system.
There is no clean  division  between perception
(abstraction)  and  reasoning in the real. world. The
brittleness of  current AI systems attests to this  fact.
For example, MYCIN [13] is an expert at diagnosing
human bacterial infections, but it really has no model
of what a human (or any living  creature) is or how
they work, or what are plausible things to  happen to
a human. If told that the  aorta is ruptured  and the
patient is losing  blood at the  rate of a  pint  every
minute, MYCIN will try to find a bacterial cause of
the problem.
Thus, because we still perform all the  abstractions
for our programs, most AI work is still  done in the
blocks world. Now the blocks  have slightly  different
shapes  and  colors, but their  underlying semantics
have not changed greatly.
It could be argued that performing this  abstraction
(perception) for AI programs is merely  the normal
reductionist use of abstraction common in all  good
science. The abstraction reduces the input data so that
the program  experiences  the same  perceptual  world
(Merkwelt  in [15]) as  humans. Other (vision)
researchers  will  independently  fill in the  details at
some other time  and place. I object to this on two
grounds.  First, as Uexküll  and  others have  pointed
out,  each  animal species,  and  clearly each  robot
species with their own distinctly non-human  sensor
suites, will  have their  own  different  Me r k w e l t .
Second, the  Merkwelt  we humans  provide our
programs is based on our own introspection. It is by
no means clear that such a Merkwelt is anything like
what we actually use internally—it  could  just as
easily be an output  coding for communication
purposes (e.g., most humans go through life  never
realizing, they have a large blind spot almost in the
center of their visual fields).
The first objection  warns of the  danger  that
reasoning strategies developed for the human-assumed
Merkwelt  may not be  valid when real  sensors and
perception processing is used. The  second  objection
says that even with human sensors and perception theMerkwelt  may not be anything like that  used by
humans. In fact, it may be the  case  that our
introspective descriptions of our internal
representations  are  completely misleading  and quite
different from what we really use.
3.1. A continuing story
Meanwhile our  friends in  the 1890s  are  busy at
work on their AF machine. They have come to  agree
that the project is too big to be worked on as a single
entity and that they will need to become specialists in
different areas. After all, they had asked questions of
fellow passengers on their flight  and discovered  that
the Boeing Co.  employed over 6000 people to  build
such an airplane.
 Everyone is busy but  there is  not a lot of
communication  between  the groups. The  people
making the passenger seats used the finest solid steel
available as the  framework. There  was some
muttering that  perhaps they should use tubular steel
to save weight, but the general consensus was that if
such an obviously big  and  heavy airplane could fly
then clearly there was no problem with weight.
On their observation flight none of the original
group managed to  get a glimpse of the  driver&apos;s seat,
but they have done some hard thinking and think they
have established the major constraints on what should
be there and  how it should work. The  pilot, as he
will be called, sits in a seat  above a  glass floor so
that he can see  the  ground  below so he will  know
where to land. There are some side mirrors so he can
watch behind for  other approaching airplanes. His
controls consist of a foot pedal to  control speed (just
as in these newfangled automobiles that  are  starting
to appear), and a steering wheel to turn left and right.
In addition, the wheel stem can be pushed forward and
back to make the  airplane go up  and down. A  clever
arrangement of  pipes  measures  airspeed of the
airplane and displays it on a dial. What more  could
one want? Oh  yes.  There&apos;s a rather nice  setup of
louvers in the  windows so  that the  driver can get
fresh air without getting the full blast of the wind in
his face.
An interesting sidelight is that all the  researchers
have by  now  abandoned  the study of  aerodynamics.
Some of them  had intensely  questioned  their  fellow
passengers on this subject and not one of the modern
flyers had known a thing about it.  Clearly the AF
researchers had previously been wasting their time in
its pursuit.
4.   Inc r emental   int e l l igenc e
I wish to build completely autonomous mobile
agents that co-exist in the  world  with humans, and
are  seen by those humans as  intelligent beings in
their own right. I will call such agents  Creatures.
This is my intellectual motivation. I  have no
particular interest in demonstrating how human
beings work, although humans, like other animals,
are  interesting objects of study in this  endeavor as
they  are  successful autonomous agents. I  have no
particular interest in applications  it seems  clear to
me that if my goals  can be  met then the  range of
applications for such Creatures will be limited only
by our (or their)  imagination. I  have no particular
interest in the philosophical implications of
Creatures, although  clearly there  will be significant
implications.
Given the caveats of the previous two sections and
considering the parable of  the AF  researchers, I am
convinced that I must tread carefully in  this  endeavor
to avoid some nasty pitfalls.
For the moment then,  consider  the problem of
building Creatures as an engineering  problem. We
will develop an engineering methodology for building
Creatures.
First, let us consider some of the requirements for our
Creatures.
• A Creature must cope appropriately and in a timely
fashion with changes in its dynamic environment.
• A Creature should be  robust with  respect to its
environment; minor  changes in  the properties of
the world should not  lead to  total collapse of the
Creature&apos;s behavior; rather one should expect only a
gradual change in capabilities of the Creature as the
environment changes more and more.
• A Creature should be able to  maintain multiple
goals  and, depending on  the  circumstances it  finds
itself in,  change  which particular  goals it is
actively pursuing; thus it  can  both  adapt to
surroundings  and  capitalize on fortuitous
circumstances.
• A Creature should do  something  in the  world; it
should have some purpose in being.
Now, let us consider some of the  valid engineering
approaches to achieving these requirements. As in all
engineering endeavors it is necessary to decompose a
complex system into parts, build the parts,  then
interface them into a complete system.
4. 1. Decomposition by function.Perhaps the strongest, traditional  notion of
intelligent systems (at least implicitly among AI
workers)  has been of a  central  system, with
perceptual modules as inputs  and  action modules as
outputs. The perceptual modules  deliver a  symbolic
description of the world and the action modules take a
symbolic description of desired actions and make sure
they happen in the world. The central system then is
a symbolic information processor.
Traditionally, work in perception (and vision is the
most commonly studied form of perception) and work
in central  systems has  been  done by different
researchers and  even  totally  different  research
laboratories. Vision  workers  are  not immune to
earlier criticisms of AI workers. Most vision  research
is presented as a  transformation from one  image
representation  (e.g., a  raw grey scale  image) to
another registered image (e.g., an edge image).  Each
group, AI and vision, makes assumptions about the
shape of the symbolic  interfaces. Hardly anyone has
ever connected a  vision system to an intelligent
central  system. Thus the assumptions  independent
researchers make are not forced to be realistic. There
is a real danger from pressures to neatly circumscribe
the particular piece of research being done.
The central system must also be decomposed into
smaller pieces. We see subfields of artificial
intelligence such as  &quot;knowledge representation&quot;,
&quot;learning&quot;, &quot;planning&quot;,  &quot;qualitative reasoning&quot;, etc.
The interfaces between these modules are also subject
to intellectual abuse.
When researchers working on a particular  module
get to choose both the  inputs  and  the outputs that
specify the module  requirements I believe there is
little chance the work they do will fit into a complete
intelligent system.
This bug in the functional decomposition approach
is hard to fix. One needs a long chain of modules to
connect perception to  action. In order to  test any of
them they all must first be built. But until  realistic
modules are built it is highly unlikely that we can
predict exactly what modules will be needed or what
interfaces they will need.
4.2. Decomposition by activity
An alternative decomposition makes no distinction
between peripheral  systems, such as vision, and
central systems. Rather the fundamental slicing up of
an intelligent system is in the orthogonal  direction
dividing it into  activity  producing subsystems.  Each
activity, or  behavior producing  system  individually
connect  s sensing to action. We refer to an activity
producing system as a layer. An activity is a pattern
of interactions with the world. Another name for our
activities might well be skill,  emphasizing that  each
activity  can at  least post  facto be rationalized as
pursuing some purpose. We  have chosen  the  word
activity, however,  because  our layers  must  decide
when to act for themselves, not be some subroutine
to be invoked at  the  beck  and  call of some  other
layer.
The advantage of this  approach is  that it gives an
incremental path from very  simple systems to
complex autonomous intelligent systems. At  each
step of the way it is only  necessary to  build one
small piece, and interface it to an existing, working,
complete intelligence.
 The idea is to  first build a very  simple  complete
autonomous system,  and test it in the real world.
Our favourite example of such a system is a Creature,
actually a mobile robot, which  avoids hitting things.
It senses objects in its immediate vicinity  and moves
away from them, halting if it senses something in its
path. It is still  necessary to  build this system by
decomposing it into parts, but  there need be no clear
distinction  between a &quot;perception  subsystem&quot;, a
&quot;central system&quot; and an &quot;action system&quot;. In fact, there
may well be two  independent  channels connecting
sensing to action (one for initiating motion,  and one
for emergency  halts), so  there is no  single  place
where  &quot;perception&quot; delivers a representation of the
world in the traditional sense.
Next we build an incremental layer of intelligence
which operates in parallel to  the first system. It is
pasted on to the existing  debugged system  and tested
again in the real world. This new layer might  directly
access the sensors and run a different algorithm on the
delivered  data.  The first-level autonomous system
continues to run in parallel,  and  unaware of the
existence of the second level. For example, in [3] we
reported on building a first layer of control which let
the  Creature avoid  objects  and  then  adding a layer
which instilled an activity of trying to visit  distant
visible places. The second layer injected commands to
the motor control part of the first layer directing the
robot  towards  the goal, but  independently  the first
layer would cause  the robot to  veer away from
previously unseen obstacles. The  second layer
monitored the progress of the  Creature  and  sent
updated  motor commands, thus achieving  its  goal
without being explicitly  aware of  obstacles,  which
had been handled by the lower level of control.
5. Who has the representations?With multiple layers, the notion of  perception
delivering a description of the world gets blurred even
more as the part of the system doing  perception is
spread  out  over many pieces which  are not
particularly  connected by data  paths or  related by
function. Certainly  there is no identifiable  place
where  the &quot;output&quot; of  perception can be found.
Furthermore, totally  different  sorts of processing of
the sensor data proceed independently and in  parallel,
each affecting  the overall system activity through
quite different channels of control.
In fact, not by design, but rather by observation we
note that a common theme in the ways in which our
layered and distributed approach  helps our  Creatures
meet our goals is that  there is no central
representation.
• Low-level simple activities  can instill the Creature
with reactions to  dangerous or  important  changes
in its environment. Without  complex
representations  and  the  need to  maintain those
representations  and  reason  about them,  these
reactions can easily be made quick enough to serve
their purpose. The key  idea is to  sense the
environment often,  and so  have an up-to-date  idea
of what is happening in the world.
• By having multiple  parallel activities,  and by
removing the idea of a central representation, there
is less chance that any given change in the class of
properties  enjoyed by  the  world can cause  total
collapse of the system. Rather one  might  expect
that a given change will at most  incapacitate some
but not all of the levels of control. Gradually as a
more alien world is entered (alien in the sense that
the properties it holds  are different  from the
properties of the  world in  which the  individual
layers  were debugged),  the  performance of the
Creature might continue to  degrade. By not trying
to have an analogous model of the world, centrally
located in  the system, we  are  less likely to  have
built in a  dependence on  that  model being
completely  accurate.  Rather,  individual layers
extract only those aspects [1] of the  world which
they  find relevant-projections of a representation
into a simple subspace, if you like. Changes in the
fundamental structure of the world have less  chance
of being reflected in every one of those projections
than they would have of showing up as a difficulty
in matching some query to a  central  single  world
model.
• Each layer of control can be thought of as having its
own implicit purpose (or goal if you insist).  Since
they are active layers, running in parallel and with
access to  sensors, they  can  monitor the
environment and  decide on  the appropriateness of
their goals. Sometimes goals  can be  abandoned
when circumstances seem  unpromising,  and  other
times fortuitous  circumstances can be taken
advantage of. The key idea here is to be using the
world as its own model and to  continuously match
the preconditions of  each  goal against the  real
world. Because there is separate  hardware  for  each
layer we can match as many goals as  can exist in
parallel,  and do  not pay any  price for higher
numbers of goals as we  would if we tried to add
more and more sophistication to a single processor,
or even  some multiprocessor with a
capacity-bounded network.
• The purpose of the  Creature is  implicit in its
higher-level purposes, goals or layers.  There  need
be no explicit  representation of  goals that some
central (or distributed) process  selects from to
decide what. is most appropriate for the Creature to
do next.
5.1. No representation versus no  central
representation
Just as there is no central representation there is not
even a central system.  Each activity  producing  layer
connects perception to action directly. It is  only the
observer of  the  Creature  who imputes a  central
representation or central control. The  Creature  itself
has none; it is a collection of competing behaviors.
Out of the local chaos of their interactions  there
emerges, in the eye of an observer, a coherent pattern
of behavior. There is no central purposeful locus of
control. Minsky [10] gives a similar  account of how
human behavior is generated.
Note carefully that we are not claiming that  chaos
is a necessary ingredient of  intelligent behavior.
Indeed, we advocate  careful  engineering of all the
interactions within the system (evolution  had the
luxury of incredibly long time  scales  and  enormous
numbers of individual experiments and  thus  perhaps
was able to do without this careful engineering).
We do claim however, that  there  need be no
explicit  representation of either the  world or the
intentions of the system to  generate  intelligent
behaviors for a Creature. Without such explicit
representations,  and  when  viewed  locally, the
interactions may  indeed  seem chaotic  and  without
purpose.
I claim there is more than this, however. Even at a
local, level we do not  have traditional AI
representations. We never use tokens which have any
semantics that can be attached to them. The best that
can be said in our implementation is that one number
is passed from a process to another. But it is only bylooking at the state of both the first  and second
processes  that that number  can be  given any
interpretation at all. An extremist might say that we
really do have representations, but that they  are  just
implicit. With an  appropriate  mapping of the
complete system and its state to  another domain, we
could define a representation that these numbers and
topological connections between processes somehow
encode.
However we are  not happy with calling  such
things a  representation. They  differ  from  standard
representations in too many ways.
There are no  variables  (e.g.  see [1] for a  more
thorough treatment of this) that  need instantiation in
reasoning processes. There are no rules which need to
be selected  through pattern matching.  There are no
choices to be made. To a large extent the state of the
world determines the action of the Creature.  Simon
[14] noted  that the complexity of behavior of a
system was not  necessarily inherent in the
complexity of the  creature,  but Perhaps in the
complexity of the environment. He  made  this
analysis in his  description of an  Ant  wandering the
beach, but  ignored  its implications in the  next
paragraph  when he  talked  about humans. We
hypothesize (following  Agre  and  Chapman) that
much of  even  human level activity is similarly a
reflection of the  world  through very  simple
mechanisms without detailed representations.
6. The methodology, in practice
In order to  build systems  based on an  activity
decomposition so that they  are truly robust we must
rigorously follow a careful methodology.
6. 1. Methodological maxims
First, it is vitally important to test the  Creatures
we build in the  real world;  i.e., in the  same  world
that we humans inhabit. It is  disastrous to fall  into
the temptation of testing them in a simplified world
first,  even  with the best intentions of  later
transferring activity to an unsimplified world. With a
simplified  world  (matte  painted  walls,  rectangular
vertices everywhere,  colored  blocks as the only
obstacles) it is very easy to accidentally  build a
submodule of the system which happens to rely on
some of those simplified properties. This reliance can
then easily be  reflected in  the  requirements on the
interfaces between  that submodule  and  others. The
disease spreads and the complete system  depends in a
subtle way on the simplified world. When it comes
time to move to the, unsimplified  world, we
gradually and painfully realize that  every piece of the
system must be rebuilt. Worse than that we may need
to rethink the total  design as the issues may  change
completely. We are not so concerned that it  might be
dangerous to  test simplified Creatures  first  and  later
add  more sophisticated layers of control  because
evolution has been successful using this approach.
Second, as  each layer is  built it must be  tested
extensively in the  real world.  The system  must
interact with the real world over extended periods. Its
behavior  must be  observed  and be  carefully and
thoroughly debugged. When a second layer is added to
an existing  layer there are three potential sources of
bugs: the first layer, the  second  layer, or the
interaction of the two layers. Eliminating the first of
these source of bugs as a possibility makes  finding
bugs much easier. Furthermore,  there is  only one
thing possible to  vary in  order to fix the bugs—the
second layer.
6.2. An instantiation of the methodology
We have built a series of four robots based on the
methodology of task decomposition. They all  operate
in an unconstrained dynamic world (laboratory and
office areas in  the MIT Artificial  Intelligence
Laboratory). They successfully  operate  with  people
walking by,  people  deliberately  trying to  confuse
them,  and  people just  standing by watching  them.
All four robots  are  Creatures in  the sense that on
power-up they exist in the world and interact with it,
pursuing multiple goals determined by  their control
layers implementing  different  activities. This is in
contrast to other mobile robots that  are  given
programs or plans to follow for a specific mission,
The four robots  are  shown in Fig. 1.  Two are
identical, so there are really three, designs. One uses
an offboard  LISP machine for  most of its
computations, two use  onboard  combinational
networks,  and  one uses a custom  onboard parallel
processor. All the robots implement the  same
abstract architecture, which we call the  subsumption
architecture  which embodies  the  fundamental  ideas
of decomposition into  layers of task  achieving
behaviors,  and  incremental  composition through
debugging in the  real world.  Details of  these
implementations can be found in [3].
Each layer in  the subsumption  architecture is
composed of a fixed-topology network of  simple
finite state machines. Each finite state machine has a
handful of states, one or two internal registers, one or
two internal timers,  and  access to  simple
computational machines, which can  compute  things
such as vector sums. The finite state  machines run
asynchronously, sending  and  receiving  fixed  length
messages (1-bit messages on the two small  robots,and 24-bit messages on the larger  ones) over wires.
On our first robot these  were  virtual wires; on our
later robots we have used physical wires to  connect
computational components.
There is no central locus of control. Rather, the finite
state machines are data-driven by  the messages they
receive. The arrival of messages or the expiration of
designated  time  periods cause  the finite state
machines to  change state. The finite state  machines
have access to  the contents of the messages and
might output them, test them with a  predicate and
conditionally branch to a different state, or pass them
to simple computation elements.  There is no
possibility of  access to  global  data,  nor of
dynamically established communications links. There
is thus no possibility of global control. All  finite
state machines are equal, yet at the same time  they
are prisoners of their fixed topology connections.
Layers are combined through mechanisms we  call
suppression  (whence  the name  subsumption
architecture) and inhibition.  In both  cases as a new
layer is  added,  one of the new wires is  side-tapped
into an existing wire. A pre-defined time constant is
associated  with  each  side-tap. In the  case of
suppression the side-tapping occurs on the input side
of a finite state machine. If a message arrives on the
net wire it is  directed to  the input port of the finite
state machine as though it had arrived on the existing
wire. Additionally, any new messages on the existing
wire  are suppressed  (i.e.,  rejected)  for the  specified
time period. For inhibition the side-tapping occurs on
the output side of a finite state machine. A message
on the new wire  simply inhibits  messages being
emitted on the  existing  wire for the  specified  time
period. Unlike suppression the new message is not
delivered in their place.
As an example, consider the three layers of  Fig. 2.
These are three layers of control that we have run on
our first mobile robot for well over a year. The robot
has a ring of twelve ultrasonic sonars as its  primary
sensors.  Every  second  these sonars  are  run to give
twelve radial depth measurements. Sonar is extremely
noisy  due to  many objects being mirrors to sonar.
There are thus problems with specular reflection and
return paths following  multiple  reflections  due to
surface skimming with low angles of incidence (less
than thirty degrees).
In more detail the three layers work as follows:
Fig. 1. The four MIT AI laboratory Mobots. Left-most is the first
built  Allen, which relies on an offboard  LISP  machine for
computation support. The right-most one is Herbert, shown with a
24 node  CMOS  parallel processor surrounding  its  girth. New
sensors and fast early  vision  processors are  still to be built and
installed. In the middle  are  Tom and  Jerry, based on a
commercial toy chassis, with single PALs (Programmable  Ar r ay
of Logic) as their controllers.
 (1) The lowest-level layer implements a behavior
which makes the robot (the physical embodiment of
the  Creature) avoid  hitting objects. It both  avoids
static objects and moving objects, even those that are
actively attacking it. The finite state machine labelled
sonar simply runs the sonar devices and every second
emits an instantaneous map with the  readings
converted to polar coordinates. This map is passed on
to the collide and feelforce finite state machine. The
first of these simply  watches to see if there is
anything dead ahead, and if so sends a halt message to
the finite state machine in  charge of  running the
robot forwards—if that finite state machine is  not in
the  correct  state the message may well be  ignored.
Simultaneously, the other finite state  machine
computes a repulsive force on the robot,  based on an
inverse  square  law,  where each  sonar return is
considered to  indicate  the  presence of a repulsive
object. The contributions from each sonar are added to
produce an  overall  force  acting on the robot. The
output is  passed to  the  runaway  machine which
thresholds it  and  passes it on to the  turn  machine
which orients the robot  directly away  from the
summed repulsive force.  Finally, the  forward
machine  drives  the robot  forward. Whenever  this
machine receives a halt message while the robot is
driving forward, it commands the robot to halt.
This network of finite state machines  generates
behaviors which let the robot  avoid  objects. If it
starts in the middle of an empty room it simply  sits
there. If someone walks up to  it, the robot  moves
away. If it moves in the direction of other obstacles it
halts. Overall, it  manages to  exist in a  dynamic
environment without hitting or being hit by objects.The next layer makes the robot wander about,
when not busy  avoiding objects. The  wander  finite
state machine  generates a random heading  for the
robot  every  ten  seconds or  so. The  avoid  machine
treats that  heading as an attractive force and sums it
with the repulsive force computed from the sonars. It
uses the result to suppress the lower-level behavior,
forcing the robot to move in a direction close to what
wander  decided  but at the same time  avoid any
obstacles. Note that if the.  turn  and  forward  finite
state machines are  busy running the robot the new
impulse to wander will be ignored.
(3) The third layer makes the robot try to explore.
It looks for distant places, then tries to  reach  them.
This  layer suppresses the wander layer, and observes
how the bottom  layer diverts  the robot  due. to
obstacles,  (perhaps dynamic). It corrects for any
divergences and the robot achieves the goal.
Fig. 2. We  wire, finite state machines together  into  layers of
control. Each layer is built on top of existing layers. Lower level
layers never rely on the existence of higher level layers.
The  whenlook  finite state machine notices  when
the robot is not busy moving,  and starts up, the free
space finder (labelled  stereo in the  diagram)  finite
state machine. At the same time it inhibits wandering
behavior so that the observation  will  remain valid.
When a path is  observed it is  sent to the  pathplan
finite state machine, which injects a  commanded
direction to  the  avoid  finite state machine. In  this
way,  lower-level obstacle  avoidance  continues to
function. This may  cause  the robot to go in a
direction different to  that  desired by  pathplan.  For
that reason the actual path of the robot is  monitored
by the  integrate  finite state machine, which  sends
updated  estimates to the  pathplan  machine. This
machine then acts as a difference  engine forcing the
robot in the  desired  direction  and  compensating for
the actual path of the robot as it avoids obstacles.
These particular layers  were  implemented on our
first robot. See  [3] for more details. Brooks and
Connell [5]  report on another three  layers
implemented on that particular robot.
7. What this is not
The subsumption  architecture with its  network of
simple machines is reminiscent, at the  surface level
at least, with a number of mechanistic approaches to
intelligence, such as connectionism  and  neural
networks. But it is  different in  many  respects for
these endeavors, and  also  quite  different  from many
other post-Dartmouth traditions in  artificial
intelligence. We very briefly explain those differences
in the following sections.
7.1. It isn&apos;t connectionism
Connectionists try to make networks of  simple
processors. In that regard,  the things they build (in
simulation only—no connectionist has  ever driven a
real  robot in a  real  environment, no matter how
simple) are similar to the subsumption networks we
build.  However, their processing  nodes tend to be
uniform and they are looking (as their name suggests)
for revelations from  understanding  how to  connect
them  correctly (which is  usually  assumed to mean
richly at least). Our nodes are all  unique  finite state
machines and the density of connections is very much
lower, certainly not uniform,  and  very  low  indeed
between layers. Additionally, connectionists seem to
be looking for explicit  distributed representations to
spontaneously arise from their networks. We  harbor
no such hopes because we believe representations are
not  necessary and appear only in the  eye or mind of
the observer.
7.2. It isn&apos;t neural networks
Neural networks is  the parent discipline of  which
connectionism is a  recent  incarnation.  Workers in
neural networks claim that there is  some biological
significance to their network nodes, as models of
neurons. Most of the,  models seem  wildly
implausible given the paucity of modeled connections
relative to the thousands found in real  neurons. We
claim no biological significance in our  choice of
finite state machines as network nodes.
7.3. It isn&apos;t production rules
Each individual  activity  producing layer of our
architecture could be viewed as an implementation of
a production rule. When the right conditions are met
in the environment a certain action will be performed.We feel that analogy is a little like saying that any
FORTRAN program with IF statements is
implementing a  production rule  system. A  standard
production system really is more—it has a rule base,
from which a rule is  selected based on  matching
preconditions of  all the rules to some  database. The
preconditions may  include variables  which must be
matched to individuals in the database, but  layers run
in parallel  and  have no variables or  need for
matching. Instead, aspects of the world are  extracted
and these directly trigger or modify certain behaviors
of the layer.
7.4. It isn&apos;t a blackboard
If one, really wanted, one could make an analogy
of our networks to a blackboard, control architecture.
Some of the finite state machines would be localized
knowledge sources. Others would be processes acting
on these knowledge sources by finding them on the
blackboard. There is a  simplifying point in our,
architecture however: all the processes know  exactly
where to   look on the  blackboard as  they are
hard-wired to  the  correct  place. I think this  forced
analogy indicates  its own  weakness.  There is no
flexibility at all on  where a  process can  gather
appropriate knowledge.  Most  advanced  blackboard
architectures make heavy use of the  general sharing
and  availability of almost all  knowledge.
Furthermore, in spirit at least,  blackboard  systems
tend to hide from a consumer of knowledge who the
particular producer   was. This is the primary  means
of abstraction in  blackboard systems. In our system
we make such connections explicit and permanent.
7.5. It isn&apos;t German philosophy
In some  circles much  credence is  given to
Heidegger as  one who  understood  the  dynamics of
existence. Our  approach  has  certain  similarities to
work inspired by  this  German philosopher (e.g.  [1])
but our work was not so inspired. It is  based purely
on engineering considerations. That does not  preclude
it from being  used in  philosophical  debate as an
example on any side of any fence, however.
8. Limits to growth
Since our approach is a performance-based one, it
is the  performance of  the systems we build  which
must be used to measure its usefulness and to  point
to its limitations.
We claim that as of mid-1987 our  robots, using
the subsumption  architecture to  implement  complete
Creatures,  are  the most  reactive real-time  mobile
robots in existence. Most other mobile robots are
still at the stage of  individual &quot;experimental runs&quot; in
static environments, or at best in completely mapped
static environments. Ours, on the other hand, operate
completely autonomously in complex  dynamic
environments at the flick of their on switches, and
continue until their batteries are drained. We believe
they operate at a level closer to  simple insect  level
intelligence than to  bacteria  level intelligence. Our
goal (worth nothing if we don&apos;t  deliver) is  simple
insect level intelligence within two years. Evolution
took 3 billion  years to  get from single cells to
insects,  and  only  another  500 million  years from
there to humans. This statement is not  intended as a
prediction of  our future  performance,  but  rather to
indicate  the nontrivial  nature of insect  level
intelligence.
Despite this  good performance to date, there are a
number of serious questions about our approach. We
have beliefs  and  hopes about how these  questions
will be  resolved,  but  under  our  criteria  only
performance truly counts. Experiments  and  building
more complex systems take time, so with the  caveat
that the experiments  described  below have  not yet
been performed we outline how we currently see our
endeavor progressing. Our intent in discussing this is
to indicate  that  there is at  least a plausible path
forward to more intelligent machines from our current
situation.
Our belief is that the sorts of activity  producing
layers of control we are developing (mobility, vision
and survival related tasks)  are necessary prerequisites
for higher-level intelligence in the style we attribute
to human beings.
The most natural and serious questions concerning
limits of our approach are:
• How many layers can be built in the subsumption
architecture before  the interactions  between layers
become too complex to continue?
• How complex  can  the behaviors be that are
developed without the aid of central representations?
• Can higher-level functions such as learning occur in
these fixed topology networks of simple finite  state
machines?
We outline our current thoughts on these questions.
8.1. How many layers?
The highest number of layers we  have run on a
physical robot is three. In simulation we have run six
parallel layers. The  technique of completely
debugging the  robot on all existing activityproducing layers before  designing  and adding a new
one seems to have been practical till now at least.
8.2. How complex?
We are  currently working  towards a complex
behavior pattern on our fourth robot which  will
require  approximately fourteen individual  activity
producing layers.
The robot has infrared proximity sensors for local
obstacle  avoidance. It  has an  onboard  manipulator
which  can grasp  objects at  ground  and  table-top
levels,  and  also  determine  their rough weight. The
hand has depth sensors
mounted on it so  that homing in on a target  object
in order to grasp it can be controlled directly. We are
currently working on a structured light  laser  scanner
to determine rough depth maps in the forward looking
direction from
the robot.
The high-level behavior we  are trying to instill in
this  Creature is to  wander around the office areas of
our laboratory, find open office doors, enter, retrieve
empty  soda  cans from  cluttered desks in  crowded
offices and return them to a central repository.
In order to achieve this  overall behavior a number
of simpler task achieving behaviors  are necessary 
They include: avoiding  objects, following walls,
recognizing  doorways  and  going through  them,
aligning on  learned  landmarks, heading in a
homeward  direction, learning  homeward  bearings at
landmarks  and  following them, locating  table-like
objects,  approaching such  objects, scanning  table
tops for cylindrical objects of roughly the height of a
soda can, serving the manipulator arm, moving the
hand above sensed objects, using the hand sensor to
look for objects of soda can size sticking up from a
background,  grasping objects if they  are  light
enough, and depositing objects.
The individual tasks  need  not be  coordinated  by
any central controller. Instead  they  can index off of
the state of the world. For instance the grasp behavior
can cause the manipulator to grasp any object of the
appropriate size seen by the  hand sensors. The robot
will not  randomly grasp  just any object  however,
because it  will only be  when other layers or
behaviors have noticed an object of roughly the right
shape on top of a table-like object that the grasping
behavior  will  find  itself in a position  where its
sensing of the world tells it to  react. If, from above,
the object no longer looks like a soda can, the grasp
reflex will not happen and other lower-level behaviors
will  cause  the robot to look  elsewhere  for new
candidates.
8.3. Is learning and such possible?
Some insects demonstrate a simple type of  learning
that has  been dubbed &quot;learning by instinct&quot; [7]. It is
hypothesized  that honey  bees for example are
pre-wired to learn how to. distinguish  certain classes
of flowers, and to  learn routes to  and  from a home
hive and sources of nectar. Other insects, butterflies,
have been shown to be able to  learn to  distinguish
flowers, but in an information limited way [8]. If
they are forced to learn about a second sort of flower,
they forget what they already knew about the first, in
a manner  that suggests the total amount of
information which they know, remains constant.
We have found a  way to build  fixed  topology
networks of our finite state machines which can
perform learning, as an isolated subsystem, at levels
comparable to  these examples. At the moment of
course we are in the very position we lambasted most
AI workers for earlier in  this paper. We  have an
isolated module of a system working,  and the inputs
and outputs have been left dangling.
We are  working to  remedy  this situation, but
experimental work  with physical  Creatures is a
nontrivial and time consuming activity. We find that
almost any  pre-designed  piece of equipment or
software has so many preconceptions of how they are
to be used built into them, that they are not flexible
enough to be a part of our complete systems.  Thus,
as of mid-1987, our work in learning is  held up by
the  need to  build a new sort of  video camera and
high-speed low-power processing box to run specially
developed vision algorithms at 10 frames per second.
Each of  these steps is a significant  engineering
endeavor  which we  are  undertaking as  fast as
resources permit.
Of course, talk is cheap.
8.4. The future
Only experiments with real Creatures in real worlds
can answer  the natural doubts about our  approach.
Time will tell.
A c k n o w l e d g e m e n t
Phil Agre,  David  Chapman, Peter  Cudhea, Anita
Flynn,  David Kirsh  and Thomas Marill  made  many
helpful comments on earlier drafts of this paper.
Re f e r e n c e s
[1] P.E.  Agre and D. Chapman, Unpublished memo, MIT
Artificial Intelligence Laboratory, Cambridge, MA (1986).[2] R.J.  Bobrow and  J.S.  Brown, Systematic understanding:
synthesis, analysis, and contingent knowledge in  specialized
understanding systems, in: R.J. Bobrow and A.M, Collins,  eds.,
Representation and Understanding  (Academic  Press, New
York, 1975) 103-129.
[3] R.A. Brooks, A robust layered control  system  for a  mobile
robot, IEEE J. Rob. Autom. 2 (1986) 14-23.
[41 R.A. Brooks, A  hardware retargetable  distributed  layered
architecture for  mobile robot control,  in: Proceedings  IEEE
Robotics and Automation, Raleigh, NC (1987) 106-110.
 [5] R.A. Brooks and J.H. Connell, Asynchronous distributed
control system  for a  mobile robot,  in:  Proceedings  SPIE,
Cambridge, MA (1986) 77-84.
[6] E.A. Feigenbaum and J.A. Feldman, eds.,  Computers and
Thought (McGraw-Hill, San Francisco, CA, 1963).
[7] J.L. Gould and P. Marler, Learning by  instinct, Sci.  Am.
(1986) 74-85.
[8] A.C. Lewis, Memory constraints and Rower  choice in pieris
rapae, Science 232 (1986) 863-865.
[9] M.L.  Minsky,  ed.,  Semantic Information Processing  (MIT
Press, Cambridge, MA, 1968).
[10] M.L. Minsky, Society of Mind  (Simon  and Schuster, New
York, 1986).
[11]  H.P. Moravec,  Locomotion, vision  and intelligence,  in: M.
Brady and R. Paul, eds.,  Robotics Research  1 (MIT Press,
Cambridge, MA, (1984) 215-224.
[12] N.J.  Nilsson, Shakey the robot,  Tech. Note 323,  SRI AI
Center, Menlo Park, CA (1984).
[13] E.H. Shortliffe,  MYCIN: Computer-Based  Medical
Consultations (Elsevier, New York, 1976).
[14] H.A.  Simon,  The Sciences of the  Artificial  (MIT Press,
Cambridge, MA, 1969).
[15] J. Von Uexküll, Umwelt and Innenwelt der Tiere  (Berlin,
1921).</doc>
        <id>6
</id>
    </owl:Thing>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pb -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pb">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_consistency_and_parallel_DB"/>
        <doc>title : Improving Quality in Business 
Process Outsourcing through Technology ###
Hongyan Li
CORAL, Department of Business Studies, Aarhus School of Business, Aarhus V 8210, Denmark, Hojl@asb.dk
Joern Meissner*
Department of Management Science, Lancaster University Management School, Lancaster LA1 4YX, UK, joe@meiss.com 
Sufﬁcient evidence shows that Business Process Outsourcing (BPO) is growing rapidly. Technological and communication advances help realize the wide-spread adoption of BPO, due to their quality and cost improvements. Technology
applied in BPO through the adopted software, applications and platform has a substantial long-term impact on the whole
process, affecting the quality, cost and associated risks of the operation of outsourced activities.
In a previous SAP white paper, the impact of technology to cost has been addressed. As one of a series of SAP white
papers, we concentrate here on how technology impacts quality in BPO. In this paper, we identify the quality structure of
BPO and distinguish the unique quality characteristics that are different from the traditional service industry. The quality
structure includes the major quality measurement criteria, which BPO buyers should consider and BPO service providers
should be focus on when offering their services. In conclusion, technology enhances the value of BPO from perspectives
of enabling and leveraging the values of standardization, automation, integration, ﬂexibility and innovation.
To both BPO service providers and customers, quality and technology are two equally important elements which need
to be concerned carefully. In this paper, we present a quality framework of a BPO service including seven quality dimensions: reliability, tangibility, conformance, responsiveness, ﬂexibility, assurance and security, and four quality enablers:
standardization, integration and automation, innovation.
Key words: Technology, Quality, BPO, E-learning process outsourcing, Recruiting process outsourcing ###
1. Introduction
BPO is an important branch and trend of outsourcing that many management theories and methodologies
generated and developed for outsourcing can be applied to. Many corporations, like Dell, AIG, IBM and
Citi Group, have been using BPO and leveraging the larger scale of outside service providers to cut costs,
improve process quality and speed time to market. Also, many IT service vendors, like IBM, EDS, Accenture, and SAP, have integrated BPO services into their systems and models.
* We are indebted to Sebastian Burgarth from the SAP BPO team, who provided fantastic support and valuable input to this research
project. We would also like to thank Bassem El-Gawly and Ashwin Sathyanesan Girija, two MBA Students at Lancaster University
Management School who were involved in the initial stages of this study.
12 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
BPO started with non-core processes and is now moving towards more critical applications. It has boomed
with call centers and customer support processes, and now is happening with software development, Human
Resources (HR), Finance and Accounting (F&amp;A), training, payroll, and procurement. The trend is moving
beyond the outsourcing of typical back-ofﬁce functions into middle-ofﬁce functions. BPO is catching up
with industries like medical transcription, animation production, and even disaster recovery management
systems. Moreover, it is not just India or the Philippines that are booming with BPO. Central &amp; Eastern
Europe’s markets are aggressively chasing near shore outsourcing from Europe.
Terra (2005) showed that 73% of BPO customers surveyed believe BPO is improving their outsourced
processes. The study concludes that BPO is increasingly moving to be about efﬁciency and effectiveness. In
order to deliver a quality and stable service, the provider has a service methodology, the needed infrastructure, people, and skills, technology, and metrics capabilities. Among those requisites, the technological and
communication advances is the fundamental factor leading to the wide spread adoption of BPO. The internet and the low cost of communication, in addition to the move into a more standardized applications, open
IT platforms, and more integrated systems gave BPO the tools needed to advance and spread. All this will
and has been helping the adoption of BPO by reducing risks, increasing the transparency, and improving
the process quality while lowering costs.
BPO is growing rapidly. International Data Company (IDC) predicts that BPO will grow at a compound
annual rate of 10.9%. With $382.5 billion in annual sales in 2004, global BPO will likely grow to $641.2
billion in 2009 (Gibson (2005)). Moreover, nearly all processes outsourced are highly IT intensive. Donniel
Schulman, from IBM’s Business Transformation Outsourcing (BTO), highlighted the essential role of IT in
BPO, demonstrating how IT should be involved in deciding where BPO investments go (Erlanger (2006)).
The sustainable success of any BPO process depends on technology. As technology impacts almost every
area of operations management (Slack et al. (2004)), technology profoundly affects BPO services.
Applied BPO technology, like the adopted software, applications and platform, have a substantial longterm impact on the whole process, affecting the quality, cost and associated risks of the operation of the
outsourced activities. Technology directly drives the process automation through workﬂows, paperless document management and online interactive. It facilitates innovative solutions to be implemented and allows
providers to create creative new models of processes operations while minimizing cost and disruption ofHongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 3
execution. The right technology enables the balance of personalization and standardization of the outsourced
activities, creating the capacity to design processes that suit the speciﬁcities of each client, while maintaining a standardization level that enables sustainable long term advantages for both the provider and clients.
Both on a process and IT level, technology affects the level of integration between the BPO buyers and
the service provider. The underlying technology adopted by the service provider can drive his ﬂexibility
capabilities to adapt and accommodate for any needed scope or scale changes requested by the BPO buyer
or even changes in the sourcing strategy.
Given the potential economic impact of BPO and the critical role of technology in improving BPO service, it is necessary to conduct research on the impact of technology on BPO. However, most of the relatively
recent research is concerned with how outsourcing affects the companies’ competitive advantages. There is
a lack of academic research on BPO quality, as most ﬁndings are obtained by BPO industry insiders and
advisors. The focus in BPO is changing from just operational cost cutting, into a transformational process
where extra beneﬁts are realized by buyers who demand higher level of quality to be supplied by the service
providers processes. In this paper, we identify the mechanisms that would contribute to the perception of
the customer in judging service quality, explore the relevance of technology and service quality for BPO,
and provide managerial insights to BPO practitioners.
To construct a pragmatic service quality framework for the BPO industry, we borrow the European
Foundation for Quality Management Excellence Model (EFQMEM), which divides the quality criteria into
enablers and results. Although the EFQMEM is usually used to assess the performance of an organization,
it provides insights on how to deﬁne a quality framework for BPO industry. In this industry, technology
can be regarded as one of the enabling resources to create the excellent customer perceptions on quality.
Therefore, in this study, a technology-driven customer quality framework for BPO is developed and shown
in Figure 1 below.
The remainder of this paper is organized as follows. In section 2, we develop a multiple dimensional
quality framework and present the performance measures on each dimension. The impact of the technology
to quality in BPO are addressed in section 3. Finally, the industrial implications and conclusion are presented
in Section 4.4 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
Figure 1 Technology-driven customer quality framework for BPO
Assurance/Empathy
2. Quality framework in BPO
2.1. Related research on service quality
Quality improvements affect operations performance in various ways, such as increasing revenue, reducing
costs and improving productivity. Quality has been regarded as one of the major drivers of competitive
strategy in every industry. There is no exception to the BPO service industry. However, as Reeves and
Bednar (1994) stated that “no universal, Parsimonious, or all-encompassing deﬁnition or model of quality
exists”. The quality construct space is very broad and characterized by industry. The American National
Standards Institute (ANSI) and American Society for Quality (ANQ) deﬁne quality as:
the totality of features and characteristics of a product or service that impact its ability to satisfy given
needs.
At the level of strategic operations, many researchers have developed different quality frameworks. For
example, Garvin (1987) developed a quality framework considering an eight dimension product quality, and
Parasuraman et al. (1991) derived a ﬁve dimension model of service quality, SERVQUAL (see below table
1).
It is difﬁcult, however, to measure service quality due to three unique natures of services: intangibilityservice cannot be measured, counted, inventoried, tested and veriﬁed in advance of sale; heterogeneity-the
consistency of service from a personnel is difﬁcult to measure; and inseparability-the difﬁculty in separating consumption from production (Ma et al. (2005)). Zeithaml et al. (1993) states that customers not onlyHongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 5
Table 1 Dimensions of Quality [Source: Ma et al. (2005)].
Framework Dimension Deﬁnition
1. Performance Primary operating characteristics
2. Feature Supplements to basic functioning characteristics
Product quality 3. Reliability Does not malfunction during speciﬁed period
(Garvin (1987)) 4. Conformance Meets established standards
5. Durability A measure of product life
6. Serviceability The speed and ease of repair
7. Aesthetics How a product looks, feels, tastes and smells
8. Perceived quality As seen by a customer
1. Tangibility Physical facilitates, equipment and appearance of personnel
Service Quality 2. Reliability Ability to perform the promised service dependably and accurately
(Parasuraman et al. (1991)) 3. Responsiveness Willingness to help customers and provide prompt service
4. Assurance Knowledge and courtesy of employees and their ability to inquire trust
and conﬁdence
5. Empathy Caring, individualized attention the ﬁrm provider gives its customers
judge service quality based on the outcome of the service but also consider the process of service delivery. Speciﬁcally, service quality perceptions stem from how well a provider performs vis-`a-vis customers’
perception about how the provider should perform. Cronin and Taylor (1992) investigated and developed
a performance-based measure — SERVPERF. Kettinger and Lee (1997) recommended a revised model as
SERVQUAL+, which assesses the service quality in desired level and adequate level. While there have
been attempts by numerous researchers to give service quality a tangible aspect to make service quality
measurable and determinable, there has yet to be a viable theory developed.
Although existing quality frameworks are not directly appropriate for BPO, previous studies on quality
measures are useful in developing a more accurate quality dimension for BPO industry. BPO shares various
features with IT outsourcing and is highly technology dependent. Therefore, some of the general and IT
outsourcing theories and concepts are applicable to BPO research. The quality research on IT/IS outsourcing
and ASP are the most relevant to the BPO industry. Grover et al. (1996) explained that outsourcing involves
the quality expectations of both the service provider and the service receiver, and that service quality is
measured with tangibility and reliabilities. Additionally, Ma et al. (2005) implemented an exploratory study
on service quality of ASPs and identiﬁed seven factors to measure service quality: features, availability,
reliability and assurance, empathy, conformance and security. Despite of the similarities between BPO,
IT/IS outsourcing and ASP, their differences are also evident. Based on the comparison analysis on IT/IS
and ASP in Ma et al. (2005), we can extend the comparison to BPO industry (see Table 2).
Given the differences, it is inappropriate to adopt either of the quality dimensions from IT/IS outsourcing
and ASP. Thereby, referring to the ﬁndings from the IT/IS outsourcing quality and ASP service quality, we6 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
Table 2 The comparison of IT/IS outsourcing, ASP and BPO model.
Traditional IT/IS
Perspectives outsourcing model ASP model BPO model
Client The quality and availability of support staff, the ability of the vendor to grow,
expectation indication of some vendor competence, and tangible evidence of success
Similarities Applications Standard, non-critical applications/processes
Contract Service level was speciﬁed in a contract to govern the services that providers rendered
Client- Maintain a good relationship with clients
relationship
Target market Large clients with SMEs with low IT Any clients with focus
IT departments experience on core competence and
expectation to stay lean.
Vendor “Name” vendors, with Entrepreneurs and Technically advanced, global
Characteristics potential global span start-ups expertise in speciﬁc functions
Differences Contract type Long, broad, strategic Short, standard, usage- Long, standard, and strategic,
based, and non strategic broader, deeper
Available Pick your services from Web-based application Any non-core functions,
functions application development services to efﬁciently manage critical
to infrastructure operation information and intellectual
property.
Product Tailored or client- Standard packages with Tailored and Standardized
customization determined one site ﬁts all
Resource Mixed bag Vendor server hardware Vendor technology and
ownership and application ownership application ownership, customer
intellectual property, data
and information ownership
identify the BPO service quality dimensions in Section 2.2.
Cronin and Taylor (1992) describe service quality as the difference between the product or service performance and customer expectations. In other words, the realization of service quality is the gap between
the customer expected quality and perceived quality. Therefore, for a quality BPO service, it is important
to identify the quality criteria from the perspective of the customer so that effective services are provided
to satisfy and surpass the customer’s expectations. In section 2.2, with respect to each dimension of quality,
the client quality expectations are clariﬁed. Two typical types of BPO services, E-Learning processes and
Human Resource Management processes, are employed as examples to help analysis.
E-learning is “the use of technology to manage, design, deliver, select, transact, coach, support and extend
learning” (Elliot Masie, The Masie Center
1
). The development and implementation of a course must involve
several technically proﬁcient people or a programming expert, and therefore, it has been a popular option
to outsource the entire process, from course analysis to design and development until implementation.
1
Source: http://www.masieweb.comHongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 7
The total human resource recruitment spending in the United States in 2000 was around $40 billion.
Recent surveys reveal the growing concern among senior management regarding the recruitment and retention of talent in the organization. Innovation, integration, and attention to detail are key factors to a quality
Human Resource Recruitment Process (HRRP). It is costly for organizations to have a dedicated team for
improving the activities in the recruitment process, and most companies’ human resource departments spend
only 10% of their time on the acquisition of talented recruits; in contrast, HRRP providers are dedicated
to enhancing the process by creating access for their clients to the most talented applicants (PricewaterhouseCoopers (2002)). Insead of simply sourcing and screening candidates as recruitment agencies do, a
full HRRP service covers an end to end recruitment process.
2.2. Quality dimension in BPO
As a special service provider, BPO service quality is the degree and direction of variation between the service receiver’s expectations and perceptions. As Kumar (2004) pointed out “Service excellence has become
the basic instinct and real value differentiator that drives client satisfaction. Operational excellence, product/service leadership and highly effective client relationship management are keys to assuring superior
service delivery. The service provider must have a clearly deﬁned service vision in line with outsourcers’
priorities and hones on creating measurable values”. Therefore, we develop a BPO quality structure including seven dimensions shown in Table 3 below.
In each dimension, the sub-measures are identiﬁed based on the nature of BPO service. These criteria are
strongly acknowledged as some of the main factors leading to BPO success.
1. Reliability
Reliability is how to manage the outsourcing relationship, assuring the successful service delivery after
the deal is signed and the outsourced process activities are in operation. Grover et al. (1996) suggested that
the success of outsourcing is heavily dependent on the reliability of the service a service provider provides.
In Human Capital Management, the ﬁnalized and easy-to-use results should be available. A full HRRP
service begins with the job requisition through hiring the new employee, including: information collection
and applications management, candidate sourcing (internally and externally), recruitment agencies management, screening, interviewing and testing, reference checking, offer and contracts management, on-boarding
and even any needed initial training.8 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
Table 3 Dimensions of BPO service quality.
Quality dimension Qualitative Description Sub-measures
Reliability Ability to perform the promised service On-time
dependably and accurately Accuracy
Accessibility
Correct historical record
Disaster recovery
Tangibles Physical facilitates, equipment, and Advanced technology
application, appearance of personnel Global expertise
Application’s friendly user interface
Ease of data reporting and extracting
Application scalability
Application interoperability
Conformance The degree a service’s design and operating Systematically process design
characteristics meet established standards Consistent process delivery and manage
Efﬁciency and Effectiveness
Added value
Responsiveness The timeliness of service Speed
Competence
Ease of repair
Customer relationship
Flexibility The process ability to deal with changes Re-scalability
Upgrade
Innovation
Transition
Assurance/ Client-focused process Shared approach to problem solving
Empathy development and management Helping customers in improving their entire operations
Quality assurance systems or tools
Expertise availability and know how
Fit of work practices with that of customers, etc.
Security The freedom from danger, risk, or doubt Conﬁdentiality
Physical safety
Financial safety
In outsourced E-Learning activities, customer expectations include:
• Service providers will deliver promised service by a certain time;
• Accurate learning resources and tools are delivered;
• Sufﬁcient capacity is provided to record the learner’s learning history;
• The speed and effects will not be impacted by the amount of the users;
• When customers have a problem, excellent service providers will resolve the issue as soon as possible.
2. Tangibility
Tangibility includes the physical evidence of service. Technology is the ﬁrst visible quality feature in
BPO. Technology and applications allow the monitoring of process operations. Technology adds to the BPO
quality and success by making the operations more visible by different views suited for different role-based
levels in the client’s organization, e.g. operational vs. managerial vs. strategic and planning level.Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 9
For example, in outsourced E-Learning activities, cutting-edge technology is one of the critical factors in
matching the learning architecture a company needs to provide the best solution for the business process.
When delivering the learning system to a business, the service prodivers’ technology choices should bear
in mind the existing technical infrastructure and the needs and skill levels of the employees who will work
with whatever products and systems are chosen.
Other customer expectations include:
• Physical advances - global expertise in the course relevant ﬁeld;
• Assessment tools - diversity should be provided, etc.;
• Courses - well designed and structured;
• Easy to apply - a competency model that really works;
• Comprehensive - covers all job descriptions and management levels;
• Automated - most of the analysis can be automated using online assessment tools.
3. Responsiveness
Responsiveness concerns the willingness or readiness of employees to provide service. Customer-facing
staff can be brought up to speed with new offerings far more rapidly, and brand service values can be communicated to customer touch points more efﬁciently and consistently. Providers cannot escape errors at any
stage of BPO service, so the ability to discover errors swiftly and track down their causes are important. This
will result in the quick operation restoration, and thereby increase client satisfaction due to minimum interruption and early discovery of problems. This is aligned with Tax and Brown’s (1998) arguments regarding
how effective service recovery from failure is one of the major demands that customers expect.
Using outsourced E-Learning process as an example, other customer expectations include:
• When a break-down happens, an excellent E-Learning service provider should be able to resolve it
promptly;
• Speciﬁc individuals should be available to provide help on any application problems.
4. Conformance
Conformance is the degree to which the design and operation of an application of service meets its
established standards. Although BPO is usually considered service ﬁrst, it is actually providing applications
to organizations. Any outsourced business process involves some speciﬁcations which are usually restricted10 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
by a contract. The BPO activities should align the outsourcing objectives with the appropriate contract
reviews, performance monitoring and measurement systems. Therefore, quality BPO service should provide
the needed framework for driving the behavior of the BPO provider towards sustaining the quality level of
service required.
Despite the crucial role of the contract and the Service Level Agreement (SLA) in controlling the relationship, BPO governance should be built around a partnership to support the increasing dependency between
the client and the service provider. Such a trusted provider/supplier relationship would revolve around open
communication, fairness and the belief in mutual beneﬁt and interdependence. The quality monitoring and
measurement should be viewed as a beneﬁt for both parties.
The conformance performance in BPO involves how the provider is able to align his activities with
the evolving goals of the client. Only then will the providers’ activities be completely client focused and
will add real value. For this to succeed the client should provide the needed resources for managing the
relationship, or the provider will not be able to fulﬁll his part of the relationship. For example, in the HRRecruitment process, the governance team should not only control the cycle time or the cost-per-hire but
more importantly, the new-hire efﬁcacy and turnover. Quality in the recruitment process does not end with
the hiring step but would also measure how the newly hired employee ﬁts the job, and how the selection was
accurate, reliable, and effective. HRRP providers are dedicated to enhancing the process creating access for
their clients to the best of the talents.
In a particular HRRP deal, the buyer and the provider used to hold a monthly virtual meeting between
senior managers to measure the each side’s satisfaction (Center-Everest (1992)). This way any quality drop
will be more visible at senior levels and more likely to be rapidly resolved.
For the aspect of conformance, the clients of the E-Learning Process Outsourcing service often have the
following expectations:
• The excellent e-learning service providers understand the speciﬁc needs of their customer (learners);
• The learning process should be more efﬁcient and effective comparing with the legacy system or content;
• The learning activity is more pleasant.
5. FlexibilityHongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 11
Quality is not only delivering the service but consistency in delivery. BPO contracts are relatively longtermed, between 3 and 7 years, and the clients’ requirements and needs frequently change. In order to
achieve sustainable quality, BPO service providers need to cope with scope or scale changes including
clients’ organizational restructuring, Merge and Acquisition (M&amp;A) activities or even the sourcing strategy
change. In other words, ﬂexibility is one of the important qualities of BPO service.
BPO agreements should be designed to allow for adaptability. Contracts should enable elements to evolve
and adapt to organizations’ service level requirements, like service level improvement clauses with incentives schemes. Quality entails service providers to proactively enhance value, hence the need to adapt to
their clients’ changing needs. This is an element of being customer focused and aligned to clients’ strategic
needs. In addition to day-to-day operations, the service providers should engage in frequent benchmarking
and innovation for their own processes to be able to cope with the increasing quality demands while maintaining their efﬁciencies and economies of skill and scale to drive long-term proﬁtability. While the BPO
relationship should be built on partnership, the contracts must be built with the view of the possibility of a
breakup, in case of re-sourcing the processes back in-house or move to another BPO provider.
The challenge also comes from the change in outsourcing from just operational cost cutting into a strategic way to drive corporate performance and competitiveness. To gain this, outsourcing is changing to
become more of a transformational process where performance is monitored and the beneﬁts measurable,
not the mere transfer of an existing operation. It is now a process of continuous improvement by both
the outsourcing organization and the outsourcing vendor to meet the ever changing business needs. This
requires that the IT innovation and deployments to catch up with the operational continuous change and
improvement.
Outsourcing, especially in the process of HR-Recruitment, provides ﬂexibility to the organization’s
stafﬁng function (RES (2005)). For example, ﬂexibility reﬂects the ability of BPO service provider to adapt
to their clients’ seasonal peaks, and scale to maintain their service metrics, like time-to-ﬁll and cost- perhire, even at times of talent shortages.
An E-learning process should be able to support both synchronous and asynchronous learning, accommodate diversity course resources and assessment tools, etc. Synchronous learning has a signiﬁcant strength in
the number of different applications it can be applied to. Thinking of live E-learning as not solely a training12 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
medium, but as a highly ﬂexible tool for everything from one-to-one collaborative working to corporate
communications across an entire organization could be beneﬁcial.
Customer expectations include:
• Flexible learning forms and time should be available;
• Advanced Learning management system is very important;
• Dynamic systems that are easily adapted to ﬁt organizational change.
6. Assurance and Empathy
Assurance and empathy are both client focused quality criteria. Assurance is reﬂected by the knowledge
and courtesy of employees and their ability to instill trust and conﬁdence. Empathy is the degree of individualized attention the service provider gives its customers. As the quality advantages of BPO service,
assurance and empathy are the critical view organizations gain when they outsource their processes to the
provider. In other words, assurance and empathy help improve the perceived service quality, customer value
and customer satisfaction. In a HRO deal, the buyer and the provider can hold a monthly virtual meeting
between senior managers to measure the each side’s satisfaction. This way, any quality drop can be more
visible at senior levels and more likely to be rapidly resolved.
Based the perspective of assurance and empathy, quality BPO service should support the increasing
dependency between the client and the service provider. A trusted relationship between BPO service
providers and their clients should be built around open communication, fairness, belief in the mutual beneﬁt
and interdependence. In addition, quality BPO service requires that the service provider has formal systems and procedures to consistently fulﬁll the requirements of different customers and deliver services to
the agreed service levels. Moreover, higher levels of quality capability require that the providers must have
quality and performance measures in place to monitor performance progress and proactively enhance the
quality of service they are providing. Other customer expectations include:
• Providing effective customer training and education programs;
• Sharing work practices and problem solving approaches with their clients;
• Deploying more staff to improving the functions of the system;
• Helping customer in strategic planning and setting proper expectations;
• Detailed fee-for services, no hidden costs, etc.Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 13
7. Security
In BPO, the outsourced business process often interacts with other business processes of the customer, or
intervene important or conﬁdential business information. BPO service has to guarantee the security of the
customer business. Concretely, the quality of security should be analyzed from the following aspects:
• Conﬁdential data and information;
• Security auditing;
• Encryption and anti-virus protection;
• Secure physical environment.
3. Enabling Quality by Technology
The ability of the service provider to deliver quality service beyond the expectation of the outsourcing
ﬁrm has a signiﬁcant impact on the success of BPO. Technology plays an important role in improving
quality performance in BPO during the entire BPO life cycle (see Figure 2). Technology allows the rapid
development of various ready-to-use best-practice templates that suits most needed business processes. It
offers ready-to-run user interfaces and screens, in addition to the generic built-in out of the box interfaces
and integration scenarios that are compatible with most business applications and software.
Figure 2 BPO service life cycle
Proper technology used by the service provider could allow for saving a blueprint of the clients business
process, which would be used in later stages. Technology could allow for a smooth, efﬁcient transition of
data, processes and knowledge and real-time documentation updates. By enabling and leveraging the values
of standardization, automation, integration and innovation, technology impacts BPO and enhances the value
of BPO. All this shortens the transition phase and minimizes the impact and duration of the transition, and
so accelerates the time to beneﬁt, till reaching the agreed quality levels and a streamlined operation. It also
reduce the risks associated with custom code, hence increasing the quality level and customer satisfaction
during this crucial initial phase.14 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
3.1. Standardization
For the BPO provider, technology impacts the service quality through affecting the ability to standardize.
Providers can not drive any standardization strategy without having a standard technology, a uniform platform that empowers his standardization activities, while still allowing for “personalization”. For the BPO
clients, technology accommodates the exceptions related to the language or the country speciﬁc rules and
laws with the ability to satisfy the country-speciﬁc requirements and dissimilarities. Thus, allowing the
company to have a uniform standardized process execution throughout all the different divisions, units, and
countries.
The right technology enables the balance of personalization and standardization of the outsourced activities, creating the capacity to design a process that suits the speciﬁcities of each client, while maintaining
a standardization level that enables sustainable long term advantages for both the provider and the clients.
The standardization reduces the complexity of operations, and thus, helps in reaching higher reliability,
responsiveness and conformance.
Technology allows the adoption of best-practices which are built on ready-to-use templates that cover
most of the needed process practices. These templates allow for the same scenarios to be standardized across
different countries, with the availability of different language support and country speciﬁc tuning. All this
is achieved with no need for any custom-made user interface or screens; this consistent standardized userinterface decreases the time to deploy, the error rate and training needs. Moreover, this facilitates the use of
systems by occasional users due to its simplicity and intuitive design that ensures fast adoption and reduces
the risks linked to occasional users use.
While offering the gains of an integrated single system instance, multi-tenant technology allows BPO
providers to adapt the business processes for an individual customer without impacting any of the processes
of the other customers sharing the same common platform. While in BPO the gain is extended to that of
simpler operations, better support, and easier maintenance. Applying this service provider standardization
across the different clients, BPO providers provide their clients with benchmarking data that can be used to
compare their processes to other companies and other industrial standards. This can not be utilized without
following a relatively standardized process, or else the comparison would not be applicable. For example,
in an HR context, HRRP providers can provide their clients with aggregated HR key ﬁgure reporting andHongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 15
analytic data with all personal and company identiﬁers are removed. This service gives great insight into
performance and quality levels of the different business processes. For example, companies can contrast
their head count cost, their training budgets, their hiring costs and turnover. So adopting standard processes
allowed by technology not only enhances the quality levels of the service providers but also that of the BPO
buyers.
Following the standardization created by technology, the BPO providers also achieve scale beneﬁts
through a virtual centralization. In other words, technology enables the decentralized execution on the entire
business or inter-country level while ensuring the high quality through the centralization of the standarized
process blueprint. This also leads to the effect technology driven integration.
3.2. Integration
Standard interfaces and open standards are built to make it easier to integrate processes together. Their
function is not to glue a series of systems together but rather to enable consistency. Technology allows an
accurate and centralized consolidation of candidates’ information gathered from various sources. Following
standard practices and abiding to industry standard processes and applications helps create a better integrated environment. This not only reduces the need for custom code, but also allows the use of software
generic templates and interfaces with only minor conﬁguration changes. The integration between the different outsourced processes is easy, neat and tight, and the same for integration with the retained in-house
processes and systems.
Evolving technology is providing this kind of support integration. Business software providers and BPO
providers are promoting the embrace of open standard and inter-operable interfaces. The industry is moving
forward to embed web services and Service Oriented Architecture (SOA) into solutions offering, in addition
to supporting standard technologies like Business Process execution Language (BPEL). Another example
is HR–XML Consortium,
2
of which SAP is a charted member, which involves most of the HR industry
software and applications companies. This consortium is building the XML speciﬁcations for e-business and
automation of HR related data exchanges. This means more integrated business processes, and smoother
systems interfaces which will lead to better integration and performance view, and therefore, better business
2
source: http://www.hr-xml.org16 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
decisions and more effective strategies. Through these common standards total integration is becoming
more feasible, rapid and efﬁcient.
An integrated system provides the most effective data and information storage, and thereby, improves the
efﬁciency and reliability of service. For example, in an HR management context, an employee does not have
to ﬁll his/her timesheets more than once or track his/her tasks and assignment in more than one interface.
The company does not hold different versions of the employee addresses or bank accounts or does not hold
it in different places.
For the clients, technology facilitates access to a diversiﬁed pool of talent gathered from the organization career website, powered by the BPO provider, from the organization’s internal employee’s database,
employee referral programs, and the recruiters website contracted by the BPO provider. Abundance of candidates sometimes leads to hiring without posting the position, using only skill and job matching through the
uniﬁed candidates’ pool. All this enhances the quality of the candidates available for selection, and provides
BPO clients with higher quality new hires. Technology created and developed the potential information and
other resource utilization.
Quality in BPO is often linked to how the service provider can offer an integrated system on which
the client processes will run. An integrated system approach ensures greater leverage of the potential of
automation and streamlined workﬂow across different processes. A fully integrated system also ensures
a sustainable quality for the BPO buyers, maintained throughout systems deployment, maintenance and
upgrades. Furthermore, an integrated solution makes achieving centralized operations possible without the
need of location centralization but through “virtual” centralization (BPO Excellence (2006)). The virtual
centralization allows location decisions to be opportunistic in enabling BPO providers to not sacriﬁce quality. For example, in HR service delivery, the contact agents and case workers can be located in off-shore
locations, while policy experts and accounts managers can be located near the customer.
Moreover, integrated processes, applications and systems allow for more automation and more integrated
workﬂows that link different business processes together and streamline the different activities. For example, production scheduling needs to be linked with job requisition, which in turn needs integration with
project budgeting and accounts payable. This kind of integration enhances the accuracy and effectiveness
of the process execution, but it is impossible without proper support from the underlying technology and
knowledge which are the key advantages of BPO providers.Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 17
3.3. Automation
Standardization and centralization facilitate and leverage another driver for quality called automation. Technology directly drives the process automation through workﬂows, paperless document management and
online interaction. Technology can automate change requests through real-time workﬂows that make process governance more effective and more efﬁcient. Another effect of applying technology to the BPO process control is the remote monitoring and supervision of the outsourced activities execution. Technology
enables BPO buyers to closely monitor and supervise the process operations. Through automation, technology facilitates this control mechanism, making the process steps clearer.
Automation not only speeds the process and decreases cost, but also delivers a more secure and agile
service with an enhanced quality of process and new-hires. For example, workﬂow automation increases the
quality perceived by BPO buyers, as well as their job candidates by making the process simpler. Automation
transforms processes from the inefﬁcient batch process ﬂow into a synchronized real-time ﬂow. It provides
quicker access to accurate and real-time information through streamlined data routing, thereby increasing
the quality of decision making. The process activities are more visible, so the process and its operational
quality are more transparent, better measured and better managed.
Another supporting example for advancement in workﬂow management and self-service techniques
applies for the HR-Recruitment process. Through self-service, technology allows automated workﬂows to
be driven by the employees, or even by candidates triggering a job application. Moreover, technology allows
for alerts and reminders to be automatically sent to streamline the ﬂow along the process steps. In addition, the scheduling and communication with candidates is managed through the same workﬂow interface.
Through a self-service and single user interface, hiring managers can take control of the screening process,
starting with job requisition and continuing through accessing candidates’ information, testing, scheduling
appointments, and hiring. Other systems allow candidates to self-manage their information and applications
submitted for various vacancies, like the career sections of Shell, Yahoo, Microsoft, and IBM. Technology enables these seamless ﬂows with the added protection of secured authorization systems with tracking
capabilities.
The impact of automation is also evident in the HR-Recruitment process. The different BPO service
providers conﬁrmed the value and emphasis of automation. For example, the job requisition process is
totally automated through different workﬂow routes which include:18 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
• Direct online job deﬁnition by the hiring manager;
• Candidate pool screening and skill mining;
• Existing and new candidates’ data ﬂows directly to and from the candidate pool database;
• Testing, assessment and sometime background checks by third party providers.
In addition, as the task of training, evaluating and retaining employees has moved to the center of corporate strategic planning, new technology tools have emerged to enable a changing role for learning within
business such as the Learning Management System (LMS) in outsourced E-Learning processes. It provides
end users with a single point of access to disparate learning sources. It also has the functionality for design,
management and assessment of learning, enabling an excellent control for direct Human Resource Development (HRD). From a technical perspective, LMS is a technology to link and integrate all the other technical
components, and also other existing ERP and HR applications.
In general, organizations can leverage the technologies of automatic data extraction and data mining techniques provided by service providers to reduce the HR recruitment cycle, increasing its accuracy and reliability, and hence, the quality of the recruitment outcome. The customer speciﬁc conﬁgurations, switches,
and options results in more effective yet standardized implementations that are easier to implement, maintain, and upgrade. In summary, the effect of process automation is the reduction in the frequency of errors,
and improvement in error detection and correction. This improves the reliability and accuracy of the process
outsourced and adds to its perceived quality.
3.4. Innovation
Though innovation was never a main drivers for outsourcing, many buyers now perceive innovation as one
of the biggest advantages of outsourcing. Technology drives innovation and business processes adoption
directly, such as the development of different internet tools. Business practices could also lead the process innovation, but technology facilitates it a self-service technique. Technology enables innovations like
intelligent collection, analysis, and mining of information to create better visibility, which in tur creates
competitive advantage. Recently, in HR management, innovations have been created and applied, such as
corporate blogs used as a recruitment marketing tool in the recruitment trends of sharing information with
candidates. Another example is Really Simple Syndication (RSS), an innovative technology to publicize
current vacancies to potential candidates.Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 19
Given that the BPO underling technology evolves at an acceptable cost, innovation would lead to higher
levels of quality. Newly created innovation supports a continuous increase in operational efﬁciency by
allowing easy adoption of changes in processes through the high level of personalization and conﬁguration options. Innovation allows companies to connect with candidates faster and select and recruit more
accurately. An example of technological innovation that will affect the BPO quality is the use of Wiki
web technology that enhances the quality of data support and knowledge management processes. Another
example is the advancements in Interactive Voice Response (IVR) and voice recognition technologies. The
service reliability of contact centers will increase dramatically by adding the feature of human dialogue,
while achieving the economical gains of IVR.
In the HR-Recruitment process, innovative practices improve the quality of the recruitment process. Gartner uses data extraction techniques, powered by BrassRing recruitment solutions,
3
to extract job candidates’
information from uploaded ﬁles into structured ﬁelds, which is then presented to and conﬁrmed by the candidates in real-time. This eliminates the need for internal data entry and the associated risk of data errors.
Many, if not all, of the BPO providers offer data mining techniques to automate the initial screening process, by intelligent matching of the vacancies’ requirements with the candidates’ skill information. This is
another example of innovation and automation adding to the efﬁciency and effectiveness of the BPO process.
Additionally, HRRP providers offer their business process modeling and predictive analysis techniques
based on HR historical data. These predictive analysis, modeling, and planning tools allow organizations
to analyze the effect of tactical decisions, using tools to compare the results to budgets and forecasts, use
what-if analysis to model plan modiﬁcations, and ﬁne-tune business plans (White (2005)). Using these innovative tools in an HR context, the process ﬂexibility is increased, so that managers can optimize temp-labor
recruitment schedules and ensure they match work peaks and hiring demands, like at times of marketing
initiative or new product campaigns. These provider-led innovations help companies to analyze possible
scenarios and related consequences to make better decisions.
4. Conclusion
This study developed the theoretical dimensions of service quality for the BPO industry and explored how
technology affects these quality factors. Technology is a main enabler of BPO and a major factor of its
3
Source: http://www.kenexa.com/Solutions/RecruitmentProcessOutsourcing20 Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology
success. The effect of technology in BPO is built upon standardization, integration, automation and innovation. These factors not only drive and maintain but also improve the reliability, tangible, and conformance,
responsiveness and ﬂexibility, security along the whole of process activities.
As the application objective of BPO is shifting towards that of enhancing the competitiveness of BPO
buyers, the value of the technology driving BPO is increasing. The use of standard integration interfaces,
best-practice templates and conﬁguration options enhance BPO quality. Technology provides the execution
platform to enable beneﬁts, like speeding up the adoption of best-practices, interfaces and new upgrades,
and providing a sustainable quality level during maintenance activities. The underlying technology provides
business value to both the buyers and the service providers. Due to this value, companies seeking BPO are
actively seeking ways to leverage the advantages by making the most of technology. Therefore, this study
provides the quality factor and standards driven by technology when selecting a BPO service provider.
However, the quality dimensions and quality enablers discussed above are not exhaustive and overlapping
exists among them. We hope to provide managerial implications to practitioners by this study. Given the
boom of BPO applications and the limitations on the research of BPO quality theory, it is also expected to
invite further study in greater depth and width. A broadly recognized quality framework for BPO would be
helpful to push BPO development forward.
References
Center-Everest Outsourcing. 1992. Governing attitudes: 12 best practices in managing outsourcing relationships. Everest Group. Available at http://jobfunctions.bnet.com/abstract.aspx?docid=
58913&amp;tag=content;col1.
Cronin, J J, S A Taylor. 1992. Measuring service quality: A reexamination and extension. Journal of Marketing 56(3)
55–68.
BPO Excellence. 2006. How to make BPO sustainable and the role of technology in achieving this. SAP Seminar
June.
Erlanger, L. 2006. Business process outsourcing: putting IT in the director’s chair. InforWorld 27 Feb.
Garvin, D A. 1987. Competing on the eight dimensions of quality. Havard Business Review Nov-Dec 101–109.
Gibson, S. 2005, BPO: The Next frontier. IT Management Articles in eWeek. Available at http://www.eweek.
com/c/a/IT-Management/BPO-The-Next-Frontier/.Hongyan Li and Joern Meissner: Improving Quality in Business Process Outsourcing through Technology 21
Grover, V, M J Cheon, J T C Teng. 1996. The effect of service quality and partnership on the outsourcing of information
systems function. Journal of Management Information Systems 12(4) 89–116.
Kettinger, W J, C C Lee. 1997. Pragmatic perspectives on the measurement of information systems service quality.
MIS Quarterly 21(2) 223–240.
Kumar, D R. 2004. Secrets of organization success — business process outsourcing. Emerio Corporation Pet Ltd,
Singapore. Available at http://www.Emeriocorp.com.
Ma, Q, J M Pearson, S Tadisina. 2005. An exploratory study into factors of service quality for application service
providers. Information and Management 42 1067–1080.
Parasuraman, A, L L Berry, V A Zeithaml. 1991. Reﬁnement and reassessment of the SERQUAL scale. Journal of
Retailing 67(4) 420–450.
PricewaterhouseCoopers. 2002. Global human capital survey. Available at http://www.retentionengine.
com/pdf/IBM\%20Human\%20Capital\%20Survey.pdf.
Reeves, C, D Bednar. 1994. Deﬁning quality: alternatives and implications. Academy of Management Review 19(3)
419–445.
RES. 2005. The business case for RPO. White paper. Available at http://www.resjobs.com/RPO/.
Slack, N, S Chambers, R Johnston. 2004. Operations Management (4th edition). Prentice Hall, Essex.
Terra Equa. 2005. Study on BPO satisfaction conducted by EquaTerra, managing offshore and information week.
Available at http://ubmtechnology.mediaroom.com/index.php?s=43&amp;item=413.
White, C. 2005. Bridging the planning and business performance gap. BI Research . Available at http://www.
sap.com/platform/netweaver/pdf/BWP_AR_BI_Research.pdf.
Zeithaml, V, L Berry, A Parasuraman. 1993. The nature and determinants of customer expectations of service quality.
Journal of the Academy of Marketing Science 21(1) 1–12.</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_consistency_and_parallel_DB"/>
        <doc>A Latency and Fault-Tolerance Optimizer
for Online Parallel Query Plans
Prasang Upadhyaya
University of Washington
prasang@cs.uw.edu
YongChul Kwon
University of Washington
yongchul@cs.uw.edu
Magdalena Balazinska
University of Washington
magda@cs.uw.edu ###
ABSTRACT
We address the problem of making online, parallel query
plans fault-tolerant: i.e., provide intra-query fault-tolerance
without blocking. We develop an approach that not only
achieves this goal but does so through the use of dier-
ent fault-tolerance techniques at dierent operators within a
query plan. Enabling each operator to use a dierent fault-
tolerance strategy leads to a space of fault-tolerance plans
amenable to cost-based optimization. We develop FTOpt, a
cost-based fault-tolerance optimizer that automatically se-
lects the best strategy for each operator in a query plan
in a manner that minimizes the expected processing time
with failures for the entire query. We implement our ap-
proach in a prototype parallel query-processing engine. Our
experiments demonstrate that (1) there is no single best
fault-tolerance strategy for all query plans, (2) often hybrid
strategies that mix-and-match recovery techniques outper-
form any uniform strategy, and (3) our optimizer correctly
identies winning fault-tolerance congurations.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Fault tolerance, modeling
techniques; H.2.4 [Database Management]: Systems|
Parallel databases,Query processing
General Terms
Performance ###
1. INTRODUCTION
The ability to analyze large-scale datasets has become a
critical requirement for modern business and science. To
carry out their analyses, users are increasingly turning to-
ward parallel database management systems (DBMSs) [14,
41, 45] and other parallel data processing engines [10, 15, 20]
deployed in shared-nothing clusters of commodity servers.
In many systems, users can express their data processing
needs using SQL or other specialized languages (e.g., Pig
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGMOD’11, June 12–16, 2011, Athens, Greece.
Copyright 2011 ACM 978-1-4503-0661-4/11/06 ...$10.00.
Latin [30], DryadLINQ [48]). The resulting queries or scripts
are then translated into a directed acyclic graph (DAG)
of operators (e.g., relational operators, maps, reduces, or
other [20]) that execute in the cluster.
An important challenge faced by these systems is fault-
tolerance. When running a parallel query at large scale,
some form of failure is likely to occur during execution [9].
Existing systems take two radically dierent strategies to
handle failures: parallel DBMSs restart queries if failures oc-
cur during their execution. The limitation of this approach
is that a single failure can cause the system to reprocess a
query in its entirety. While this is not a problem for queries
running across a small number of servers and for a short
period of time, it becomes undesirable for long queries us-
ing large numbers of servers. In contrast, MapReduce [10]
and similar systems [15] materialize the output of each op-
erator and restart individual operators when failures occur.
This approach limits the amount of work repeated in the
face of failures, but comes at the cost of materializing all
intermediate data, which adds signicant overhead even in
the absence of failures. Furthermore, because MapReduce
materializes data in a blocking fashion, this approach pre-
vents users from seeing results incrementally. Partial results
are a desirable feature during interactive data analysis now
commonly performed with these systems [43].
In this paper, we study the problem of providing users
both the ability to see early results as motivated by online
query processing [17, 18] and achieve a low expected total
runtime. We thus seek to enable intra-query fault-tolerance
without blocking and we want to do so in a manner that min-
imizes the expected total runtime in the presence of failures.
Other objective functions could also be useful (e.g., minimize
runtime without failures subject to a constraint on recovery
time.) We choose to minimize the sum of time under normal
processing and time in failure recovery. This function com-
bines high-performance at runtime with fast failure recovery
into a single objective. We want to minimize this function
while preserving pipelining.
Recent work [43] has also looked at the problem of com-
bining pipelining and fault-tolerance: they developed tech-
niques for increased data pipelining in MapReduce. This
system both pipelines and materializes data between opera-
tors. We observe, however, that data materialization is only
one of several strategies for achieving fault-tolerance in a
pipelined query plan. Other strategies are possible including
restarting a query or operator but skipping over previously
processed data [21, 24] or checkpointing operator states and
restarting from these checkpoints [11, 21]. Additionally, themost appropriate fault-tolerance method may depend on the
available resources, failure rates, and query plan properties.
For example, an expensive join operator may need to check-
point its state while an inexpensive lter may simply skip
over previously processed data after a failure.
Given these observations, we develop (1) a framework that
enables mixing-and-matching of fault-tolerance techniques in
a single, pipelined query plan and (2) FTOpt, a cost-based
fault-tolerance optimizer for this framework. Our frame-
work enables intra-query fault-tolerance without blocking,
thus preserving pipelining. Given a query plan and infor-
mation about the cluster and expected failure rates, FTOpt
automatically selects the fault-tolerance strategy for each
operator in a query plan such that the overall query runtime
with failures is minimized. We call the resulting congura-
tion a fault-tolerance plan. In our fault-tolerance plans, each
operator can individually recover after failure and it can re-
cover using a dierent strategy than other operators in the
same plan. In summary, we make the following contribu-
tions:
1. Extensible, heterogeneous fault-tolerance framework.
We propose a framework that enables the mixing and
matching of dierent fault-tolerance techniques in a
single distributed, parallel, and pipelined query plan.
Our framework is extensible in that it is agnostic of the
specic operators and fault-tolerance strategies used.
We also describe how three well-known strategies can
be integrated into our framework (Section 4).
2. Fault-tolerance optimizer. We develop a cost-based
fault-tolerance optimizer. Given a query plan and a
failure model, the optimizer selects the fault-tolerance
strategy for each operator that minimizes the total
time to complete the query given an expected num-
ber of failures (Section 5).
3. Operator models for pipelined plans. We model the
processing and recovery times for a small set of repre-
sentative operators. Our models capture operator per-
formance within a pipelined query plan rather than in
isolation. They are suciently accurate for the fault-
tolerance optimizer to select good plans yet suciently
simple for global optimization using a Geometric Pro-
gram Solver [3]. We also develop an approach that
simplies the modeling of other operators within our
framework thus simplifying extensibility (Section 5.3).
We implemented our approach in a prototype parallel
query processing engine. The implementation includes our
new fault-tolerance framework, specic per-operator fault-
tolerance strategies for a small set of representative opera-
tors (select, join, and aggregate
1
), and a MATLAB mod-
ule for the FTOpt optimizer. Our experiments demon-
strate that dierent fault-tolerance strategies, often hybrid
ones, lead to the best performance in dierent settings: for
the congurations tested, total runtimes with as little as
one failure diered by up to 70% depending on the fault-
tolerance method selected. These results show that fault-
tolerance can signicantly aect performance. Additionally,
our optimizer is able to correctly identify the winning fault-
tolerance strategy for a given query plan. Overall, FTOpt
1
As in online aggregation [18], aggregates can occur at top
of plans. Our prototype uses a standard aggregate operator
but it could be replaced with an online one.
Data 
part. 1
O11
Repartition
O21
Input data
on disk
Compute node
Partition 1 of operator 1 
O31
Data 
part. 2
O12
Data 
part. N
O1N
…
O22
O2X
Fault-tolerance strategy 1
Repartition
O31
O3Y
Partition 1 of operator 2 
strategy 2 strategy 3
… …
Figure 1: Parallel query plan comprising three op-
erators (O1, O2, O3) and one input from disk. Each
operator is partitioned across a possibly dierent
number of nodes. Data can be re-partitioned be-
tween operators. Fault-tolerance strategies are se-
lected at the granularity of operators.
is thus an important component of parallel data processing,
enabling performance gains similar in magnitude to several
other recently proposed MapReduce optimizations [22, 26].
2. MODEL AND ASSUMPTIONS
Query Model and Resource Allocation. In paral-
lel data processing systems, queries take the form of di-
rected acyclic graphs (DAGs) of operators that are dis-
tributed across servers in a cluster as illustrated in Figure 1.
Servers are also referred to as nodes. Each operator can
be partitioned and these partitions then execute in parallel
on the same or on dierent nodes. Multiple operators can
also share the same nodes. In this paper, we focus on non-
blocking query plans, which take the form of trees (rather
than DAGs), where operators are scheduled and executed at
the same time, and where data is pipelined from one opera-
tor to the next, producing results incrementally. We assume
that aggregation operators, if any, appear only at the top
of a plan. Since input data comes from disk, it can be read
and consumed at a steady pace (there are no unexpected
bursts as in a streaming system for example). If a query
plan is too large for all operators to run simultaneously, our
approach will optimize fault-tolerance for pipelined subsets
of the plan, materializing results at the end.
Fault-tolerance choices and resource allocation are in-
tertwined: an operator can perform more complex fault-
tolerance if it is allocated a greater fraction of the compute
resources. In addition to fault-tolerance strategies, our op-
timizer computes the appropriate allocation of resources to
operators. Due to space constraints, however, in this paper,
we assume that each operator is partitioned across a given
number of compute nodes and is allocated its own core(s)
and disk on that node. For the resource allocation details,
we refer the reader to Appendix A.5.
Failure Model. In a shared-nothing cluster, dierent
types of failures occur. Our approach handles a variety of
failures from process failures to network failures. To simplify
the presentation, we rst focus on process failures: i.e., we
assume that each operator partition runs in its own process
and that these processes crash and are then restarted (with
an empty state) independently of one another. We come
back to more complex failures in Section 5.5.
To make fault-tolerance choices, our optimizer must know
the likelihood for dierent types of failures. If ni is the total
number of processes allocated to operator i, we assume thatthe expected number of failures during query execution for
that operator is given by: zi =
ni
n Z where n =
P
j2O nj , O
is the set of all operators in the plan, and Z is the expected
number of process failures for the query. Z can be estimated
from the observed failure rates for previous queries and ad-
ministrators typically know this number [9]. We assume
Z to be independent of the chosen fault-tolerance plan. Z
depends on the query runtime, whose order of magnitude
can be estimated by FTOpt as the total runtime without
fault-tolerance and without failures (we show that results
are robust to small errors in Z&apos;s value in Section 6.6).
Operator Determinism. We assume that individual
operator partitions are deterministic, i.e., an operator par-
tition produces an identical output when it processes the
same input tuples in the same order. This is a common
assumption [20, 47, 37, 2, 36, 23] and most relational opera-
tors are deterministic. In a distributed system, however, the
order in which input tuples reach an operator partition may
not be deterministic. Our approach handles this scenario.
3. RELATED WORK
Fault-Tolerance in Relational DBMSs. Commercial
relational DBMSs provide fault-tolerance through replica-
tion [6, 34, 40]. Similarly, parallel DBMSs [14, 41, 45] use
replication to handle various types of failures. Neither, how-
ever, provides intra-query fault-tolerance [32].
Main-memory DBMSs [25, 35, 28] use a variety of check-
pointing strategies to preserve the in-memory state of their
databases. In contrast, our approach preserves and recovers
the state of ongoing computations.
Fault-Tolerance in MapReduce-type systems. The
MapReduce framework [10] provides intra-query fault-
tolerance by materializing results between operators and
re-processing these results upon operator failures. This
approach, however, imposes a high runtime overhead and
prevents users from seeing any output until the job com-
pletes. In Dryad [20], data between operators can either be
pipelined or materialized. In contrast, we strive to achieve
both pipelining and fault-tolerance at the same time. We
also study how to decide when to materialize or check-
point data. Recent work [47] applies MapReduce-style fault-
tolerance to distributed databases by breaking long-running
queries into small ones that execute and can be restarted in-
dependently. This approach, however, supports only a spe-
cic type of queries over a star schema. In contrast, we
explore techniques that are more generally applicable. Re-
cent work also introduced the ability to partly pipeline data
in Hadoop [43], a MapReduce-type platform. This work is
complementary to ours as it retains the use of materializa-
tion throughout the query plan for fault-tolerance purposes.
Other Fault-Tolerance Strategies. In the distributed
systems and stream processing literatures, several additional
fault-tolerance strategies have been proposed [11, 21, 37].
All these strategies involve replication. One set of tech-
niques is based on the state-machine approach. Here, the
same computation is performed in parallel by two process-
ing nodes [2, 36, 37]. We do not consider such techniques in
this paper because of their overhead: to tolerate even a sin-
gle failure, they require twice the resources. The second set
of techniques uses rollback recovery methods [11, 21], where
the system takes periodic snapshots of its state that it copies
onto stable storage (i.e., into memory of other nodes or onto
disk). We show how to integrate the latter techniques into
our fault-tolerance optimization framework (Section 4.2).
Recently, Simitsis et. al. [38] studied the problem of
selecting fault-tolerance strategies and recovery points for
ETL 
ows. Similar to us they consider using dierent fault-
tolerance strategies within a single 
ow. In contrast to our
work, they do not propose a general heterogeneous fault-
tolerance framework, do not have individually recoverable
operators, and do not optimize for overall latency nor show
how fault-tolerance choices aect processing latencies.
Additional Related Work. Hwang et al. [19] studied
self-conguring high-availability methods. Their approach
is orthogonal to our work as it is based on a uniform check-
pointing strategy and optimizes the time when checkpoints
are taken and the backup nodes where they are saved.
Techniques for query suspend and resume [4, 5] use roll-
back recovery but are otherwise orthogonal to our work.
Phoenix/App [27] explores the problem of heterogeneous
fault-tolerance in the context of web enterprise applications.
This approach identies three types of software compo-
nents: Persistent, Transactional, and External depending on
the fault-tolerance strategy that each uses (message logging
with checkpointing, transactions, or nothing respectively).
Phoenix/App then denes dierent \interaction contracts&quot;
for each combination of component types. Each contract
implements a dierent protocol with dierent guarantees.
Thus in Phoenix/App, the protocol depends on the fault-
tolerance capabilities of the communicating components. In
contrast, our approach enables the mixing-and-matching of
fault-tolerance strategies without changes to the protocol.
4. FRAMEWORK FOR HETEROGENEOUS FAULT-TOLERANCE
We present a framework for mixing and matching fault-
tolerance techniques. Our framework relies on concepts
from the literature including logging, acknowledging, and re-
playing tuples as previously done in uniform fault-tolerance
settings [21, 37] and \contract-based&quot; methods for query
suspend-resume [4]. Our contribution lies in articulating
how these strategies can be used to enable fault-tolerance het-
erogeneity. We also discuss how three fault-tolerance tech-
niques from the literature can be used within our framework.
4.1 Protocol
To enable heterogeneous fault-tolerance between consec-
utive operators in a query plan, we isolate these operators
by xing the semantics of their interactions through a set
of four rules. These rules enable each operator to be indi-
vidually restartable without requiring any blocking materi-
alization as in MapReduce and also without requiring that
all operators use the same fault-tolerance strategy.
In our framework, as in any parallel data processing sys-
tem, operators receive input tuples from their upstream
neighbors; they process these tuples and send results down-
stream. For example, in Figure 1, each partition of operator
O2 receives data from each O1 partition and sends data to
all O3 partitions. If an operator partition such as O21 fails,
a new instance of the operator partition is started with an
empty state. To recover the failed state, in our framework,
the new instance can read any state persistently captured by
the operator&apos;s fault-tolerance strategy. It can also ask up-
stream operators to resend (a subset) of their data. To en-able such replays, tuples must have unique identiers, which
may or may not be visible to applications, and operators
must remember the output they produced. For this, we de-
ne the following two rules:
Rule 4.1. Each relation must have a key.
Rule 4.2. Producer replay guarantee. Upon request, an
operator, must regenerate and resend in order and without
duplicates any subset of unacknowledged output tuples.
Acknowledgments mentioned in this rule help reduce the
potential overhead of storing old output tuples by bound-
ing how much history must be retained [21, 37]. In our
framework, acknowledgments are optional and are sent from
downstream operators to upstream ones. For example, once
all operator partitions O21 through O2X that have received
an input tuple t from operator partition O11 acknowledge
this tuple, the tuple need no longer be retained by O11.
Upon sending an acknowledgment, an operator promises
never to ask for the corresponding tuple again. Formally,
Rule 4.3. Consumer progress guarantee. If an operator
acknowledges a tuple rx, it guarantees that, even in case of
failure, it will never ask for rx again.
Most parallel data processing systems use in-order com-
munication (e.g., TCP) between operators. In that case, an
operator can send a single message with the identier of a
tuple rx to acknowledge all tuples up to and including rx.
When a failure occurs and an operator restarts with an
empty state, most fault-tolerance techniques will cause the
operator to produce duplicate tuples during recovery. To en-
sure that an operator can eliminate duplicates before send-
ing them downstream, we add a last requirement:
Rule 4.4. Consumer Durability Guarantee. Upon re-
quest, an operator Od must produce the identier of the
most recent input tuple that it has received from an upstream
neighbor Ou.
Together, these four rules enable a parallel system to en-
sure that it produces the same output tuples in the same
order with and without failure (the tuples may still be de-
layed due to failure recovery.) They also enable operators
to be individually restartable and the query plan to be both
pipelined and fault-tolerant, since data can be transmitted
at anytime between operators. Finally, the framework is
agnostic of the fault-tolerance method used as long as the
method works within the pre-dened types of interactions.
From the above four rules, only the \Producer replay guar-
antee&quot; rule potentially adds a visible overhead to the system
since it requires that a producer be able to re-generate (part
of) its output.
2
A no-cost solution to satisfy this rule is
for an operator to restart itself upon receiving a replay re-
quest. With this strategy, an operator failure can cause a
cascading rollback eect, where all preceding operators in
the plan get also restarted. This approach is equivalent to
restarting a subset of the query plan after a failure occurs
and is no worse than what parallel DBMSs do today. Alter-
natively, an operator could write its output to disk. Finally,
2
Our framework also requires unique identiers for tuples.
In our implementation, we create unique identiers consist-
ing of 3 integers [44]; for the 512 byte tuples used in our
experiments the space overhead is less than 2.5%.
some operators, such as joins, can also easily re-generate
their output from their state without the need to log their
output. Each of these solutions leads to dierent expected
query runtimes with and without failures. Our optimizer
is precisely designed to select the correct strategy for each
operator (from a pre-dened set of strategies) in a way that
minimizes the total runtime with failures for a given query
plan as we discuss further below.
4.2 Concrete Framework Instance
We now discuss how three fault-tolerance strategies from
the literature can be integrated into our framework.
Even though the operators in our framework are deter-
ministic (see Section 2), in a distributed setting tuples may
arrive in dierent interleaved order on dierent inputs. We
develop a low-overhead method {based on lightweight log-
ging of information about input tuple processing order{ to
ensure determinism in this case, but we omit it due to space
constraints and refer the reader to Appendix A.4.
Strategy NONE. Within our framework, an operator
can choose to do nothing to make itself fault-tolerant. We
call this strategy NONE. To ensure that it can recover from
a failure, such an operator can simply avoid sending any ac-
knowledgments upstream. Upon a failure, that operator can
then request that its upstream neighbors replay their entire
output. This strategy is analogous to the upstream backup
approach developed for stream processing engines [21].
As in upstream backup, operators such as select or project
that do not maintain state between consecutive tuples (i.e.,
\stateless operators&quot;) can send acknowledgments in some
cases: e.g., if an input tuple r makes it through a selection to
generate the output q and is acknowledged by all operators
downstream, then r can be acknowledged. Unlike upstream
backup, which uses dierent types of acknowledgments [21],
our approach uses only one type of acknowledgments facili-
tating heterogeneous fault-tolerance. This approach of skip-
ping over input data during recovery has also been used for
resumptions of interrupted warehouse loads [24].
To handle a request for output tuples, a stateless oper-
ator can fail and restart itself to reproduce the requested
data. For stateful operators (i.e., operators such as joins
that maintain state between consecutive tuples), a more ef-
cient strategy is to maintain an output queue and replay
the requested data [21]. Such a queue, however, can still
impose a signicant memory overhead and an I/O overhead
if the queue is written to disk. We observe, however, that
stateful relational operators need not keep such output queue
but, instead, can re-generate the data from their state. We
implement this strategy and use it in our evaluation.
Strategy MATERIALIZE. An alternate rollback re-
covery approach consists in logging intermediate results be-
tween operators as in MapReduce [10]. While CHCKPT
speeds-up recovery for the checkpointed operator itself, MA-
TERIALIZE potentially speeds-up recovery for downstream
operators: to satisfy a replay request, an operator can sim-
ply re-read the materialized data. Since materialized output
tuples need never be generated again, an operator can use
the same acknowledgement and recovery policy as in NONE.
Strategy CHCKPT. This strategy is a type of rollback
recovery strategy where operators&apos; state is periodically saved
to stable storage. Because our framework recovers operators
individually, it requires what is called uncoordinated check-
pointing with logging [11]. One approach that can directly beapplied is passive standby [21], where operators take periodic
checkpoints of their state, independently of other operators.
Our framework requires that an operator save sucient
information to guarantee the consumer progress, consumer
durability, and producer replay guarantees. For this, the op-
erator must log its state (e.g., partial aggregates, join hash
tables) and, when applicable, its output queue. The opera-
tor can acknowledge checkpointed input tuples. Upon fail-
ures, the operator restarts from the last checkpoint. As an
optimization, operators can checkpoint only delta-changes
of their state [11]. Other optimizations are also possible [11,
19, 23] and can be used with our framework.
Unlike NONE and MATERIALIZE, with CHCKPT
blocking operators also benet from fault-tolerance provi-
sioning as they can checkpoint their state periodically and
restart from the latest checkpoint after a failure.
3
In summary, while our framework imposes constraints
on operator interactions, all three of these common fault-
tolerance strategies can easily be incorporated into it.
5. FTOpt
FTOpt is an optimizer for our heterogeneous fault-
tolerance framework. FTOpt runs as a post-processing step:
it takes as input (a) a query plan selected by the query
optimizer and annotates it with the fault-tolerance strate-
gies to use. The optimizer also takes as input (b) informa-
tion about the cluster resources and cluster failure model,
and (c) models for the operators in the plan under dier-
ent fault-tolerance strategies. FTOpt produces as output
a fault-tolerance plan that minimizes an objective function
(i.e., the expected runtime with failures) given a set of con-
straints (that model the plan).
FTOpt&apos;s fault-tolerance plans have three parts: (1) a
fault-tolerance strategy for each operator, (2) checkpoint fre-
quencies for all operators that should checkpoint their states,
and (3) an allocation of resources to operators. As indicated
in Section 2, however, we do not discuss resource allocation
in this paper due to space constraints. We assume a given
resource allocation to operators.
For a given query plan, the optimizer&apos;s search space thus
consists of all combinations of fault-tolerance strategies. In
this paper, we use a brute-force technique to enumerate
through that search space and leave more ecient enumera-
tion algorithms for future work. For each such combination,
FTOpt estimates the expected total runtime with failures,
the optimal checkpoint frequencies and, optionally, an allo-
cation of resources to operators [44]. It then chooses the
plan with the minimum total runtime with failures.
5.1 Geometric Model
As it enumerates through the search-space, given a po-
tential fault-tolerance plan, in order to select optimal check-
point frequencies and estimate the total runtime with fail-
ures for the plan, FTOpt uses a geometric programming
(GP) framework. GP allows expressions that model resource
scaling (for resource allocation) and non-linear operator be-
havior, but still nds a global minima for the model [3].
In a geometric optimization problem, the goal is to mini-
mize a function f0(~x), where ~x is the optimization variable
3
FTOpt works irrespective of a blocking operator&apos;s place-
ment in a query plan. We focus on online query processing
since FTOpt is especially useful for such plans: it enables
fault-tolerance without creating unnecessary blocking.
vector. The optimization is subject to constraints on other
functions fi(~x) and gi(~x). All of ~x, g(~x), and f(~x) are con-
strained to take the following specic forms:
 ~x = (x1; : : : ; xn) such that 8i xi &gt; 0; xi 2 R.
 g(~x) must be a monomial of the form cx
a1
1 x
a2
2
: : : x
an
n
with c &gt; 0 and ai 2 R.
 f(~x) must be a posynomial dened as a sum of one or
more monomials. Specically, with ck &gt; 0 and aik 2
R: f(~x) =
Pk=K
k=1
ckx
a1k
1 x
a2k
2
: : : x
ank
n .
The optimization is then expressed as follows:
minimize f0(~x)
subject to fi(~x)  1; i = 1; : : : ; m
gi(~x) = 1; i = 1; : : : ; p
In our case, ~x = [i2O(ci; Ni; x
N
i
; x
RD
i
; x
RS
i ) where O is
the set of all operators in the query plan and [ denotes
concatenation. Each operator has a vector of variables that
includes: ci, its checkpoint frequency, Ni, the number of
nodes assigned to it (we assume that it is xed in this pa-
per), and x
N
i
, x
RD
i
, and x
RS
i
, which capture the average time
between two consecutive output tuples, requested replay tu-
ples, and \units of recovery&quot; (a measure of recovery speed),
respectively. Tables 1 and 2 summarize these parameters
(we come back to the tables shortly).
Our objective function, f0(~x) = Ttotal, is the total time
to execute the query including time spent recovering from
failures. We dene it more precisely in Sections 5.2 and 5.3.
Our constraints comprise framework and operator con-
straints. The former constrain how operator models are
composed: (a) the average input and output rates of con-
secutive operators must be equal since the query plan is
pipelined, (b) aggregate input and output rates for opera-
tors cannot exceed the network and processing limits, and
(c) if an operator uses an output queue, it must either check-
point its output queue to disk frequently enough, or must
receive acknowledgements from downstream operators fre-
quently enough to never run out of memory. Individual op-
erators can add further constraints (see Section 5.3).
5.2 Objective Function
FTOpt minimizes the following cost function, that cap-
tures the expected runtime of a query plan:
Ttotal = max
p2P
 
Tpd +
i=Xd1
i=1
Dpi
!
+
X
i2O
zi  Ri (1)
The rst term is the total time needed to completely pro-
cess the query including the overhead of fault-tolerance if no
failures occur The second term is the expected time spent
in recovery from failures. Failure recovery can be added on
top of normal processing because, with our approach, when
a failure occurs, it blocks the entire pipeline. Indeed, even
if one operator partition fails, operators upstream from that
partition stop executing normally and take part in the re-
covery. A side-eect of this approach is that recovering a
single operator partition or recovering all partitions yield
approximately the same recovery time.
In more detail, for the rst term, P is the set of all paths
from the root of the query tree to the leaves. For a given
path p 2 P of length d, the root is labeled with p1 and
the leaf with pd; Dpi
is the delay introduced by operator pi Table 1: Functions capturing operator behavior.
Delay to produce the rst tuple
DN
() Average delay to output rst tuple during normal pro-
cessing (with fault-tolerance overheads).
DRD
() Average delay to produce rst tuple requested by a down-
stream operator during a replay.
DRS
() Average delay to the start of state recovery on failure.
Average processing time
x
N
() Average time interval between successive output tuples
during normal runtime (with fault-tolerance overheads).
x
RD
() Average time interval between successive output tuples
requested by a downstream operator.
x
RS
() Average time-interval between strategy-specic \units of
recovery&quot; (e.g., checkpointed tuples read from disk).
Acknowledgement interval, a(), sent to upstream nodes.
Table 2: Operator behavior parameters().
Query parameters
jIuj Number of input tuples received from upstream operator u.
jIj Number of tuples produced by current operator.
Operator parameters
t
cpu Operator cost in terms of time to process one tuple.
t
io
The time taken to write a tuple to disk.
Runtime parameters
x
N
u Average inter-tuple arrival time from upstream operator u in
normal processing.
F Fault-tolerance strategy.
c Number of tuples processed between consecutive checkpoints.
N The number of nodes assigned to the operator.
Surrounding fault-tolerance context
ad Maximum number of unacknowledged output tuples.
x
RD
u Average inter-tuple arrival time from upstream operator u
during a replay.
where the delay is dened as the time taken to produce its
rst output tuple from the moment it receives its rst input
tuple; and, Tpd
is the time taken by the leaf operator to
complete all processing after receiving its rst input tuple.
Tpd = Dpd +x
N
pd
jIj, where jIj is the number of output tuples
produced by the leaf operator. Dpi and Tpd depend on the
input tuple arrival rate at an operator, which depend on how
fast previous operators are in the pipeline. We capture these
dependencies with constraints as mentioned in Section 5.1.
For the second term, O is the set of all operators in the
tree. For operator i 2 O, zi is the expected number of fail-
ures during query execution, and Ri is the expected recovery
time from a failure. We estimate zi using an administrator-
provided failure model as described in Section 2.
To adapt Ttotal to be a posynomial, we need to eliminate
the max operator. For this, we introduce a variable T for
Ttotal and decompose the objective to be \minimize T with
a constraint for each path such that: (T)
1
[expected total
time for the path] &lt;= 1&quot;. Since the expected total time (for
a single path) is a posynomial, the constraints are posyno-
mials, and the entire program is a GP.
5.3 Operator Modeling
To compute the objective function, FTOpt thus requires
that each operator provide expressions that characterize
its delay and processing time during normal operation and
when failures occur. These expressions must be provided
for each fault-tolerance strategy that the operator supports.
Formally, FTOpt needs to be given the functions in Table 1
expressed in terms of the parameters, represented by  in
Table 2 (these parameters capture the inter-dependencies
between operators). FTOpt combines these functions to-
gether to derive the overall processing time Ttotal.
In this section, we show how to express such functions in
Actual runtime
Number of output  tuples
Time
Total output
t
a
cpu
Point of change (tangent)
NBout
(nin
)
Figure 2: Data output curve (comprised of NBout()
curve till the \Point of change&quot; and the dashed line
after it) for a symmetric-hash join operator.
our framework. We proceed through an example: we derive
the constraint equations for join (symmetric hash-join). The
models for select and aggregate are similarly derived [44].
5.3.1 Modeling Basic Operator Runtime
To model our problem as a GP, we must (a) derive the
operator output rate (given by the inter-output-tuple delay,
x
N
) in the absence of failures, and (b) derive the delay, D
N
.
The delay, however, is simply either negligible for selects and
the symmetric hash-join that we model or equal to the total
processing time for aggregates.
4
The challenge in expressing an operator&apos;s output rate is
that x
N
can follow a complex curve for some operators such
as certain non-blocking join algorithms as illustrated in Fig-
ure 2. The gure shows the data output curve for a sym-
metric hash-join operator. For this operator, the more tu-
ples that it has already processed, the more likely the join
is to nd matching tuples, and thus it outputs tuples at an
increasingly faster rate. As a result, at the beginning of
the computation, the bottleneck is the input data rate (the
NBout(nin) curve) and the operator produces increasingly
more output tuples for each input tuple. Eventually, the
CPU at the join becomes the bottleneck (t
cpu
a curve) and
the output rate 
attens.
We found that ignoring such eects and assuming a con-
stant output signicantly underestimated the total runtime
for the operator. Alternatively, modeling these eects and
exposing them to downstream operators signicantly com-
plicated the overall optimization problem. We thus opted
for the following middle-ground: we model the non-uniform
output rate of an operator to derive its total runtime. Given
the total runtime, we compute the equivalent average out-
put rate that we use as a constant input arrival rate for the
next operator. The GP framework is helpful here to express
these non-linear behaviors.
Interestingly, we nd that we can automatically derive the
above curve from the following operator properties:
 t
cpu
a : Average time to generate one output tuple if all
input is available with no delay.
 NBout(nin): This function provides the total number
of output tuples produced for a given number of tuples
(nin) received across all input streams.
The above functions can easily be derived (hence simpli-
fying optimizer extensibility). Both these functions are ex-
tensions of parameters of standard query optimizers: (a)
t
cpu
a corresponds to the standard query optimizer function
for computing an operator&apos;s cost, except that we then di-
vide this cost by the operator output cardinality, and (b)
4
To compute total query times, we ignore any partial results
that an online aggregate may produce. NBout(nin) is similar to computing the cardinality of an
operator output, except that it also captures how that out-
put is produced as the input data arrives. Simple operators
like select or merge join have NBout = nin, where  is
the operator selectivity. For blocking operators such as ag-
gregates, after the delay D
N
() all the output tuples are
produced at once and hence NBout = jIj. For other non-
blocking operators the relationship can be more complex as
we discuss next using our symmetric hash-join as example.
For the symmetric hash-join operator, dene Iutot to be
the set of all tuples received from both upstream input chan-
nels. Hence, jIutotj = jI1j + jI2j. For this operator:
t
cpu
a = jIj
1
(jIutotj + jIj)t
cpu
The expression is a product of the average time taken
to process either an input or output tuple (t
cpu
, obtained
through micro-benchmarks) and the total number of tuples
seen by the operator, including the input tuples (Iutot) and
the output join tuples (I). This number is then divided by
the total number of output tuples (jIj) to get the average
time per output tuple.
To get the NBout function for a symmetric hash-join we
assume that the input tuples from the two input channels
can arrive in any order, each order being equally likely. Let ^
(a function of the join selectivity , jI1j and jI2j [44]) be the
probability that two tuples from dierent channels join and
pi be the probability that a tuple belongs to the i
th
channel.
In this case, the function NBout(nin) is dened as follows:
NBout(nin) = ^p1p2nin(nin  1)  p^ 1p2n
2
in
Intuitively, nin(nin1) is the count of pairs of distinct tuples
to join, p1p2 is the probability that they come from dierent
channels, and ^ is the probability that they join.
We now show how our optimizer translates these functions
into a set of inequalities that characterize the average time
interval between successive output tuples produced by an
operator. For this, we require that the NBout(nin) function
take the form: NBout(nin) = 
n
k
in, in order to t into the
GP framework. Thus, for our join operator: 
 = ^p1p2 and
k = 2. Informally, as the operator sees more input tuples,
the number of the output tuples produced after processing
a single new input tuple should never decrease.
Given the above, the average time interval between con-
secutive output tuples, x
N
, is given by the following inequal-
ities:
me = 
(x
IN
)
k
kt
k1
f
me  (t
cpu
a )
1
me  

1
k (x
IN
)
1
kjIj
1 1
k
(1  k
1
)tf + jIjm
1
e  x
N
jIj
The above inequalities take x
IN
as input, which is the time
interval at which input tuples are arriving. x
IN
depends on
the current execution context. If we are operating normally,
it is the average time interval between tuples produced by
the upstream operators; if we are recovering from a failure,
we might read the input tuples from disk at the maximum
bandwidth possible for the disk.
For the exact derivation of this mode, we refer the reader
to Appendix B. Here, we only provide the intuition behind
it.
In the above equations, jIj is the output cardinality; 

and k come from the NBout(nin) function; me is the num-
ber of output tuples produced per second at the instant the
processing ends and tf is the rst time at which the output
produces tuples at the rate me. The rst equation realizes
this relationship between me and tf . The following inequal-
ity states that the operator can not take less than t
cpu
a time
to produce an output tuple, since this is the least amount
of time the processor needs per tuple, given the resources
it has. For the second inequality, its right hand side is the
maximum rate at which output could be produced if the
only bottleneck was the rate of arrival of input tuples. Note
that, since we require the NBout() function to have a non-
negative rate of change, the fastest output production rate
will be at the end of the computation and the derivative of
the function NBout(), with respect to x
IN
, at the end gives
us this value. Since, in a real computation the processing
cost is positive, the actual observed rate has to be less than
the derivative (the right hand side in the second inequality).
The third inequality states that the total time to process all
tuples (which is equal to the average output rate times the
number of output produced) must be higher than the actual
processing time, which is its left hand side.
To model a dierent operator, the functions for t
cpu
a and
NBout(nin) would change, while the form of the inequalities
and equalities used by the optimizer would remain the same.
They simply use the above as parameters.
We model a partitioned operator as a single operator that
scales linearly with allocated resources. This approach suf-
ces to show the feasibility and impact of fault-tolerance
optimization. We leave extensions to more complex models,
including data skew between partitions, for future work.
5.3.2 Modeling Overhead of Fault-tolerance
Fault-tolerance overhead only aects t
cpu
a , the time an op-
erator needs to produce an output. The model depends on
the operator implementation. For MATERIALIZE, our join
writes all output tuples to disk. For CHCKPT, it logs the
incoming tuples to disk incrementally
5
. The join does not
maintain any output queue.
For brevity, we use the notation that I
N
, I
M
and I
C
are 1 if
NONE, MATERIALIZE or CHCKPT is the chosen fault tol-
erance option, respectively, and are 0 otherwise. Although
we need one equation per fault-tolerance strategy we repre-
sent them as a single one.
t
cpu
a = jIj
1

t
cpu
(jIutotj + jIj) + I
C
t
io
jIutotj + I
M
t
io
jIj

Here t
io
is the time to write a tuple to disk and is also
obtained through micro-benchmarks.
5.3.3 Modeling Replay Request Times
FTOpt also needs to know the average rate at which out-
put tuples are produced to satisfy a replay request and the
delay in generating the rst requested tuple. The replay
rate may depend on when, during the course of the query,
the downstream fails. For example, if the replay behaves
as during normal operations for the symmetric hash-join, it
might be slower if the downstream fails early on and be faster
later. To approximate the recovery rate we nd the time it
takes to replay all output tuples and divide that number by
5
Out of simplicity, our join checkpoints input tuples as they
arrive rather than checkpointing the hash table. When it
rebuilds the hash table from a checkpoint, the operator does
not redo the join.            the total number of output tuples. During this replay phase,
the operator has no fault-tolerance overheads.
As before, the exact model depends on the implementa-
tion details. Our join implementation uses its in-memory
hash table to regenerate outputs and hence the delay is neg-
ligible. But it could be signicant for a join that can not use
either its state or its output to answer tuple requests.
To get the average output rate, we reuse the framework
we developed in the previous section. Thus we only need to
specify t
cpu
a and NBout(nin) for the replay mode.
Since, during replay, we only reprocess the inputs without
any fault tolerance overhead: t
cpu
a = jIj
1
(jIutotj + jIj)t
cpu
.
The form of the NBout() remains the same as for the nor-
mal processing. Also, during reprocessing the input tuples
are already in memory, hence the inter-tuple arrival time of
inputs x
IN
is at least t
cpu
and we take x
IN
= t
cpu
.
5.3.4 Modeling Recovery Time
To compute the total time to recover from a failure, we
need to know the average rate at which recovery proceeds.
As before, the exact recovery model depends on the imple-
mentation. For our join, upon failure the MATERIALIZE
and the NONE options have to request all the input from
the upstream nodes and rebuild the hash table exactly as it
was before (using Rule 4.2 and operator determinism [44]),
while CHCKPT rebuilds it from the input tuples logged to
disk.
In all cases, during recovery, no output is produced when
the input tuples are processed to remake the hash table.
Thus, t
cpu
a = t
cpu
since we look at each input tuple once.
To dene the function NBout(nin) we think of the hash
table being rebuilt as the desired \output&quot; and the input
tuples as the inputs. Since all the input tuples are used
to generate the \output&quot; hash table: NBout(nin) = nin.
For MATERIALIZE and NONE, x
IN
is the average time
interval in which requested tuples from the upstream nodes
arrive. For CHCKPT, since we directly read tuples from the
disk: x
IN
= t
io
.
The delay in getting the rst input is negligible if we use
CHCKPT and is equal to the delay of the upstream tuples
in the case of NONE and MATERIALIZE.
We approximate the expected hash table size to recover
to be
1
2
jIutotj. Thus, the expected time to recover is the
sum of (1) the delay to receive the rst input tuple, and (2)
the product of the expected hash table size and the average
time per tuple spent in adding a tuple to that hash table.
In summary, compared to existing cost models for paral-
lel query runtime estimation [13, 12] and fault-tolerance in
streaming engines [21], our models capture the dynamic op-
erator interactions in pipelined queries, which we observed
to aect runtime predictions and fault-tolerance optimiza-
tion. For example, a fast operator following a slow one in a
pipeline will produce its output slowly. At the same time,
we do not require that an operator&apos;s output tuples be uni-
formly spread across the entire execution time of the oper-
ator [16, 49]. Indeed, because we use a GP framework, we
support simple types of non-uniform outputs such as that
of asymmetric hash-join. Of course, our GP framework may
not cover all cases. In particular, for multi-phase operators
(e.g., a symmetric hash-join that spills state to disk), we may
still need to split the operator into multiple sub-operators
for more accurate modeling of each phase.
5.4 Approach Implementability
Our approach consists of (1) a protocol that enables het-
erogeneous fault-tolerance in a parallel query plan and (2)
an optimizer that automatically selects the fault-tolerance
strategy that each operator should use. We now discuss the
diculty of implementing this approach in a parallel data
processing system.
To implement our approach, developers need to (a) imple-
ment desired fault-tolerance strategies for their operators in
a manner that follows our protocol. In Section 4.2, however,
we showed, how to eciently implement three well-known
fault-tolerance strategies for generic stateless and stateful
operators. Existing libraries can also help with such imple-
mentation (e.g., [23]). Developers must also (b) model their
operator costs within a pipelined query plan. To simplify
this latter task, we develop an approach that requires only
that developers specify well-known functions under dierent
fault-tolerance strategies and during recovery: an operator
cost function and a function that computes how the output
size of an operator grows with the input size. Our opti-
mizer derives the resulting operator dynamics automatically.
For parallel database systems [41, 14] and MapReduce-type
systems such as Hive [1] or Pig [30], which come with pre-
dened operators, the above overhead needs only be paid
once and we thus posit that it is a reasonable requirement.
For user-dened operators (UDOs), the above may still
be too much to ask. In that case, the simplest strategy
is to treat UDOs as if they could only support the NONE
or MATERIALIZE strategies (depending on the underlying
platform) without ever producing acknowledgments. With
this approach, UDO writers need not do any extra work
at all, yet the overall query plan can still be optimized and
achieve higher performance than without fault-tolerance op-
timization as we show in Section 6.4.
Finally, our approach relies on a set of parameters includ-
ing IO cost (expressed as the time t
io
spent in a byte sized
disk IO), per-operator CPU cost (expressed as the time t
cpu
spent processing each tuple), and total network bandwidth.
Commercial database systems already automate the collec-
tion of such statistics (e.g., [31]), though t
cpu
is typically
expanded into a more detailed formula.
Other necessary information includes the expected num-
ber of failures for the query (see Section 2), operator selectiv-
ities (standard optimizer-provided metric), and an estimate
of the total checkpointable state. As shown in Section 6.6,
our optimizer is insensitive to small errors in these estimates.
Overall, the requirements of our fault-tolerance optimiza-
tion framework are thus similar to those of existing cost-
based query optimizers.
5.5 Handling Complex Failure Scenarios
So far, we have focused on process failures. However, our
approach also handles other types of failures.
Our approach still works when entire physical machines
fail (e.g., due to a disk failure, a power failure, or a net-
work failure). To support such failures, checkpoints must be
written to remote nodes instead of locally [19], which adds
network and CPU costs that must be taken into account by
the optimizer. Given that the optimizer knows the size of
these checkpoints, it can take that cost into account. Sec-
ond, when a physical machine fails or becomes disconnected,
the number of nodes in the cluster is reduced by one, which
must also be taken into account by the optimizer. 2
2 2
σ
2
!&quot;
2
!&quot;
8
8
!&quot;
16
(a) Query 1
160
8 8
σ
8
!&quot;
8
!&quot;
8
8
!&quot;
8
(b) Query 2
160
8 16
σ
8
!&quot;
16
!&quot;
160
γ0.008
(c) Query 3
160
8 40
σ
8
!&quot;
40
!&quot;
80
γ0.008
(d) Query 4
160
8 8
σ
8
!&quot;
8
!&quot;
8
8
!&quot;
32
8.96 20.1
σ
8.96
!&quot;
20.1
!&quot;
79.4
γ0.008
(e) Query 5
Figure 3: Query plans used in experiments. , ./, 

denote Select, Join, and Aggregation, respectively.
All numbers are in millions.
Our approach does not currently handle failures that af-
fect a large number of machines. Indeed, such failures can
cause the temporary loss of input data or checkpointed data.
In such cases, the query needs to be restarted in its entirety
once the input data becomes available again. In general,
however, large-scale rack and network failures are infrequent,
while single machine failures are common. For example,
Google reports 5 average worker deaths per MapReduce job
in March 2006 [9], but only approximately 20 rack failures
per year (and similarly few network failures) [8].
Even though our approach does not handle large-scale fail-
ures that cause the loss of input or checkpointed data, it does
handle multiple operators failing at the same time. The only
requirement in such cases is that operators be restarted from
downstream to upstream, ensuring that each operator knows
where to start recovering from before asking upstream neigh-
bors to replay data.
6. EVALUATION
We evaluate FTOpt by answering the following questions:
(1) Does the choice of fault-tolerance strategy for a parallel
query matter? (2) Are there congurations where a hybrid
plan, where dierent operators use dierent fault-tolerance
techniques, outperforms uniform plans? (3) Is our optimizer
able to nd good fault-tolerance plans automatically? (4)
How do user-dened operators aect FTOpt? (5) What is
the scalability of our approach? (6) How sensitive is FTOpt
to estimation errors in its various parameters?
We answer these questions through experiments with a va-
riety of queries in a 17-node cluster. Each node has dual 2.5
GHz Quad Core E5420 processors and 16 GB RAM running
Linux kernel 2.6.18 with two 7.2K RPM 750 GB SATA hard
disks. The cluster runs a simple parallel data processing en-
gine that we wrote in Java. The implementation includes
our new fault-tolerance framework and specic per-operator
fault-tolerance strategies for a small set of representative op-
erators. All fault-tolerance strategies were moderately opti-
mized (see Section 4.2). We implemented the optimizer in
MATLAB using the cvx package [7].
The query plans that we use in the experiments are shown
in Figure 3. They include an SJJJ and SJJA query (we also
test a more complex query later in this section). For both
queries we have 8 partitions per operator with 2 cores and
1 disk per partition. Partitions of the same operator run on
dierent machines. The input data is synthetic and without
skew. Tuples are 0.5 KB in size. The schema consists of 4
0 
50 
100 
150 
200 
250 
NM  NN  MM  CC  cc  MM  cc 
Select-Join  Select-Average  Select-Join 
Runtime (s) 
Configurations 
Real  Predicted 
Figure 4: Runtime without failures for various two-
operator queries. X-axis labels show the fault-
tolerance strategy chosen: N for NONE, M for MA-
TERIALIZE, C for CHCKPT with a total of 10
checkpoints, and c for CHCKPT with 1K check-
points.
attributes used to hash-partition tuples for each operator, a
5th attribute for grouping the aggregates, and a 6th one for
the join predicates. A separate producer process generates
input tuples. For a given plan, we get the expected recovery
time by injecting a failure midway through the time the
plan takes to execute with no failures. We inject exactly one
failure per run and show the recovery time averaged over all
distinct operators in the plan.
6.1 Model Validation Experiments
FTOpt requires the t
cpu
and the t
io
values for each opera-
tor. It also requires the network bandwidth for each machine
in the cluster. Through micro-benchmarks, we nd that the
average time to read a tuple from disk (sequential read) is
t
io
= 13:0 s for a 0.5 KB tuple. This number is equivalent
to a disk throughput of 37 MBps. For select and aggregate
operators, we measure t
cpu
to be 1:82s. The join operator,
internally, works in two parts: (1) hashing the input tuple
and storing it in one of the tables for a cost of t1 = 8s
and (2) joining the hashed input tuple to the corresponding
tuples from the other table for a cost of t2 = 1s. We use
t1, t2, and the operator&apos;s selectivity to estimate its t
cpu
. Fi-
nally, we measure the network I/O time per 0:5 KB tuple
to be 4:7s, which is equivalent to a network bandwidth of
109:4 MBps and is close to the theoretical maximum of 1
Gbps network bandwidth for each machine in the cluster.
These parameters along with our operator models enable
FTOpt to predict the runtime for an entire query plan.
Figure 4 shows the runtime without failure for a few two-
operator queries. While the median percentage dierence
between real and predicted runtime is 9:5%, this error is
small given the overall dierences in runtime between var-
ious congurations. We measure the sensitivity of our ap-
proach to the benchmarked parameter values in Section 6.6.
6.2 Impact of Fault-Tolerance Strategy
The rst question that we ask is whether a fault-tolerance
optimizer is useful: how much does it really matter what
fault-tolerance strategy is used for a query plan?
Figures 5 through 7 show the actual and predicted run-
times for Queries 1 through 3 from Figure 3 with 8 partitions
per operator. Note that, each join receives input from two
sources: its upstream operator in the plan and a producer
process. In all our experiments, an equal number of tuples
was received from each source. Whenever FTOpt selects
CHCKPT as a strategy, it also chooses the checkpoint fre-
quency (Query 3). In other cases, we use 100 checkpoints, aCKPT MAT NONE(OPT) RESTART
0
10
20
30
40
50
60
70
Runtime (s)
Predicted Recovery Predicted Normal
Observed Recovery Observed Normal
250 CKPTFigure 5: Query 1 (SJJJ) MAT NONE(OPT) RESTART
CKPT MAT(OPT) NONE RESTART
0
50
100
150
200
250
Runtime (s)
Predicted Recovery Predicted Normal
Observed Recovery Observed Normal
Figure 6: Query 2 (SJJJ with lower selectivities)
manually selected value that we found to give high perfor-
mance in these experiments.
The most important result from these experiments is that,
while these queries are all similar to each other, each one re-
quires a dierent fault-tolerance plan to achieve best perfor-
mance. For Query 1, a uniform NONE strategy is best. For
Query 2, uniform MATERIALIZE wins. Finally, for Query
3, uniform CHCKPT outperforms the other options.
Second, restarting a query is at most 50% slower than a
strategy with more ne-grained fault-tolerance. The ne-
grained strategy gains the most when it reduces recovery
times with minimal impact on runtime without failures.
For some queries, the appropriate choice of fault-tolerance
gets close to this theoretical upper bound. For Query 2,
RESTART is 31% worse than the best strategy while for
Query 3, restarting is 44% slower than the best strategy.
Achieving such gains, however, requires fault-tolerance op-
timization. Indeed, dierent strategies win for dierent
queries and a wrong fault-tolerance strategy choice leads to
much worse performance than restarting a query. Overall,
the dierences between the best and worst plan are high:
58% for Query 1, 31% for Query 2, and 72% for Query 3.
Finally, in all cases, FTOpt is able to identify the winning
strategy! Predicted runtimes do not always match the ob-
served ones exactly. Most of the dierence is attributable to
our simple model for the network and FTOpt&apos;s predictions
are thus more accurate when either CPU or disk IO is the
bottleneck in a query plan and less accurate when it is the
network. While we could further rene our models, to pick
the optimal strategy, we only need to have correct relative
order of predicted runtimes for dierent plans. As shown
in Figures 4 through 8, FTOpt preserves that order when
runtime dierences are large. When two congurations lead
to very similar runtimes, FTOpt may not nd the best of
these plans but the choice of plan matters less in such a case
and FTOpt always suggests one of the good plans.
In summary, the correct choice of fault-tolerance strategy
can signicantly impact query runtime and that choice is
not obvious as similar query plans may require very dierent
strategies. FTOpt can automatically select a good plan.
6.3 Beneﬁts of Hybrid Conﬁgurations
We now consider a query (Query 4), similar to Query
CKPT(OPT) MAT NONE RESTART
0
50
100
150
200
250
300
350
400
450
500
Runtime (s)
Predicted Recovery Predicted Normal
Observed Recovery Observed Normal
Figure 7: Query 3 (SJJA query)
CKPT MAT NONE HYBRID(OPT) RESTART
0
50
100
150
200
250
300
350
400
450
Runtime (s)
Predicted Recovery Predicted Normal
Observed Recovery Observed Normal
Figure 8: Query 4 (SJJA with more expensive joins).
The hybrid strategy is to materialize after select, do
nothing for joins, and checkpoint the aggregate
3, but with the joins processing and producing much more
data, making checkpointing expensive. Figure 8 shows that
a hybrid strategy that materializes the select&apos;s output, does
nothing for the joins, and checkpoints the aggregate&apos;s state
for a total of 40 checkpoints (value selected by the opti-
mizer), yields the best performance. The uniform strate-
gies are 15% slower at best and 21% slower at worst while
RESTART is 35% slower.
We observe similar gains for a longer query (Query 5)
with eight operators. Figure 9 shows that the hybrid plan
(chosen by the optimizer) materializes both selects&apos; outputs,
does nothing for joins and takes 20 checkpoints of the aggre-
gate. The best and worst uniform strategies and RESTART
are 16%, 23% and 36% slower, respectively. Manually, we
found that checkpointing the rst two joins in the hybrid
plan led to another hybrid plan that was 2% faster. While
the optimizer did not choose this better plan, the plan it
chose performs similarly. Further, both the observed and
predicted best plans are hybrid.
The experiments thus show that hybrid fault-tolerance
strategies can be advantageous and the best strategy for an
operator depends not only on the operator but on the whole
query plan: the same operator can use dierent strategies
in dierent query plans: e.g., select in Queries 3 and 4.
Note that we inject only one failure per experiment. Thus,
our graphs show the minimum guaranteed gains. Additional
failures amplify dierences between strategies.
6.4 Performance in Presence of UDOs
We look at the applicability of heterogeneous fault-
tolerance when an operator is a user-dened function with
limited fault-tolerance capabilities. We experiment with
Query 3, but treat its last operator, the aggregate, as a
UDO that can only restart from scratch if it fails. Note that
Rule 4.2 and operator determinism [44] allow restarting a
UDO in isolation without restarting the entire query.
Figure 10 shows the results. Previously, the best fault-
tolerance strategy, with a single failure, was to checkpoint
every operator (\With CKPT&quot;) and checkpointing aggregate
provided signicant savings in recovery time. Now that the
aggregate can use NONE as sole strategy, we nd that ma-0
200
400
600
OPT NONE MAT CKPT RESTART
Runtime (in s)
Observed Normal Observed Recovery
Figure 9: Query 5 (SJJJSJJA Query). The opti-
mal hybrid strategy is MNNNMNNC where M de-
notes MATERIALIZE and N denotes NONE and C
denotes CHCKPT. In the optimal conguration 20
checkpoints are taken.
0
100
200
300
400
500
With CKPT UDO-OPT MAT NONE RESTART
Runtimes (in s)
Observed Normal Observed Recovery
Figure 10: Impact of aggregate becoming a UDOs
without fault-tolerance capabilities on Query 3. The
optimal strategy is to materialize after select and do
nothing elsewhere.
terializing the rst operator&apos;s output and using NONE for
the remaining operators outperforms uniformly materializ-
ing, none and RESTART by 48%, 12%, and 24%, respec-
tively. The hybrid strategy is itself 16% slower than the
optimal strategy for Query 3 (\With CKPT&quot;).
Hence even in the presence of fault-tolerance agnostic
UDOs, FTOpt can generate signicant runtime savings.
6.5 Scalability
FTOpt currently uses a brute force search algorithm, but
we nd that simple heuristics can signicantly prune the
search space. Indeed, we observe that the best hybrid plans
use the NONE strategy for many operators and using an-
other strategy in place of NONE will always increase the
runtime without failures. Thus, if the runtime without fail-
ure for a plan exceeds the runtime with failures for another
plan, we can prune the former plan. Hence, evaluating plans
in the decreasing order of the number of operators that use
the NONE strategy can prune signicant fractions of the
search space. For example, with this heuristic, the optimizer
examines only 28 out of 81 congurations for Query 4. In
addition, the search essentially computes the least costly of
a set of independent optimization problems and all of these
problems can be optimized in parallel.
FTOpt&apos;s MATLAB implementation uses the cvx pack-
age, which oers a successive approximation solver using
SDPT3 [42]. In our prototype, the average time to solve the
optimization problem per plan is around 25s for the 4 oper-
ator plans in the previous sections. However, an optimized
solver can solve a larger problem in a sub millisecond [29].
The behavior of an operator for a fault-tolerance strategy
is modeled using at most 12 inequality and 4 equality con-
straints of 11 variables. Thus, a query with n operators can
be modeled using 11n variables, 13n + 1 inequality and 4n
equality constraints. Further, all but one of the constraints
are sparse: they depend on just a few variables indepen-
dent of n. For example, with 4 operators, our models use
44 variables, 16 equalities, and 53 inequalities. The existing
0
100
200
300
400
500
1 11 21 31 41 51 61 71 81
Runtime (in s)
Rank
Predicted Observed
Figure 11: Observed and predicted runtimes for
Query 3, sorted on the predicted runtime, for all
81 fault-tolerance plans for the query.
Table 3: Real rankings of top 5 plans from perturbed
congurations.
Perturbation Rankings
Failing thrice instead of once 1 2 3 4 5
IO cost 2.0x of true value 1 6 8 9 18
IO cost 0.5x of true value 2 1 3 4 5
IO cost 10x of true value 6 18 20 21 24
IO cost 0.1x of true value 2 28 31 30 29
Selectivity of all operators 1.1x 1 2 3 4 5
Selectivity of all operators 0.9x 1 2 3 4 5
Selectivity of all operators 2.0x 1 2 3 4 5
Selectivity of all operators 0.5x 56 1 66 67 10
optimized solvers can solve a problem of 140 variables, 120
equalities, and 60 inequalities in 0.425 ms on average [29].
To sum up, with an optimized solver, and a parallelized
heuristic search algorithm, FTOpt could be scalable enough
to handle larger query plans within a few seconds.
6.6 Optimizer Sensitivity
We evaluate FTOpt&apos;s sensitivity to inaccuracies in param-
eter estimates. We experiment with Query 3 since it is most
sensitive to wrong choices: Figure 11 shows that runtimes
vary from about 250s to 400s depending on the chosen plan.
To evaluate the sensitivity for a given parameter, we re-
run FTOpt, feeding it a perturbed parameter value. We
only perturb a single parameter at a time while keeping the
other parameters at their true values. We then compute the
top 5 plans with the perturbed value and report the ranks of
these plans in FTOpt&apos;s original ranking (Figure 11). Table 3
shows the results. As an example, in this table, when IO cost
increases to 2X its true value, the second best plan identied
by FTOpt was ranked 6th with the real IO costs.
Table 3 shows that FTOpt is very robust to small errors in
the number of failures and it is fairly robust to even large er-
rors in IO cost: a 10x change still leads to a good plan (with
true rank 6) being chosen, though the subsequent plans have
poor true rankings. FTOpt is least robust to cardinality esti-
mation errors. In our experiments, we varied the selectivities
of all the operators in tandem (and with the join always pro-
cessing the same number of tuples from both streams). In
this scenario, our predictions were unchanged for changes of
1.1x, 2x and 0.9x in selectivity but for a 0.5x change, the top
choice&apos;s true rank was 56 with an observed runtime about
70% worse than that of the best conguration possible.
The robustness to I/O cost errors and failure errors can be
explained by the fact that the eect of these errors is mostly
linear on the optimizer. However, imprecise selectivity es-
timates have an exponential eect (the further an operator
is from the beginning, the less data it processes and it pro-duces even less output) on FTOpt. Thus, the optimizer is
more sensitive to perturbations in selectivity estimates.
7. CONCLUSION
In this paper, we presented a framework for heterogeneous
fault-tolerance, a concrete instance of that framework, and
FTOpt, a latency and fault-tolerance optimizer for parallel
data processing systems. Given a pipelined query plan, a
shared-nothing cluster, and a failure model, FTOpt selects
the fault-tolerance strategy for each operator in a query plan
to minimize the time to complete the query with failures. We
implemented our approach in a prototype parallel query pro-
cessing engine. Our experimental results show that dierent
fault-tolerance strategies, often hybrid ones, lead to the best
performance in dierent settings and that our optimizer is
able to correctly identify a winning strategy.
Acknowledgments
We thank Phil Bernstein, Bill Howe, Julie Letchner, Dan
Suciu, and the anonymous reviewers for helpful comments
on the paper&apos;s early drafts. This work is supported in part
by the National Science Foundation grants NSF CAREER
IIS-0845397 and IIS-0713123, gifts from Microsoft Research,
and Balazinska&apos;s Microsoft Research Faculty Fellowship.
8. REFERENCES
[1] Ashish Thusoo et. al. Hive - a petabyte scale data warehouse
using hadoop. In Proc. of the 26th ICDE Conf., 2010.
[2] M. Balazinska, H. Balakrishnan, S. Madden, and
M. Stonebraker. Fault-tolerance in the Borealis distributed
stream processing system. In Proc. of the SIGMOD Conf.,
June 2005.
[3] S. P. Boyd, S. J. Kim, L. Vandenberghe, and A. Hassibi. A
tutorial on geometric programming. Technical report, Stanford
University, Info. Systems Laboratory, Dept. Elect. Eng., 2004.
[4] B. Chandramouli, C. N. Bond, S. Babu, and J. Yang. Query
suspend and resume. In Proc. of the SIGMOD Conf., 2007.
[5] S. Chaudhuri, R. Kaushik, A. Pol, and R. Ramamurthy.
Stop-and-restart style execution for long running decision
support queries. In Proc. of the 33rd VLDB Conf., 2007.
[6] Chen et. al. High availability and scalability guide for DB2 on
linux, unix, and windows. IBM Redbooks
http://www.redbooks.ibm.com/redbooks/pdfs/sg247363.pdf,
Sept. 2007.
[7] Cvx. http://www.stanford.edu/~boyd/cvx/.
[8] J. Dean. Software engineering advice from building large-scale
distributed systems. http:
//research.google.com/people/jeff/stanford-295-talk.pdf.
[9] J. Dean. Experiences with MapReduce, an abstraction for
large-scale computation. Keynote I: PACT, 2006.
[10] J. Dean and S. Ghemawat. MapReduce: simplied data
processing on large clusters. In Proc. of the 6th OSDI Symp.,
2004.
[11] E. N. M. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B. Johnson.
A survey of rollback-recovery protocols in message-passing
systems. ACM Computing Surveys, 34(3), 2002.
[12] S. Ganguly, A. Goel, and A. Silberschatz. Ecient and
accurate cost models for parallel query optimization (extended
abstract). In Proc. of the 15rd PODS Symp., 1996.
[13] S. Ganguly, W. Hasan, and R. Krishnamurthy. Query
optimization for parallel execution. In Proc. of the SIGMOD
Conf., pages 9{18, 1992.
[14] Greenplum database. http://www.greenplum.com/.
[15] Hadoop. http://hadoop.apache.org/.
[16] W. Hasan and R. Motwani. Optimization algorithms for
exploiting the parallelism-communication tradeo in pipelined
parallelism. In Proc. of the 20th VLDB Conf., 1994.
[17] J. M. Hellerstein, R. Avnur, and V. Raman. Informix under
CONTROL: Online query processing. Data Mining and
Knowledge Discovery, 4(4):281{314, 2000.
[18] J. M. Hellerstein, P. J. Haas, and H. J. Wang. Online
aggregation. In Proc. of the SIGMOD Conf., 1997.
[19] J.-H. Hwang, Y. Xing, U. Cetintemel, and S. Zdonik. A
cooperative, self-conguring high-availability solution for
stream processing. In Proc. of ICDE Conf., Apr. 2007.
[20] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:
Distributed data-parallel programs from sequential building
blocks. In Proc. of the EuroSys Conf., pages 59{72, 2007.
[21] Jeong-Hyon Hwang et. al. High-availability algorithms for
distributed stream processing. In Proc. of the 21st ICDE
Conf., Apr. 2005.
[22] S. Y. Ko, I. Hoque, B. Cho, and I. Gupta. Making cloud
intermediate data fault-tolerant. In Proc. of the 1st ACM
symposium on Cloud computing (SOCC), pages 181{192, 2010.
[23] Y. Kwon, M. Balazinska, and A. Greenberg. Fault-tolerant
stream processing using a distributed, replicated le system. In
Proc. of the 34th VLDB Conf., 2008.
[24] W. J. Labio, J. L. Wiener, H. Garcia-Molina, and V. Gorelik.
Ecient resumption of interrupted warehouse loads. SIGMOD
Record, 29(2):46{57, 2000.
[25] A.-P. Liedes and A. Wolski. Siren: A memory-conserving,
snapshot-consistent checkpoint algorithm for in-memory
databases. In Proc. of the 22nd ICDE Conf., page 99, 2006.
[26] D. Logothetis, C. Olston, B. Reed, K. C. Webb, and K. Yocum.
Stateful bulk processing for incremental analytics. In Proc. of
the 1st ACM symposium on Cloud computing (SOCC), 2010.
[27] D. Lomet. Dependability, abstraction, and programming. In
DASFAA &apos;09: Proc. of the 14th Int. Conf. on Database
Systems for Advanced Applications, pages 1{21, 2009.
[28] Marcos Vaz Salles et. al. An evaluation of checkpoint recovery
for massively multiplayer online games. In Proc. of the 35th
VLDB Conf., 2009.
[29] J. Mattingley and S. Boyd. Automatic code generation for
real-time convex optimization. In Convex Optimization in
Signal Processing Optimization. Cambridge U. Press, 2009.
[30] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins.
Pig latin: a not-so-foreign language for data processing. In
Proc. of the SIGMOD Conf., pages 1099{1110, 2008.
[31] Oracle database. http://www.oracle.com/.
[32] A. Pavlo et. al. A comparison of approaches to large-scale data
analysis. In Proc. of the SIGMOD Conf., 2009.
[33] L. Raschid and S. Y. W. Su. A parallel processing strategy for
evaluating recursive queries. In W. W. Chu, G. Gardarin,
S. Ohsuga, and Y. Kambayashi, editors, VLDB&apos;86 Twelfth
International Conference on Very Large Data Bases, August
25-28, 1986, Kyoto, Japan, Proceedings, pages 412{419.
Morgan Kaufmann, 1986.
[34] A. Ray. Oracle data guard: Ensuring disaster recovery for the
enterprise. An Oracle white paper, Mar. 2002.
[35] K. Salem and H. Garcia-Molina. Checkpointing
memory-resident databases. In Proc. of the 5th ICDE Conf.,
pages 452{462, 1989.
[36] F. B. Schneider. Implementing fault-tolerant services using the
state machine approach: a tutorial. ACM Computing Surveys,
22(4):299{319, 1990.
[37] M. Shah, J. Hellerstein, and E. Brewer. Highly-available,
fault-tolerant, parallel data
ows. In Proc. of the SIGMOD
Conf., June 2004.
[38] A. Simitsis, K. Wilkinson, U. Dayal, and M. Castellanos.
Optimizing etl work
ows for fault-tolerance. In Proc. of the
26th ICDE Conf., 2010.
[39] U. Srivastava and J. Widom. Flexible time management in data
stream systems. In Proc. of the 23rd PODS Symp., June 2004.
[40] R. Talmage. Database mirroring in SQL Server 2005.
http://www.microsoft.com/technet/prodtechnol/sql/2005/
dbmirror.mspx, Apr. 2005.
[41] Teradata. http://www.teradata.com/.
[42] R. H. Tut  unc  u, K. C. Toh, and M. J. Todd. Solving 
semidenite-quadratic-linear programs using SDPT3.
Mathematical programming, 95(2):189{217, 2003.
[43] Tyson Condie et. al. MapReduce online. In Proc. of the 7th
NSDI Symp., 2010.
[44] P. Upadhyaya, Y. Kwon, and M. Balazinska. A latency and
fault-tolerance optimizer for online parallel query plans.
Technical report, Department of Computer Science and
Engineering, Univ. of Washington, 2010.
[45] Vertica, inc. http://www.vertica.com/.
[46] A. N. Wilschut and P. M. G. Apers. Data
ow query execution
in a parallel main-memory environment. In Proceedings of theFirst International Conference on Parallel and Distributed
Information Systems (PDIS 1991), Fontainebleu Hilton
Resort, Miami Beach, Florida, December 4-6, 1991, pages
68{77. IEEE Computer Society, 1991.
[47] C. Yang, C. Yen, C. Tan, and S. R. Madden. Osprey:
Implementing MapReduce-style fault tolerance in a
shared-nothing distributed database. In Proc. of the 26th
ICDE Conf., 2010.
[48] Yuan Yu et. al. DryadLINQ: A system for general-purpose
distributed data-parallel computing using a high-level language.
In Proc. of the 8th OSDI Symp., 2008.
[49] M. Zat, D. Florescu, and P. Valduriez. Benchmarking the
DBS3 parallel query optimizer. IEEE Parallel Distrib.
Technol., 4(2):26{40, 1996.Input Batch
Log
Checkpoint/
Materialized output
Upstream
handler
Down-stream
handler
Output
queue
Input
buffer
Input Batch 
Processor
Input
batch
Operator
Figure 12: Architecture of the operator frame-
work. The operator processes the incoming data
in a pipelined manner. Threads are assigned to
each stage of the pipeline and thus each stage runs
concurrently. Network IO is handled by a pool
of threads. The dashed components represent in-
memory data structures and the implementation of
(user-supplied) operator logic.
APPENDIX
In this Appendix, we provide additional information about
various aspects of our framework and the FTOpt optimizer.
A. IMPLEMENTATION
The prototype is written in Java and built on top of the
Apache MINA framework (http://mina.apache.org/) to
implement ecient network IO. The current implementa-
tion can run a directed acyclic graph (DAG) of operators.
At runtime, each operator in the DAG is replicated across
multiple machines and executed in parallel. The data com-
munication between upstream and downstream operators is
done using all-to-all TCP connections.
A.1 Operator Framework Architecture
Figure 12 illustrates the framework of an operator. The
framework has three concurrently executing components:
Upstream Handler, Batch Processor, and Downstream Han-
dler. The three components make up a data processing
pipeline connected by queues. We now walk through how
the incoming data is processed by this pipeline.
First, for each upstream partition, the Upstream Handler
buers the input tuples and creates a batch of input tu-
ples whenever there are enough tuples or when the stream
is stalled (which is detected by a timeout.) Both the size
of an input tuple batch and the timeout are congurable
parameters.
Next, the input batch is handed to the Input Batch Pro-
cessor (IBP). Before running the core operator algorithm,
the batch processor logs the summary information for the
current batch for deterministic replay of the input stream.
Because the tuples in an input tuple batch are all from the
same upstream partition, in the batch summary, we only
need to record the upstream partition identier, the rst
tuple identier in the batch, and the number of tuples in
the batch for deterministic replay. In Appendix A.4, we dis-
cuss the details of logging and show that logging imposes a
minimal overhead. The output of core operator algorithm
is also collected in a batch and handed to the Downstream
Handler.
Finally, the Downstream Handler streams the output tu-
ples to the downstream operators and completes the pro-
cessing of a batch of input tuples. The output tuples are
routed to designated downstream operators according to a
partition function. The downstream handler also takes the
required fault-tolerance action such as materializing output
before writing to the network or triggering checkpoints to
capture the current state of the operator at the end of pro-
cessing the current output batch.
Our prototype supports the three fault-tolerance strate-
gies we mention in Section 4.2: NONE, MATERIALIZE,
and CHCKPT. CHCKPT is supported only when the op-
erator algorithm implements necessary hooks (serialize and
de-serialize state.) The other two strategies are supported
automatically by the framework. For stateless operator such
as Select and Project, when the strategy NONE is chosen, the
prototype supports skipping over the previously processed
input during recovery and replay.
A.2 Operator Implementation
The current prototype implements three representative re-
lational operators: Select, Aggregate, and Join. For fault-
tolerance strategy, we only describe the detail of CHCKPT
because NONE and MATERIALIZE strategies are automat-
ically supported by the framework.
Select: This operator evaluates a given predicate on each
input tuple. We did not implement checkpoint hooks for it
since it is stateless.
Aggregate: This operator computes the average of a spe-
cic column in a group. It keeps track of the partially ag-
gregated states using an in-memory hash table. We imple-
mented checkpoint hooks to store the in-memory hash table
into a checkpoint and load it from a checkpoint.
Join: We implemented a binary symmetric hash join op-
erator using two in-memory hash tables [33, 46]. We imple-
mented incremental checkpoints: input tuples are buered
in memory, written to disk when a checkpoint occurs, and
then deleted from memory. During recovery, the operator re-
builds its hash tables by reading input tuples from the disk.
No joins need to be performed at this point. During replay,
the operator rst locates the oldest input tuple to replay,
then joins the following input tuples with the in-memory
state.
A.3 Synthetic Benchmark Setup
We implemented the synthetic workload in Section 6 as
follows:
 Data: All elds are randomly generated integers or
strings.
 Partition: We hash-partition the output of each op-
erator on a dierent attribute. We use the randomly
generated value for that attribute to determine where
to route each tuple.
 Select: We send a tuple to the output if the random
value of the select eld -of type double and taking val-
ues from 0 through 1- is less than the given selectivity.
 Aggregate: We vary the state size by controlling the
number of groups to which the input tuples are aggre-
gated.
 Join: Given the join selectivity , we join two input
tuples when the join attribute, for the two tuples, is
congruent modulo d
1
e.
A.4 Ensuring Operator Determinism
Our framework requires that the operator partitions be
deterministic. In particular, rule 4.2 requires that, in re-
sponse to a valid request, a partition must always return the same sequence of tuples, irrespective of any failures it
experiences.
Most relational operators (and hence their partitions) can
be made deterministic as long as when they restart, they
process the same tuples in the same order across all their in-
puts. The challenge is that these inputs come from dierent
machines in the cluster and may thus arrive with dierent
latencies when they are replayed. One approach to ensure
a deterministic input-data order is to buer and interleave
tuples using a pre-dened rule [2, 39]. These techniques,
however, can impose signicant memory and latency over-
heads due to tuple buering.
Instead, we adopt the approach of logging determi-
nants [11]. As the operator receives input tuples, it accu-
mulates them into small batches, with one batch per input
relation partition. For example, consider an operator with
two inputs coming from parent operators p
1
and p
2
. Tuples
arrive on these inputs starting with tuple id p
1
1 from p
1
and
p
2
1 from p
2
. Tuples arrive in interleaved order and the op-
erator accumulates them into batches, buering these two
batches separately in memory while maintaining the tuple
arrival order within a batch. Whenever a particular batch
exceeds a pre-dened size or receives an end-of-stream sig-
nal, the operator writes a log entry to disk that contains:
the identier of the stream for this batch, the identier of
the rst tuple in the batch, and the number of tuples in
the batch. Each log entry also has an implicit log sequence
number (lsn) that is not written to disk. The logging is
done before processing a batch. The operator processes the
batches in the same order in which it writes their log en-
tries to disk. In our example, if we use a batch size of
2500, and the operator receives 3500 tuples from p
1
and
4000 tuples from p
2
, the logged entries might look as follows:
h2; p
2
1; 2500i;h1; p
1
1; 2500i;h1; p
1
2501; 1000i;h2; p
2
2501; 1500i.
Log entries are force-written to stable storage but, as we
show below, this logging overhead is negligible even for batch
sizes as small as 512 tuples per batch. If the operator needs
to reprocess its input, it uses the log to ensure the reprocess-
ing occurs in the same order as before. To avoid expensive
disk IOs when possible (i.e., when the operator itself does
not fail but its downstream neighbor fails), recent determi-
nants are cached in memory.
Before processing an input tuple, the operator tags it with
hlsn; psni, where lsn corresponds to the log entry sequence
number of the corresponding batch and psn is the tuple
order within that batch. This information is used to assign
unique tuple identiers to output tuples. Note that all log
entries are of a constant size and a lsn is enough to index a
log entry.
Output tuple identiers consist of three integer elds:
hlsn; psn; seqi. The rst two elds identify one of the input
tuples that contributed to this output tuple. A sequence
number, seq, is added since one input tuple can contribute
to many output tuples (as in the case of joins.)
As an example, we show how we use this mechanism to
generate unique identiers for tuples produced by the fol-
lowing operators:
 Select: Our select always has a selectivity less than or
equal to one and can thus propagate the input tuple
identier onto the output tuple, setting seq to zero.
 Join: The latest tuple that led to the creation of this
tuple is used to populate the rst two elds. The third
0
10
20
30
40
50
60
1 3 6 12
Runtime (s)
Number of Producers
With input log Without input log
Figure 13: Each pair of bars represents the time to
complete processing, with and without logs, with a
dierent number of upstream producers for a select
operator. There is virtually no overhead even for 12
input streams.
0
5
10
15
20
25
512 1024 2048 4096 8192
Runtime (s)
Batch Size
With input log Without input log
Figure 14: Each pair of bars represents the time
to complete processing, with and without logs, with
dierent batch sizes for a join operator. The mini-
mum overhead occurs with a batch size of 2048.
eld is a count of the number of matches for any given
tuple.
 Aggregate: Since aggregates are blocking operators,
they do not need a log. In case we use CHCKPT, we
can store the last tuple identiers received from each of
the upstream partitions when we make the checkpoint.
To validate that logging overhead is negligible, we execute
a select operator on a single machine with an input of size
2:5  10
6
tuples (or 1.19 GB) and we vary the number of
upstream producers while keeping the batch size xed at
512 tuples. Figure 13 shows the time to process all tuples
with and without logging enabled. The results show that the
logging mechanism scales well with the number of upstream
producers. The average runtimes of three runs rounded to
the nearest second are identical.
To select the optimal log batch size we execute a join
operator that processes 1 million tuples from each of its two
inputs. It is a 1x1 foreign key join and produces 1 million
output tuples. We have a total of four producers generating
all the data and we vary the log batch size from 512 to 8192.
As Figure 14 shows, the smallest runtime overhead was 3%
for a batch size of 2048 tuples. As expected the runtime with
no logs for smaller batch sizes remains the same as that for
2048 while the runtime with logging increases since we write
more log entries if batch sizes are smaller and more cpu
time is spent in writing the log entries to disk. It should be
noted that the runtime with and without logs increases for
batch sizes of 4096 and 8192. This is because of an increased
buering delay for each input batch. In all our experiments,
we use a batch size of 2048 and a tuple size of 0.5 KB.
A.5 Resource Allocation in FTOptIn addition to fault-tolerance strategies, FTOpt also pro-
duces an allocation of resources to operators because re-
source allocation and choice of fault-tolerance strategy are
tightly interconnected. Resource allocation is expressed as
a fraction of all available CPU and network bandwidth re-
sources. Bandwidth is further constrained by network topol-
ogy.
In this paper, we make several simplifying assumptions
to implement and test our proof-of-concept optimizer. We
assume a simple setting where the set of compute nodes
are connected through a single switch. The current version
of our optimizer abstracts out the resource allocation by
assuming that the time to process each tuple and the disk IO
costs scale linearly with the amount of resources allocated to
an operator. Thus, if a single operator partition of operator
i takes t
cpu
time to process a tuple, then with ni partitions
each assigned exclusively to a machine, the eective time the
operator (i.e., all the operator partitions together) takes to
process a single tuple is
t
cpu
ni
time. Similarly the time taken
to write a tuple to disk is taken to be
t
io
ni
. Our optimizer
handles fractional resource assignments.
Given a resource allocation, operators can either be co-
scheduled on the same physical nodes (i.e., all nodes exe-
cute all operators) or separated onto dierent nodes (i.e.,
each node executes a subset of all operators.) In the latter
case, resource allocation must be rounded-o to the gran-
ularity of machines, which can lead to lower performance.
In the former case, operators may end-up writing their logs
and checkpoints to the same disks for a more complex per-
formance curve for these interleaved IO operations. While
our optimizer handles both strategies and computes frac-
tional resource assignments, in our experiments, we pinned
each operator partition to its own core and its own disk on
each node to keep our models simple.
B. OPERATOR MODELING STRATEGY
We refer the reader to Section 5.3.1 for an overview of
what information about each operator in the pipeline we re-
quire so as to automatically infer the runtimes for the entire
pipeline. To recap: We only require (a) the expected num-
ber of output tuples produced given the number of input
tuples received, and (b) the average cpu time required to
produce each output tuple.
We remind the reader that we have abstracted away the
dierent number of partitions of each operator by dividing
the time to process each tuple and the time to read or write
each tuple by the number of machines allocated to the op-
erator.
We now restate the equations and explain how the equa-
tions are derived.
me = 
(x
IN
)
k
kt
k1
f
(2)
me  (t
cpu
a )
1
(3)
me  

1
k (x
IN
)
1
kjIj
1 1
k (4)
(1  k
1
)tf + jIjm
1
e  x
N
jIj (5)
In the above equations, jIj is the output cardinality; 

and k come from the NBout(nin) function; me is the num-
ber of output tuples produced per second at the instant the
processing ends and tf is the rst time at which the output
produces tuples at the rate me.
Equality 2 realizes this relationship between me and tf .
Specically, the rate at which output tuples are produced
by the operator after time tf is
me =
d
dt
NBout(nin)
t=tf
=
d
dt

n
k
in
t=tf
=
d
dt



t
xIN
k
t=tf
= 
(x
IN
)
k
kt
k1
f
Inequality 3, me  (t
cpu
a )
1
, states that the operator can
not take less than t
cpu
a time to produce an output tuple,
since this is the least amount of time the processor needs per
tuple, given the resources it has. The inequality becomes an
equality when the operator reaches a stage in its processing
when the processor is working at its full capacity and can
not keep up with the theoretically maximum rate at which
output tuples could be produced given the rate at which the
input tuples are being received.
For inequality 4, me  

1
k (x
IN
)
1
kjIj
1 1
k , its right hand
side is the maximum rate at which output could be pro-
duced if the only bottleneck was the rate of arrival of input
tuples. Note that, since we require the NBout() function to
have a non-negative rate of change, i.e., the fastest output
production rate will be at the end of the computation and
the derivative of the function NBout() at the end gives us
this value. Formally,
rhs =
d
dt



t
xIN
k
t=tend
= 

k
xIN

tend
xIN
k1
where, jIj = 


tend
xIN
k
)
tend
xIN =

jIj


 1
k
) rhs = 

1
k (x
IN
)
1
kjIj
1 1
k (By substitution.)
Since, in a real computation the processing cost is positive,
the actual observed rate has to be less than the derivative
(the right hand side in the second inequality.)
The last inequality states that the total time to process
all tuples (which is equal to the average output rate times
the number of output produced and appears on the right-
hand side of the inequality) must be higher than the actual
processing time, which is its left hand side. Let S be the
number of output tuples produced until the operator was
bound by the input arrival rate. After this point, either
all the output tuples have been produced or the subsequent
processing is limited by the compute resources. In other
words, S is the number of output tuples produced until time
tf . Specically,
tf + m
1
e (jIj  S)  x
N
jIj
where
S = 


tf
xIN
k
(6)
In the inequality above the rst term is the time spent in
the input-limited stage of operation while the second term is
the time spent in the compute-limited stage of the operator&apos;s                  operation. Substituting the value of S from Equation 6 and
the value of m1
e from Equation 2 we get Equation 5.
The analysis above yields a form suitable for geometric
programs as long as t
cpu
a is a posynomial and NBout(nin) =

n
k
in for a constant k  1 and 
 being a monomial.
B.1 First Tuple Delay
The delay to produce the rst tuple is represented by
D
N
; D
RD
; D
RS
in Table 1. These quantities are only present
as an additive term in the objective function and thus, to
conform with the requirements of geometric programs, they
are required to be posynomials.
For our implementations of the select and the symmetric
hash join operators the additional delay introduced in gener-
ating the rst output tuple (over the delay in obtaining the
rst input tuple from the upstream operators) either when
processing normally or during recovery is negligible and so
we equal them to zero. For the case of blocking aggregates,
the additional delay introduced by the aggregate could be
signicantly large as discussed in Appendix B.4.1.
We now illustrate the approach by generating a model for
Selects, Aggregates, and Joins. We rst provide the missing
details of Section 5.3.1 about the modeling of a symmetric
hash join and then we present the detailed models for our
remaining two operators: Select and Aggregate.
B.2 Join
We construct a simplied analytical model for our sym-
metric hash join. The model is: Given the cardinality of
the input channels as jI1j and jI2j we assume that the input
tuples belong to the rst and the second relation with prob-
ability p1 =
jI1j
jI1j+jI2j
and p2 =
jI2j
jI1j+jI2j
, respectively. We let
^ represent the probability that a pair of tuples, one from
each input relation, have the same join attribute. Let Xi be
a random variable that denotes the number of output tuples
produced due to processing the i
th
input tuple. Thus, with
this model, on receiving the i
th
input tuple, the expected
number of new output tuples produced is:
E[Xi] =
Xi1
j=1
Ij;i
=
Xi1
j=1
2p1p2^
= 2p1p2^(i  1)
Here, Ij;i is an indicator random variable that is 1 when the
tuple i joins with tuple j. Note that this happens when i
and j belong to distinct input channels (with probability
2p1p2, the multiplier 2 being for the two ways of exclusively
assigning the tuples to the two input channels) and they
have the same value for the join attribute.
Equating the expected number of total tuples generated
after processing all the input to jI1jjI2j we get that:
jI1Xj+jI2j
i=1
E[Xi] = jI1jjI2j
p1p2^(jI1j + jI2j)(jI1j + jI2j  1) = jI1jjI2j
=) ^ = 
jI1j + jI2j
jI1j + jI2j  1
Thus, after seeing nin input tuples the expected number
of total output tuples generated is:
NBout(nin) =
Xnin
i=1
E[Xi]
= ^p1p2nin(nin  1)
 p^ 1p2n
2
in
We approximate the function NBout(nin) since for large
values of nin (which is very likely during large scale data
processing) the approximation is very close to the actual
function. Note that the approximation is required to con-
form to the requirements of a Geometric Program.
For joins, the other parameters of interest for dierent op-
erating mode, i.e., normal, during the recovery of the down-
stream, and during the recovery of the join) are discussed in
Sections 5.3.2, 5.3.3, and 5.3.4, respectively.
B.3 Select
We model a select operator that has no output queues and
can skip over input tuples during recovery.
B.3.1 Modeling Overhead of Fault Tolerance
A select operator with selectivity  processes, on average,

1
input tuples to generate a single output tuple where
each input tuple takes t
cpu
time to process. For the strat-
egy CHCKPT, since there is no output queue or state for
select, each checkpoint is assumed to cost a xed time (some
multiple of t
io
we refer to be t
ckpt
.)
Thus,
t
cpu
a = 
1
t
cpu
+ I
M
t
io
+ I
C
t
ckpt
(c)
1
where c is the number of tuples processed between consecu-
tive checkpoints (a measure of checkpoint frequency.)
To understand how the values for 
 and k are set, note
that, on average, the select operator produces nin output
tuples for every nin input tuples it processes. Thus,
NBout(nin) = nin = 
n
k
in
Hence, 
 =  and k = 1.
We use a similar reasoning for deriving 
 and k for other
operators too.
Integration within the generic operator model:
Once we have determined t
cpu
a , 
, and k we can plug these
constants in the generic operator model template we derived
in the previous subsection to get the model for the select op-
erator.
B.3.2 Modeling Replay Request Times
For NONE and CHCKPT we process on an average 
1
input tuples to generate each output tuple and each input
tuple takes t
cpu
time to process. For MATERIALIZE we
read tuples from disk. Thus,
t
cpu
a = (I
N
+ I
C
)t
cpu

1
+ I
M
t
io
For strategies NONE and CHCKPT, we need to process
the input tuples to generate the tuples to output for the
downstream node. Thus the NBout(nin) function is identi-
cal to the case for normal processing. For MATERIALIZE,
we read the output tuples from disk (the materialized tu-
ples act as the \input&quot; tuples for this recovery operation)
and send them downstream and hence NBout(nin) = nin.            Thus, the overall function is dened as follows.

 = (I
N
+ I
C
) + I
M
1
k = 1
B.3.3 Modeling Recovery Time
In the existing model we recreate everything from the up-
stream tuples. Thus the recovery output prole looks sim-
ilar to the normal processing. Hence, 
 =  and k = 1
and the minimum amount of work done per output tuple is:
t
cpu
a = 
1
t
cpu
.
We will need to generate at most 1 tuple for NONE and
MATERIALIZE and c tuples for CHCKPT.
B.4 Aggregate
For the aggregate operators used in our experiments the
nal output ts in memory and that is the case that we
model in this section. The aggregate operator as dened
in this section can also perform aggregates after grouping
the input tuples on a certain attribute. Note that we as-
sume that the aggregate operator keeps the output tuples
it has produced and sent downstream in memory (and by
our previous assumption, this information can be stored in
memory.)
The aggregate operator works in two distinct phases. In
the rst phase, no output is produced as the aggregated out-
put tuples are incrementally computed, while in the second
phase, the computed aggregates from the rst phase are sent
downstream. The second phase can be viewed as a select op-
erator with selectivity one and that receives all of its input
at an innite rate and can process each input at a rate de-
termined by the time it takes to access a tuple in memory.
The time spent in the rst phase is included as the delay
terms D
N
; D
RD
; D
RS
.
We take the checkpoint size to be jIuj. Here, jIuj is the
cardinality of the input tuples and  is the selectivity of
the aggregate. Note that there is an initial buildup phase,
as the number of groups in the output are determined. For
example, if there are 8192 groups, we will need to see at least
8192 input tuples before these groups are discovered. Since
we experiment with output groups of sizes 8192 while the
input tuples are in the order of a million tuples, we ignore
this buildup phase and approximate the state to checkpoint
to consist of 8192 tuples form the outset. The average state
size for a blocking operator like sort will be half of the nal
output (since the size of the state will linearly increase with
time.)
B.4.1 First Tuple Delay
To compute the delay in producing the rst tuple, it
should be noted that an aggregate rst processes all of its
input tuples before producing the rst tuple. Further, in
case of CHCKPT, the operator also takes checkpoints of the
aggregated state before the rst tuple is produced and thus
the production of the rst tuple is further delayed. As we as-
sumed during the analysis of the Select operator, we assume
that each checkpoint incurs an extra cost of t
ckpt
.
D
N
= jIuj max(x
N
u ; t
cpu
) + I
C
c
1
jIuj(jIuj)t
io
+
I
C
t
ckpt
jIujc
1
The rst term represents the total amount of time required
to only process the input tuples; the second term represents
the amount of time required to only write out the total size
of all checkpoints to disk, assuming sequential IO; and the
last term is the seek time at the beginning of each checkpoint
multiplied by the total number of checkpoints taken.
B.4.2 Modeling Overhead of Fault Tolerance
Before we model the overhead of fault tolerance we note
that the aggregate operator is blocking and works in two
stages: in the rst the tuples are processed and aggregated
while in the second the aggregated tuples are sent down-
stream. Since the operator is blocking, the time to produce
the rst tuple (captured by the term D
N
) accounts for the
rst stage. For the second stage, we start with the set of
aggregated tuples (grouped-by certain attributes) and send
them downstream. It is this second stage that we model
now. The output tuples are produced only after processing
all the input tuples. Each output tuple takes t
cpu
time to be
processed. The processing required by each tuple to output
is essentially the cost of looking it up in memory and sending
it downstream. We take t
cpu
to be this cost per tuple.
t
cpu
a = t
cpu
+ I
M
t
io

 = 1
k = 1
The 
 and k values are each 1 since each output tuple corre-
sponds to an aggregate on exactly one grouped-by attribute
(that was computed in the rst stage of operation) and thus,
NBout(nin) = nin. Materialization of output incurs an ex-
tra cost of t
io
.
B.4.3 Modeling Replay Request Times
When a downstream node requests a replay of older tu-
ples, the aggregate resends the requested tuples from mem-
ory. Thus, the processing is equivalent to the processing of
the second stage of the aggregate operator as observed in
the normal mode of processing. Hence,
t
cpu
a = t
cpu

 = 1
k = 1
B.4.4 Modeling Recovery Time
To model the recovery time for the aggregate operator it is
necessary to know when the failure occurred since the recov-
ery times, on average, dier in the two dierent stage of the
computation. This is because if a failure occurs randomly
in the rst stage, the amount of work lost is half of the total
(in expectation.) On the other hand, if the failure occurs in
the second half, the loss is always the total work.
If the blocking operator is the last operator in the query
plan (i.e., top operator), or if the operator is selective (an
example of a non-selective blocking operator is sort) then the
amount of time spent in the second stage is negligible and
we can assume that the operator fails only in the rst stage.
We derive our model using this assumption and discuss the
modications required for the cases where the assumptions
are violated later.
In the case of CHCKPT, the operator reads tuples from
disk and recreates the state. For this recovery operation
the input are the tuples in the checkpointed state and the
output is the state in memory. If t
io
is the time it takes to
read one tuple from disk then with CHCKPT,
t
cpu
a = t
io   and since each tuple read from disk becomes a part of the
state in memory,

 = 1
k = 1
Note that the above discussion is only applicable for
CHCKPT, for NONE and MATERIALIZE there is no state
to read back. In the case of the CHCKPT the average state
size has to be read back into memory which is of size jIj.
After reading this state in memory the operator also needs
to process the input tuples from the upstream nodes that
were processed before but whose change on the in-memory
state was not checkpointed. On an average, we need to pro-
cess
1
2
c tuples with a cost of t
cpu
per tuple. We include this
extra cost of
1
2
ct
cpu
to the delay term for recovery (D
RS
).
Since we don&apos;t generate any output tuples, we represent
the time taken to recover using the delay function D
RS
.
Upon failure, NONE and MATERIALIZE need to reprocess,
on an average, half the input tuples, and CHCKPT needs to
reprocess, on average, half of the tuples it processes between
consecutive checkpoints. Thus,
D
RS
=

(I
N
+ I
M
)
1
2
jIuj + I
C
0:5c

max(x
RD
; t
cpu
)
When the blocking operator is not at the end of the
pipeline, the only modication required in our model deriva-
tion is to change the expected number of tuples to reprocess.
As the time spent in the second stage increases, it becomes
more likely that on failure NONE and MATERIALIZE have
to reprocess their entire input to recover state and that the
CHCKPT has to only read the checkpointed state to recover.
To deal with this situation:
1. The optimizer can be used to get an lower and a upper
bound on the expected runtime by assuming rst that
failures only occur during the rst stage and then that
the failures only occur during the second stage.
2. Another alternative would be to make an estimate of
relative runtime of the two stages. For example, if the
aggregate operator is at the beginning of a pipeline
(rather than at the end), it is more likely that upon
a failure, the aggregate was in its second stage; in such
a situation we can take the expected tuples to be repro-
cessed to be the entire input (or the entire checkpoint,
for the strategy CHCKPT.) Note that the time to send
the tuples will not change. After sending the tuples,
the operator will sit idle. But if it fails, recovering it is
equivalent to re-evaluating all the tuples and regener-
ating the state.
Note that the presence of blocking operators does not af-
fect the operator models for normal processing and when
the downstream operator fails.</doc>
    </owl:NamedIndividual>
</rdf:RDF>



<!-- Generated by the OWL API (version 3.2.5.1912) http://owlapi.sourceforge.net -->

