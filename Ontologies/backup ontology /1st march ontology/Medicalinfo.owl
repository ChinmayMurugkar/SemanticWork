<?xml version="1.0"?>


<!DOCTYPE rdf:RDF [
    <!ENTITY owl "http://www.w3.org/2002/07/owl#" >
    <!ENTITY xsd "http://www.w3.org/2001/XMLSchema#" >
    <!ENTITY owl2xml "http://www.w3.org/2006/12/owl2-xml#" >
    <!ENTITY rdfs "http://www.w3.org/2000/01/rdf-schema#" >
    <!ENTITY rdf "http://www.w3.org/1999/02/22-rdf-syntax-ns#" >
]>


<rdf:RDF xmlns="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#"
     xml:base="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
     xmlns:owl2xml="http://www.w3.org/2006/12/owl2-xml#"
     xmlns:owl="http://www.w3.org/2002/07/owl#"
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    <owl:Ontology rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Datatypes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Object Properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a -->

    <owl:ObjectProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Data properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Classes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Cloud_computing_and_internet_scale_computing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Cloud_computing_and_internet_scale_computing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Data_provenance_workflow_and_cleaning -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Data_provenance_workflow_and_cleaning">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Indexing_and_storage_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Indexing_and_storage_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Information_retrieval -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Information_retrieval">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Leveraging_hardware_for_data_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Leveraging_hardware_for_data_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Location_and_sensor_based_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Location_and_sensor_based_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_and_uncertain_databases -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_and_uncertain_databases">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_data__fuzzy_data_and_data_provenance -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_data__fuzzy_data_and_data_provenance">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Schema_mapping_and_data_integration -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Schema_mapping_and_data_integration">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Service_oriented_computing_data_management_in_the_cloud -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Service_oriented_computing_data_management_in_the_cloud">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Shortest_paths_and_sequence_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Shortest_paths_and_sequence_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Social_networks_&amp;_community_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Social_networks_&amp;_community_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Spatial_and_temporal_data_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Spatial_and_temporal_data_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Stream_and_complex_event_processing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Stream_and_complex_event_processing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#books -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#books">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_mining -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_mining">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_in_cloud -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_in_cloud">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_stream -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_stream">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#distributed_database -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#distributed_database">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#energy_performance -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#energy_performance">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#incomplete_information_and_award -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#incomplete_information_and_award">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#mining_and_complex_events -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#mining_and_complex_events">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#olap_and_decision_support -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#olap_and_decision_support">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pubmed -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pubmed"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_database_optimization -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_database_optimization">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_query_processing_on_semi-structured_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_query_processing_on_semi-structured_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_skyline_query_processing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_skyline_query_processing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_understanding_data_and_queries -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_understanding_data_and_queries">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2012 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2012">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#xml_and_semistructured_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#xml_and_semistructured_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.w3.org/2002/07/owl#Thing -->

    <owl:Class rdf:about="&owl;Thing"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Individuals
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dhp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dhp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>LazyFTL  A Page level Flash Translation Layer Optimized for NAND Flash Memory ### Dongzhe Ma Jianhua Feng Guoliang Li Department of Computer Science and Technology Tsinghua University  Beijing 100084  P R  China mdzfirst yahoo com cn   fengjh  liguoliang  tsinghua edu cn ABSTRACT Flash is a type of electronically erasable programmable readonly memory  EEPROM   which has many advantages over traditional magnetic disks  such as lower access latency  lower power consumption  lack of noise  and shock resistance  However  due to its special characteristics     ash memory cannot be deployed directly in the place of traditional magnetic disks  The Flash Translation Layer  FTL  is a software layer built on raw    ash memory that carries out garbage collection and wear leveling strategies and hides the special characteristics of    ash memory from upper    le systems by emulating a normal block device like magnetic disks  Most existing FTL schemes are optimized for some speci   c access patterns or bring about signi   cant overhead of merge operations under certain circumstances  In this paper  we propose a novel FTL scheme named LazyFTL that exhibits low response latency and high scalability  and at the same time  eliminates the overhead of merge operations completely  Experimental results show that LazyFTL outperforms all the typical existing FTL schemes and is very close to the theoretically optimal solution  We also provide a basic design that assists LazyFTL to recover from system failures  Categories and Subject Descriptors D 4 2  Operating Systems   Storage Management   Secondary storage  B 7 1  Integrated Circuits   Types and Design Styles   Memory technologies  B 8 2  Performance and Reliability   Performance Analysis and Design Aids General Terms Design  Experimentation  Performance  Reliability     This work is partly supported by the National Natural Science Foundation of China under Grant No  60873065  the National Grand Fundamental Research 973 Program of China under Grant No  2011CB302206  and the National S T Major Project of China  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specif c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  Keywords Flash translation layer  LazyFTL  address translation  garbage collection ### 1  INTRODUCTION Recent years have witnessed a rapid development of    ash technologies  Advantages of    ash memory  such as high density  low access latency  low power consumption  and shock resistance  greatly bene   t database systems and other dataintensive applications  However     ash memory cannot be deployed directly in the place of traditional magnetic disks due to its special characteristics  Like other EEPROM devices  if a page has been programmed  an erase operation needs to take place before new data can be written  To make things worse  the granularity of    ash erase operations is much larger than that of read and write operations  If an in place update is to be performed  we need to copy all valid pages in the corresponding block into the RAM  update the page in the RAM  erase the block  and write all valid pages back  This update method will not only degrade performance of the    ash memory  but also reduce the life span of the chip and bring a potential consistency problem  To solve these problems  out of place updates are adopted  That is  when a page is to be overwritten  we allocate a new free or erased page  put the new data there  and use a software layer called FTL to indicate the physical location change of the page  In addition  after about 10 000     100 000 erase write cycles  some blocks may become unstable and malfunction  Although a few blocks are reserved to replace broken ones  these extra blocks will eventually be exhausted  A technology named wear leveling is usually employed in the FTL to prolong the life span of    ash memory by distributing erase cycles across the entire memory  A lot of work from the database community focuses on designing e   cient index structures  such as BFTL  29      Tree  13   FlashDB  25   LA Tree  2   and FD Tree  22   Some of these technologies are built upon the FTL while others work directly on raw    ash memories  realizing functionalities of the FTL themselves  for example  address translation  garbage collection  and wear leveling  In all cases  the FTL is a crucial factor to all    ash based technologies  Besides  since    ash memories are purely electronic devices and have no moving parts  they have no seek or rotation latency like magnetic disks  Therefore  random access of    ash memory can be as fast as sequential access and the latency of    ash memory is almost linearly proportional to the amount of data being accessed  no matter where the data 1Table 1  Magnetic Disk vs  NAND Flash  19  Read Write Erase Magnetic Disk 12 7 ms 13 7 ms N A NAND Flash 80   s  2 KB  200   s  2 KB  1 5 ms  128 KB  is located  Another feature making    ash memories di   erent is that the write latency of    ash memory is usually several times larger than the read latency as shown in Table 1  we adopt the con   guration described in  19   since it takes longer to physically inject electrons into a storage cell than sense its status  Therefore  most    ash technologies tend to focus on optimization of write performance  even if increase read operations sometimes  Our study makes several contributions as follows      In this paper  we propose a novel FTL scheme named LazyFTL  which is optimized for NAND type    ash memories  To avoid the heavy overhead of merge operations in existing block level and hybrid mapping FTL schemes  LazyFTL employs a page level mapping table  This makes LazyFTL a high performance FTL scheme compared with other existing ones  However  a page level mapping scheme is hard to deploy on NAND type    ash memories since they can only be programmed in pages  If any part of the mapping table is immediately written in    ash memory whenever it is modi   ed  performance will be a   ected  If dirty data is kept in the SRAM and only written in    ash memory when it is swapped out  we risk losing critical information and leaving the system in an inconsistent state  To solve this problem  LazyFTL keeps two small areas in    ash memory and updates the page level mapping table in a lazy manner      We implement a trace driven simulator to help evaluate the performance of LazyFTL and six other typical FTL schemes  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  Our empirical evaluation demonstrates that LazyFTL outperforms all typical FTL schemes while achieving consistency and reliability at the same time  Experimental results also show that LazyFTL successfully approaches the theoretically optimal solution      We test and measure the performance of LazyFTL when the ratio of the capacity of    ash memory to that of the SRAM is increased  We discover that within a certain scope  LazyFTL can still achieve an excellent performance  This experiment indicates that the scalability of LazyFTL is high since it does not require that the capacity of the SRAM is enlarged as fast as    ash memory  We also analyze the reliability of LazyFTL theoretically and present an algorithm that assists LazyFTL to recover from system failures e   ciently  The rest of this paper is organized as follows  In Section 2  we make a short introduction of    ash memory and previous FTL designs  In Section 3  we provide an overview of the proposed LazyFTL scheme  A detailed description of the major functionalities of LazyFTL is given in Section 4 and Section 5 de   nes the states of pages and demonstrates the transition of states using a simple example  Experimental results are presented and analyzed in Section 6  We also analyze the scalability and reliability issues in Section 6  Finally  Section 7 draws some conclusions and directions for future work  2  BACKGROUND 2 1 Introduction of Flash Memory There are two types of    ash memories  namely NOR and NAND  NOR provides independent address and data buses  allowing random access to any location of the memory  which makes NOR a perfect replacement of the traditional readonly memory  ROM   such as the BIOS chip of computers  On the other hand  address and data share the same I O interface in NAND  which means that NAND can only be accessed in pages  though it has a higher density and lower cost per bit than NOR  NAND is usually used as a secondary storage device  1   In the rest of this paper  we use the term    ash to refer to NAND type    ash memory unless we explicitly indicate NOR type    ash memory  Each    ash chip consists of a constant number of blocks that are basic units of erase operations  And each block consists of a constant number of pages that are basic units of read and write operations  Most    ash memories also provide a spare area for each page to store out of band  OOB  data  such as the error correction code  ECC   the logical page number  and the state    ag for the page  As technology advances  di   erent    ash memory organizations have been developed as shown in Table 2  Table 2  Organization of Flash Chips  23  28  13  Block Size Page Size OOB Size Small block SLC 16 KB 512 bytes 16 bytes Large block SLC 128 KB 2 KB 64 bytes Large block MLC 1 512 KB 4 KB 128 bytes 2 2 Overview of FTL According to the granularity of the mapping unit  existing FTL schemes can be divided into four categories  page level  block level  hybrid  and variable length mappings  Just as the name implies  in page level FTLs  a logical page number  LPN  can be directly translated to a physical page number  PPN   In other words  page level FTLs need to maintain a mapping entry for every logical page  which means that the mapping table of page level FTLs is much larger than any other types  In fact  all page level FTL schemes store the entire mapping table in    ash memory and load the currently used parts into the SRAM dynamically using the LRU algorithm or some other strategy  However  since hot and cold data can be easily separated  page level FTLs are quite e   cient and    exible  On the contrary  in block level FTLs  an LPN is    rst divided into a logical block number  LBN  and an in block o   set  Then the LBN is translated to a physical block number  PBN  and    nally some search algorithm is employed to    nd the target page  It is obvious that the mapping table of block level FTLs is quite small and can be easily stored in the SRAM  Nevertheless  due to the mixture of hot and cold data and the overhead of moving valid pages during garbage collection  the performance of block level FTL schemes is limited compared with other mapping methods  Hybrid mapping schemes try to achieve the    exibility of page level FTLs while keeping the mapping table relatively small and comparable to the block level methods by dividing the    ash memory into a data block area  DBA  and a log block area  LBA   Block level mapping is applied to the 1 An MLC device is capable of storing more than one bit of information in a single storage cell  2Figure 1  Three Types of Merge Operations DBA which occupies most of the    ash memory while each valid page in the LBA is traced by another page level mapping  The LBA is very small and generally takes less than 5 percent of the entire    ash memory  In hybrid FTLs  except HFTL  17    the LBA is used to store overwriting data and di   erent schemes adopt di   erent strategies to merge data in the LBA to the DBA to generate new space for the LBA  There are three types of merge operations as illustrated in Figure 1  A full merge is a general but expensive operation in which all up to date pages need to be copied to a new allocated block and then old blocks are erased and put back into the free block pool  The partial and switch merges are e   cient but can only be done in special cases since they can only be done when pages in the log block or the replacement block are all free or valid and each valid page is written in their own place  Although many hybrid FTL schemes try to do partial or switch merges whenever possible  full merges are di   cult to avoid with di   erent access patterns  This makes an insuperable bottleneck for all hybrid FTL schemes  It is also possible to map variable length continuous logical pages to continuous physical pages in    ash memory  In this case  granularity can be adjusted dynamically when access pattern changes  However  since sizes of di   erent mapping units are not identical and are changing  mapping entries can only be stored in some type of search tree  and as a result  the table look up overhead of variable length mappings is higher than other schemes of which the mapping table is nothing more than a simple address array  2 3 Page level FTL Schemes The    rst FTL scheme was patented by Ban in 1995  3  and was adopted by the PCMCIA as a standard for NORbased    ash memories several years later  12   There is one issue that NOR based FTLs should handle in the    rst place  When a page is overwritten  the relevant entry in    ash memory needs to be updated to keep the operation atomic and reliable   Remember that page level FTL schemes keep an entire mirror of the mapping table in    ash memory to reduce the SRAM overhead   This presents no di   culty to the NOR based FTL since NOR type    ash memories can be programmed in bytes  By assigning a replacement page list for the relevant mapping page when necessary  this mapping page can be updated  written in the    rst free entry of the same o   set in the replacement page list  several times as long as the length of the list without rewriting the entire mapping page  12  9   DFTL  Demand based FTL   10   another page level FTL scheme  makes the    rst attempt to transfer the former NORbased FTL to NAND type    ash memories  omitting the replacement page part  This scheme  though e   cient  faces a serious reliability problem since all modi   ed information in the SRAM will be lost if a system failure occurs  In this case  spare areas of all data pages need to be scanned until the system recovers to a consistent state  Therefore  DFTL is not suitable  we believe  for circumstances where    ash memory is regarded as a permanent and reliable storage device  2 4 Block level FTL Schemes Ban patented two other FTL schemes in 1999  4  8  9   These schemes are designed for NAND type    ash memories and also known as the NFTLs  In this paper  they will be cited as NFTL 1 and NFTL N  NFTL 1 is designed for    ash memories that have a spare area for each page and NFTL N is for devices without such storage  When a page is overwritten  NFTL 1    rst allocates a replacement block for the relevant logical block if there is none and writes overwriting pages one after another from the beginning of the replacement block  Since pages are written in an out of place manner in replacement blocks  NFTL 1 needs to scan all the spare areas in the replacement block in reversed order to    nd the most up to date version of a requested page  Fortunately  the spare areas in NAND type    ash memory are using a di   erent addressing algorithm that is optimized for fast reference and the overhead of this search process is relatively low  On the other hand  since some models of NAND    ash memories have no spare areas to support fast search  NFTL N keeps a replacement block list for some of the logical blocks when necessary and write requests for each logical page are    rst handled by the    rst block in the list and then the next one  keeping the in block o   set identical with that of the logical address  If all pages in the list with the request o     set have been programmed  a new block is allocated and appended to the back of the list  2 5 Hybrid FTL Schemes BAST  Block Associative Sector Translation  is the    rst hybrid FTL scheme proposed in 2002  15   which is essentially an altered version of NFTL 1  As mentioned earlier  hybrid FTL schemes build a page level mapping for the LBA  To keep this table small enough to reside in the SRAM  BAST limits the total number of replacement blocks  also known as log blocks   Obviously  the read performance of BAST is better than NFTL 1 because the SRAM is several orders of magnitude faster than    ash memories  However  BAST does not work well with random overwrite patterns which may result in a block thrashing problem  20   Since each replacement block can accommodate pages from only one logical block  BAST can easily run out of free replacement blocks and be forced to reclaim replacement blocks that have not been    lled  Therefore  the utilization ratio of replacement blocks in BAST is low both theoretically and experimentally  To solve the block thrashing problem  another hybrid FTL scheme named FAST  Fully Associative Sector Translation  was put forward  20   FAST goes to the other extreme by allowing a log block to hold updates from any data block  Although FAST successfully delays garbage collections as much as possible  the system wide latency for reclaiming a single log block may turn out to be longer than BAST  since the associativity of log blocks is only limited by the number of pages in a block  The associativity of a log block is de   ned as the number of di   erent data blocks whose most up to date pages are located in the log block  The higher the associativity of a log block is  the more expensive it is to 3reclaim it  To increase the proportion of partial and switch merges  FAST reserves a sequential log block to perform sequential updates  This optimization is also limited since in modern multi process environments  a sequential write is often interrupted by random writes and other sequential writes  18   In the following years  researchers tried to    nd some intermediate proposals to balance between the log block utilization and the reclamation overhead  There are some typical representatives such as Superblock FTL  14   SAST  SetAssociative Sector Translation   26   LAST  Locality Aware Sector Translation   18   and A SAST  Adaptive SAST   16   Both Superblock FTL and SAST share  at most  K log blocks among N data blocks  The di   erence is that Superblock FTL keeps a page level map in the spare areas of the superblock while SAST restricts the number of log blocks and maintains the page level map in the SRAM  Due to the size limitation of spare areas  Superblock FTL needs to search at most three spare areas to    nd a requested page  And in SAST  di   erent data block sets may compete for log blocks as a result of the small LBA  A common problem with these two schemes is that they both need to be tuned beforehand  which means that their performance may get worse if access pattern changes  Unlike Superblock FTL and SAST  LAST divides the LBA into several functional segments to fully utilize the log blocks while keeping the reclamation overhead as low as possible  Longer requests are written in the sequential log bu   er to perform partial or switch merges  Hot data that might be overwritten soon is written in the hot partition of the random log bu   er and other write requests are served by the cold partition  A SAST is an optimized version of SAST which loosens the restriction of maximum number of log blocks shared within a data block set and can merge and split data block sets dynamically  KAST  K Associative Sector Translation   6  is the same as FAST in essence but requires that the associativity of all log blocks should never exceed K  The scheme is designed for real time systems since its reclamation latency is controllable  KAST can be considered as another tradeo    between the log block utilization and the reclamation overhead  Unlike other hybrid schemes  HFTL  Hybrid FTL   17  does not treat the page mapping area as a bu   er of updates  Instead  HFTL employs a hash based hot data identi   cation technique  11  and traces pages from hot blocks with the page level mapping as long as they remain hot  However  when access pattern changes  some hot pages will need to be swapped out  which will introduce an extra overhead  2 6 Other FTL Schemes It is also possible to implement a variable length mapping  One such scheme was proposed in 2004  5  and in 2008 another one named    FTL  which adopts    Tree  13  as the mapping structure  was published  21   The main disadvantage of these schemes is the address translation cost since variable length mappings can only be implemented in search trees  JFTL  proposed in 2009  is a technique to e   ectively deploy journal    le systems on    ash memory using the outof place characteristic  7   which can be built on any other FTL schemes  However  JFTL cannot do anything about the consistency problem of DFTL  Figure 2  Architecture of LazyFTL 3  LAZYFTL OVERVIEW 3 1 Design Principles After explaining the merits and demerits of di   erent types of existing FTL schemes in Section 2  some design principles and considerations will be presented at the beginning of this section  First of all  a storage system should guarantee the reliability of its operations  therefore dirty or altered data should be    ushed into    ash memory before an operation can return  DFTL violates this rule in order to obtain high performance  Although the system can recover by scanning the spare area of all pages  the resulting bootup delay is unacceptable along with the increase of the density and the capacity of    ash memories  To design a highly e   cient FTL scheme  the mapping granularity should be decided in the    rst place  Among all the FTL schemes discussed in Section 2  the block level mapping cannot distinguish cold data from hot ones and has to move cold data unnecessarily during the garbage collection procedure  The variable length mapping can adjust its mapping granularity dynamically but the high complexity of address translation makes an inherent weakness  The hybrid mapping is feasible since costly full merge operations can be avoid as much as possible by partitioning the LBA or by sharing log blocks  However  no matter how subtly they are designed  hybrid mapping schemes can hardly eliminate full merge operations completely  The page level mapping is the most e   cient and e   ective mapping granularity but can hardly be applied to NAND    ash without violating the    rst rule  This is not true however  The LazyFTL scheme proposed in this paper proves that by adopting an update bu   er  like the LBA in hybrid mapping schemes  the pagelevel FTL can be transferred to NAND    ash while keeping reliability and consistency at the same time  3 2 LazyFTL Architecture The architecture of the proposed LazyFTL scheme is presented in Figure 2  As illustrated  LazyFTL divides the entire    ash memory into four parts  a data block area  DBA   a mapping block area  MBA   a cold block area  CBA   and an update block area  UBA   All these parts except the MBA are used to store user data  Pages in the DBA are tracked by a page level mapping table called the global mapping table  GMT   The GMT is organized in pages and stored in the MBA  A small cache adopting the LRU algorithm or similar is reserved in the SRAM to provide e   cient reference of the most frequently 4accessed parts of the GMT  A secondary table named the global mapping directory  GMD  is stored in the SRAM and keeps physical locations of all valid mapping pages of the GMT  The CBA is used to accommodate cold blocks and the UBA is used to accommodate update blocks as the names indicate  The main di   erence between LazyFTL and the original page level FTL scheme  3  12  is that LazyFTL reserves two small partitions  the CBA and the UBA  to delay modi     cations of the GMT caused by write requests or valid page movements  The total size of the CBA and the UBA is relatively small compared with the entire    ash memory and  like the LBAs in hybrid FTL schemes  another page level mapping table which is called the update mapping table  UMT  is built on these two areas  The UMT can be implemented as a hash table or a binary search tree to support e   cient insertion  deletion  modi   cation  and reference  The number of entries in the UMT is quite small  so these operations will not introduce too much overhead  A block in the UBA called the current update block  CUB  is used to handle write operations  When the CUB over     ows  another free block is allocated and becomes the new CUB  Similarly  there is a current cold block  CCB  in the CBA dealing with moved data pages  As a matter of fact  LazyFTL treats    lled cold blocks in the CBA and    lled update blocks in the UBA in the same way  In other words  the relative size of the CBA and the UBA can be adjusted dynamically  If the proportion of hot data rises  the convert cost  see 4 1  of blocks in the UBA will decrease more slowly and the UBA will expand  If space utilization increases  the CCB will be    lled faster than the CUB and the CBA will be enlarged  In this way  LazyFTL can tune itself automatically for di   erent access patterns  It is necessary to mention that it is also feasible to divide the CBA and the UBA into smaller functional segments like LAST  18   However  we decide to keep the design as simple as possible since the current design is quite e   cient and there is no room for performance improvement  We also maintain two bitmaps in the SRAM  the update    ag and the invalidate    ag  These two bitmaps help mark the states of all pages in the CBA and the UBA  Each bit in the update    ag indicates whether the translation information of the corresponding page needs to be updated to the GMT  And each bit in the invalidate    ag indicates whether the target page that the corresponding GMT entry points to needs to be invalidated  4  MAJOR FUNCTIONALITIES 4 1 Convert Operation Since the UMT is stored in the SRAM to support e   cient reference  the CBA and the UBA cannot be too large and will eventually over   ow  in which case  a convert operation is carried out  In hybrid FTL schemes  a merge operation needs to copy valid pages in the victim log block out of the LBA and reorganize relevant data blocks most of the time due to the in place storage pattern in the DBA  However  LazyFTL only has to convert the victim block to a normal data block logically since pages in the DBA are also stored in an out of place manner  The only overhead of the convert operation is caused by the GMT updates which will be proved to be much cheaper than reorganizing data blocks  A convert operation is achieved in four steps as illustrated in Algorithm 1  First  a    lled block in the CBA or the UBA Algorithm 1 Convert block B Input  B  a victim block in the CBA or the UBA Output  B  a normal data block 1  mapping pages         2  update entries            Gather relevant information    3  for each valid page P in B do 4  E      LPNP  PPNP  5  remove E from the UMT 6  if the update    ag of P is set then 7  P            LPNP   number of entries per page    8  mapping pages     mapping pages      P       9  update entries     update entries      E  10  end if 11  end for    Gather entries that can also be updated    12  for each entry E     in the UMT do 13  if the relevant mapping page     mapping pages and the update    ag of E     is set then 14  update entries     update entries      E       15  the update    ag of E         0 16  end if 17  end for    Make sure that each page is loaded only once    18  sort update entries by LPN    Update the GMT and invalidate old pages    19  for each entry E            update entries do 20  load the relevant mapping page P        if necessary 21  o   set     LPNE       mod number of entries per page 22  if the invalidate    ag of E        is set then 23  invalidate P         o   set  24  the invalidate    ag of E            0 25  end if 26  P         o   set      PPNE       27  if no more updates to P        then 28  write P        to the MBA 29  update the GMD 30  invalidate the old page of P        31  end if 32  end for with the lowest convert cost is selected as the victim  The convert cost of each candidate block is de   ned as the number of di   erent mapping pages that valid pages in this block whose translation information need to be updated to the GMT belong to  Second  all relevant mapping pages are found and all mapping entries in the UMT that belong to these mapping pages are collected  including entries pointing to other blocks in the CBA and the UBA  Then modi   cations of the mapping pages are performed  Finally  mapping entries in the UMT that point to the victim block are removed and the victim block is converted to a normal data block logically  One thing that should take our attention is that for the sake of e   ciency an entry in the UMT is removed only when the block where the target page is located is converted  no matter whether this entry is updated in that operation  In other words  all valid pages in the CBA and the UBA are tracked by the UMT  even if some of them have already been updated when other blocks in the CBA or the UBA are converted  As mentioned earlier  to help identify pages whose physical locations have not been updated to the GMT  an update 5Algorithm 2 Reclaim block B Input  B  a victim block in the DBA or the MBA Output  B  a free block 1  if B is a mapping block then 2  for each valid page P in B do 3  move P to the MBA 4  end for 5  else 6  for each valid page P in B do 7  if LPNP can be found in the UMT then 8  the invalidate    ag of UMT LPNP      0 9  else 10  if the CCB is    lled up then 11  if the UBA   CBA are    lled up then 12  select a victim block and convert it 13  end if 14  allocate a new block for the CCB 15  end if 16  move P to the CBA 17  add  LPNP  PPNP  to the UMT 18  the update    ag of P     1 19  the invalidate    ag of P     0 20  end if 21  end for 22  end if 23  erase B 24  put B into the free block pool    ag bitmap is maintained in the SRAM  Each bit in this bitmap is related to a page in the CBA or the UBA  If the update    ag of a page is 1  we should modify the corresponding entry in the GMT whenever possible and at least before the block this page belongs to is converted  When a page is written to the UBA or moved to the CBA  its physical location changes which means that the initial update    ag of all pages should be 1  And after the relevant GMT entry of a page in the CBA or the UBA is updated or if it is overwritten  its update    ag should be cleared  4 2 Garbage Collection When the number of free blocks decreases to a prede   ned threshold  a victim block from the DBA or the MBA is selected to be erased  The cost to reclaim a certain block B can be de   ned as  Cread   Cwrite      NB   Cerase where Cread  Cwrite  and Cerase indicate the    ash read  write  and erase operation latencies  respectively and NB represents the number of valid pages in block B  Obviously  to reduce the overhead of garbage collection process  the block with the lowest reclaimation cost should be selected as the victim most of the time  After the victim block is chosen  all pages of this block should be scanned and the valid ones should be moved to some other block  If the victim block stores mapping pages of the GMT  valid pages should be moved to a current mapping block  CMB  in the MBA that handles mapping page rewriting and the GMD is modi   ed to track these changes  If the victim block stores user data  the valid pages should be moved to the CCB in the CBA  The philosophy is that these valid pages should be relatively colder than the invalid ones  Note that there are two cases indicating a data page is invalid  If this page has been overwritten and the new verAlgorithm 3 Write page P Input  P  new data to be written 1  if the CUB is    lled up then 2  if the UBA   CBA are    lled up then 3  select a victim block and convert it 4  end if 5  allocate a new block for the CUB 6  end if 7  write P in the UBA    Set the update    ag    8  the update    ag of P     1    Inherit or set the invalidate    ag    9  if LPNP can be found in the UMT then 10  P         UMT LPNP  11  the invalidate    ag of P     the invalidate    ag of P     12  invalidate P     13  the update    ag of P         0 14  the invalidate    ag of P         0 15  else 16  the invalidate    ag of P     1 17  end if 18  add  LPNP  PPNP  to the UMT sion is still located in the UBA  the spare area of this page may have not been marked  However  if the new version has been converted  the old page in the DBA should have been invalidated  Therefore  when the state    ag in the spare area indicates that a page is valid  we should further check whether its LPN can be found in the UMT  If this page has been overwritten  we should ignore it  and at the same time  we should clear the invalidate    ag of the most up to date page since the target that the corresponding GMT entry points to has been erased and will be used to accommodate other data  It is seriously wrong to invalidate an empty page or an innocent one  After all the valid pages have been moved  the victim block is erased and put into the free block pool again  An algorithmic description of garbage collection operations is presented in Algorithm 2  4 3 Write Operation The write operation of LazyFTL is much simpler than the convert operation and the garbage collection operation  We only need to write the new data in the UBA and do some bookkeeping  That is to say  we set the update    ag  inherit or set the invalidate    ag  invalidate the old page in the CBA or the UBA if there is one  and clear its two    ags  The pseudo code of the write operation is given in Algorithm 3  5  STATE TRANSITION 5 1 State Def nition As described earlier  the update and invalidate    ags represent the state of the corresponding page in the CBA or the UBA  By taking pointers in the GMT and the UMT with the same LPN into consideration  we can    gure out all possible states a page may have in LazyFTL  To help readers understand the di   erent page states and their transition paths and conditions  a state transition diagram is given in Figure 3  In this diagram  some states have a two digit binary number on its upper right corner  The    rst digit stands for the update    ag and the second one is the invalidate    ag  Invalid pages are represented by a small 6Figure 3  State Transition of Pages square with a cross inside  such as in state G and H  and those that are pointed neither by the GMT nor by the UMT are omitted  In state F  a pointer in the GMT is pointing to nothing since its target has been moved to the CBA and the block has been erased  Transition conditions are labeled on the path  WRITE means a write operation  GC means the block this page is located in is reclaimed  CONVERT means the corresponding block is converted and UPDATE means that some other block is converted and the entry of this page in the UMT is updated  Among the eight states in Figure 3  state C is the updated state since all update paths arrive at C  When a block is reclaimed  the relevant pages should be in reclaimed state F  All CONVERT paths except one point to D quali   es state D as the converted state  Between state H and state G  there is a path labeled as CONVERT which seems to violate the rule  This is a conversion of another block that contains a valid page which needs to be updated and shares the same mapping page with the one in our discussion  and therefore  this convert operation is di   erent from others  5 2 An Example of State Transition In this section  we give an example to help readers understand how the update and invalidate    ags are manipulated  We will start from a page that has never been written and follow its state transitions as illustrated in Figure 4      A     B As demonstrated in Algorithm 3  the update    ag of each new written page is set as 1 and since no previous written page is found in the UBA or the CBA  the initial value of the invalidate    ag is also set as 1      B     D When the corresponding block is selected as the convert victim  address translation information needs to be updated to the GMT since the update    ag is set  However  no page needs to be invalidated though the invalidate    ag is set since the relevant entry in the GMT has not been used      D     E This operation is similar to the    rst write request  The only thing we should pay attention to is that we do not try to alter the GMT entry or invalidate the old page at this time      E     F In this operation  a data block in the DBA where the old page is located is reclaimed  To tell whether a page is valid  we should    rst check the state    ag in its spare area  If the    ag indicates a valid page  we should further check whether this page has been overwritten in the UBA  In this case  the same LPN is found in the UMT  meaning that this page has been overwritten and thus should be discarded  Meanwhile  the invalidate    ag of the up to date page is cleared since the old GMT entry is currently pointing to an erased page which should not be invalidated again      F     C This time  some other block is converted and the mapping information of this page is updated in passing  Do not forget to clear the update    ag      C     H This page is overwritten again  Unlike the third operation  the old page is invalidated immediately after new data is written in  Note that invalidate    ag of the new page is cleared not because the old page has just been invalidated but because its invalidate    ag is not set  This is so called the inheritance of invalidate    ags in LazyFTL      H     G The block that holds the old page is converted and nothing needs to be done      G     F Another block related to the current page is reclaimed  This time  the old page is already invalidated and there is no need to check the UMT  The di   erence is in the    rst state F of Figure 4  the invalidate    ag is cleared by the GC operation  while in the other state F  the    ag is unchanged      F     D Finally  the block that holds the up to date page is converted and we need to modi   ed the GMT entry but do not try to invalidate the page that the old entry points to just as the two    ags indicate  6  PERFORMANCE EVALUATION To help evaluate the performance and understand other characteristics of the proposed LazyFTL scheme  we implement a trace driven simulator for LazyFTL  For comparison  we also implement six other FTL schemes that are comparable with LazyFTL  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  6 1 Experimental Setup The simulator is built on a large block SLC    ash  see Table 2  which is widely used in enterprise grade    ash memories  The capacity is 1 GB and the access latencies are set as Table 1  7Figure 4  An Example of State Transition We use the Microsoft Research Cambridge block I O traces as described in  24   These traces are collected from 13 enterprise servers for di   erent applications  such as user home directories  print server     rewall web proxy  source control  web SQL server  and media server  which should cover all major access patterns  We have tested all the 36 traces in the package and obtained similar results  The space utilization of the adjusted traces varies from almost empty to 82 87   And the relative standard deviation  RSD  of the numbers of accesses of all touched addresses ranges from 35 12  to 2526 16   The larger the RSD value is  the more frequently hot data in the trace is accessed  It is necessary to point out that though our experiment touches no more than 82  of the address space  the relative performance of LazyFTL will not degrade if the device is    lled up  In fact  we believe that the performance gap between LazyFTL and other existing schemes will expand because other FTLs do not fully utilize every page in a block and a higher space utilization means more frequent garbage collection calls for them  The results presented in this paper were obtained by using trace usr 2 csv  The trace is scaled down to    t our 1 GB    ash memory and 75 9  of the entire address space is touched  with an RSD of 193 60   The largest request size is 256 pages  which are equivalent to 4 blocks or 0 5 MB  However  most requests involve less than 32 pages  We also implement six comparative experiments  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  All these schemes are typical block level or hybrid mapping FTL schemes that focus on optimization of average access performance and do not have to be tuned for speci   c access patterns  We do not try to compare LazyFTL with DFTL  another NAND based page level FTL scheme  On one hand  DFTL has a consistent disadvantage  and on the other  the performance of LazyFTL and DFTL should be similar since neither of them can overcome the theoretical barrier  Suppose that 4 bytes are used to store a single page address or block address 2   then it takes    1 GB    128 KB    4 bytes      32768 bytes or 16 pages in the SRAM to store the block level mapping table in comparative experiments  Another 32768 bytes are allocated in the SRAM to accommodate the page level mapping table of hybrid FTL schemes  which means that    32768 bytes    4 bytes      8192 pages or 128 blocks can be assigned to the LBA  These 128 blocks take only 1 56  of the entire    ash memory  To keep the results comparable  the total size of the UBA and the CBA of LazyFTL is also limited to 128 blocks and at most 14 pages  another 2 pages are used to store the GMD  of the GMT can be cached in the LRU cache in the SRAM  Other data structures are either small or employed in all the schemes and therefore are not considered  All implemented schemes adopt the greedy strategy to 2 The block address is shorter than the page address and may be stored in less than 4 bytes  select garbage collection victims  NFTL 1 selects the block which has the most used pages in its replacement block  NFTL N selects the block which has the longest replacement block list  All hybrid FTL schemes and LazyFTL select the block that has the least valid pages since these pages need to be moved and thus are considered as the overhead of the garbage collection  Some may argue that by taking the access pattern into consideration  we may    gure out which free page is going to be used or which valid page is going to be invalidated  Garbage collection strategy is not within our discussion  however  and we only choose a block that has the least overhead or the most pro   t for a single operation as the victim  In addition  wear leveling is omitted in all our implements because the wear leveling mechanism is relatively independent from other components  Wear leveling involves many issues  such as how to identify worn out blocks  which blocks to reclaim and where to put valid data  Upper applications and the system architechture should also be taken into consideration  If multi process is supported  garbage collection and wear leveling can be carried out in the background without interrupting other operations  However  in embedded environments or real time systems these functions can only be done on demand  since the background way is either impossible or inacceptable  respectively  All in all  wear leveling is another interesting research topic and many existing works have studied this problem  It is unrealistic to permute all the combinations and di   cult to    nd a representative strategy  Another reason why wear leveling is omitted is to provide a clear view of the performance comparison of di   erent FTLs  For the sake of wear leveling  some data will need to be moved from cold blocks to worn out ones on purpose  which will introduce many noise operations  Since our paper focuses on address translation and data organization  it is better not to be distracted from other components  We believe that an identical wear leveling strategy will similarly in   uence all the schemes and will not a   ect our simulation results  To help evaluate the possibility of further improvement of the proposed LazyFTL  we also compare LazyFTL with the theoretically optimal solution  That is to say  each page read request causes a single page read operation  each page write request causes a single page write operation and a block erase operation is invoked every 64 page write operations  When implementing NFTL N  we discover that in the beginning  the response time for write requests decreases quickly when the length limit of replacement block lists is enlarged  However  after a certain point around 7  the write performance becomes stable and constant  Another issue that surprised us is that it seems that the search cost of read requests does not increase much when the limit is relaxed  This is probably because when a certain proportion of    ash memory is used  the replacement block list has little 8Figure 5  Comparison with Existing Schemes chance to get longer before it is reclaimed  even though the limit has been enlarged  In our experiment  the maximum length of the replacement block lists is set to 16  Tuning for LAST is relatively complex  In our experiments  8 blocks are assigned to the sequential log block area and the other 120 blocks in the LBA are random log blocks  The threshold for the hot partition utility and that of the cold partition are set to 0 2 and 0 9 respectively  Other schemes do not need special tuning and will not be presented in this section  6 2 Comparison with Other Schemes The results of our simulation are shown in Figure 5  We will focus on four parameters  the block utilization which indicates the average number of pages that have been used when a block is erased  the total time  the write response time  and the number of erase operations which will directly a   ect the life span of the    ash memory  Among all the implemented schemes  the performance of NFTL 1 is acceptable compared with NFTL N and BAST and its simplicity makes a great selling point  However  the read performance is inferior to other schemes since it needs to search in the replacement block to    nd a certain page  In our experiment  17 565 spare areas need to be scanned on average to serve a one page read request  NFTL N is designed for NAND    ash that does not have a spare area for each page  As a result  the block utilization ratio of NFTL N is very low since pages can only be written in an in place manner  This also implies that NFTL N needs to perform more erase operations than other schemes as illustrated in Figure 5 d   BAST is the    rst hybrid FTL scheme that tries to avoid the search overhead when reading a page  Due to the block thrashing problem  the block utilization ratio of BAST is also very low compared with other hybrid FTL schemes and the performance is worse than any other scheme including NFTL N  That is because NFTL N can always perform partial or switch merges while BAST needs to perform full merge almost all the time  In our experiment  we can hardly    nd any chance for BAST to perform a much cheaper partial or switch merge  The simulation results of the other three hybrid FTLs are nearly the same  A SAST is better than LAST  which is in turn better than FAST  We notice that the performance of FAST is very close to that of NFTL 1  Although FAST tries to delay merge operations as much as possible and reserves a sequential log block to perform partial and switch merges  the advantages gained are counteracted by the heavy overhead of full merges  LAST and A SAST successfully make a tradeo    between the log block utilization and the reclamation overhead  Experimental results indicate that LAST and A SAST achieve a much higher log block utilization than BAST and a much lower write response time than FAST at the same time  As Figure 5 shows  the performance of LazyFTL is much better than those of other schemes and is very close to the optimal result  First of all  LazyFTL does not have to reclaim a block before it is    lled  This implies that the block 9Figure 6  Convert Cost and Scalability of LazyFTL utilization of LazyFTL always equals the number of pages in a block  which also means that LazyFTL needs to perform fewer erase operations than other FTL schemes  If a proper wear leveling strategy is employed and the erase operations are distributed through the entire memory  LazyFTL will also prolong the life span of the    ash chip  One may argue that although LazyFTL successfully avoids merge operations  it has to convert a victim block in the UBA or the CBA when these two areas over   ow  Nevertheless  Figure 6 a  indicates that in current con   guration  LazyFTL needs to rewrite only a small number of mapping pages  We can also see from Figure 5 c  that LazyFTL has a much lower response latency for write requests than other schemes  This issue will be further d</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining"/>
        <doc>Advancing Data Clustering via Projective Clustering Ensembles ### Francesco Gullo DEIS Dept  University of Calabria 87036 Rende  CS   Italy fgullo deis unical it Carlotta Domeniconi Dept  of Computer Science George Mason University 22030 Fairfax     VA  USA carlotta cs gmu edu Andrea Tagarelli DEIS Dept  University of Calabria 87036 Rende  CS   Italy tagarelli deis unical it ABSTRACT Projective Clustering Ensembles  PCE  are a very recent advance in data clustering research which combines the two powerful tools of clustering ensembles and projective cluster  ing  Speci cally  PCE enables clustering ensemble methods to handle ensembles composed by projective clustering so  lutions  PCE has been formalized as an optimization prob  lem with either a two objective or a single objective func  tion  Two objective PCE has shown to generally produce more accurate clustering results than its single objective counterpart  although it can handle the object based and feature based cluster representations only independently of one other  Moreover  both the early formulations of PCE do not follow any of the standard approaches of clustering en  sembles  namely instance based  cluster based  and hybrid  In this paper  we propose an alternative formulation to the PCE problem which overcomes the above issues  We investigate the drawbacks of the early formulations of PCE and de ne a new single objective formulation of the prob  lem  This formulation is capable of treating the object  and feature based cluster representations as a whole  essentially tying them in a distance computation between a projective clustering solution and a given ensemble  We propose two cluster based algorithms for computing approximations to the proposed PCE formulation  which have the common merit of conforming to one of the standard approaches of clustering ensembles  Experiments on benchmark datasets have shown the signi cance of our PCE formulation  as both the proposed heuristics outperform existing PCE methods  Categories and Subject Descriptors H 3 3  Information Storage and Retrieval   Information Search and Retrieval clustering  I 2 6  Arti cial Intelli  gence   Learning  I 5 3  Pattern Recognition   Cluster  ing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  General Terms Algorithms  Theory  Experimentation Keywords Data Mining  Clustering  Clustering Ensembles  Projective Clustering  Subspace Clustering  Dimensionality reduction  Optimization 1 ### INTRODUCTION Given a set of data objects as points in a multi  dimensional space  clustering aims to detect a number of homogeneous  well separated subsets  clusters  of data  in an unsupervised way  18   After more than four decades  a considerable corpus of methods and algorithms has been developed for data clustering  focusing on di erent aspects such as data types  algorithmic features  and application tar  gets  14   In the last few years  there has been an increased interest in developing advanced tools for data clustering  In this respect  clustering ensembles and projective clustering represent two of the most important directions of research  Clustering ensemble methods  28  13  36  29  17  aim to ex  tract a  consensus  clustering from a set  ensemble  of clus  tering solutions  The input ensemble is typically generated by varying one or more aspects of the clustering process  such as the clustering algorithm  the parameter setting  and the number of features  objects or clusters  The output con  sensus clustering is usually obtained using instance based  cluster based  or hybrid methods  Instance based methods require a notion of distance measure to directly compare the data objects in the ensemble solutions  cluster based meth  ods exploit a meta clustering approach  and hybrid methods attempt to combine the  rst two approaches based on hybrid bipartite graph clustering  Projective clustering  32  35  30  34  aims to discover clusters that correspond to subsets of the input data and have di erent  possibly overlapping  dimensional subspaces associated with them  Projected clusters tend to be less noisy because each group of data is represented in a sub  space that does not contain irrelevant dimensions and more understandable because the exploration of a cluster is eas  ier when few dimensions are involved  Clustering ensembles and projective clustering hence ad  dress two major issues in data clustering distinctly  projec  tive clustering deals with the high dimensionality of data  whereas clustering ensembles handle the lack of a priori knowledge on clustering targets  The  rst issue arises due to the sparsity that naturally occurs in data representation As such  it is unlikely that all features are equally relevant to form meaningful clusters  The second issue is related to the fact that there are usually many aspects that character  ize the targets of a clustering task  however  due to the al  gorithmic peculiarities of any particular clustering method  a single clustering solution may not be able to capture all facets of a given clustering problem  In  16   projective clustering and clustering ensembles are treated for the  rst time in a uni ed framework  The underlying motivation of that study is that the high  dimensionality and the lack of a priori knowledge problems usually co exist in real world applications  To address both issues simultaneously   16  hence formalizes the problem of projective clustering ensembles  PCE   the objective is to de ne methods that  by exploiting the information provided by an ensemble of projective clustering solutions  are able to compute a robust projective consensus clustering  PCE is formulated as an optimization problem  hence the sought projective consensus clustering is computed as a so  lution to that problem  Speci cally  two formulations of PCE have been proposed in  16   namely two objective PCE and single objective PCE  The two objective PCE formula  tion consists in the simultaneous optimization of two ob  jective functions  which separately consider the data object clustering and the feature to cluster assignment  A well  founded heuristic developed for this formulation of PCE  called MOEA PCE  has been found to be particularly ac  curate  although it has drawbacks concerning e ciency  pa  rameter setting  and interpretability of results  By contrast  the single objective PCE formulation embeds in one objec  tive function the object based and feature based representa  tions of candidate clusters  Apart from being a weaker for  mulation than two objective PCE  the developed heuristic for single objective PCE  called EM PCE  is outperformed by two objective PCE in terms of e ectiveness  while show  ing more e ciency  Both the early formulations of PCE have their own draw  backs and advantages  however none of them refers to any of the common approaches of clustering ensembles  i e   the aforementioned instance based  cluster based  and hybrid approaches  This may limit the versatility of such early formulations of PCE and  eventually  their comparability with existing ways of solving clustering ensemble problems at least in terms of experience gained in some real world scenarios  Besides this common shortcoming  an even more serious weakness concerns the inability of the two objective PCE of treating the object based and feature based cluster representations as interrelated  This fact in principle may lead to projective consensus clustering solutions that contain conceptual  aws in their cluster composition  In this work  we face all the above issues revisiting the PCE problem  For this purpose  we pursue a di erent ap  proach to the study of PCE  focusing on the development of methods that are closer to the standard clustering ensem  ble methods  By providing an insight into the theoretical foundations of the early two objective PCE formulation  we show its weaknesses and propose a new single objective for  mulation of PCE  The key idea underlying our proposal is to de ne a function that measures the distance of any pro  jective clustering solution from a given ensemble  where the object based and feature based cluster representations are considered as a whole  The new formulation enables the development of heuristic algorithms that are easy to de ne and  at the same time  are well founded as they can ex  ploit a corpus of research results obtained by the majority of existing clustering ensemble methods  Particularly  we investigate the opportunity of adapting each of the various approaches of clustering ensembles to the new PCE prob  lem  We de ne two heuristics that follow a cluster based approach  namely Cluster Based Projective Clustering En  sembles  CB PCE  and a step forward version called Fast Cluster Based Projective Clustering Ensembles  FCB PCE   We show not only the suitability of the proposed heuristics to the PCE context but also their advantages in terms of computational complexity w r t  the early formulations of PCE  Moreover  based on an extensive experimental evalua  tion  we assessed e ectiveness and e ciency of the proposed algorithms  and found that both outperform the early PCE methods in terms of accuracy of projective consensus clus  tering  In addition  FCB PCE reveals to be faster than the early two objective PCE and comparable or even faster than the early single objective PCE in the online phase  The rest of the paper is organized as follows  Section 2 provides background on clustering ensembles  projective clustering  and the PCE problem  Section 3 describes our new formulation of PCE and presents the two developed heuristics along with an analysis of their computational com  plexities  Section 4 contains experimental evaluation and results  Finally  Section 5 concludes the paper  2  BACKGROUND 2 1 Clustering Ensembles  CE  Given a set D of data objects  a clustering solution de ned over D is a partition of D into a number of groups  i e   clus  ters  A set of clustering solutions de ned over the same set D of data objects is called ensemble  Given an ensemble de ned over D  the goal of CE is to derive a consensus clus  tering  which is a  new  partition of D derived by suitably exploiting the information available from the ensemble  The earliest CE methods aim to explicitly solve the label correspondence problem to  nd a correspondence between the cluster labels across the clusterings of the ensemble  10  11  12   These approaches typically su er from e ciency is  sues  More re ned methods fall into instance based  cluster  based  and hybrid categories  2 1 1 Instance based CE Instance based CE methods perform a direct comparison between data objects  Typically  instance based methods operate on the co occurrence or co association matrix W  which resembles the pairwise object similarities according to the information available from the ensemble  For each pair of objects   o 0    o 00    the matrix W stores the number of solutions of the ensemble in which  o 0 and  o 00 are assigned to the same cluster divided by the size of the ensemble  Instance based methods derive the  nal consensus clustering by applying one of the following strategies   i  performing an additional clustering step based on W  using this matrix either as a new data matrix  20   or as a pairwise similarity matrix in  volved in a speci c clustering algorithm  13  22  15    ii  constructing a weighted graph based on W and partitioning the graph according to well established graph partitioning algorithms  28  3  29  2 1 2 Cluster based CE Cluster based CE lies on the principle  to cluster clus  ters   7  28  6   The key idea is to apply a clustering al  gorithm to the set of clusters that belong to the clustering solutions in the ensemble  in order to compute a set of meta  clusters  i e   sets of clusters   The consensus clustering is  nally computed by assigning each data object to the meta  cluster that maximizes a speci c criterion  such as the com  monly used majority voting  which assigns each data object  o to the metacluster that contains the maximum number of clusters which  o belongs to  2 1 3 Hybrid CE Hybrid CE methods combine ideas from instance based and cluster based approaches  The objective is to build a hybrid bipartite graph whose vertices belong to the set of data objects and the set of clusters  For each object  o and cluster C  the edge   o  C  of the bipartite graph usually as  sumes a unit weight  if the object  o belongs to the cluster C according to the clustering solution that includes C  and zero otherwise  36   Some methods use weights in the range  0  1   which express the probability that object  o belongs to cluster C  29   The consensus clustering of hybrid CE methods is obtained by partitioning the bipartite graph ac  cording to well established methods  e g   METIS  19    The nodes representing clusters are  ltered out from the graph partition  2 2 Projective Clustering  PC  Let D be a set of data objects  where each  o 2 D is de ned on a feature space F   f1          jFjg  A projective cluster C de ned over D is a pair hC   Ci such that   C denotes the object based representation of C  It is a jDj dimensional real valued vector whose component C  o 2  0  1   8 o 2 D  represents the object to cluster assignment of  o to C  i e   the probability Pr Cj o  that object  o belongs to C     C denotes the feature based representation of C  It is a jFj dimensional real valued vector whose component  C f 2  0  1   8f 2 F  represents the feature to cluster assignment of the feature f to C  i e   the probabil  ity Pr fjC  that feature f belongs to the subspace of features associated with C  Note that the above de nition addresses all possible types of projective clusters handled by existing PC algorithms  In fact  both soft and hard object to cluster assignments are taken into account the assignment is hard when C  o 2 f0  1g rather than  0  1   8 o 2 D  Similarly  feature to cluster assignments may be equally weighted  i e    C f   1 R  where R is the number of relevant features for C   if f is recognized as relevant   C f   0 otherwise  This repre  sentation is suited for dealing with the output of all those PC algorithms which only select the relevant features for each cluster  without specifying any feature to cluster as  signment probability distribution  Such algorithms fall into bottom up  34  25   top down  32  31  2  37  5   and hybrid ap  proaches  24  35  1   On the other hand  the methods de ned in  34  8  30  handle projective clusters having soft object  to cluster assignment and or feature to cluster assignment unequally weighted  The object based  C  and the feature based   C  repre  sentations of any projective cluster C are exploited to de ne the projective cluster representation matrix  for brevity  pro  jective matrix   XC of C  XC is a jDj jFj matrix that stores  8 o 2 D  f 2 F  the probability of the intersection of the events  object  o belongs to C  and  feature f belongs to the subspace associated with C   Under the assumption of inde  pendence between the two events  such a probability is equal to the product of Pr Cj o    C  o with Pr fjC     C f   Hence  given D   f o1           ojDjg and F   f1          jFjg  ma  trix XC can be formally de ned as  XC   0 B   C  o1   C 1       C  o1   C jFj             C  ojDj   C 1       C  ojDj   C jFj 1 C A  1  The goal of a PC method is to derive from an input set D of data objects a projective clustering solution denoted by C  which is de ned as a set of projective clusters that satisfy the following conditions  X C2C C  o   1  8 o 2 D and X f2F  C f   1  8C 2 C The semantics of any projective clustering C is that for each projective cluster C 2 C  the objects belonging to C are actually close to each other if  and only if  they are projected onto the subspace associated with C  2 3 Projective Clustering Ensembles  PCE  A projective ensemble E is de ned as a set of projective clustering solutions  No information about the ensemble generation strategy  algorithms and or setups   nor original feature values of the objects within D are provided along with E  Moreover  each projective clustering solution in E may contain in general a di erent number of clusters  The goal of PCE is to derive a projective consensus clus  tering by exploiting information on the projective solutions within the input projective ensemble  2 3 1 Two objective PCE In  16   PCE is formulated as a two objective optimiza  tion problem  whose objectives take into account the object  based  function  o  and the feature based  function  f   cluster representations of a given projective ensemble E  C     arg min C2E f o C  E    f  C  E g  2  where  o C  E    X C2E    o  C  C     f  C  E    X C2E    f  C  C    3  Functions  o and  f are de ned as  o  C 0   C 00       o C 0   C 00     o C 00   C 0    2 and  f  C 0   C 00       f  C 0   C 00      f  C 00   C 0     2  respectively  where  o C 0   C 00     1 jC 0 j X C02C0   1  max C002C00 J  C0   C00     f  C 0   C 00     1 jC 0 j X C02C0   1  max C002C00 J   C0    C00    J   u  v       u    v      k uk 2 2   k vk 2 2   u    v   2  0  1  denotes the extended Jaccard similarity coe cient  also known as Tani  moto coe cient  between any two real valued vectors  u and  v  26                       The problem de ned in  2  is solved by a well founded heuristic  in which a Pareto based Multi Objective Evolu  tionary Algorithm  called MOEA PCE  is used to avoid com  bining the two objective functions into a single one  2 3 2 Single objective PCE To overcome some issues of the two objective PCE for  mulation  such as those concerning e ciency  parameter setting  and interpretation of the results    16  proposes an alternative PCE formulation based on a single objective function  which aims to consider the object based and the feature based cluster representations in E as a whole  C     arg min C2E X C2C X  o2D    C  o X C2E   X C 2C  C  o   X f2F    C f   C f    2 where     1 is a positive integer that ensures non linearity of the objective function w r t  C  o  To solve the above problem  the EM based Projective Clus  tering Ensembles  EM PCE  heuristic is de ned  EM PCE iteratively looks for the optimal values of C  o  resp   C f   while keeping  C f  resp  C  o   xed  until convergence  3  CLUSTER BASED PCE 3 1 Problem Statement Experimental results have shown that the two objective PCE formulation is much more accurate than the single  objective counterpart  16   Nevertheless  two objective PCE su ers from an important conceptual issue that has not been discussed in  16   proving that the accuracy of two objective PCE can be further improved  We unveil this issue in the following example  Example  Let E be a projective ensemble de ned over a set D of data objects and a set F of features  Suppose that E contains only one projective clustering solution C and that C in turn contains two projective clusters C 0 and C 00   whose object  and feature based representations are di erent from one another  i e   9  o 2 D s t  C0   o    6 C00   o  and 9 f 2 F s t   C0  f     6 C00  f   Let us consider two candidate projective consensus clus  terings C1   fC 0 1  C 00 1 g and C2   fC 0 2  C 00 2 g  We assume that C1   C  whereas C2 is de ned as follows  Cluster C 0 2 has object  and feature based representations given by C0  i e   the object based representation of the  rst cluster C 0 within C  and  C00  i e   the feature based representation of the second cluster C 00 within C   respectively  cluster C 00 2 has object  and feature based representations given by C00  i e   the object based representation of the second cluster C 00 within C  and  C0  i e   the feature based representation of the  rst cluster C 0 within C   respectively  According to  3   it is easy to see that   o C1  E    o C2  E   0 and  f  C1  E    f  C2  E   0 Both the candidates C1 and C2 minimize the objectives of the early two objective PCE formulation reported in  2   and hence  they are both recognized as optimal solutions  This conclusion is conceptually wrong  because only C1 should be recognized as an optimal solution  since only C1 is exactly equal to the unique solution of the ensemble  Conversely  C2 is not well representative of the ensemble E  as the object  and feature based representations of its clusters are inversely associated to each other w r t  the associations present in C  Indeed  in C2  C 0 1   hC0    C00 i and C 00 1   hC00    C0 i  whereas  the solution C 2 E is such that C 0   hC0    C0 i and C 00   hC00    C00 i  The issue described in the above Example arises because the two objective PCE formulation ignores that the object  based and feature based representations of any projective cluster are strictly coupled to each other  and hence  need to be considered as a whole  In other words  in order to ef  fectively evaluate the quality of a candidate projective con  sensus clustering  the objective functions  o and  f cannot be kept separated from each other  We attempt to solve the above drawback by proposing the following alternative formulation of PCE  which is based on a single objective function  C     arg min C2E  of  C  E   4  where  of is a function designed to measure the  distance  of any well de ned projective clustering solution C from E in terms of both data clustering and feature to cluster assign  ment  To carefully take into account e ciency  we de ne  of based on an asymmetric function  which has been de  rived by adapting the measure de ned in  16  to our setting   of  C  E    X C2E    of  C  C    5  where  of  C 0   C 00     1 2    of  C 0   C 00      of  C 00   C 0      6  and  of  C 0   C 00     1 jC 0 j X C02C0   1 max C002C00 J   XC0   XC00     7  In  7   the similarity between any pair C 0   C 00 of projective clusters is computed in terms of their corresponding pro  jective matrices XC0 and XC00  cf   1   Sect  2 2   For this purpose  the Tanimoto similarity coe cient can easily be generalized to operate on real valued matrices  rather than vectors   J  X  X      Pjrows X j i 1 Xi   X  i kXk 2 2   kX  k 2 2  Pjrows X j i 1 Xi   X  i  8  where Xi   X  i denotes the scalar product between the i th rows of matrices X and X     From a dissimilarity viewpoint  as J  2  0  1   we adopt in this work the measure 1  J   We hereinafter refer to 1  J  as Tanimoto distance  It can be noted that the proposed formulation based on the function  of ful ls the requirement of measuring the quality of a candidate consensus clustering in terms of both data clustering and feature to cluster assignments as a whole  In particular  we remark that the issue described in the previous Example does not arise in the proposed formu  lation  Indeed  considering again the two candidate projec  tive consensus clusterings C1 and C2 of the Example  it is easy to see that   of  C1  E    0 and  of  C2  E    0 Thus  C1 is correctly recognized as an optimal solution  whereas C2 is not                    3 2 Heuristics Apart from solving the critical issue of two objective PCE previously explained  a major advantage of the proposed PCE formulation w r t  the early ones de ned in  16  is its close relationship to the classic formulations typically em  ployed by CE algorithms  Like standard CE  the problem de ned in  4  can be straightforwardly proved to be a special version of the median partition problem  4   which is de ned as follows  given a number of partitions  clusterings  de   ned over the same set of objects and a distance measure between partitions   nd a  new  clustering that minimizes the distance from all the input clusterings  The only di er  ence between  4  and any standard CE formulation is that the former deals with projective clustering solutions  and hence  it needs a new measure for comparing projective clus  terings   whereas the latter involves standard clustering so  lutions  The closeness to CE is a key point of our work  as it enables the development of heuristic algorithms for PCE following standard approaches to CE  The advantage in this respect is twofold  heuristics for PCE can be de ned by ex  ploiting the extensive and well established work so far given for standard CE  which enables the development of solutions that are simple and easy to understand  and e ective at the same time  Within this view  a reasonable choice for de ning proper heuristics for PCE is to adapt the standard CE approaches  i e   instance based  cluster based  and hybrid  cf  Sect  2 1   to the PCE context  However  it is arguable if all such CE approaches are well suited for PCE  In fact  de ning an instance based PCE method is intrinsically tricky  and this also holds for the hybrid approach  which is essentially a combination of the instance based and cluster based ones  We explain the issues on de ning instance based PCE in the following  First  as the focus of any hypothetical instance based PCE is primarily on data objects  performing the two PCE steps of data clustering and feature to cluster assignment alto  gether would be hard  Indeed  focusing on data objects may produce information about data clustering only  for instance  by exploiting a co occurrence matrix properly re  de ned for the PCE context   This would force the assign  ment of the features to the various clusters to be performed in a separate step  and only once the objects have been grouped in clusters  Unfortunately  performing the two PCE steps of data clustering and feature to cluster assignment distinctly may negatively a ect accuracy of the output con  sensus clustering  According to the de nition of projective clustering  the information about the various objects belong  ing to any projective cluster should not be interpreted as absolute  but always in relation to the subspace associated to that cluster and vice versa  Thus  data clustering and feature to cluster assignment should be interrelated  at each step of the heuristic algorithm to be de ned  A more crucial issue arises even accepting to perform data clustering and feature to cluster assignment separately  Given a set of data objects to be included in any projec  tive cluster  the feature to cluster assignment process should take into account that the notion of subspace of any given projective cluster makes sense only if it refers to the whole set of objects belonging to that cluster  In other words  say  ing that any set of data objects forms a cluster C having a subset S of features associated with it does not mean that each object within C is represented by S  but rather that Algorithm 1 CB PCE Input  a projective ensemble E  the number K of clusters in the output projective consensus clustering  Output  the projective consensus clustering C   1   E   S C2E   C  2  P   pairwiseClusterDistances  E   f 8 g 3  M   metaclusters  E   P  K  4  C      5  for all M 2 M do 6     M   object basedRepresentation  E  M  f 12 g 7     M   f eature basedRepresentation  E  M  f 22 g 8  C    C     fhM   Mig 9  end for the entire set C is represented by S  Unfortunately  perform  ing feature to cluster assignment apart from data clustering contrasts with the semantics of a subspace associated to a set of objects in a projective cluster  Indeed  the various fea  tures could be assigned to any given cluster C only by con  sidering the objects within C independently of one another  Let us consider  for example  the case where the assignment is performed by averaging over the objects within C and over the feature based representations of all the clusters within the ensemble E  i e    C f   avg  o2C C 2C  C2E   fC  o      C f   g  8f 2 F  This case clearly shows that each feature f is assigned to C by considering each object within C indepen  dently from the other ones belonging to C  Within this view  we discard instance based and hybrid approaches to embrace a cluster based approach  In the fol  lowing  we describe our cluster based proposal in detail and also show how this is particularly appropriate to the PCE context  3 2 1 The CB PCE algorithm The Cluster Based Projective Clustering Ensembles  CB  PCE  algorithm is proposed as a heuristic approach to the PCE formulation given in  4   In addition to the notation provided in Sect  2  CB PCE employs the following symbols  M denotes a set of metaclusters  i e   a set of sets of clusters   M 2 M denotes a metacluster  i e   a set of clusters   and M 2 M denotes a cluster  i e   a set of data objects   The outline of CB PCE is reported in Alg  1  Similarly to standard cluster based CE  the  rst step of CB PCE aims to group the set  E of clusters from each solution within the input ensemble E into metaclusters  Lines 1 2   A clustering step over the set  E is performed by the function metaclus  ters  This step exploits the matrix P of pairwise distances between the clusters within  E  Line 1   The distance be  tween any pair of clusters is computed by resorting to the Tanimoto similarity coe cient reported in  8   The set M of metaclusters is  nally exploited to derive the object  and feature based representations of each projective cluster to be included into the output consensus clustering C    Lines 3 8   Such representations are denoted by    M and    M  8M 2 M  respectively  more precisely     M  resp     M  denotes the object based  resp  feature based  representa  tion of the projective cluster within C   corresponding to the metacluster M     M and    M are derived by focusing on the optimization of a criterion easy to solve  which enables the  nding of reasonable and e ective approximations at the same time  In particular  we adapt the widely used majority voting to the context at hand  Let us  rst consider    M values  If       the projective clustering solutions within the ensemble are all hard at a clustering level  the majority voting criterion leads to the de nition of the following optimization problem  f   M j M 2 Mg   argmin fMjM2Mg X M2M X  o2D M  o jMj X M2M 1  M  o s t  X M2M M  o   1  8 o 2 D M  o 2 f0  1g  8M 2 M  8 o 2 D whose solution can be easily proved to be as follows  8M  8 o      M  o   8     1 if M   arg min M02M 1 jM0 j X M2M0 1  M  o 0 otherwise that is  each object  o is assigned to the metacluster M con  taining the maximum number of clusters to which  o belongs  i e   such that M  o   1   If the ensemble contains projective clusterings that are soft at clustering level  the following problem can be de ned  f   MjM2Mg   argmin fMjM2Mg Q  9  s t  X M2M M  o   1  8 o 2 D  10  M  o   0  8M 2 M  8 o 2 D  11  where Q  X M2M X  o2D    M  o AM  o   AM  o   1 jMj X M2M 1  M  o and     1 is an integer that guarantees the non linearity of the objective function Q w r t  M  o  needed to ensure    M  o 2  0  1   rather than f0  1g   1 The solution for such a problem however is not as straightforward as that of the traditional case  i e   hard data clustering   We derive the solution in the following  Theorem 1  The optimal solution of problem P de ned in  9   11  is given by  8M  8 o      M  o     X M02M   AM  o AM0   o   1  1  1  12  Proof  The optimal    M  o can be found by means of the conventional Lagrange multipliers method  To this end  we  rst consider the relaxed problem P 0 of P obtained by tem  porarily discarding the inequality constraints from the con  straint set of P  i e   the constraints de ned in  11    We de ne the new  unconstrained  objective function Q 0 for P 0 as follows  Q 0   Q   X  o2D   o   X M02M M0   o  1    13  The optimal    M  o are computed by  rst retrieving the stationary points of Q 0   i e   the points for which rQ 0       Q 0   M  o     Q 0     o     0 1 Alternatively  to obtain    M  o 2  0  1   properly de ned reg  ularization terms can be introduced  see  e g    21    Thus  we solve the following system of equations    Q 0   M  o     AM  o  M  o   1     o   0  14    Q 0     o   X M02M M0   o  1   0  15  Solving  14  w r t  M  o and substituting such a solution in  15   we obtain  X M02M     o   AM0   o   1  1   1  16  Solving  16  w r t    o and substituting such a solution in  14   we obtain    AM  o  M  o   1    X M2M   1   AM0   o   1  1    1    0  17  Finally  solving  17  w r t  M  o  we obtain a stationary point whose expression is exactly equal to that in  12      M  o     X M02M   AM  o AM0   o   1  1  1  18  As it holds that  i  the stationary points of the Lagrangian function Q 0 are also stationary points of the original objec  tive function Q   ii  the feasible region of P and hence  the feasible region of P 0 is a convex set  and  iii  Q is convex w r t  M  o  it follows that such a stationary point repre  sents a global minimum of Q  and  accordingly  the optimal solution of P 0   Moreover  as AM  o   0  8M  8 o  it is trivial to observe that    M  o   0  8M  8 o  Therefore  the solution in  18  satis es the inequality constraints that were tem  porarily discarded in order to de ne the relaxed problem P 0  cf   11    thus  it represents the optimal solution of the original problem P  which proves the theorem  An analogous reasoning can be carried out for    M f   In this case  the problem to be solved is the following  f    MjM2Mg  arg min f MjM2Mg X M2M X f2F     M f BM f  19  s t  X f2F  M f   1  8M 2 M  20   M f   0  8M 2 M  8f 2 F  21  where BM f   jMj 1 P M2M 1 M f and   plays the same role as   in function Q  The solution of such a problem is similar to that derived for    M  o   Theorem 2  The optimal solution of the problem de ned in  19   21  is given by the following  8M  8f       M f   2 4 X f02F   BM f BM f0   1  1 3 5 1  22  Proof  Analogous to Theorem 1                                                       Rationale of CB PCE  Let us now informally show that CB PCE is well suited for PCE  thus supporting one of the claim of this work  i e   cluster based approaches are particularly appropriate to the PCE context  unlike instance based and hybrid ones   Looking at the PCE formulation reported in  4   it is easy to see that function  of retrieves the consensus clustering C   so that each cluster within C   is ideally  assigned  to ex  actly one cluster of each projective clustering solution in the input ensemble E  where the  assignments  are performed by minimizing the Tanimoto distance 1  J   cf   8    Thus  considering all the solutions in the ensemble  any cluster C 2 C   is assigned to a set of clusters  metacluster  M that contains exactly one cluster of each solution in the ensem  ble  that is jMj   jEj  and M0 2 C   M00 2 C   M0   M00   8M0   M00 2M  8C 2 E  Clearly  if one would know in advance the optimal set of metaclusters to be assigned to the clusters within C     the problem in  4  would be optimally solved by computing  for each metacluster M  the cluster C   that minimizes the Tanimoto distance from all the clusters within M  that is  C     arg min C X M2M 1  J  XC  XM   23  However  it holds that   i  the metaclusters are not known in advance  as their computation is part of the optimization process   ii  the problem in  23  is hard to solve  it falls into the class of median problems in which the distance to be minimized is the Tanimoto distance  this kind of problems has been recently proved to be NP hard  9   The validity of CB PCE as a heuristic approach to the PCE formulation proposed in  4  lies in that it exactly fol  lows the scheme reported above  i e   it  rst recognizes meta  clusters and then assigns objects and features to metaclus  ters   following some approximations  These approximations are needed for solving two critical points  1  a sub optimal set of metaclusters is computed by clus  tering the overall set of projective clusters within the ensemble  where the distance measure used for com  paring clusters is the Tanimoto distance  which is the measure employed by the proposed formulation in  4   2     M and    M values  for each metacluster M  are com  puted by optimizing an easy to solve criterion that ef  fectively approximates the problem in  23   3 2 2 Speeding up CB PCE  FCB PCE Given a set D of data objects and a set F of features  the computational complexity of the measure J  reported in  8   used for computing the similarity between two projective clusters  is O jDj jFj   as it involves a comparison between two jDj jFj matrices  For e ciency purposes  we can lower the complexity by de ning an alternative measure working in O jDj   jFj   Given any two projective clusters C 0 and C 00   such a measure  called J  fast  exploits the object based  C0 and C00   and the feature based   C0 and  C00   rep  resentation vectors of C 0 and C 00   respectively  rather than their corresponding projective matrices  Formally  J  fast C 0   C 00     1 2   J  C0   C00     J   C0    C00      24  where J        denotes again the Tanimoto similarity coe   cient de ned in  8   which is in this case applied to real  valued vectors rather than matrices  It is easy to observe that  like J   J  fast 2  0  1   Taking into account J  fast  we de ne a version of the CB  PCE algorithm which is similar to that de ned in Sect  3 2 1  except for the measure involved for comparing the projec  tive clusters  which is  in this case  based on J  fast  We here  inafter refer to this alternative version of the algorithm as Fast Cluster Based Projective Clustering Ensembles  FCB  PCE  algorithm  Although clearly advantageous in terms of e ciency  a major drawback of FCB PCE concerns accuracy  In fact  a major weakness of the measure J  fast exploited by FCB  PCE is that it is less accurate than its slow counterpart J  exploited by CB PCE  This essentially depends on the fact that comparing any two projective clusters C 0 and C 00 by in  volving their projective matrices XC0 and XC00   respectively  is generally more e ective than involving their object  and feature based representation vectors C0   C00    C0   and  C00  23   2 Indeed  although it can be trivially proved that XC0   XC00   C0   C00    C0    C00   the vectors C0    C0   and C00    C00 are in general a factorization of the matrices XC0 and XC00   respectively  i e   XC0    T C0  C0 and XC00    T C00  C00    Thus  only matrices XC0 and XC00 provide the whole information about the representation of the corresponding projective clusters  Although J  fast is less accurate than J   it still allows the comparison of projective clusters by taking into account their object  and feature based representations altogether  Hence  the proposed FCB PCE heuristic based on J  fast still represents a valuable heuristic to the PCE formulation pro  posed in this work  as it overcomes the main issue of two  objective PCE explained in Sect  3 1  3 2 3 Computational Analysis Here we discuss the computational complexity of the pro  posed CB PCE and FCB PCE algorithms  We are given  a set D of data objects  each one de ned over a feature space F  a projective ensemble E de ned over D and F  and a positive integer K representing the number of clusters in the output projective consensus clustering  We also assume that the size jCj of each solution C in E is O K   For both the algorithms  we may distinguish three steps  1  pre processing  it concerns the computation of the pairwise distances between clusters  by involving mea  sures J   cf   8   for CB PCE and J  fast  cf   24   for FCB PCE  this step takes O K2 jEj 2 jDj jFj  and O K2 jEj 2  jDj   jFj   for CB PCE and FCB PCE  respectively  because computing J   resp  J  fast  is O jDj jFj   resp  O jDj   jFj    cf  Sect  3 2 2   and the clusters to be compared to each other are O K jEj   2  meta clustering  it concerns the clustering of the O K jEj  clusters of all the solutions in the ensemble  assuming to employ a clustering algorithm which is at most quadratic w r t  the size of the dataset to be par  titioned  this step takes O K2 jEj 2   for both CB PCE and FCB PCE  3  post processing  it concerns the assignment of objects and features to the metaclusters  and is exactly the 2  23  deals with hard projective clusters  however  the rea  soning therein involved can be easily extended to a soft case                Table 1  Computational complexities total online o ine MOEA PCE O ItK2 jEj jDj   jFj   O ItK2 jEj jDj   jFj     EM PCE O KjEjjDjjFj  O IKjDjjFj  O KjEjjDjjFj  CB PCE O K2 jEj 2 jDjjFj  O KjEj KjEj   jDj   jFj   O K2 jEj 2 jDjjFj  FCB PCE O K2 jEj 2  jDj   jFj   O KjEj KjEj   jDj   jFj   O K2 jEj 2  jDj   jFj   same for both CB PCE and FCB PCE  According to  12  and  22   both the object and the feature assign  ments need to look up all the clusters in each meta  cluster only once  thus  for each object and for each feature  the needed step costs O KjEj   Accordingly  performing this step for all objects and features leads to a total cost of O KjEj  jDj   jFj   for the entire post processing step  It can be noted that the  rst step is an o ine phase  i e   a phase to be performed only once in case of a multi run exe  cution  whereas the second and third are online steps  Thus  as summarized in Table 1  where we also report the com  plexities of the earlier MOEA PCE and EM PCE methods de ned in  16  3    we can  nally state that    the o ine  online  and total  i e   o ine   online  complexities of CB PCE are O K2 jEj 2 jDj jFj   O KjEj KjEj   jDj   jFj    and O K2 jEj 2 jDj jFj   respectively    the o ine  online  and total  i e   o ine   online  complexities of FCB PCE are O K2 jEj 2  jDj   jFj    O KjEj KjEj jDj jFj    and O K2 jEj 2  jDj jFj    respectively  Interpretation of the complexity results  Let us now provide an insight for the comparison between the  total  complexities derived above  For the sake of read  ability  we hereinafter omit the su x   PCE  from the names of the various PCE algorithms  We denote with r a1  a2  the ratio between the complexities of the PCE algorithms a1 and a2  Clearly  a ratio smaller  resp  greater  than 1 means that the complexity of a1 is smaller  resp  greater  than that of a2  Our main observations are summarized in the following    As expected  FCB PCE is always faster than CB PCE  as it holds that r FCB  CB     jDj jFj   jDjjFj    1  8 jDj  jFj   1    CB PCE    it holds that r CB EM    K jEj   1  thus  CB  PCE is always slower than EM PCE    the ratio r CB MOEA  is equal to  jEj jDj jFj   I t  jDj   jFj    This implies that r CB MOEA    1 if  2 jDj jFj   jDj   jFj    2 I t jEj  i e   as  jDj   jFj  2    2 jDj jFj   jDj   jFj   that r CB MOEA    1 if jDj   jFj   4 I t jEj  The latter condition is true only in a small number of real cases  as an example  considering the numerical values for I  t and jEj suggested in  16  3 In Table 1  I denotes the number of iterations to conver  gence  for MOEA PCE and EM PCE   whereas t is the pop  ulation size  for MOEA PCE only   16    i e   200  30 and 200  respectively   CB PCE is faster than MOEA PCE if jDj   jFj   120  i e   when the input dataset is very small and or low dimensional  For this purpose  CB PCE can be recognized as in practice always slower than MOEA PCE    FCB PCE    it holds that the ratio r FCB EM     K jEj  jDj   jFj    jDj jFj  is greater than 1 if  2 jDj jFj   jDj   jFj    2 K jEj  which essentially means that FCB PCE is slower than EM PCE if jDj   jFj   4 K jEj  as  jDj   jFj  2    2 jDj jFj   jDj   jFj   Thus  for large and or high dimensional datasets  i e   for datasets having jDj and jFj such that jDj   jFj   4 K jEj  FCB PCE may be faster than EM PCE  whereas for small and or low dimensional datasets may not    r FCB MOEA    jEj  I t   assuming to set t equal to 15  of the ensemble size jEj as suggested in  16   it holds that r FCB MOEA    20  3 I   Thus  as it typically holds that I   7  e g   in  16  I   200   r FCB MOEA  is always smaller than 1 and  therefore  FCB PCE is always faster than MOEA PCE  To summarize  we can state that CB PCE is the slowest method  FCB PCE is faster than MOEA PCE  whereas  compared to EM PCE  it is faster  resp  slower  for large  resp  small  and or high dimensional  resp  low  dimensional  datasets  4  EXPERIMENTAL EVALUATION We conducted an experimental evaluation to assess the ac  curacy and e ciency of the consensus clusterings obtained by the proposed CB PCE and FCB PCE  The comparison also involved the previous existing PCE algorithms  i e   MOEA PCE and EM PCE   16  as baseline methods  4 4 1 Evaluation methodology Following  16   we used eight benchmark datasets from the UCI Machine Learning Repository  27   namely Iris  Wine  Glass  Ecoli  Yeast  Segmentation  Abalone and Letter  and two time series datasets from the UCR Time Series Clas  si cation Clustering Page  33   namely Tracedata and Con  trolChart  Table 2 reports the main characteristics of the datasets  the interested reader is referred to  27  33  for a description of the datasets  4 Experiments were conducted on a quad core platform Intel Pentium IV 3GHz with 4GB memory and running Microsoft WinXP Pro Table 2  Datasets used in the experiments dataset objects attributes classes Iris 150 4 3 Wine 178 13 3 Glass 214 10 6 Ecoli 327 7 5 Yeast 1 484 8 10 Segmentation 2 310 19 7 Abalone 4 124 7 17 Letter 7 648 16 10 Tracedata 200 275 4 ControlChart 600 60 6 4 1 1 Ensemble generation We generated ensembles as suggested in  16   In particu  lar  for each set of experiments and dataset we considered 20 di erent ensembles  all results we present in the following refer to averages over these ensembles  Ensemble generation was carried out by running the LAC projective clustering al  gorithm  30   in which the diversity of the solutions was en  sured by randomly choosing the initial centroids and varying the parameter h  here we recall that this parameter controls the incentive for clustering on more features depending on the strength of the local correlation of data  To test the ability of the proposed algorithms to deal with soft clus  tering solutions and with solutions having equally weighted feature to cluster assignments  we generated each ensem  ble E as a composition of four equal sized subsets  denoted as E1  hard data clustering  feature to cluster assignments unequally weighted   E2  hard data clustering  feature to  cluster assignments equally weighted   E3  soft data clus  tering  feature to cluster assignments unequally weighted   and E4  soft data clustering  feature to cluster assignments equally weighted   4 1 2 Setting of the PCE algorithms We set the parameters of MOEA PCE and EM PCE as reported in  16   In particular  as far as MOEA PCE  the population size  t  was set equal to 15  of the ensemble size and the number I of maximum iterations equal to 200  The random noise needed for the mutation step was obtained via Monte Carlo sampling on a standard Gaussian distribution  Regarding EM PCE  the parameter   was set equal to 2  this value also represented the optimal value for the parameters   and   of our CB PCE and FCB PCE  4 1 3 Assessment criteria We assessed the quality of a consensus clustering C using both an external and an internal validity approach  specif  ically  we carried out two evaluation stages  the  rst based on the similarity of C w r t  a reference classi cation and the second based on the average similarity w r t  the solutions in the input ensemble E  Similarity w r t  the reference classi   cation  We denote with Ce a reference classi cation  where the object based representations Ce of each projective cluster Ce within Ce are provided along with D  the selected datasets are all available with a reference classi cation   whereas the feature based representations  C f e   8Ce 2 Ce  8f 2 F  are computed as suggested in  30    C f e   exp   U C  f e   h   P f02F exp   U C  f e 0   h   where the LAC s parameter h was set equal to 0 2 and  U C      f     X  o2D C  o    1 X  o2D C  o    c C      f   o f   2 c C      f     X  o2D C  o    1 X  o2D C  o     o f  with o f  denoting the   f th feature value of object  o  Similarity between C and Ce was computed in terms of the Normalized Mutual Information  by taking into account their object based  NMIo  representations  feature based representations  NMIf    or both  NMIof    and by adapting the original de nition given in  28  to handle soft solutions  Here we report the formal de nition of NMIof   NMIo and NMIf can be derived in a similar way  NMIof  C  Ce    X C2C X Ce2Ce a C Ce  T  C Ce    log   jDj 2 a C Ce  T  C Ce  b C  b Ce    q H C    H Ce  where a C 0   C 00     X  o2D X f2F C0   o  C0  f C00   o  C00  f b C     X  o2D X f2F C  o    C f   H C      X C 2C  b C   jDj log b C   jDj T C 0   C 00     X  o2D X f2F   X C02C0 C0   o  C0  f    X C002C00 C00   o  C00  f   We now explain the rationale of this evaluation stage  Let us consider NMIof   where analogous considerations hold for NMIo and NMIf   Since no additional information is provided along with any given input projective ensemble E the reference classi cations associated to the benchmark datasets are indeed exploited only for testing purposes  randomly extracting a projective solution from E is the only fair way to proceed in case no PCE method is used  Within this view  in order to establish the validity of a projective consensus C computed by any PCE algorithm  we compare the results achieved by C w r t  those obtained by any pro  jective clustering randomly chosen from E  Such a compari  son can be performed according to the following expression  which aims to compute the  expected di erence  between the results by C and those by E   of  C  E  Ce    X C2E     NMIof  C  Ce   NMIof  C   Ce    Pr C   where Pr C   is the probability of randomly choosing C  from E  Since no prior knowledge is provided along with E  we can assume a uniform distribution for the probabilities Pr C    i e   Pr C     jEj 1   8C 2 E     Computing  of hence becomes equal to computing the similarity between C and Ce minus                   Table 3  Evaluation w r t  the reference classi cation  of  o  f MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris   146   168   218   185   319   228   309   297   198   095   139   117 Wine   136   083   275   224   201   130   272   253   152   030   211   206 Glass   105   162   158   157   092   134   180   167   048   060   001   009 Ecoli   164   086   211   232   245   125   223   213   042   042   023   017 Yeast   049   021   092   095   090   066   113   110   006   090   102   010 Segmentation   137   144   148   141   102   206   194   185   075   079   098   150 Abalone   116   111   134   130   141   116   185   182   093   092   123   120 Letter   111   107   141   134   146   122   188   185   092   097   131   124 Trace   097   019   125   140   032   026   154   132   007   114   112   115 ControlChart   091   204   345   276   050   011   027   051   233   416   287   283 min   049   019   092   095   032   011   027   051   007   095   001   009 max   164   204   345   276   319   228   309   297   233   416   287   283 avg   115   110   185   171   142   116   185   178   093   093   123   122 the average similarity between Ce and the solutions within E  as proved by the following   of  C  E  Ce    X C2E     NMIof  C  Ce   NMIof  C   Ce    Pr C       NMIof  C  Ce   X C2E   NMIof  C   Ce    jEj 1     NMIof  C  Ce   avg C2E   NMIof  C   Ce   25   o and  f can be de ned analogously  The larger  of    o and  f   the better the quality of C  Similarity w r t  the ensemble solutions  The goal of this evaluation stage was to assess how well a consensus clustering complies with the solutions in the input ensemble  For this purpose  we evaluated the average similarity NMIof  C  E    avgC02ENMIof  C  C 0   between the consensus clustering C and the solutions in the ensemble E  NMIo and NMIf are de ned analogously   To improve the readability of the results  we normalize NMIof   NMIo and NMIf by dividing them by the average pairwise similarity of the solutions in the ensemble  Formally  we de ne the ratios  coe cients of variation   of    o  and  f    of  C  E    NMIof  C  E  avgC0  C002ENMIof  C 0   C 00    26   o and  f are de ned similarly  The larger these quantities are  the better the quality of C is  4 2 Results 4 2 1 Accuracy For each algorithm  dataset and ensemble  we performed 50 di erent runs  We reported average clustering results obtained by CB PCE and FCB PCE  as well as by the early MOEA PCE and EM PCE in Tables 3 and 4  Evaluation w r t  the reference classi   cation  Both CB PCE and FCB PCE achieved higher  of re  sults   rst 4 column groups in Table 3  than MOEA PCE on all datasets  In particular  CB PCE obtained an aver  age improvement of 0 070  with a maximum gain of 0 254  ControlChart   whereas FCB PCE obtained an average im  provement of 0 056  with a maximum of 0 185  ControlChart again   EM PCE was on average less accurate than MOEA  PCE  thus  the average gains of CB PCE and FCB PCE w r t  EM PCE were higher than those achieved w r t  MOEA PCE  0 075 and 0 061  respectively   Comparing the two proposed CB PCE and FCB PCE  the former achieved higher quality on nearly all datasets  all but Ecoli  Yeast and Trace   with an average gain of about 0 014 and peaks on ControlChart  0 069  and Wine  0 051   The higher perfor  mance of CB PCE vs  FCB PCE con rms one of the major claims of this work  cf  Sect  3 2 2   The superior performance of CB PCE and FCB PCE w r t  the early MOEA PCE and EM PCE was also con   rmed in terms of object based   o  and feature based   f   representations  In particular  CB PCE achieved average  o equal to 0 185 and average improvements w r t  MOEA  PCE and EM PCE of 0 043 and 0 069  respectively  Also  CB PCE outperformed MOEA PCE  resp  EM PCE  on seven  resp  eight  out of ten datasets  As far as FCB  PCE  the average  o was 0 178  with average gains w r t  MOEA PCE and EM PCE equal to 0 036 and 0 062  respec  tively  FCB PCE performed better than MOEA PCE and EM PCE on eight and nine out of ten datasets  respectively  In terms of  f   both CB PCE and FCB PCE were on average comparable to each other  in fact  they achieved average  f equal to 0 123 and 0 122  respectively  The av  erage improvements obtained by CB PCE  resp  FCB PCE  w r t  both MOEA PCE and EM PCE were equal to 0 030  resp  0 029   Like  of and  o  both the proposed CB PCE and FCB PCE performed better than MOEA PCE and EM  PCE on the majority of the datasets also in terms of  f   Evaluation w r t  the ensemble solutions  Concerning the coe cients of variation due to the consen  sus clustering w r t  the average pairwise similarity of the input ensemble  Table 4   CB PCE and FCB PCE led to average values respectively equal to 1 110 and 1 108   of    1 318 and 1 316   o   1 049 and 1 030   f    Particularly  in the case  of   CB PCE improved MOEA PCE and EM PCE by 0 062 and 0 114 on average  respectively  whereas the av  erage improvements obtained by FCB PCE w r t  MOEA  PCE and EM PCE were equal to 0 060 and 0 112  respec  tively  Also  CB PCE was able to obtain peaks of improve  ment up to 0 297  w r t  MOEA PCE  and 0 454  w r t  EM PCE   The maximum gains of FCB PCE were instead equal to 0 3 and 0 457 w r t  MOEA PCE and EM PCE  respectively  Both CB PCE and FCB PCE outperformed MOEA PCE and EM PCE on nearly all datasets  CB PCE results were better than those of MOEA PCE and EM PCE on seven and nine out of ten datasets  respectively  As far    Table 4  Evaluation w r t  the ensemble solutions  of  o  f MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris 1 019  914  984  989 1 025 1 004 1 044 1 039  953  906  986  977 Wine  993  960 1 074 1 072 1 060  991 1 057 1 056 1 018  952 1 001 1 001 Glass 1 023  918 1 1 003 1 114  971 1 064 1 066  979  915 1 004 1 004 Ecoli 1 074 1 052 1 058 1 015 1 034 1 023 1 027 1 028  975  924  986  992 Yeast 1 074 1 050 1 217 1 189 1 189 1 182 1 310 1 297  960 1 021 1 036 1 037 Segmentation 1 008  851 1 305 1 308 1 367 1 304 1 788 1 786  971  969 1 032 1 013 Abalone 1 044 1 001 1 068 1 071 1 121 1 102 1 208 1 208  982  902  980  986 Letter 1 040 1 001 1 045 1 088 1 118 1 099 1 277 1 274  981  891 1 169  998 Trace 1 170 1 207 1 196 1 196 1 325 1 501 1 503 1 503  949  927 1 062 1 062 ControlChart 1 034 1 006 1 152 1 152 1 162 1 237 1 903 1 903 1 085  577 1 234 1 234 min  993  851  98  989 1 025  971 1 027 1 028  949  577  980  977 max 1 170 1 207 1 305 1 308 1 367 1 501 1 903 1 903 1 085 1 021 1 234 1 234 avg 1 048  996 1 110 1 108 1 152 1 141 1 318 1 316  985  898 1 049 1 030 Table 5  Execution times  milliseconds  TOTAL ONLINE OFFLINE MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris 17 223 55 13 235 906 17 223 53 343 372   2 12 892 534 Wine 21 098 184 50 672 993 21 098 153 306 323   31 50 366 670 Glass 61 700 281 110 583 3 847 61 700 239 1 713 1 713   42 108 870 2 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining"/>
        <doc>Sampling Based Algorithms for Quantile Computation in Sensor Networks ### Zengfeng Huang Lu Wang Ke Yi Yunhao Liu Hong Kong University of Science and Technology  huangzf  luwang  yike  liu  cse ust hk ABSTRACT We study the problem of computing approximate quantiles in large scale sensor networks communication e   ciently  a problem previously studied by Greenwald and Khana  12  and Shrivastava et al   21   Their algorithms have a total communication cost of O k log 2 n     and O k log u      respectively  where k is the number of nodes in the network  n is the total size of the data sets held by all the nodes  u is the universe size  and    is the required approximation error  In this paper  we present a sampling based quantile computation algorithm with O      kh     total communication  h is the height of the routing tree   which grows sublinearly with the network size except in the pathological case h      k   In our experiments on both synthetic and real data sets  this improvement translates into a 10 to 100 fold communication reduction for achieving the same accuracy in the computed quantiles  Meanwhile  the maximum individual node communication of our algorithm is no higher than that of the previous two algorithms  Categories and Subject Descriptors F 2  Analysis of Algorithms and Problem Complexity   Nonnumerical Algorithms and Problems General Terms Algorithms Keywords Sensor networks  quantiles ### 1  INTRODUCTION Sensor networks are large ad hoc networks of interconnected  battery powered  wireless sensors  They are now being widely deployed to monitor diverse physical variables  such as temperature  sound  activities of wild life and so forth  15  17  27   As technologies mature  sensor networks Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  have reached the scale of thousands of nodes  1  2  and will get even larger in the near future  However  power consumption remains the biggest obstacle for large scale deployment for sensor networks as the on board battery is still the only power source for a sensor node  Since wireless transmission of data is the biggest cause of battery drain  20   in network aggregation techniques that prevent the nodes from forwarding all the data to the base station are extremely useful for energy conservation in sensor networks  The observation is that for many monitoring tasks  we do not actually need all the measurement data collected by all the sensors  often a succinct aggregate su   ces  Computing an aggregate could be much more communication e   cient than transmitting all the data to the base station  in terms of both the total communication  as well as the maximum individual node communication  Simple aggregates such as max  min  sum  count can be computed easily and e   ciently  14   The nodes    rst organize them into a spanning tree rooted at the base station  Then starting from the leaves  the aggregation propagates upwards to the root  When a node receives the aggregates from its children  it computes the aggregate of these aggregates and its own data  which equals the aggregate of all the data in the node   s subtree  and forwards it to its parent  Such a simple approach works due to the decomposable property of these aggregates  for any two disjoint sets S1 and S2  the aggregate of S1     S2 can be computed from the individual aggregates of S1 and S2  Some other aggregates such as average can also be computed this way since it is derived from two decomposable aggregates sum and count  though it is not decomposable itself  Letting k be the number of nodes in the sensor network  it is clear that a decomposable aggregate can be computed with O k  total communication and O 1  maximum node communication  assuming each node has O 1  children   However  these simple aggregates are not expressive enough  A quantile summary  which allows one to extract the     quantile for any 0        1 of the underlying data  much better characterizes the data distribution  Recall that the    quantile of a set S of n data values from a totally ordered universe is the value a with rank r a         n    in S  the rank of any value x  r x   is the number of values in S smaller than x  For ease of presentation we assume that all values in S are distinct  such an assumption can be easily removed by using any consistent tie breaker  The quantiles are also called the order statistics and equi depth histograms  and are very useful in a variety of data analytical tasks as they o   er a lot more insight into the underlying data than the simple single valued aggregates  Because a quantile summary that returns the accurate quantiles must contain the entire data set  an    approximation is usually allowed  an    approximate    quantile is a value with rank between          n and        n  This additive error de   nition is the one that has been mostly adopted in the literature  5  6  10   12  18  21  26   though multiplicative errors have also been considered  7  28   Note that since the error is additive  the    should be set small  usually on the order of 0 01  to 1   6   With an    approximation allowed  an  approximate  quantile summary could just retain the 1    values that rank at 0    n  2  n  3  n          respectively  Then for any     we return the value with the largest rank in the summary that is no larger than   n  One can also easily argue that     1     is the theoretical minimum size of such a summary  1 1 Previous results In 2004  two back to back  well cited papers by Greenwald and Khanna  12  and Shrivastava et al   21  studied the problem of computing quantile summaries in a communicatione   cient manner in sensor networks  Both of them still follow the    decomposable    approach described earlier  But unlike sum or max  a quantile by itself is not decomposable  for this reason it is also called a holistic aggregate in the literature  5    So the challenge was to design a decomposable summary that contains enough information so that the approximate quantiles can be extracted  Their solutions only differ in the decomposable quantile summaries used  The GK summary  12  has size O log 2 n     where n is the total number of data values in the network  while the q digest  21  has size O log u     where u is the size of the universe from which the data values are drawn  The two extra factors O log 2 n  and O log u  are not strictly comparable  but the GK summary is theoretically more general as it supports unbounded universes  assuming  of course  any value in the universe takes one unit of storage   The GK summary has two additional variants with size O h     and O log n log h          respectively  where h is the height of the routing tree  They can be better than the basic version when the routing tree is well balanced  Speci   cally  both algorithms  12  21  start from the leaves and compute the GK summary  resp  q digest  upwards  Each node    rst computes a summary of its own data  and then combines it with all the summaries it receives from its children  This produces an aggregated summary that incorporates all the data in the node   s subtree  which is then forwarded to its parent  It is clear that the individual node communication is equal to the summary size  while the total communication is k times that  1 2 Our results In this paper  we present a new algorithm for computing quantile summaries in sensor networks  with an expected total communication of O      kh      The computed quantile summary allows one to extract an    approximate    quantile with constant probability for any 0        1  Depending on various physical situations  the height of the tree  h  could range from log k to k  but usually does not exceed     k  which happens when the sensor nodes form a grid structure   Thus the communication is always less than that of the previous algorithms  More importantly  except in the pathological case h      k   the cost of our algorithm grows sublinearly in the size of the network  leading to excellent scalability  total comm  max node comm  q digest  21  k log u    log u    GK algorithm  12  k log 2 n    log 2 n    GK algorithm v2 k log n log h        log n log h        GK algorithm v3 kh    h    new     kh    log k h     Table 1  The asymptotic communication costs of the algorithms  where n is the total number of data values  u is the universe size  k is the network size  h is the height of the routing tree  and    is the error parameter  Meanwhile  the maximum individual node communication of our algorithm is O log k h       which is also strictly better than any of the previous algorithms  as log k   h   k   n   u  A comparison of the communication costs of these algorithms is given in Table 1  Note that for relatively large     say a constant  O      kh     could be much smaller than k  which means that we can compute the quantiles without contacting all nodes  This may seem surprising  as even computing or approximating a simple aggregate  like sum or count  needs    k  communication  This    surprise    comes with the assumption that our algorithm knows the values of n and k  otherwise  we need O k  communication to compute them    rst  We separated the cost of computing n and k from that of the quantile problem itself for the following reasons   1  It simpli   es the bound and makes the core problem of quantile computation stand out  otherwise we should always include an additive O k  term   2  Applications usually use quantiles to track the distribution of the underlying data  so will compute quantile summaries periodically  The values of n and k from period to period are unlikely to change much  while our algorithm actually only needs to know n and k within a constant factor  So we can usually repeatedly execute it without refreshing n and k  Anyways  in practice we typically have k         kh     and the O k  term can be neglected  Note that for any algorithm using decomposable summaries  the total communication is always at least     k      as any such summary must have size     1      To break this barrier  we deviate from the decomposable framework  The message a node sends to its parent does not necessarily contain a quantile summary of the data in its subtree  Nevertheless  we will make sure that the base station in the end will have a valid quantile summary for the entire data set  In particular  our algorithm uses larger messages  but of size at most O log k h       for nodes near the base station  while nodes far away send small or even no messages  This is in contrast with the previous approaches where all nodes use essentially the same message size  ignoring polylog factors   The improvement of our algorithm for the maximum individual node communication is not as drastic as that for the total communication  In fact  it is not di   cult to show an     1     lower bound on the maximum node communication for any quantile algorithm  decomposable or not  and all the algorithms already come close to this theoretical limit within polylog factors  with our log factor being the smallest  Some may say that the maximum node cost is more important  12   but we would argue the other way round  for the following reasons  1  The network will not be disconnected due to a single node exhausting its battery  A sensor network usually has enough redundancy so that rerouting is possible fora permissible number of node failures  2  We can install larger batteries for sensor nodes that are expected to have larger power consumptions  e g  those near the base station  or simply deploy more sensors there to increase redundancy  When we have good provisioning  the total communication cost  rather than the maximum individual node cost  will become the determining factor for the longevity of the network  In fact  most previous papers on data aggregation indeed used total communication as the primary measure of energy e   ciency  e g   4  16  22   The previous work on the quantile problem  12  21  did not emphasize it as much because for those algorithms  the total communication is just k times the     xed  message size  Our algorithm is based on sampling  However  simply taking a random sample of the data and computing the quantiles on the sample is not accurate enough  as it is well known that to achieve an    error with constant probability  a random sample of size    1    2   needs to be drawn  23   This results in O h    2   total communication and O 1    2   maximum node communication  To improve accuracy  or equivalently  to reduce size   we augment the random sample with additional information about the data  together with several new ideas that we brie   y outline in Section 1 4 and develop in stages in later sections  In fact  the total communication of our algorithm is O min      kh     h    2     i e   it is always no worse than simple random sampling  but we will avoid carrying along with the    min    throughout the paper to simplify exposition  Since our algorithm is based on random sampling  it provides a probabilistic guarantee  that any    quantile can be extracted within error    with a constant probability which can be made arbitrarily close to 1  While the GK algorithm and the q digest provide a worst case    error guarantee  However  we would be happy with a probabilistic guarantee  since transmission errors and link failures are common in sensor networks  a theoretical worst case guarantee becomes a probabilistic one in practice anyway  Nevertheless  to ensure a fair comparison  in the experiments  Section 5  we did not simply set the required    and compare the communication costs  Instead  we measure the actual average and maximum error of 99 quantiles for      1   2           99   and compare all the algorithms in terms of the communication error trade o    curve  1 3 Related work Data aggregation in sensor networks has been a topic of intensive studies for the past years  Below we only review the most relevant work  please refer to the surveys  9  24  for more comprehensive results in this area  Data aggregation techniques can be broadly classi   ed into two categories  tree based approaches and multi path approaches  In a tree based approach  the nodes organize themselves into a routing tree  Typically the tree is built from the base station in a breadth    rst manner  All nodes within communication range with the base station become level one nodes  Then a node u that can reach a level one node v becomes a level two node  with v being the parent of u  and so on so forth  When a link fails  the child will try to    nd a new parent  but at any time  a node has only one parent and the aggregation is performed along a tree  Most data aggregation techniques  including all the quantile algorithms  are tree based  In a multi path approach  a node has multiple parents and will broadcast its message to all of them  Such an approach is more robust against link failures  but also requires more communication  as now the algorithm has to be designed to be insensitive to the duplication of messages  With a multi path approach  even a simple aggregate like count or sum cannot be computed exactly  To obtain an    approximation of count or sum  each node will need to send a message of size O 1    2    4  19   Currently there are no multi path algorithms for the quantile problem  There is also a hybrid approach  16  that combines the bene   ts of tree based and multi path approaches  where it uses tree aggregation when the link failure rate is low to gain better communication e   ciency  while adopts a multi path strategy when the failure rate is high  The closely related heavy hitters problem has also received a lot of attention  where the goal is to compute a summary of a multiset of size n from which we can estimate the frequency of any item up to an additive error of   n  It is well known  6  that this problem can be reduced to the         approximate quantile problem for some                  by simply using some tie breaker to convert the multiset into a set  say  padding di   erent lower order bits   and then asking quantile queries with               2                 Thus the previous quantile algorithms  12  21   as well as our new algorithm  also solve the heavy hitters problem  But of interest is whether the heavy hitters problem can be solved more e   ciently  as in the case of the streaming model  where the heavy hitters problem can be solved in O 1     space  6  while the best quantile algorithm needs O log n     space  11   Manjhi et al   16  proposed a deterministic algorithm for computing the heavy hitters with total communication O k      but the bound only holds for a class of    nice    routing trees that they de   ne  It is still an open question whether there are better deterministic heavy hitter algorithms for arbitrary routing trees  For randomized algorithms  one can use the count min sketch  8  in the decomposable framework  leading to total communication O k     for any routing tree  Our algorithm improves this to O      kh      Quantile summaries  as fundamental statistics and a useful data analytical tool  have been extensively studied in several other settings  Munro and Paterson  18  studied how to compute an exact quantile with limited memory and multiple passes  and also showed that approximation is necessary if only one pass is allowed  The best one pass  i e   streaming  algorithm for computing approximate quantile summaries is due to Greenwald and Khana  11   whose algorithm uses O log n     space  Gilbert et al   10  and Cormode and Muthukrishnan  8  studied how to maintain a quantile summary under both insertions and deletions using small space  Finally  Cormode et al   5  and Yi and Zhang  26  studied how to track the quantile summary over distributed data sets as they evolve  1 4 Roadmap We develop our algorithm in three stages  In Section 2 we    rst present the algorithm in the    at model  in which all nodes are directly connected to the base station  This algorithm has O      k     total communication and O 1     maximum individual node communication  Simply running the algorithm on a spanning tree of height h would result in O h     k     total communication  even worse  the maximum individual node communication could be as high as O      k     since all the tra   c might have to go through a single node  In Section 3 we develop techniques that combinethe messages a node receives before it forwards its message to its parent  This results in an O log k     maximum message size  Finally in Section 4  we use a tree partitioning technique to improve the total communication cost to the claimed O      kh     bound  2  THE FLAT MODEL In this section we    rst describe our algorithm in the    at model  in which each node is directly connected to the base station  Let the set of data values at node i be Si  i   1          k  and let n be the total number of data values  The algorithm  The algorithm is very simple  Each node    rst independently samples each of its data values with some probability p  to be determined later   For each sampled value a  it computes its local rank r a  i  at node i  i e   the rank of a in set Si  Then it simply sends all the sampled values and their local ranks to the base station  We    rst show how a value to rank query can be answered at the base station from the information it receives  namely  given any value x  we need to estimate r x   the rank of x in S i Si  After this  quantile  rank to value  queries can be easily answered  Let pred x  i  be the predecessor of x in the sampled values sent from node i  namely  the largest value no larger than x  note that pred x  i  may not exist  We show below that r   x  i        r pred x  i   i    1 p  if pred x  i  exists  0  else is an unbiased estimator of r x  i   the local rank of x in Si  Then  we can estimate the global rank of x as r   x    X i r   x  i   Node 1 10 33 42 68 101 132 Node 2 52 97 125 Node 3 21 74 111 Figure 1  There are 3 nodes  each of which holds a set of integers  and the shaded integers are sampled  the sample rate here is 1 2   See Figure 1 for an example  Suppose we want to query for the rank of 80  In this example    r 80  1    2 2   4 since in the sample from Node 1  the local rank of 80   s predecessor  42  is 2 and the sample probability is 1 2  Similarly  r   80  2    0 and   r 80  3    0   2   2  so the estimated global rank of 80 is   r 8    4   0   2   6  whereas its actual global rank is 7  Analysis  The key to showing that   r x  estimates the global rank of x accurately is the following lemma  Lemma 1  For any x  r   x  i  is an unbiased estimator for r x  i   with variance Var   r x  i       1      1     p  r x i  p 2   Proof  Consider the random variable X       r x  i      r pred x  i   i   if pred x  i  exists  r x  i    1 p  else  Note that   r x  i    r x  i      X   1 p  So we just need to show that E X    1 p and bound Var X   Starting from x and walking to smaller values  we observe that X represents the number of values we see until the    rst sampled value pred x  i  in set Si  when it exists  When all the r x  i  values smaller than x are not sampled  X is set to r x  i    1 p  Therefore  we have  detailed derivations are omitted  r x  i  is shorthanded as r  E X    Xr     1    p 1     p        1    1     p  r  r   1 p    1 p  The variance is Var X    E X 2       E X  2   Xr     1     2 p 1     p        1    1     p  r  r   1 p  2     1 p 2    1      1     p  r   1     p  p 2     1      1     p  r x i  p 2   Since the global rank of x is the sum of the local ranks    r x  is an unbiased estimator of r x  with variance Pk i 1 Var   r x  i    We bound Var   r x  i   in two ways  First it is clear that Var   r x  i       1 p 2   and hence P i Var   r x  i       k p 2   Thus  by setting p           k   n   the variance becomes O    n  2    By Chebyshev   s inequality  this means that   r x  approximates r x  within an additive error of   n with constant probability  This constant probability can be made arbitrarily close to 1 by enlarging p by appropriate constant factors  In this case the total communication is O pn    O      k      Alternatively we can bound Var   r x  i   as Var   r x  i       1      1     pr x  i   p 2   r x  i  p   so Var   r x     X i Var   r x  i       1 p X i r x  i      n p   This means that when p is set to    1    2 n   Var   r x   is also O    n  2    In this case the total communication is O 1    2    namely the same as simple random sampling  Therefore  the algorithm has a total communication of O min      k     1    2     In the rest of the paper  we will only consider the interesting and typical case when     k      1    2   i e   k   1    2   to avoid carrying the    min    around  Reducing individual node communication  The algorithm described above has the desired total communication O      k      but all this tra   c could be from one node  if it dominates the entire data set  Below we show how to limit the individual node communication to O 1      We classify the nodes into those with more than n      k data values and those with at most that  If a node i has  Si      n      k values  it still uses the previously determined sample probability pi   p  if  Si    n      k  it will use a smaller sample probability pi   1    Si   The estimator correspondingly becomes r   x  i        r pred x  i   i    1 pi  if pred x  i  exists  0  else  It is clear that now a node samples at most O 1     values in expectation  To see that the estimator is still accurate by Lemma 1    r x  is still an unbiased estimator of r x  with variance Var   r x       Xk i 1 1 p 2 i   X i    Si      n k    ni  2   X i    Si        n k    n  2 k     0 B   X  i    Si      n k      ni  1 C A 2      n  2     2   n  2   Thus   r x  is still an    approximation of r x  with constant probability  Quantile queries  We have shown how to answer value torank queries using the summary structure at the base station  Quantile  rank to value  queries can also be answered easily  as follows  For each sampled data value a received by the base station from node i  we    rst estimate its global rank   r a  as r   a    r a  i    X j 6 i r   a  j   Now  given a required rank r  we simply return the sampled value x that has the closest estimated rank   r x  to r  Below we argue that its true rank  r x   is away from r by at most   n with constant probability  Assume that r is between the estimated ranks of two consecutive values x and y in the sample  i e     r x      r     r   y   Consider the following three events  1    r x      1 2   n     r x      r   x    1 2   n  2    r y      1 2   n     r y      r   y    1 2   n  3  r y      r x        n  When all three events happen  one can verify that x or y  whoever has the closest estimated rank to r  must have its true rank within   n to r  By appropriately adjusting the constants  we can make sure that events 1  and 2  each happen with probability 8 9  say   Since the sample probability is at least 1    Si       1   n  the number of missed values between x and y is no more than   n with constant probability  Again this constant can be boosted to 8 9  Then by a union bound  all three events happen together with probability at least 2 3  Theorem 1  Our algorithm in the    at model has O      k     total communication and O 1     maximum individual node communication  and answers an    approximate quantile query with constant probability  3  THE TREE MODEL There are two challenges in extending the    at model algorithm to a general routing tree  First  if each node simply sends its message through its ancestors in the routing tree to the base station without any data reduction  an intermediate node might see too much tra   c going through  This could result in an O      k     maximum individual node communication  Second  in terms of total communication  simply running the    at model algorithm in the tree model would result in O h     k     communication as a message needs O h  hops to reach the base station  This section will resolve the    rst issue while Section 4 the second  3 1 Basic ideas From Theorem 1 we know that each node   s own message has size at most O 1      Problems arise when a node has too many descendants whose messages need to be forwarded  Our idea is to merge these messages in a systematic way so as to reduce their size  The unit of our merge operation is a sample s taken from a ground set D s   Let n s  denote the size of the ground set D s   and we store n s  together with s  We say s is a small sample if n s    n      k and a large sample if n s      n      k  Initially  each such sample s is generated by a node i from its own data set D s    Si  Recall from the previous section that the initial samples have the following properties   P1  The ground sets of the samples are disjoint  and their union is the entire data set   P2  Each value in D s  has been sampled to s with some equal probability p s   in particular  p s        k   n if s is a small sample and p s    1   n s  if it is a large sample   P3  Each sampled value a in s is associated with r a  D s    the local rank of a in D s   Recall that an immediate consequence of  P2  is that each sample has size at most O 1      For a large sample s  we de   ne its class number as c s       log n s      k n      It is clear that 0     c s      log     k  When a node has received a number of samples from its children  together with one of its own  it will    rst check if the ground sets of all the small samples have a combined size of at least n      k  If so it will merge all the small samples into a large sample  Next it will repeatedly merge two large samples of the same class into one in the next class  until no two large samples are in the same class  As a result  there will be at most one large sample per class left  During the merge operation  we will also ensure that the three properties above are maintained  As a result  since all the small samples  if there are any  have their combined ground set smaller than n      k  the total size of all the small samples is O 1      and because each large sample has size at most O 1      the node will eventually send out a message of size O log k      When merging two samples s1 and s2 and producing a merged sample on the ground set D s1      D s2   properties  P1  and  P2  are relatively easy to maintain  by appropriately subsampling the values in s1 and s2 based on p s1  and p s2   However   P3  is di   cult to guarantee  In fact  because we only have a sample s2 from D s2   for any value a subsampled to the merged sample from s1  we cannot really compute its exact rank in D s2   and vice versa  So we will instead estimate its rank in D s2  based on s2  Thus  we will relax property 3  to the following   P3       Each sampled value a in s is associated with   r a  D s    which is an unbiased estimator of r a  D s    the local rank of a in D s   Now we need to be careful since the errors in these estimated ranks will propagate as more merges are performed  and we need to make sure that when the samples reach the base station  the accumulated error should not exceed   n  Below we    rst present the relatively simple merging algorithm  and defer the more complicated analysis to later 3 2 The merging algorithm As described above  if all the small samples have their combined ground set smaller than n      k  we will not do any merges since their total sample size is O 1      Otherwise  we use the following merge small operation to merge them into a large sample  merge small  Let s1  s2          sm be all the small samples  such that P i n s      n      k  Let s be the merged sample  and let n s    P i n si  be the size of the combined ground set  Note that s will be a large sample  so its sample probability should be p s    1   n s   To form the sample  we can simply subsample each data value in all the si   s with probability p s  p si    n n s      k   For each a thus sampled  if it is from si  we estimate its local rank in the combined ground set as r   a  D s     r a  i    X j 6 i r   a  D sj     where r   a  D sj      8     r pred a  sj    D sj      1 p sj    if pred a  sj   exists  0  else  As before  here pred a  sj   denotes the predecessor of a in the sample sj   It is easy to see that merge small maintains properties  P1    P2   and  P3       since by Lemma 1    r a  D sj    is an unbiased estimator of r a  D sj     After executing merge small  we will repeatedly execute merge large to merge the large samples  Unlike mergesmall  we apply merge large only on pairs of large samples of the same class  one pair at a time  progressively from the low classes to high  More precisely  starting from class c   0  as long as there are at least two large samples in this class  we merge them with merge large  to form a sample in class c   1  When there is one or no sample left in class c  we move on to class c   1  This idea is similar to that in  12   but because of our way of sampling and the fact that we deviate from the decomposable framework  the total size of our merged samples is smaller than that of  12  by a logarithmic factor  merge large  Let s1 and s2 be two large samples to be merged  Let s be the merged sample  and set n s    n s1   n s2   As s has a sample probability p s    1   n s   we subsample each data value in s1 with probability p s  p s1    n s1  n s  and subsample each data value in s2 with probability p s  p s2    n s2  n s   For each subsampled value a  if it is from s1  we estimate its rank in D s  as r   a  D s       r a  D s1       r a  D s2    where   r a  D s1   is the rank  either exact or approximate  that a carries from s1  and   r a  D s2   is computed from s2 similarly as before r   a  D s2     8     r   pred a  s2   D s2     1 p s2   if pred a  s2  exists  0  else  except that   r pred a  s2   D S2   could now also be an estimate rather than the exact rank of pred a  s2  in D S2   The case where a is from s2 is handled symmetrically  Let us see an example of merging two messages  Suppose n      k   100  and the two messages contain summaries  s1  s2  s3  and  t1  t2  t3   with ground set sizes 80  400  800  and 60  400  3200 respectively  So s1 and t1 are small samples  and we merge them into s     1 using merge small  The resulting merged sample has class number 0  We then    nd that s2 and t2 are both in class 2  and merge them together into a new summary with class number 3  It is further merged with s3  getting a summary s     2 with class number 4  Now we are done  since the summaries left are  s     1  s     2  t3   all of which have di   erent class numbers  When all the samples have been sent to the base station  we can use exactly the same query algorithms as in the    at model to answer value to rank and rank to value queries using these samples  just that now the local ranks for the sampled values are estimates of the actual local ranks in the respective ground sets  3 3 Error analysis It easily follows from the merging algorithm that properties  P1    P2   and  P3       are all maintained  but it remains to show that when the base station has received all the samples  an    approximate quantile query can still be answered with constant probability  Lemma 2  For any large sample s resulted from mergesmall  the estimated local rank r   a  D s   has variance at most m   n  2  k  where m is the number of merged small samples  Proof  Because each small sample is an initial sample with sample probability p       k   n  the lemma directly follows from Lemma 1  Lemma 3  Let s be any large sample of class c s   For any data value a     s  its estimated rank r   a  D s   has variance at most m   n  2  k      2 c s  1 n  2  k  where m is the number of small samples whose ground sets are included in D s   Proof  We will prove by induction on c s   The base case c s    0 is easy to verify  For any large sample of class 0  it is either an initial sample or one produced from mergesmall  The variance of   r a  D s   is 0 in    rst case  and at most m   n  2  k in the second case by Lemma 2  Now we assume the lemma holds for all large samples of class i and proceed to prove it for class i   1  A large sample s with c s    i   1 could be produced from mergesmall  or merged from two large samples s1 and s2 with c s1    c s2    i  In the    rst case  again by Lemma 2  the variance is at most m   n  2  k  in the second case  by the induction hypothesis  we have Var   r a  D sj         mj    n  2  k      2 i 1 n  2  k  for any a     sj   where mj is the number of small samples whose ground sets are included in D sj    j   1  2  Consider a data value a     s  W l o g   assume that it is subsampled from s1  The rank of a in D s  is estimated as r   a  D s       r a  D s1       r a  D s2    The variance of   r a  D s1    by the induction hypothesis  is at most m1   n  2  k      2 i 1 n  2  k   1 The variance of   r a  D s2    if the local ranks in s2 were accurate  by Lemma 1 is at most 1 p s2  2      n s2   2        2 i 1 n  2  k   2  Since now we only have an estimate for the rank of pred a  s2  with variance m2   n  2  k      2 i 1 n  2  k   3  by the law of total variance  Var   r a  s   is the sum of  1    2  and  3   Thus Var   r a  s        m1   m2    n  2  k   3   2 i 1 n  2  k     m   n  2  k      2 i 2 n  2  k  which completes the induction  Theorem 2  Our algorithm in the tree model has O h     k     total communication and O log k     maximum individual node communication  and answers an    approximate quantile query with constant probability  Proof  The communication bounds follow directly from the algorithm description  so we only prove correctness here  Below we only focus on a value to rank query  i e   estimating the rank r x  of any given value x within error   n  after that  a quantile query can be answered in the same way as in Section 2  Recall that we use the same algorithm as in the    at model to estimate r x  from a number of small samples and at most one large sample per class  The total variance from all the small samples is at most O    n  2   according to the analysis in Section 2  since they are the initial samples without merging  Let s0  s1          s log     k be the large samples for each of the classes  The estimated local ranks of x in these samples have two sources of variance  the variance due to the sampling  which is 1 p si  2 as in Lemma 1  and the variance of the estimated local rank   r pred x  s   D s    which can be bounded by Lemma 3  The total variance from the    rst source is at most log     Xk i 0 1 p si  2     log     Xk i 0    2 i 1 n  2  k   O    n  2    The total variance from the second source  by Lemma 3  is at most log     Xk i 0  mi   n  2  k      2 i 1 n  2  k      k   n  2  k   log     Xk i 0    2 i 1 n  2  k   O    n  2    Again  the constant in the big Oh can be made arbitrarily small by enlarging the sample probabilities by constant factors  This means that we can estimate r x  within   n error with a constant probability  4  TREE PARTITIONING In this section we describe our    nal improvement of the algorithm  reducing the total communication by another O      h  factor to O      kh      which is sublinear in k for all h   o k   The idea is to partition the routing tree into t connected components  each of which contains O k t  nodes  Then we conceptually shrink each component into a    super node     These t super nodes form a tree of size t but whose height still could be h  Now  if we apply our algorithm of Section 3 on these super nodes  the total communication would be O h     t      This seems to suggest a t that is as small as possible  However  since a super node is not really one node  but O k t  nodes that are connected  To produce an initial sample for a super node and compute the local ranks for the sampled values within the super node  we have to send messages among the O k t  nodes  It turns out preparing the initial samples now takes communication O k        t   Thus  setting t   k h balances these two terms and yields the desired O      kh     bound  We next elaborate on this idea in the rest of this section  4 1 Tree partitioning We    rst partition the routing tree into O t    O k h  components  each of which has O h  nodes  To ensure that each component is connected  we may introduce a few virtual nodes  by cloning the actual nodes  A virtual node has no data  It sits inside the actual node but logically operates on its own  Note that the tree partitioning phase depends only on the topology of the routing tree  so we can separate it from the actual quantile algorithm and only run it when the tree topology changes  Below we present a distributed algorithm that does the partitioning in O k  total communication  We assume that each node is aware of its subtree size  if not this information can be obtained easily using a bottom up computation with O k  communication  Each node u maintains a weight w u  which is initially set to u   s subtree size  during the algorithm w u  will represent the number of unpartitioned nodes in u   s subtree  The partitioning algorithm starts by calling partition root of the tree   and proceeds recursively  as outlined in Algorithm 1  Algorithm 1  partition u  1 foreach child v of u do 2 if w v      h then partition v   set w u     P 3 v w v    1 for all children v of u  4 while w u    h do if u has children v1          vl s t  h 2     P 5 i w vi      h then 6 put all the unpartitioned nodes in the subtrees of v1          vl into one component  7 mark all these nodes as    partitioned     8 if l     2  create a virtual node at u as the root of this component  9 set w u     w u      w v1                   w vl   10 set w vi     0 for i   1          l  Note that when partition root  returns  the root might still have at most h unpartitioned nodes below  Then we simply allocate these nodes into the last component  An example of the partitioning obtained by this algorithm is shown in Figure 2  Lemma 4  The partitioning algorithm partitions an arbitrary routing tree into O k h  connected components  each of size O h   At most one virtual node is created for each component a b d i j e f k l c g h a b     d i j e b f k l c g h Figure 2  An example of the tree partition algorithm when k   12 and h   4  The whole tree is partitioned into 3 components  Node b     is a virtual node added by the algorithm  and in real life its role can be played by node b  Proof  It is clear that the algorithm only produces components of size between h 2 and h  and at most one component  the last one  of size between 1 and h   1  But we still need to argue that all nodes must have been partitioned eventually  To do so  we show that when partition u     nishes  u has at most h unpartitioned nodes in its subtree  We prove it by induction  When u is a leaf  partition u  does nothing and the claim is certainly correct  Now consider an internal node u  By the induction hypothesis when lines 1   2 are done  each of u   s children v has at most h unpartitioned nodes below  namely  w v      h  Thus  as long as the sum of their weights is at least h  line 5 will always evaluates to true  In fact  we can    rst check if there is any v with h 2     w v      h  If there is one that already satis   es the condition  Otherwise all of them have w v    h 2  We can then simply collect them one by one until the sum falls between h 2 and h  Therefore  we can continue producing components of size between h 2 and h until w u      h  This    nishes the induction and hence the proof  4 2 Quantile algorithm on the partitioned tree On the partitioned tree  we run our tree model algorithm of Section 3 by treating each component as a super node  Recall that there are only t   O k h  super nodes  Let Si be the set of data values in the i th super node  The previous analysis suggests a sampling rate of p       t   n for a super node with less than n      t data values  and p   1    Si  if  Si      n      t  After the sample is drawn  we also needed to compute the local ranks of the sampled values in Si  However  now Si does not reside on one single node  but distributed among O h  nodes in the component  so we have to pay communication to compute the local ranks  Speci     cally  each node in a super node    rst samples its own data  and then the sampled values to the root of the component  The root of the component  after receiving all the samples  broadcasts them to everyone in the component  Now every node in the components has a copy of the sample drawn from the whole component  and thus can compute their ranks within its own data set  Finally we aggregate these local ranks in a bottom up fashion to the root of the component  which is actually the same as performing multiple sum aggregations within the component  one per sampled value  After the root of each component has prepared its initial sample and the associated local ranks  we can simply run our previous tree model algorithm on these initial samples  More precisely  starting from these component roots  we send the samples hop by hop to the base station  As before  when an intermediate node has received multiple samples  it tries to merge them before propagating them upwards  Theorem 3  Our quantile algorithm  when running on a partitioned tree  has O      kh     total communication and O log k h      maximum individual node communication  Proof  Since we sample the data values with probability at most     t   n  where t   O k h   the total sample size is O  p k h      in expectation   In the    rst phase of the algorithm  all the sampled data values are sent to the component roots  then broadcast to all nodes in the component  and      nally aggregated back to the component roots to compute their local ranks  Thus each sampled data value could travel to all the O h  nodes in a component in the worst case  This results in O  p k h       h    O      kh     communication in total  In the second phase  the component roots send their initial samples to the base station  Even if we do not do any merging of the samples  the cost would be at most O  p k h     h    O      kh      since each sample takes at most h hops to reach the base station  Thus the total communication cost is O      kh      In terms of maximum individual node communication  we still use the same algorithm in Section 3  except that a sample is said to be large if its ground set is of size at least n      t  This results in O log     t    O log k h   classes  So the maximum individual node communication is O log k h       expected  according to the analysis in Section 3  Remarks  The partitioning approach well    ts the case where the sensor network already uses a clustered structure  as in LEACH  13  and COUGAR  25   In this case a cluster naturally corresponds to a connected component  Note that we set the size of the component to O h  and the number of component to O k h  just for optimizing the communication cost  The correctness of the algorithm and its probabilistic guarantees on the returned quantiles do not depend on these parameters  When the cluster sizes deviate from O h   the total communication cost might be a   ected slightly but not the quality of the computed quantile summary  However  when the sensor network does not deploy a clustered structure  the algorithm on the partitioned tree might introduce additional overhead since it is no longer a oneround algorithm as in  12  14  21   It needs 3 rounds of communication within each component plus one    nal round from the component roots to the base station  5  EXPERIMENTS In this section we evaluate our algorithm experimentally  comparing with the two previous algorithms  the q digest  21  and the GK algorithm v2  12   See Table 1   We denote our Sampling Based algorithms as SB 1  for the one round version in Section 3  and SB p  for the improved version based on tree partitioning in Section 4   5 1 Experiment setup We built a simulator which simulates a sensor network and implemented all four algorithms on top of the same platform  The network topology is generated in the same way as in  21   i e   sensors are distributed over a certain area uniformly at random  Sensors are assumed to have a    xed radio range  and two sensors may communicate with each other if and only if they are within range of each other  The root of the network is chosen from the sensors randomly  after that a routing tree is generated by a breadth    rst search10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7  1e 05 0 0001 0 001 0 01 0 1 1 Maximum node communication Maximum error of queries q digest GK SB p SB 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10  1e 05 0 0001 0 001 0 01 0 1 1 Total communication Maximum error of queries q digest GK SB p SB 1 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7  1e 05 0 0001 0 001 0 01 0 1 1 Maximum node communication Average error of queries q digest GK SB p SB 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10  1e 05 0 0001 0 001 0 01 0 1 1 Total communication Average error of queries q digest GK SB p SB 1 Figure 3  Error Communication trade o   s on the synthetic data set with k   16384  starting from the root  Since our main goal is to improve the scalability of the algorithms in terms of network size  the experiments are done on relatively large networks  with k   1024  2048  4096  8192  16384 nodes  respectively  We used both synthetic and real data to test the performance of the algorithms  For the synthetic data sets  we    rst generated a total of n   1 billion values from a Gaussian distribution with mean 0 and variance 1  and then scaled them to the range  0  2 32     1  and round them to integers  since q digest cannot handle    oating point numbers  though the other algorithms can  Next  we deployed the sensors over a unit square area  and randomly distributed these n integers to the sensors  For the real data set  we used a terrain data for the Neuse River Basin  available at  3   This data set contains roughly 0 5 billion LIDAR points which measure the elevation of the terrain  In this case we randomly deploy the sensors on the terrain  and assume that each sensor collects the elevation data nearby  We adjusted the radius of the area from which a sensor collects data such that the total size of data set  i e   n  is around 1 billion  Note that this data set is highly correlated  since close sensors will observe similar elevations  To perform a quantile computation  we set some    and run all four algorithms  However  as mentioned earlier this may not be a fair comparison  since our algorithms give an    error with a constant</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis">
        <doc>Data Serving in the Cloud ### Raghu Ramakrishnan Chief Scientist  Audience and Cloud Computing Brian Cooper Adam Silberstein Utkarsh Srivastava Yahoo  Research Joint work with the Sherpa team in Cloud Computing2 Outline     Clouds     Scalable serving   the new landscape     Very Large Scale Distributed systems  VLSD      Yahoo    s PNUTS Sherpa     Comparison of several systems     Preview of upcoming Y  Cloud Serving  YCS   benchmark3 Types of Cloud Services     Two kinds of cloud services      Horizontal     Platform     Cloud Services     Functionality enabling tenants to build applications or new  services on top of the cloud     Functional Cloud Services      Functionality that is useful in and of itself to tenants  ### E g    various SaaS instances  such as Saleforce com  Google  Analytics and Yahoo    s IndexTools  Yahoo  properties aimed  at end users and small businesses  e g   flickr  Groups  Mail   News  Shopping      Could be built on top of horizontal cloud services or from  scratch     Yahoo  has been offering these for a long while  e g   Mail for  SMB  Groups  Flickr  BOSS  Ad exchanges  YQL 5 Yahoo  Horizontal Cloud Stack Provisioning   Self serve  YCS Horizontal YCPI  Cloud ServicesBrooklyn     EDGE Monitoring Metering Security Hadoop Horizontal   Cloud Services BATCH STORAGE PNUTS SherpaHorizontal MOBStor Cloud Services    OPERATIONAL STORAGE VM OS Horizontal Cloud Services     APP VM OS Horizontal Cloud Services yApache WEB Data Highway Serving Grid PHP App Engine6 Cloud Power   Yahoo  Ads  Optimization Content  Optimization Search  Index Image Video  Storage   Delivery Machine  Learning   e g  Spam  filters  Attachment Storage7 Yahoo    s Cloud   Massive Scale  Geo Footprint     Massive user base and engagement     500M  unique users per month     Hundreds of petabyte of storage      Hundreds of billions of objects     Hundred of thousands of requests sec     Global     Tens of globally distributed data centers     Serving each region at low latencies     Challenging Users     Downtime is not an option  outages cost  millions      Very variable usage patterns8 New in 2010      SIGMOD and SIGOPS are starting a new annual  conference  co located with SIGMOD in 2010   ACM Symposium on Cloud Computing  SoCC   PC Chairs  Surajit Chaudhuri   Mendel Rosenblum GC  Joe Hellerstein Treasurer  Brian Cooper      Steering committee  Phil Bernstein  Ken Birman   Joe Hellerstein  John Ousterhout  Raghu  Ramakrishnan  Doug Terry  John Wilkes9 VERY LARGE SCALE   DISTRIBUTED  VLSD   DATA SERVING ACID or BASE  Litmus tests are colorful  but the picture is cloudy10 Databases and Key Value Stores http   browsertoolkit com fault tolerance png11 Web Data Management Large data analysis  Hadoop  Structured record  storage  PNUTS Sherpa  Blob storage  MObStor     Warehousing    Scan  oriented  workloads    Focus on  sequential  disk I O      per cpu  cycle    CRUD     Point lookups  and short  scans    Index  organized  table and  random I Os      per latency    Object  retrieval and  streaming    Scalable file  storage      per GB  storage    bandwidth12 The World Has Changed     Web serving applications need      Scalability      Preferably elastic     Flexible schemas     Geographic distribution     High availability     Reliable storage     Web serving applications can do without      Complicated queries     Strong transactions     But some form of consistency is still desirable13 Typical Applications     User logins and profiles     Including changes that must not be lost      But single record    transactions    suffice     Events     Alerts  e g   news  price changes      Social network activity  e g   user goes offline      Ad clicks  article clicks     Application specific data     Postings in message board     Uploaded photos  tags     Shopping carts14 Data Serving in the Y  Cloud Simple Web Service API   s Database PNUTS   SHERPA Search Vespa Messaging Tribble Storage MObStor Foreign key photo     listing FredsList com application ALTER Listings MAKE CACHEABLE Compute Grid Batch export Caching memcached 1234323   transportation   For sale  one  bicycle  barely  used 5523442   childcare   Nanny  available in  San Jose DECLARE DATASET Listings AS   ID String PRIMARY KEY  Category String  Description Text   32138   camera   Nikon  D40  USD 30015 VLSD Data Serving Stores     Must partition data across machines     How are partitions determined      Can partitions be changed easily   Affects elasticity      How are read update requests routed      Range selections  Can requests span machines      Availability  What failures are handled      With what semantic guarantees on data access       How  Is data replicated      Sync or async  Consistency model  Local or geo      How are updates made durable      How is data stored on a single machine 16 The CAP Theorem     You have to give up one of the following in  a distributed system  Brewer  PODC 2000   Gilbert Lynch  SIGACT News 2002       Consistency of data      Think serializability     Availability     Pinging a live node should produce results     Partition tolerance     Live nodes should not be blocked by partitions17 Approaches to CAP        BASE        No ACID  use a single version of DB  reconcile later     Defer transaction commit      Until partitions fixed and distr xact can run     Eventual consistency  e g   Amazon Dynamo      Eventually  all copies of an object converge     Restrict transactions  e g   Sharded MySQL      1 M c Xacts  Objects in xact are on the same machine      1 Object Xacts   Xact can only read write 1 object     Object timelines  PNUTS  http   www julianbrowne com article viewer brewers cap theorem18 18    I want a big  virtual database       What I want is a robust  high performance virtual  relational database that runs transparently over a  cluster  nodes dropping in and out of service at will   read write replication and data migration all done  automatically  I want to be able to install a database on a server  cloud and use it like it was all running on one  machine        Greg Linden   s blog19 PNUTS   SHERPA To Help You Scale Your Mountains of Data Y  CCDI20 Yahoo  Serving Storage Problem     Small records     100KB or less     Structured records     Lots of fields  evolving     Extreme data scale   Tens of TB     Extreme request scale   Tens of thousands of requests sec     Low latency globally   20  datacenters worldwide     High Availability   Outages cost  millions     Variable usage patterns   Applications and users change 2021 E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E What is PNUTS Sherpa  E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E CREATE TABLE Parts   ID VARCHAR  StockNumber INT  Status VARCHAR       Parallel database Geographic replication Structured  flexible schema Hosted  managed infrastructure A     42342               E B     42521               W C     66354               W D     12352               E E     75656               C F     15677               E 2122 What Will It Become   E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E Indexes and views23 Scalability     Thousands of machines     Easy to add capacity     Restrict query language to avoid costly queries Geographic replication     Asynchronous replication around the globe     Low latency local access High availability and fault tolerance     Automatically recover from failures     Serve reads and writes despite failures Design Goals 23 Consistency     Per record guarantees     Timeline model      Option to relax if needed Multiple access paths     Hash table  ordered table     Primary  secondary access Hosted service     Applications plug and play     Share operational cost24 Technology Elements PNUTS      Query planning and execution     Index maintenance Distributed infrastructure for tabular data     Data partitioning      Update consistency     Replication YDOT FS     Ordered tables Applications Tribble     Pub sub messaging YDHT FS      Hash tables Zookeeper     Consistency service YCA  Authorization PNUTS API Tabular API 2425 PNUTS  Key Components     Maintains map from  database table key totablet to SU     Provides load balancing     Caches the maps from the TC     Routes client requests to  correct SU     Stores records     Services get set delete  requests 2526 Storage units Routers Tablet Controller REST API Clients Local region Remote regions Tribble Detailed Architecture 2627 DATA MODEL 2728 Data Manipulation     Per record operations     Get     Set     Delete     Multi record operations     Multiget     Scan     Getrange     Web service  RESTful  API 2829 Tablets   Hash  Table Apple Lemon Grape Orange Lime Strawberry Kiwi Avocado Tomato Banana Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  How much did you pay for this lemon  Is this a vegetable  New Zealand The perfect fruit Name Description Price  12  9  1  900  2  3  1  14  2  8 0x0000 0xFFFF 0x911F 0x2AF3 2930 Tablets   Ordered Table 30 Apple Banana Grape Orange Lime Strawberry Kiwi Avocado Tomato Lemon Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  The perfect fruit Is this a vegetable  How much did you pay for this lemon  New Zealand  1  3  2  12  8  1  9  2  900  14 Name Description Price A Z Q H31 Flexible Schema Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Color Red Condition Good Fair32 32 Primary vs  Secondary Access Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Price Posted date Listing id 15 6 5 07 421133 86 6 1 07 763245 570 6 1 07 424252 1123 6 3 07 211242 Primary table Secondary index Planned functionality33 Index Maintenance     How to have lots of interesting indexes  and views  without killing performance      Solution  Asynchrony      Indexes views updated asynchronously when  base table updated34 PROCESSING READS   UPDATES 3435 Updates 1 Write key k 2 Write key k 7 Sequence   for key k 8 Sequence   for key k SU SU SU 3 Write key k 4 5 SUCCESS 6 Write key k Routers Message brokers 3536 Accessing Data 36 SU SU SU 1 Get key k 2 3 Record for key k Get key k 4 Record for key k37 Bulk Read 37 SU Scatter  gather  server SU SU 1  k1  k2      kn  Get k 2 1 Get k2 Get k338 Storage unit 1 Storage unit 2 Storage unit 3 Range Queries in YDOT     Clustered  ordered retrieval of records Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 1 Router Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Grapefruit   Pear  Grapefruit   Lime  Lime   Pear  Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 139 Bulk Load in YDOT     YDOT bulk inserts can cause performance  hotspots     Solution  preallocate tablets40 ASYNCHRONOUS REPLICATION  AND CONSISTENCY 4041 Asynchronous Replication 4142 Consistency Model     If copies are asynchronously updated   what can we say about stale copies      ACID guarantees require synchronous updts     Eventual consistency  Copies can drift apart   but will eventually converge if the system is  allowed to quiesce     To what value will copies converge       Do systems ever    quiesce         Is there any middle ground 43 Example  Social Alice User Status Alice Busy West East User Status Alice Free User Status Alice     User Status Alice     User Status Alice Busy User Status Alice         Busy Free Free Record Timeline  Network fault   updt goes to East   Alice logs on 44     Goal  Make it easier for applications to reason about updates  and cope with asynchrony     What happens to a record with primary key    Alice     PNUTS Consistency Model 44 Time Record  inserted Update Update Update Update Update Delete Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Update Update As the record is updated  copies may get out of sync 45 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write Current  version Stale version Stale version PNUTS Consistency Model 45 Achieved via per record primary copy protocol  To maximize availability  record masterships automaticlly  transferred if site fails  Can be selectively weakened to eventual consistency   local writes that are reconciled using version vectors 46 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write if   v 7 ERROR Current  version Stale version Stale version PNUTS Consistency Model 46 Te s t and set writes facilitate per record transactions47 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Current  version Stale version Stale version Read PNUTS Consistency Model 47 In general  reads are served using a local copy48 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read up to date Current  version Stale version Stale version PNUTS Consistency Model 48 But application can request and get current version49 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read     v 6 Current  version Stale version Stale version PNUTS Consistency Model 49 Or variations such as    read forward      while copies may lag the master record  every copy goes through the same sequence of changes50 OPERABILITY 5051 51 Server 1 Server 2 Server 3 Server 4 6 2 07 636353 Bike  86 6 5 07 662113 Chair  10 Distribution 6 1 07 424252 Couch  570 6 1 07 256623 Car  1123 6 7 07 121113 Lamp  19 6 9 07 887734 Bike  56 6 11 07 252111 Scooter  18 6 11 07 116458 Hammer  8000 Data shuffling for load balancing Distribution for parallelism52 Tablet Splitting and Balancing 52 Each storage unit has many tablets  horizontal partitions of the table  Overfull tablets split Tablets may grow over time Storage unit may become a hotspot Shed load by moving tablets to other servers Storage unit Tablet53 Consistency Techniques     Per record mastering     Each record is assigned a    master region        May differ between records     Updates to the record forwarded to the master region     Ensures consistent ordering of updates     Tablet level mastering     Each tablet is assigned a    master region        Inserts and deletes of records forwarded to the master region     Master region decides tablet splits     These details are hidden from the application     Except for the latency impact 54 54 Mastering A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                 E A     42342                E B     42521                E C     66354                W D     12352                E E     75656                C F     15677                 E C     66354                W B     42521                E A     42342                E D     12352                E E     75656                C F     15677                E55 55 Record vs  Tablet Master A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p1">
        <doc>A Latency and Fault Tolerance Optimizerfor Online Parallel Query ### PlansPrasang UpadhyayaUniversity of Washingtonprasang cs uw eduYongChul KwonUniversity of Washingtonyongchul cs uw eduMagdalena BalazinskaUniversity of Washingtonmagda cs uw eduABSTRACTWe address the problem of making online  parallel queryplans fault tolerant  i e   provide intra query fault tolerancewithout blocking  We develop an approach that not onlyachieves this goal but does so through the use of di er ent fault tolerance techniques at di erent operators within aquery plan  Enabling each operator to use a di erent fault tolerance strategy leads to a space of fault tolerance plansamenable to cost based optimization  We develop FTOpt  acost based fault tolerance optimizer that automatically se lects the best strategy for each operator in a query planin a manner that minimizes the expected processing timewith failures for the entire query  We implement our ap proach in a prototype parallel query processing engine  Ourexperiments demonstrate that  1  there is no single bestfault tolerance strategy for all query plans   2  often hybridstrategies that mix and match recovery techniques outper form any uniform strategy  and  3  our optimizer correctlyidenti es winning fault tolerance con gurations Categories and Subject DescriptorsC 4  Performance of Systems   Fault tolerance  modelingtechniques  H 2 4  Database Management   Systems Parallel databases Query processingGeneral TermsPerformance1 ### INTRODUCTIONThe ability to analyze large scale datasets has become acritical requirement for modern business and science  Tocarry out their analyses  users are increasingly turning to ward parallel database management systems  DBMSs   14 41  45  and other parallel data processing engines  10  15  20 deployed in shared nothing clusters of commodity servers In many systems  users can express their data processingneeds using SQL or other specialized languages  e g   PigPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for pro   t or commercial advantage and that copiesbear this notice and the full citation on the    rst page  To copy otherwise  torepublish  to post on servers or to redistribute to lists  requires prior speci   cpermission and or a fee SIGMOD   11  June 12   16  2011  Athens  Greece Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00 Latin  30   DryadLINQ  48    The resulting queries or scriptsare then translated into a directed acyclic graph  DAG of operators  e g   relational operators  maps  reduces  orother  20   that execute in the cluster An important challenge faced by these systems is fault tolerance  When running a parallel query at large scale some form of failure is likely to occur during execution  9  Existing systems take two radically di erent strategies tohandle failures  parallel DBMSs restart queries if failures oc cur during their execution  The limitation of this approachis that a single failure can cause the system to reprocess aquery in its entirety  While this is not a problem for queriesrunning across a small number of servers and for a shortperiod of time  it becomes undesirable for long queries us ing large numbers of servers  In contrast  MapReduce  10 and similar systems  15  materialize the output of each op erator and restart individual operators when failures occur This approach limits the amount of work repeated in theface of failures  but comes at the cost of materializing allintermediate data  which adds signi cant overhead even inthe absence of failures  Furthermore  because MapReducematerializes data in a blocking fashion  this approach pre vents users from seeing results incrementally  Partial resultsare a desirable feature during interactive data analysis nowcommonly performed with these systems  43  In this paper  we study the problem of providing usersboth the ability to see early results as motivated by onlinequery processing  17  18  and achieve a low expected totalruntime  We thus seek to enable intra query fault tolerancewithout blocking and we want to do so in a manner that min imizes the expected total runtime in the presence of failures Other objective functions could also be useful  e g   minimizeruntime without failures subject to a constraint on recoverytime   We choose to minimize the sum of time under normalprocessing and time in failure recovery  This function com bines high performance at runtime with fast failure recoveryinto a single objective  We want to minimize this functionwhile preserving pipelining Recent work  43  has also looked at the problem of com bining pipelining and fault tolerance  they developed tech niques for increased data pipelining in MapReduce  Thissystem both pipelines and materializes data between opera tors  We observe  however  that data materialization is onlyone of several strategies for achieving fault tolerance in apipelined query plan  Other strategies are possible includingrestarting a query or operator but skipping over previouslyprocessed data  21  24  or checkpointing operator states andrestarting from these checkpoints  11  21   Additionally  themost appropriate fault tolerance method may depend on theavailable resources  failure rates  and query plan properties For example  an expensive join operator may need to check point its state while an inexpensive  lter may simply skipover previously processed data after a failure Given these observations  we develop  1  a framework thatenables mixing and matching of fault tolerance techniques ina single  pipelined query plan and  2  FTOpt  a cost basedfault tolerance optimizer for this framework  Our frame work enables intra query fault tolerance without blocking thus preserving pipelining  Given a query plan and infor mation about the cluster and expected failure rates  FTOptautomatically selects the fault tolerance strategy for eachoperator in a query plan such that the overall query runtimewith failures is minimized  We call the resulting con gura tion a fault tolerance plan  In our fault tolerance plans  eachoperator can individually recover after failure and it can re cover using a di erent strategy than other operators in thesame plan  In summary  we make the following contribu tions 1  Extensible  heterogeneous fault tolerance framework We propose a framework that enables the mixing andmatching of di erent fault tolerance techniques in asingle distributed  parallel  and pipelined query plan Our framework is extensible in that it is agnostic of thespeci c operators and fault tolerance strategies used We also describe how three well known strategies canbe integrated into our framework  Section 4  2  Fault tolerance optimizer  We develop a cost basedfault tolerance optimizer  Given a query plan and afailure model  the optimizer selects the fault tolerancestrategy for each operator that minimizes the totaltime to complete the query given an expected num ber of failures  Section 5  3  Operator models for pipelined plans  We model theprocessing and recovery times for a small set of repre sentative operators  Our models capture operator per formance within a pipelined query plan rather than inisolation  They are su ciently accurate for the fault tolerance optimizer to select good plans yet su cientlysimple for global optimization using a Geometric Pro gram Solver  3   We also develop an approach thatsimpli es the modeling of other operators within ourframework thus simplifying extensibility  Section 5 3  We implemented our approach in a prototype parallelquery processing engine  The implementation includes ournew fault tolerance framework  speci c per operator fault tolerance strategies for a small set of representative opera tors  select  join  and aggregate1   and a MATLAB mod ule for the FTOpt optimizer  Our experiments demon strate that di erent fault tolerance strategies  often hybridones  lead to the best performance in di erent settings  forthe con gurations tested  total runtimes with as little asone failure di ered by up to 70  depending on the fault tolerance method selected  These results show that fault tolerance can signi cantly a ect performance  Additionally our optimizer is able to correctly identify the winning fault tolerance strategy for a given query plan  Overall  FTOpt1As in online aggregation  18   aggregates can occur at topof plans  Our prototype uses a standard aggregate operatorbut it could be replaced with an online one Data part  1O11RepartitionO21Input dataon diskCompute nodePartition 1 of operator 1 O31Data part  2O12Data part  NO1N   O22O2XFault tolerance strategy 1RepartitionO31O3YPartition 1 of operator 2 strategy 2 strategy 3       Figure 1  Parallel query plan comprising three op erators  O1  O2  O3  and one input from disk  Eachoperator is partitioned across a possibly di erentnumber of nodes  Data can be re partitioned be tween operators  Fault tolerance strategies are se lected at the granularity of operators is thus an important component of parallel data processing enabling performance gains similar in magnitude to severalother recently proposed MapReduce optimizations  22  26  2  MODEL AND ASSUMPTIONSQuery Model and Resource Allocation  In paral lel data processing systems  queries take the form of di rected acyclic graphs  DAGs  of operators that are dis tributed across servers in a cluster as illustrated in Figure 1 Servers are also referred to as nodes  Each operator canbe partitioned and these partitions then execute in parallelon the same or on di erent nodes  Multiple operators canalso share the same nodes  In this paper  we focus on non blocking query plans  which take the form of trees  ratherthan DAGs   where operators are scheduled and executed atthe same time  and where data is pipelined from one opera tor to the next  producing results incrementally  We assumethat aggregation operators  if any  appear only at the topof a plan  Since input data comes from disk  it can be readand consumed at a steady pace  there are no unexpectedbursts as in a streaming system for example   If a queryplan is too large for all operators to run simultaneously  ourapproach will optimize fault tolerance for pipelined subsetsof the plan  materializing results at the end Fault tolerance choices and resource allocation are in tertwined  an operator can perform more complex fault tolerance if it is allocated a greater fraction of the computeresources  In addition to fault tolerance strategies  our op timizer computes the appropriate allocation of resources tooperators  Due to space constraints  however  in this paper we assume that each operator is partitioned across a givennumber of compute nodes and is allocated its own core s and disk on that node  For the resource allocation details we refer the reader to Appendix A 5 Failure Model  In a shared nothing cluster  di erenttypes of failures occur  Our approach handles a variety offailures from process failures to network failures  To simplifythe presentation  we  rst focus on process failures  i e   weassume that each operator partition runs in its own processand that these processes crash and are then restarted  withan empty state  independently of one another  We comeback to more complex failures in Section 5 5 To make fault tolerance choices  our optimizer must knowthe likelihood for di erent types of failures  If ni is the totalnumber of processes allocated to operator i  we assume thatthe expected number of failures during query execution forthat operator is given by  zi  nin Z where n  Pj2O nj   Ois the set of all operators in the plan  and Z is the expectednumber of process failures for the query  Z can be estimatedfrom the observed failure rates for previous queries and ad ministrators typically know this number  9   We assumeZ to be independent of the chosen fault tolerance plan  Zdepends on the query runtime  whose order of magnitudecan be estimated by FTOpt as the total runtime withoutfault tolerance and without failures  we show that resultsare robust to small errors in Z s value in Section 6 6  Operator Determinism  We assume that individualoperator partitions are deterministic  i e   an operator par tition produces an identical output when it processes thesame input tuples in the same order  This is a commonassumption  20  47  37  2  36  23  and most relational opera tors are deterministic  In a distributed system  however  theorder in which input tuples reach an operator partition maynot be deterministic  Our approach handles this scenario 3  RELATED WORKFault Tolerance in Relational DBMSs  Commercialrelational DBMSs provide fault tolerance through replica tion  6  34  40   Similarly  parallel DBMSs  14  41  45  usereplication to handle various types of failures  Neither  how ever  provides intra query fault tolerance  32  Main memory DBMSs  25  35  28  use a variety of check pointing strategies to preserve the in memory state of theirdatabases  In contrast  our approach preserves and recoversthe state of ongoing computations Fault Tolerance in MapReduce type systems  TheMapReduce framework  10  provides intra query fault tolerance by materializing results between operators andre processing these results upon operator failures  Thisapproach  however  imposes a high runtime overhead andprevents users from seeing any output until the job com pletes  In Dryad  20   data between operators can either bepipelined or materialized  In contrast  we strive to achieveboth pipelining and fault tolerance at the same time  Wealso study how to decide when to materialize or check point data  Recent work  47  applies MapReduce style fault tolerance to distributed databases by breaking long runningqueries into small ones that execute and can be restarted in dependently  This approach  however  supports only a spe ci c type of queries over a star schema  In contrast  weexplore techniques that are more generally applicable  Re cent work also introduced the ability to partly pipeline datain Hadoop  43   a MapReduce type platform  This work iscomplementary to ours as it retains the use of materializa tion throughout the query plan for fault tolerance purposes Other Fault Tolerance Strategies  In the distributedsystems and stream processing literatures  several additionalfault tolerance strategies have been proposed  11  21  37  All these strategies involve replication  One set of tech niques is based on the state machine approach  Here  thesame computation is performed in parallel by two process ing nodes  2  36  37   We do not consider such techniques inthis paper because of their overhead  to tolerate even a sin gle failure  they require twice the resources  The second setof techniques uses rollback recovery methods  11  21   wherethe system takes periodic snapshots of its state that it copiesonto stable storage  i e   into memory of other nodes or ontodisk   We show how to integrate the latter techniques intoour fault tolerance optimization framework  Section 4 2  Recently  Simitsis et  al   38  studied the problem ofselecting fault tolerance strategies and recovery points forETL ows  Similar to us they consider using di erent fault tolerance strategies within a single ow  In contrast to ourwork  they do not propose a general heterogeneous fault tolerance framework  do not have individually recoverableoperators  and do not optimize for overall latency nor showhow fault tolerance choices a ect processing latencies Additional Related Work  Hwang et al   19  studiedself con guring high availability methods  Their approachis orthogonal to our work as it is based on a uniform check pointing strategy and optimizes the time when checkpointsare taken and the backup nodes where they are saved Techniques for query suspend and resume  4  5  use roll back recovery but are otherwise orthogonal to our work Phoenix App  27  explores the problem of heterogeneousfault tolerance in the context of web enterprise applications This approach identi es three types of software compo nents  Persistent  Transactional  and External depending onthe fault tolerance strategy that each uses  message loggingwith checkpointing  transactions  or nothing respectively  Phoenix App then de nes di erent  interaction contracts for each combination of component types  Each contractimplements a di erent protocol with di erent guarantees Thus in Phoenix App  the protocol depends on the fault tolerance capabilities of the communicating components  Incontrast  our approach enables the mixing and matching offault tolerance strategies without changes to the protocol 4  FRAMEWORK FOR HETEROGENEOUS FAULT TOLERANCEWe present a framework for mixing and matching fault tolerance techniques  Our framework relies on conceptsfrom the literature including logging  acknowledging  and re playing tuples as previously done in uniform fault tolerancesettings  21  37  and  contract based  methods for querysuspend resume  4   Our contribution lies in articulatinghow these strategies can be used to enable fault tolerance het erogeneity  We also discuss how three fault tolerance tech niques from the literature can be used within our framework 4 1 ProtocolTo enable heterogeneous fault tolerance between consec utive operators in a query plan  we isolate these operatorsby  xing the semantics of their interactions through a setof four rules  These rules enable each operator to be indi vidually restartable without requiring any blocking materi alization as in MapReduce and also without requiring thatall operators use the same fault tolerance strategy In our framework  as in any parallel data processing sys tem  operators receive input tuples from their upstreamneighbors  they process these tuples and send results down stream  For example  in Figure 1  each partition of operatorO2 receives data from each O1 partition and sends data toall O3 partitions  If an operator partition such as O21 fails a new instance of the operator partition is started with anempty state  To recover the failed state  in our framework the new instance can read any state persistently captured bythe operator s fault tolerance strategy  It can also ask up stream operators to resend  a subset  of their data  To en able such replays  tuples must have unique identi ers  whichmay or may not be visible to applications  and operatorsmust remember the output they produced  For this  we de  ne the following two rules Rule 4 1  Each relation must have a key Rule 4 2  Producer replay guarantee  Upon request  anoperator  must regenerate and resend in order and withoutduplicates any subset of unacknowledged output tuples Acknowledgments mentioned in this rule help reduce thepotential overhead of storing old output tuples by bound ing how much history must be retained  21  37   In ourframework  acknowledgments are optional and are sent fromdownstream operators to upstream ones  For example  onceall operator partitions O21 through O2X that have receivedan input tuple t from operator partition O11 acknowledgethis tuple  the tuple need no longer be retained by O11 Upon sending an acknowledgment  an operator promisesnever to ask for the corresponding tuple again  Formally Rule 4 3  Consumer progress guarantee  If an operatoracknowledges a tuple rx  it guarantees that  even in case offailure  it will never ask for rx again Most parallel data processing systems use in order com munication  e g   TCP  between operators  In that case  anoperator can send a single message with the identi er of atuple rx to acknowledge all tuples up to and including rx When a failure occurs and an operator restarts with anempty state  most fault tolerance techniques will cause theoperator to produce duplicate tuples during recovery  To en sure that an operator can eliminate duplicates before send ing them downstream  we add a last requirement Rule 4 4  Consumer Durability Guarantee  Upon re quest  an operator Od must produce the identi er of themost recent input tuple that it has received from an upstreamneighbor Ou Together  these four rules enable a parallel system to en sure that it produces the same output tuples in the sameorder with and without failure  the tuples may still be de layed due to failure recovery   They also enable operatorsto be individually restartable and the query plan to be bothpipelined and fault tolerant  since data can be transmittedat anytime between operators  Finally  the framework isagnostic of the fault tolerance method used as long as themethod works within the pre de ned types of interactions From the above four rules  only the  Producer replay guar antee  rule potentially adds a visible overhead to the systemsince it requires that a producer be able to re generate  partof  its output 2A no cost solution to satisfy this rule isfor an operator to restart itself upon receiving a replay re quest  With this strategy  an operator failure can cause acascading rollback e ect  where all preceding operators inthe plan get also restarted  This approach is equivalent torestarting a subset of the query plan after a failure occursand is no worse than what parallel DBMSs do today  Alter natively  an operator could write its output to disk  Finally 2Our framework also requires unique identi ers for tuples In our implementation  we create unique identi ers consist ing of 3 integers  44   for the 512 byte tuples used in ourexperiments the space overhead is less than 2 5  some operators  such as joins  can also easily re generatetheir output from their state without the need to log theiroutput  Each of these solutions leads to di erent expectedquery runtimes with and without failures  Our optimizeris precisely designed to select the correct strategy for eachoperator  from a pre de ned set of strategies  in a way thatminimizes the total runtime with failures for a given queryplan as we discuss further below 4 2 Concrete Framework InstanceWe now discuss how three fault tolerance strategies fromthe literature can be integrated into our framework Even though the operators in our framework are deter ministic  see Section 2   in a distributed setting tuples mayarrive in di erent interleaved order on di erent inputs  Wedevelop a low overhead method  based on lightweight log ging of information about input tuple processing order  toensure determinism in this case  but we omit it due to spaceconstraints and refer the reader to Appendix A 4 Strategy NONE  Within our framework  an operatorcan choose to do nothing to make itself fault tolerant  Wecall this strategy NONE  To ensure that it can recover froma failure  such an operator can simply avoid sending any ac knowledgments upstream  Upon a failure  that operator canthen request that its upstream neighbors replay their entireoutput  This strategy is analogous to the upstream backupapproach developed for stream processing engines  21  As in upstream backup  operators such as select or projectthat do not maintain state between consecutive tuples  i e   stateless operators   can send acknowledgments in somecases  e g   if an input tuple r makes it through a selection togenerate the output q and is acknowledged by all operatorsdownstream  then r can be acknowledged  Unlike upstreambackup  which uses di erent types of acknowledgments  21  our approach uses only one type of acknowledgments facili tating heterogeneous fault tolerance  This approach of skip ping over input data during recovery has also been used forresumptions of interrupted warehouse loads  24  To handle a request for output tuples  a stateless oper ator can fail and restart itself to reproduce the requesteddata  For stateful operators  i e   operators such as joinsthat maintain state between consecutive tuples   a more ef  cient strategy is to maintain an output queue and replaythe requested data  21   Such a queue  however  can stillimpose a signi cant memory overhead and an I O overheadif the queue is written to disk  We observe  however  thatstateful relational operators need not keep such output queuebut  instead  can re generate the data from their state  Weimplement this strategy and use it in our evaluation Strategy MATERIALIZE  An alternate rollback re covery approach consists in logging intermediate results be tween operators as in MapReduce  10   While CHCKPTspeeds up recovery for the checkpointed operator itself  MA TERIALIZE potentially speeds up recovery for downstreamoperators  to satisfy a replay request  an operator can sim ply re read the materialized data  Since materialized outputtuples need never be generated again  an operator can usethe same acknowledgement and recovery policy as in NONE Strategy CHCKPT  This strategy is a type of rollbackrecovery strategy where operators  state is periodically savedto stable storage  Because our framework recovers operatorsindividually  it requires what is called uncoordinated check pointing with logging  11   One approach that can directly beapplied is passive standby  21   where operators take periodiccheckpoints of their state  independently of other operators Our framework requires that an operator save su cientinformation to guarantee the consumer progress  consumerdurability  and producer replay guarantees  For this  the op erator must log its state  e g   partial aggregates  join hashtables  and  when applicable  its output queue  The opera tor can acknowledge checkpointed input tuples  Upon fail ures  the operator restarts from the last checkpoint  As anoptimization  operators can checkpoint only delta changesof their state  11   Other optimizations are also possible  11 19  23  and can be used with our framework Unlike NONE and MATERIALIZE  with CHCKPTblocking operators also bene t from fault tolerance provi sioning as they can checkpoint their state periodically andrestart from the latest checkpoint after a failure 3In summary  while our framework imposes constraintson operator interactions  all three of these common fault tolerance strategies can easily be incorporated into it 5  FTOptFTOpt is an optimizer for our heterogeneous fault tolerance framework  FTOpt runs as a post processing step it takes as input  a  a query plan selected by the queryoptimizer and annotates it with the fault tolerance strate gies to use  The optimizer also takes as input  b  informa tion about the cluster resources and cluster failure model and  c  models for the operators in the plan under di er ent fault tolerance strategies  FTOpt produces as outputa fault tolerance plan that minimizes an objective function i e   the expected runtime with failures  given a set of con straints  that model the plan  FTOpt s fault tolerance plans have three parts   1  afault tolerance strategy for each operator   2  checkpoint fre quencies for all operators that should checkpoint their states and  3  an allocation of resources to operators  As indicatedin Section 2  however  we do not discuss resource allocationin this paper due to space constraints  We assume a givenresource allocation to operators For a given query plan  the optimizer s search space thusconsists of all combinations of fault tolerance strategies  Inthis paper  we use a brute force technique to enumeratethrough that search space and leave more e cient enumera tion algorithms for future work  For each such combination FTOpt estimates the expected total runtime with failures the optimal checkpoint frequencies and  optionally  an allo cation of resources to operators  44   It then chooses theplan with the minimum total runtime with failures 5 1 Geometric ModelAs it enumerates through the search space  given a po tential fault tolerance plan  in order to select optimal check point frequencies and estimate the total runtime with fail ures for the plan  FTOpt uses a geometric programming GP  framework  GP allows expressions that model resourcescaling  for resource allocation  and non linear operator be havior  but still  nds a global minima for the model  3  In a geometric optimization problem  the goal is to mini mize a function f0  x   where  x is the optimization variable3FTOpt works irrespective of a blocking operator s place ment in a query plan  We focus on online query processingsince FTOpt is especially useful for such plans  it enablesfault tolerance without creating unnecessary blocking vector  The optimization is subject to constraints on otherfunctions fi  x  and gi  x   All of  x  g  x   and f  x  are con strained to take the following speci c forms    x    x1          xn  such that 8i xi   0  xi 2 R   g  x  must be a monomial of the form cxa11 xa22      xannwith c   0 and ai 2 R   f  x  must be a posynomial de ned as a sum of one ormore monomials  Speci cally  with ck   0 and aik 2R  f  x   Pk Kk 1ckxa1k1 xa2k2      xankn  The optimization is then expressed as follows minimize f0  x subject to fi  x    1  i   1          mgi  x    1  i   1          pIn our case   x    i2O ci  Ni  xNi  xRDi  xRSi   where O isthe set of all operators in the query plan and   denotesconcatenation  Each operator has a vector of variables thatincludes  ci  its checkpoint frequency  Ni  the number ofnodes assigned to it  we assume that it is  xed in this pa per   and xNi  xRDi  and xRSi  which capture the average timebetween two consecutive output tuples  requested replay tu ples  and  units of recovery   a measure of recovery speed  respectively  Tables 1 and 2 summarize these parameters we come back to the tables shortly  Our objective function  f0  x    Ttotal  is the total timeto execute the query including time spent recovering fromfailures  We de ne it more precisely in Sections 5 2 and 5 3 Our constraints comprise framework and operator con straints  The former constrain how operator models arecomposed   a  the average input and output rates of con secutive operators must be equal since the query plan ispipelined   b  aggregate input and output rates for opera tors cannot exceed the network and processing limits  and c  if an operator uses an output queue  it must either check point its output queue to disk frequently enough  or mustreceive acknowledgements from downstream operators fre quently enough to never run out of memory  Individual op erators can add further constraints  see Section 5 3  5 2 Objective FunctionFTOpt minimizes the following cost function  that cap tures the expected runtime of a query plan Ttotal   maxp2P Tpd  i Xd1i 1Dpi  Xi2Ozi   Ri  1 The  rst term is the total time needed to completely pro cess the query including the overhead of fault tolerance if nofailures occur The second term is the expected time spentin recovery from failures  Failure recovery can be added ontop of normal processing because  with our approach  whena failure occurs  it blocks the entire pipeline  Indeed  evenif one operator partition fails  operators upstream from thatpartition stop executing normally and take part in the re covery  A side e ect of this approach is that recovering asingle operator partition or recovering all partitions yieldapproximately the same recovery time In more detail  for the  rst term  P is the set of all pathsfrom the root of the query tree to the leaves  For a givenpath p 2 P of length d  the root is labeled with p1 andthe leaf with pd  Dpiis the delay introduced by operator pi Table 1  Functions capturing operator behavior Delay to produce the  rst tupleDN    Average delay to output  rst tuple during normal pro cessing  with fault tolerance overheads  DRD    Average delay to produce  rst tuple requested by a down stream operator during a replay DRS    Average delay to the start of state recovery on failure Average processing timexN    Average time interval between successive output tuplesduring normal runtime  with fault tolerance overheads  xRD    Average time interval between successive output tuplesrequested by a downstream operator xRS    Average time interval between strategy speci c  units ofrecovery   e g   checkpointed tuples read from disk  Acknowledgement interval  a     sent to upstream nodes Table 2  Operator behavior parameters    Query parametersjIuj Number of input tuples received from upstream operator u jIj Number of tuples produced by current operator Operator parameterstcpu Operator cost in terms of time to process one tuple tioThe time taken to write a tuple to disk Runtime parametersxNu Average inter tuple arrival time from upstream operator u innormal processing F Fault tolerance strategy c Number of tuples processed between consecutive checkpoints N The number of nodes assigned to the operator Surrounding fault tolerance contextad Maximum number of unacknowledged output tuples xRDu Average inter tuple arrival time from upstream operator uduring a replay where the delay is de ned as the time taken to produce its rst output tuple from the moment it receives its  rst inputtuple  and  Tpdis the time taken by the leaf operator tocomplete all processing after receiving its  rst input tuple Tpd   Dpd  xNpdjIj  where jIj is the number of output tuplesproduced by the leaf operator  Dpi and Tpd depend on theinput tuple arrival rate at an operator  which depend on howfast previous operators are in the pipeline  We capture thesedependencies with constraints as mentioned in Section 5 1 For the second term  O is the set of all operators in thetree  For operator i 2 O  zi is the expected number of fail ures during query execution  and Ri is the expected recoverytime from a failure  We estimate zi using an administrator provided failure model as described in Section 2 To adapt Ttotal to be a posynomial  we need to eliminatethe max operator  For this  we introduce a variable T forTtotal and decompose the objective to be  minimize T witha constraint for each path such that   T 1  expected totaltime for the path     1   Since the expected total time  fora single path  is a posynomial  the constraints are posyno mials  and the entire program is a GP 5 3 Operator ModelingTo compute the objective function  FTOpt thus requiresthat each operator provide expressions that characterizeits delay and processing time during normal operation andwhen failures occur  These expressions must be providedfor each fault tolerance strategy that the operator supports Formally  FTOpt needs to be given the functions in Table 1expressed in terms of the parameters  represented by   inTable 2  these parameters capture the inter dependenciesbetween operators   FTOpt combines these functions to gether to derive the overall processing time Ttotal In this section  we show how to express such functions inActual runtimeNumber of output  tuplesTimeTotal outputtacpuPoint of change  tangent NBout nin Figure 2  Data output curve  comprised of NBout   curve till the  Point of change  and the dashed lineafter it  for a symmetric hash join operator our framework  We proceed through an example  we derivethe constraint equations for join  symmetric hash join   Themodels for select and aggregate are similarly derived  44  5 3 1 Modeling Basic Operator RuntimeTo model our problem as a GP  we must  a  derive theoperator output rate  given by the inter output tuple delay xN  in the absence of failures  and  b  derive the delay  DN The delay  however  is simply either negligible for selects andthe symmetric hash join that we model or equal to the totalprocessing time for aggregates 4The challenge in expressing an operator s output rate isthat xNcan follow a complex curve for some operators suchas certain non blocking join algorithms as illustrated in Fig ure 2  The  gure shows the data output curve for a sym metric hash join operator  For this operator  the more tu ples that it has already processed  the more likely the joinis to  nd matching tuples  and thus it outputs tuples at anincreasingly faster rate  As a result  at the beginning ofthe computation  the bottleneck is the input data rate  theNBout nin  curve  and the operator produces increasinglymore output tuples for each input tuple  Eventually  theCPU at the join becomes the bottleneck  tcpua curve  andthe output rate attens We found that ignoring such e ects and assuming a con stant output signi cantly underestimated the total runtimefor the operator  Alternatively  modeling these e ects andexposing them to downstream operators signi cantly com plicated the overall optimization problem  We thus optedfor the following middle ground  we model the non uniformoutput rate of an operator to derive its total runtime  Giventhe total runtime  we compute the equivalent average out put rate that we use as a constant input arrival rate for thenext operator  The GP framework is helpful here to expressthese non linear behaviors Interestingly  we  nd that we can automatically derive theabove curve from the following operator properties   tcpua   Average time to generate one output tuple if allinput is available with no delay   NBout nin   This function provides the total numberof output tuples produced for a given number of tuples nin  received across all input streams The above functions can easily be derived  hence simpli fying optimizer extensibility   Both these functions are ex tensions of parameters of standard query optimizers   a tcpua corresponds to the standard query optimizer functionfor computing an operator s cost  except that we then di vide this cost by the operator output cardinality  and  b 4To compute total query times  we ignore any partial resultsthat an online aggregate may produce  NBout nin  is similar to computing the cardinality of anoperator output  except that it also captures how that out put is produced as the input data arrives  Simple operatorslike select or merge join have NBout    nin  where   isthe operator selectivity  For blocking operators such as ag gregates  after the delay DN    all the output tuples areproduced at once and hence NBout   jIj  For other non blocking operators the relationship can be more complex aswe discuss next using our symmetric hash join as example For the symmetric hash join operator  de ne Iutot to bethe set of all tuples received from both upstream input chan nels  Hence  jIutotj   jI1j   jI2j  For this operator tcpua   jIj1 jIutotj   jIj tcpuThe expression is a product of the average time takento process either an input or output tuple  tcpu  obtainedthrough micro benchmarks  and the total number of tuplesseen by the operator  including the input tuples  Iutot  andthe output join tuples  I   This number is then divided bythe total number of output tuples  jIj  to get the averagetime per output tuple To get the NBout function for a symmetric hash join weassume that the input tuples from the two input channelscan arrive in any order  each order being equally likely  Let    a function of the join selectivity    jI1j and jI2j  44   be theprobability that two tuples from di erent channels join andpi be the probability that a tuple belongs to the ithchannel In this case  the function NBout nin  is de ned as follows NBout nin      p1p2nin nin  1     p  1p2n2inIntuitively  nin nin1  is the count of pairs of distinct tuplesto join  p1p2 is the probability that they come from di erentchannels  and    is the probability that they join We now show how our optimizer translates these functionsinto a set of inequalities that characterize the average timeinterval between successive output tuples produced by anoperator  For this  we require that the NBout nin  functiontake the form  NBout nin    nkin  in order to  t into theGP framework  Thus  for our join operator       p1p2 andk   2  Informally  as the operator sees more input tuples the number of the output tuples produced after processinga single new input tuple should never decrease Given the above  the average time interval between con secutive output tuples  xN  is given by the following inequal ities me    xIN kktk1fme    tcpua  1me   1k  xIN 1kjIj1 1k 1  k1 tf   jIjm1e   xNjIjThe above inequalities take xINas input  which is the timeinterval at which input tuples are arriving  xINdepends onthe current execution context  If we are operating normally it is the average time interval between tuples produced bythe upstream operators  if we are recovering from a failure we might read the input tuples from disk at the maximumbandwidth possible for the disk For the exact derivation of this mode  we refer the readerto Appendix B  Here  we only provide the intuition behindit In the above equations  jIj is the output cardinality  and k come from the NBout nin  function  me is the num ber of output tuples produced per second at the instant theprocessing ends and tf is the  rst time at which the outputproduces tuples at the rate me  The  rst equation realizesthis relationship between me and tf   The following inequal ity states that the operator can not take less than tcpua timeto produce an output tuple  since this is the least amountof time the processor needs per tuple  given the resourcesit has  For the second inequality  its right hand side is themaximum rate at which output could be produced if theonly bottleneck was the rate of arrival of input tuples  Notethat  since we require the NBout    function to have a non negative rate of change  the fastest output production ratewill be at the end of the computation and the derivative ofthe function NBout     with respect to xIN  at the end givesus this value  Since  in a real computation the processingcost is positive  the actual observed rate has to be less thanthe derivative  the right hand side in the second inequality  The third inequality states that the total time to process alltuples  which is equal to the average output rate times thenumber of output produced  must be higher than the actualprocessing time  which is its left hand side To model a di erent operator  the functions for tcpua andNBout nin  would change  while the form of the inequalitiesand equalities used by the optimizer would remain the same They simply use the above as parameters We model a partitioned operator as a single operator thatscales linearly with allocated resources  This approach suf  ces to show the feasibility and impact of fault toleranceoptimization  We leave extensions to more complex models including data skew between partitions  for future work 5 3 2 Modeling Overhead of Fault toleranceFault tolerance overhead only a ects tcpua   the time an op erator needs to produce an output  The model depends onthe operator implementation  For MATERIALIZE  our joinwrites all output tuples to disk  For CHCKPT  it logs theincoming tuples to disk incrementally5  The join does notmaintain any output queue For brevity  we use the notation that IN  IMand ICare 1 ifNONE  MATERIALIZE or CHCKPT is the chosen fault tol erance option  respectively  and are 0 otherwise  Althoughwe need one equation per fault tolerance strategy we repre sent them as a single one tcpua   jIj1 tcpu jIutotj   jIj    ICtiojIutotj   IMtiojIj Here tiois the time to write a tuple to disk and is alsoobtained through micro benchmarks 5 3 3 Modeling Replay Request TimesFTOpt also needs to know the average rate at which out put tuples are produced to satisfy a replay request and thedelay in generating the  rst requested tuple  The replayrate may depend on when  during the course of the query the downstream fails  For example  if the replay behavesas during normal operations for the symmetric hash join  itmight be slower if the downstream fails early on and be fasterlater  To approximate the recovery rate we  nd the time ittakes to replay all output tuples and divide that number by5Out of simplicity  our join checkpoints input tuples as theyarrive rather than checkpointing the hash table  When itrebuilds the hash table from a checkpoint  the operator doesnot redo the join             the total number of output tuples  During this replay phase the operator has no fault tolerance overheads As before  the exact model depends on the implementa tion details  Our join implementation uses its in memoryhash table to regenerate outputs and hence the delay is neg ligible  But it could be signi cant for a join that can not useeither its state or its output to answer tuple requests To get the average output rate  we reuse the frameworkwe developed in the previous section  Thus we only need tospecify tcpua and NBout nin  for the replay mode Since  during replay  we only reprocess the inputs withoutany fault tolerance overhead  tcpua   jIj1 jIutotj   jIj tcpu The form of the NBout    remains the same as for the nor mal processing  Also  during reprocessing the input tuplesare already in memory  hence the inter tuple arrival time ofinputs xINis at least tcpuand we take xIN  tcpu 5 3 4 Modeling Recovery TimeTo compute the total time to recover from a failure  weneed to know the average rate at which recovery proceeds As before  the exact recovery model depends on the imple mentation  For our join  upon failure the MATERIALIZEand the NONE options have to request all the input fromthe upstream nodes and rebuild the hash table exactly as itwas before  using Rule 4 2 and operator determinism  44   while CHCKPT rebuilds it from the input tuples logged todisk In all cases  during recovery  no output is produced whenthe input tuples are processed to remake the hash table Thus  tcpua   tcpusince we look at each input tuple once To de ne the function NBout nin  we think of the hashtable being rebuilt as the desired  output  and the inputtuples as the inputs  Since all the input tuples are usedto generate the  output  hash table  NBout nin    nin For MATERIALIZE and NONE  xINis the average timeinterval in which requested tuples from the upstream nodesarrive  For CHCKPT  since we directly read tuples from thedisk  xIN  tio The delay in getting the  rst input is negligible if we useCHCKPT and is equal to the delay of the upstream tuplesin the case of NONE and MATERIALIZE We approximate the expected hash table size to recoverto be12jIutotj  Thus  the expected time to recover is thesum of  1  the delay to receive the  rst input tuple  and  2 the product of the expected hash table size and the averagetime per tuple spent in adding a tuple to that hash table In summary  compared to existing cost models for paral lel query runtime estimation  13  12  and fault tolerance instreaming engines  21   our models capture the dynamic op erator interactions in pipelined queries  which we observedto a ect runtime predictions and fault tolerance optimiza tion  For example  a fast operator following a slow one in apipeline will produce its output slowly  At the same time we do not require that an operator s output tuples be uni formly spread across the entire execution time of the oper ator  16  49   Indeed  because we use a GP framework  wesupport simple types of non uniform outputs such as thatof asymmetric hash join  Of course  our GP framework maynot cover all cases  In particular  for multi phase operators e g   a symmetric hash join that spills state to disk   we maystill need to split the operator into multiple sub operatorsfor more accurate modeling of each phase 5 4 Approach ImplementabilityOur approach consists of  1  a protocol that enables het erogeneous fault tolerance in a parallel query plan and  2 an optimizer that automatically selects the fault tolerancestrategy that each operator should use  We now discuss thedi culty of implementing this approach in a parallel dataprocessing system To implement our approach  developers need to  a  imple ment desired fault tolerance strategies for their operators ina manner that follows our protocol  In Section 4 2  however we showed  how to e ciently implement three well knownfault tolerance strategies for generic stateless and statefuloperators  Existing libraries can also help with such imple mentation  e g    23    Developers must also  b  model theiroperator costs within a pipelined query plan  To simplifythis latter task  we develop an approach that requires onlythat developers specify well known functions under di erentfault tolerance strategies and during recovery  an operatorcost function and a function that computes how the outputsize of an operator grows with the input size  Our opti mizer derives the resulting operator dynamics automatically For parallel database systems  41  14  and MapReduce typesystems such as Hive  1  or Pig  30   which come with pre de ned operators  the above overhead needs only be paidonce and we thus posit that it is a reasonable requirement For user de ned operators  UDOs   the above may stillbe too much to ask  In that case  the simplest strategyis to treat UDOs as if they could only support the NONEor MATERIALIZE strategies  depending on the underlyingplatform  without ever producing acknowledgments  Withthis approach  UDO writers need not do any extra workat all  yet the overall query plan can still be optimized andachieve higher performance than without fault tolerance op timization as we show in Section 6 4 Finally  our approach relies on a set of parameters includ ing IO cost  expressed as the time tiospent in a byte sizeddisk IO   per operator CPU cost  expressed as the time tcpuspent processing each tuple   and total network bandwidth Commercial database systems already automate the collec tion of such statistics  e g    31    though tcpuis typicallyexpanded into a more detailed formula Other necessary information includes the expected num ber of failures for the query  see Section 2   operator selectiv ities  standard optimizer provided metric   and an estimateof the total checkpointable state  As shown in Section 6 6 our optimizer is insensitive to small errors in these estimates Overall  the requirements of our fault tolerance optimiza tion framework are thus similar to those of existing cost based query optimizers 5 5 Handling Complex Failure ScenariosSo far  we have focused on process failures  However  ourapproach also handles other types of failures Our approach still works when entire physical machinesfail  e g   due to a disk failure  a power failure  or a net work failure   To support such failures  checkpoints must bewritten to remote nodes instead of locally  19   which addsnetwork and CPU costs that must be taken into account bythe optimizer  Given that the optimizer knows the size ofthese checkpoints  it can take that cost into account  Sec ond  when a physical machine fails or becomes disconnected the number of nodes in the cluster is reduced by one  whichmust also be taken into account by the optimizer  22 2  2  2  88  16 a  Query 11608 8  8  8  88  8 b  Query 21608 16  8  16  160  0 008 c  Query 31608 40  8  40  80  0 008 d  Query 41608 8  8  8  88  328 96 20 1  8 96  20 1  79 4  0 008 e  Query 5Figure 3  Query plans used in experiments         denote Select  Join  and Aggregation  respectively All numbers are in millions Our approach does not currently handle failures that af fect a large number of machines  Indeed  such failures cancause the temporary loss of input data or checkpointed data In such cases  the query needs to be restarted in its entiretyonce the input data becomes available again  In general however  large scale rack and network failures are infrequent while single machine failures are common  For example Google reports 5 average worker deaths per MapReduce jobin March 2006  9   but only approximately 20 rack failuresper year  and similarly few network failures   8  Even though our approach does not handle large scale fail ures that cause the loss of input or checkpointed data  it doeshandle multiple operators failing at the same time  The onlyrequirement in such cases is that operators be restarted fromdownstream to upstream  ensuring that each operator knowswhere to start recovering from before asking upstream neigh bors to replay data 6  EVALUATIONWe evaluate FTOpt by answering the following questions  1  Does the choice of fault tolerance strategy for a parallelquery matter   2  Are there con gurations where a hybridplan  where di erent operators use di erent fault tolerancetechniques  outperforms uniform plans   3  Is our optimizerable to  nd good fault tolerance plans automatically   4 How do user de ned operators a ect FTOpt   5  What isthe scalability of our approach   6  How sensitive is FTOptto estimation errors in its various parameters We answer these questions through experiments with a va riety of queries in a 17 node cluster  Each node has dual 2 5GHz Quad Core E5420 processors and 16 GB RAM runningLinux kernel 2 6 18 with two 7 2K RPM 750 GB SATA harddisks  The cluster runs a simple parallel data processing en gine that we wrote in Java  The implementation includesour new fault tolerance framework and speci c per operatorfault tolerance strategies for a small set of representative op erators  All fault tolerance strategies were moderately opti mized  see Section 4 2   We implemented the optimizer inMATLAB using the cvx package  7  The query plans that we use in the experiments are shownin Figure 3  They include an SJJJ and SJJA query  we alsotest a more complex query later in this section   For bothqueries we have 8 partitions per operator with 2 cores and1 disk per partition  Partitions of the same operator run ondi erent machines  The input data is synthetic and withoutskew  Tuples are 0 5 KB in size  The schema consists of 40 50 100 150 200 250 NM  NN  MM  CC  cc  MM  cc Select Join  Select Average  Select Join Runtime  s  Configurations Real  Predicted Figure 4  Runtime without failures for various two operator queries  X axis labels show the fault tolerance strategy chosen  N for NONE  M for MA TERIALIZE  C for CHCKPT with a total of 10checkpoints  and c for CHCKPT with 1K check points attributes used to hash partition tuples for each operator  a5th attribute for grouping the aggregates  and a 6th one forthe join predicates  A separate producer process generatesinput tuples  For a given plan  we get the expected recoverytime by injecting a failure midway through the time theplan takes to execute with no failures  We inject exactly onefailure per run and show the recovery time averaged over alldistinct operators in the plan 6 1 Model Validation ExperimentsFTOpt requires the tcpuand the tiovalues for each opera tor  It also requires the network bandwidth for each machinein the cluster  Through micro benchmarks  we  nd that theaverage time to read a tuple from disk  sequential read  istio  13 0  s for a 0 5 KB tuple  This number is equivalentto a disk throughput of 37 MBps  For select and aggregateoperators  we measure tcputo be 1 82 s  The join operator internally  works in two parts   1  hashing the input tupleand storing it in one of the tables for a cost of t1   8 sand  2  joining the hashed input tuple to the correspondingtuples from the other table for a cost of t2   1 s  We uset1  t2  and the operator s selectivity to estimate its tcpu  Fi nally  we measure the network I O time per 0 5 KB tupleto be 4 7 s  which is equivalent to a network bandwidth of109 4 MBps and is close to the theoretical maximum of 1Gbps network bandwidth for each machine in the cluster These parameters along with our operator models enableFTOpt to predict the runtime for an entire query plan Figure 4 shows the runtime without failure for a few two operator queries  While the median percentage di erencebetween real and predicted runtime is 9 5   this error issmall given the overall di erences in runtime between var ious con gurations  We measure the sensitivity of our ap proach to the benchmarked parameter values in Section 6 6 6 2 Impact of Fault Tolerance StrategyThe  rst question that we ask is whether a fault toleranceoptimizer is useful  how much does it really matter whatfault tolerance strategy is used for a query plan Figures 5 through 7 show the actual and predicted run times for Queries 1 through 3 from Figure 3 with 8 partitionsper operator  Note that  each join receives input from twosources  its upstream operator in the plan and a producerprocess  In all our experiments  an equal number of tupleswas received from each source  Whenever FTOpt selectsCHCKPT as a strategy  it also chooses the checkpoint fre quency  Query 3   In other cases  we use 100 checkpoints  aCKPT MAT NONE OPT  RESTART010203040506070Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed Normal250 CKPTFigure 5  Query 1  SJJJ  MAT NONE OPT  RESTARTCKPT MAT OPT  NONE RESTART050100150200250Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 6  Query 2  SJJJ with lower selectivities manually selected value that we found to give high perfor mance in these experiments The most important result from these experiments is that while these queries are all similar to each other  each one re quires a di erent fault tolerance plan to achieve best perfor mance  For Query 1  a uniform NONE strategy is best  ForQuery 2  uniform MATERIALIZE wins  Finally  for Query3  uniform CHCKPT outperforms the other options Second  restarting a query is at most 50  slower than astrategy with more  ne grained fault tolerance  The  ne grained strategy gains the most when it reduces recoverytimes with minimal impact on runtime without failures For some queries  the appropriate choice of fault tolerancegets close to this theoretical upper bound  For Query 2 RESTART is 31  worse than the best strategy while forQuery 3  restarting is 44  slower than the best strategy Achieving such gains  however  requires fault tolerance op timization  Indeed  di erent strategies win for di erentqueries and a wrong fault tolerance strategy choice leads tomuch worse performance than restarting a query  Overall the di erences between the best and worst plan are high 58  for Query 1  31  for Query 2  and 72  for Query 3 Finally  in all cases  FTOpt is able to identify the winningstrategy  Predicted runtimes do not always match the ob served ones exactly  Most of the di erence is attributable toour simple model for the network and FTOpt s predictionsare thus more accurate when either CPU or disk IO is thebottleneck in a query plan and less accurate when it is thenetwork  While we could further re ne our models  to pickthe optimal strategy  we only need to have correct relativeorder of predicted runtimes for di erent plans  As shownin Figures 4 through 8  FTOpt preserves that order whenruntime di erences are large  When two con gurations leadto very similar runtimes  FTOpt may not  nd the best ofthese plans but the choice of plan matters less in such a caseand FTOpt always suggests one of the good plans In summary  the correct choice of fault tolerance strategycan signi cantly impact query runtime and that choice isnot obvious as similar query plans may require very di erentstrategies  FTOpt can automatically select a good plan 6 3 Bene   ts of Hybrid Con   gurationsWe now consider a query  Query 4   similar to QueryCKPT OPT  MAT NONE RESTART050100150200250300350400450500Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 7  Query 3  SJJA query CKPT MAT NONE HYBRID OPT  RESTART050100150200250300350400450Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 8  Query 4  SJJA with more expensive joins  The hybrid strategy is to materialize after select  donothing for joins  and checkpoint the aggregate3  but with the joins processing and producing much moredata  making checkpointing expensive  Figure 8 shows thata hybrid strategy that materializes the select s output  doesnothing for the joins  and checkpoints the aggregate s statefor a total of 40 checkpoints  value selected by the opti mizer   yields the best performance  The uniform strate gies are 15  slower at best and 21  slower at worst whileRESTART is 35  slower We observe similar gains for a longer query  Query 5 with eight operators  Figure 9 shows that the hybrid plan chosen by the optimizer  materializes both selects  outputs does nothing for joins and takes 20 checkpoints of the aggre gate  The best and worst uniform strategies and RESTARTare 16   23  and 36  slower  respectively  Manually  wefound that checkpointing the  rst two joins in the hybridplan led to another hybrid plan that was 2  faster  Whilethe optimizer did not choose this better plan  the plan itchose performs similarly  Further  both the observed andpredicted best plans are hybrid The experiments thus show that hybrid fault tolerancestrategies can be advantageous and the best strategy for anoperator depends not only on the operator but on the wholequery plan  the same operator can use di erent strategiesin di erent query plans  e g   select in Queries 3 and 4 Note that we inject only one failure per experiment  Thus our graphs show the minimum guaranteed gains  Additionalfailures amplify di erences between strategies 6 4 Performance in Presence of UDOsWe look at the applicability of heterogeneous fault tolerance when an operator is a user de ned function withlimited fault tolerance capabilities  We experiment withQuery 3  but treat its last operator  the aggregate  as aUDO that can only restart from scratch if it fails  Note thatRule 4 2 and operator determinism  44  allow restarting aUDO in isolation without restarting the entire query Figure 10 shows the results  Previously  the best fault tolerance strategy  with a single failure  was to checkpointevery operator   With CKPT   and checkpointing aggregateprovided signi cant savings in recovery time  Now that theaggregate can use NONE as sole strategy  we  nd that ma 0200400600OPT NONE MAT CKPT RESTARTRuntime  in s Observed Normal Observed RecoveryFigure 9  Query 5  SJJJSJJA Query   The opti mal hybrid strategy is MNNNMNNC where M de notes MATERIALIZE and N denotes NONE and Cdenotes CHCKPT  In the optimal con guration 20checkpoints are taken 0100200300400500With CKPT UDO OPT MAT NONE RESTARTRuntimes  in s Observed Normal Observed RecoveryFigure 10  Impact of aggregate becoming a UDOswithout fault tolerance capabilities on Query 3  Theoptimal strategy is to materialize after select and donothing elsewhere terializing the  rst operator s output and using NONE forthe remaining operators outperforms uniformly materializ ing  none and RESTART by 48   12   and 24   respec tively  The hybrid strategy is itself 16  slower than theoptimal strategy for Query 3   With CKPT   Hence even in the presence of fault tolerance agnosticUDOs  FTOpt can generate signi cant runtime savings 6 5 ScalabilityFTOpt currently uses a brute force search algorithm  butwe  nd that simple heuristics can signi cantly prune thesearch space  Indeed  we observe that the best hybrid plansuse the NONE strategy for many operators and using an other strategy in place of NONE will always increase theruntime without failures  Thus  if the runtime without fail ure for a plan exceeds the runtime with failures for anotherplan  we can prune the former plan  Hence  evaluating plansin the decreasing order of the number of operators that usethe NONE strategy can prune signi cant fractions of thesearch space  For example  with this heuristic  the optimizerexamines only 28 out of 81 con gurations for Query 4  Inaddition  the search essentially computes the least costly ofa set of independent optimization problems and all of theseproblems can be optimized in parallel FTOpt s MATLAB implementation uses the cvx pack age  which o ers a successive approximation solver usingSDPT3  42   In our prototype  the average time to solve theoptimization problem per plan is around 25s for the 4 oper ator plans in the previous sections  However  an optimizedsolver can solve a larger problem in a sub millisecond  29  The behavior of an operator for a fault tolerance strategyis modeled using at most 12 inequality and 4 equality con straints of 11 variables  Thus  a query with n operators canbe modeled using 11n variables  13n   1 inequality and 4nequality constraints  Further  all but one of the constraintsare sparse  they depend on just a few variables indepen dent of n  For example  with 4 operators  our models use44 variables  16 equalities  and 53 inequalities  The existing01002003004005001 11 21 31 41 51 61 71 81Runtime  in s RankPredicted ObservedFigure 11  Observed and predicted runtimes forQuery 3  sorted on the predicted runtime  for all81 fault tolerance plans for the query Table 3  Real rankings of top 5 plans from perturbedcon gurations Perturbation RankingsFailing thrice instead of once 1 2 3 4 5IO cost 2 0x of true value 1 6 8 9 18IO cost 0 5x of true value 2 1 3 4 5IO cost 10x of true value 6 18 20 21 24IO cost 0 1x of true value 2 28 31 30 29Selectivity of all operators 1 1x 1 2 3 4 5Selectivity of all operators 0 9x 1 2 3 4 5Selectivity of all operators 2 0x 1 2 3 4 5Selectivity of all operators 0 5x 56 1 66 67 10optimized solvers can solve a problem of 140 variables  120equalities  and 60 inequalities in 0 425 ms on average  29  To sum up  with an optimized solver  and a parallelizedheuristic search algorithm  FTOpt could be scalable enoughto handle larger query plans within a few seconds 6 6 Optimizer SensitivityWe evaluate FTOpt s sensitivity to inaccuracies in param eter estimates  We experiment with Query 3 since it is mostsensitive to wrong choices  Figure 11 shows that runtimesvary from about 250s to 400s depending on the chosen plan To evaluate the sensitivity for a given parameter  we re run FTOpt  feeding it a perturbed parameter value  Weonly perturb a single parameter at a time while keeping theother parameters at their true values  We then compute thetop 5 plans with the perturbed value and report the ranks ofthese plans in FTOpt s original ranking  Figure 11   Table 3shows the results  As an example  in this table  when IO costincreases to 2X its true value  the second best plan identi edby FTOpt was ranked 6th with the real IO costs Table 3 shows that FTOpt is very robust to small errors inthe number of failures and it is fairly robust to even large er rors in IO cost  a 10x change still leads to a good plan  withtrue rank 6  being chosen  though the subsequent plans havepoor true rankings  FTOpt is least robust to cardinality esti mation errors  In our experiments  we varied the selectivitiesof all the operators in tandem  and with the join always pro cessing the same number of tuples from both streams   Inthis scenario  our predictions were unchanged for changes of1 1x  2x and 0 9x in selectivity but for a 0 5x change  the topchoice s true rank was 56 with an observed runtime about70  worse than that of the best con guration possible The robustness to I O cost errors and failure errors can beexplained by the fact that the e ect of these errors is mostlylinear on the optimizer  However  imprecise selectivity es timates have an exponential e ect  the further an operatoris from the beginning  the less data it processes and it pro duces even less output  on FTOpt  Thus  the optimizer ismore sensitive to perturbations in selectivity estimates 7  CONCLUSIONIn this paper  we presented a framework for heterogeneousfault tolerance  a concrete instance of that framework  andFTOpt  a latency and fault tolerance optimizer for paralleldata processing systems  Given a pipelined query plan  ashared nothing cluster  and a failure model  FTOpt selectsthe fault tolerance strategy for each operator in a query planto minimize the time to complete the query with failures  Weimplemented our approach in a prototype parallel query pro cessing engine  Our experimental results show that di erentfault tolerance strategies  often hybrid ones  lead to the bestperformance in di erent settings and that our optimizer isable to correctly identify a winning strategy AcknowledgmentsWe thank Phil Bernstein  Bill Howe  Julie Letchner  DanSuciu  and the anonymous reviewers for helpful commentson the paper s early drafts  This work is supported in partby the National Science Foundation grants NSF CAREERIIS 0845397 and IIS 0713123  gifts from Microsoft Research and Balazinska s Microsoft Research Faculty Fellowship 8  REFERENCES 1  Ashish Thusoo et  al  Hive   a petabyte scale data warehouseusing hadoop  In Proc  of the 26th ICDE Conf   2010  2  M  Balazinska  H  Balakrishnan  S  Madden  andM  Stonebraker  Fault tolerance in the Borealis distributedstream processing system  In Proc  of the SIGMOD Conf  June 2005  3  S  P  Boyd  S  J  Kim  L  Vandenberghe  and A  Hassibi  Atutorial on geometric programming  Technical report  StanfordUniversity  Info  Systems Laboratory  Dept  Elect  Eng   2004  4  B  Chandramouli  C  N  Bond  S  Babu  and J  Yang  Querysuspend and resume  In Proc  of the SIGMOD Conf   2007  5  S  Chaudhuri  R  Kaushik  A  Pol  and R  Ramamurthy Stop and restart style execution for long running decisionsupport queries  In Proc  of the 33rd VLDB Conf   2007  6  Chen et  al  High availability and scalability guide for DB2 onlinux  unix  and windows  IBM Redbookshttp   www redbooks ibm com redbooks pdfs sg247363 pdf Sept  2007  7  Cvx  http   www stanford edu  boyd cvx   8  J  Dean  Software engineering advice from building large scaledistributed systems  http   research google com people jeff stanford 295 talk pdf  9  J  Dean  Experiences with MapReduce  an abstraction forlarge scale computation  Keynote I  PACT  2006  10  J  Dean and S  Ghemawat  MapReduce  simpli ed dataprocessing on large clusters  In Proc  of the 6th OSDI Symp  2004  11  E  N  M  Elnozahy  L  Alvisi  Y  M  Wang  and D  B  Johnson A survey of rollback recovery protocols in message passingsystems  ACM Computing Surveys  34 3   2002  12  S  Ganguly  A  Goel  and A  Silberschatz  E cient andaccurate cost models for parallel query optimization  extendedabstract   In Proc  of the 15rd PODS Symp   1996  13  S  Ganguly  W  Hasan  and R  Krishnamurthy  Queryoptimization for parallel execution  In Proc  of the SIGMODConf   pages 9 18  1992  14  Greenplum database  http   www greenplum com   15  Hadoop  http   hadoop apache org   16  W  Hasan and R  Motwani  Optimization algorithms forexploiting the parallelism communication tradeo  in pipelinedparallelism  In Proc  of the 20th VLDB Conf   1994  17  J  M  Hellerstein  R  Avnur  and V  Raman  Informix underCONTROL  Online query processing  Data Mining andKnowledge Discovery  4 4  281 314  2000  18  J  M  Hellerstein  P  J  Haas  and H  J  Wang  Onlineaggregation  In Proc  of the SIGMOD Conf   1997  19  J  H  Hwang  Y  Xing  U   Cetintemel  and S  Zdonik  Acooperative  self con guring high availability solution forstream processing  In Proc  of ICDE Conf   Apr  2007  20  M  Isard  M  Budiu  Y  Yu  A  Birrell  and D  Fetterly  Dryad Distributed data parallel programs from sequential buildingblocks  In Proc  of the EuroSys Conf   pages 59 72  2007  21  Jeong Hyon Hwang et  al  High availability algorithms fordistributed stream processing  In Proc  of the 21st ICDEConf   Apr  2005  22  S  Y  Ko  I  Hoque  B  Cho  and I  Gupta  Making cloudintermediate data fault tolerant  In Proc  of the 1st ACMsymposium on Cloud computing  SOCC   pages 181 192  2010  23  Y  Kwon  M  Balazinska  and A  Greenberg  Fault tolerantstream processing using a distributed  replicated  le system  InProc  of the 34th VLDB Conf   2008  24  W  J  Labio  J  L  Wiener  H  Garcia Molina  and V  Gorelik E cient resumption of interrupted warehouse loads  SIGMODRecord  29 2  46 57  2000  25  A  P  Liedes and A  Wolski  Siren  A memory conserving snapshot consistent checkpoint algorithm for in memorydatabases  In Proc  of the 22nd ICDE Conf   page 99  2006  26  D  Logothetis  C  Olston  B  Reed  K  C  Webb  and K  Yocum Stateful bulk processing for incremental analytics  In Proc  ofthe 1st ACM symposium on Cloud computing  SOCC   2010  27  D  Lomet  Dependability  abstraction  and programming  InDASFAA  09  Proc  of the 14th Int  Conf  on DatabaseSystems for Advanced Applications  pages 1 21  2009  28  Marcos Vaz Salles et  al  An evaluation of checkpoint recoveryfor massively multiplayer online games  In Proc  of the 35thVLDB Conf   2009  29  J  Mattingley and S  Boyd  Automatic code generation forreal time convex optimization  In Convex Optimization inSignal Processing Optimization  Cambridge U  Press  2009  30  C  Olston  B  Reed  U  Srivastava  R  Kumar  and A  Tomkins Pig latin  a not so foreign language for data processing  InProc  of the SIGMOD Conf   pages 1099 1110  2008  31  Oracle database  http   www oracle com   32  A  Pavlo et  al  A comparison of approaches to large scale dataanalysis  In Proc  of the SIGMOD Conf   2009  33  L  Raschid and S  Y  W  Su  A parallel processing strategy forevaluating recursive queries  In W  W  Chu  G  Gardarin S  Ohsuga  and Y  Kambayashi  editors  VLDB 86 TwelfthInternational Conference on Very Large Data Bases  August25 28  1986  Kyoto  Japan  Proceedings  pages 412 419 Morgan Kaufmann  1986  34  A  Ray  Oracle data guard  Ensuring disaster recovery for theenterprise  An Oracle white paper  Mar  2002  35  K  Salem and H  Garcia Molina  Checkpointingmemory resident databases  In Proc  of the 5th ICDE Conf  pages 452 462  1989  36  F  B  Schneider  Implementing fault tolerant services using thestate machine approach  a tutorial  ACM Computing Surveys 22 4  299 319  1990  37  M  Shah  J  Hellerstein  and E  Brewer  Highly available fault tolerant  parallel dataows  In Proc  of the SIGMODConf   June 2004  38  A  Simitsis  K  Wilkinson  U  Dayal  and M  Castellanos Optimizing etl workows for fault tolerance  In Proc  of the26th ICDE Conf   2010  39  U  Srivastava and J  Widom  Flexible time management in datastream systems  In Proc  of the 23rd PODS Symp   June 2004  40  R  Talmage  Database mirroring in SQL Server 2005 http   www microsoft com technet prodtechnol sql 2005 dbmirror mspx  Apr  2005  41  Teradata  http   www teradata com   42  R  H  Tut   unc   u  K  C  Toh  and M  J  Todd  Solving  semide nite quadratic linear programs using SDPT3 Mathematical programming  95 2  189 217  2003  43  Tyson Condie et  al  MapReduce online  In Proc  of the 7thNSDI Symp   2010  44  P  Upadhyaya  Y  Kwon  and M  Balazinska  A latency andfault tolerance optimizer for online parallel query plans Technical report  Department of Computer Science andEngineering  Univ  of Washington  2010  45  Vertica  inc  http   www vertica com   46  A  N  Wilschut and P  M  G  Apers  Dataow query executionin a parallel main memory environment  In Proceedings of theFirst International Conference on Parallel and DistributedInformation Systems  PDIS 1991   Fontainebleu HiltonResort  Miami Beach  Florida  December 4 6  1991  pages68 77  IEEE Computer Society  1991  47  C  Yang  C  Yen  C  Tan  and S  R  Madden  Osprey Implementing MapReduce style fault tolerance in ashared nothing distributed database  In Proc  of the 26thICDE Conf   2010  48  Yuan Yu et  al  DryadLINQ  A system for general purposedistributed data parallel computing using a high level language In Proc  of the 8th OSDI Symp   2008  49  M  Za  t  D  Florescu  and P  Valduriez  Benchmarking theDBS3 parallel query optimizer  IEEE Parallel Distrib Technol   4 2  26 40  1996 Input BatchLogCheckpoint Materialized outputUpstreamhandlerDown streamhandlerOutputqueueInputbufferInput Batch ProcessorInputbatchOperatorFigure 12  Architecture of the operator frame work  The operator processes the incoming datain a pipelined manner  Threads are assigned toeach stage of the pipeline and thus each stage runsconcurrently  Network IO is handled by a poolof threads  The dashed components represent in memory data structures and the implementation of user supplied  operator logic APPENDIXIn this Appendix  we provide additional information aboutvarious aspects of our framework and the FTOpt optimizer A  IMPLEMENTATIONThe prototype is written in Java and built on top of theApache MINA framework  http   mina apache org   toimplement e cient network IO  The current implementa tion can run a directed acyclic graph  DAG  of operators At runtime  each operator in the DAG is replicated acrossmultiple machines and executed in parallel  The data com munication between upstream and downstream operators isdone using all to all TCP connections A 1 Operator Framework ArchitectureFigure 12 illustrates the framework of an operator  Theframework has three concurrently executing components Upstream Handler  Batch Processor  and Downstream Han dler  The three components make up a data processingpipeline connected by queues  We now walk through howthe incoming data is processed by this pipeline First  for each upstream partition  the Upstream Handlerbu ers the input tuples and creates a batch of input tu ples whenever there are enough tuples or when the streamis stalled  which is detected by a timeout   Both the sizeof an input tuple batch and the timeout are con gurableparameters Next  the input batch is handed to the Input Batch Pro cessor  IBP   Before running the core operator algorithm the batch processor logs the summary information for thecurrent batch for deterministic replay of the input stream Because the tuples in an input tuple batch are all from thesame upstream partition  in the batch summary  we onlyneed to record the upstream partition identi er  the  rsttuple identi er in the batch  and the number of tuples inthe batch for deterministic replay  In Appendix A 4  we dis cuss the details of logging and show that logging imposes aminimal overhead  The output of core operator algorithmis also collected in a batch and handed to the DownstreamHandler Finally  the Downstream Handler streams the output tu ples to the downstream operators and completes the pro cessing of a batch of input tuples  The output tuples arerouted to designated downstream operators according to apartition function  The downstream handler also takes therequired fault tolerance action such as materializing outputbefore writing to the network or triggering checkpoints tocapture the current state of the operator at the end of pro cessing the current output batch Our prototype supports the three fault tolerance strate gies we mention in Section 4 2  NONE  MATERIALIZE and CHCKPT  CHCKPT is supported only when the op erator algorithm implements necessary hooks  serialize andde serialize state   The other two strategies are supportedautomatically by the framework  For stateless operator suchas Select and Project  when the strategy NONE is chosen  theprototype supports skipping over the previously processedinput during recovery and replay A 2 Operator ImplementationThe current prototype implements three representative re lational operators  Select  Aggregate  and Join  For fault tolerance strategy  we only describe the detail of CHCKPTbecause NONE and MATERIALIZE strategies are automat ically supported by the framework Select  This operator evaluates a given predicate on eachinput tuple  We did not implement checkpoint hooks for itsince it is stateless Aggregate  This operator computes the average of a spe ci c column in a group  It keeps track of the partially ag gregated states using an in memory hash table  We imple mented checkpoint hooks to store the in memory hash tableinto a checkpoint and load it from a checkpoint Join  We implemented a binary symmetric hash join op erator using two in memory hash tables  33  46   We imple mented incremental checkpoints  input tuples are bu eredin memory  written to disk when a checkpoint occurs  andthen deleted from memory  During recovery  the operator re builds its hash tables by reading input tuples from the disk No joins need to be performed at this point  During replay the operator  rst locates the oldest input tuple to replay then joins the following input tuples with the in memorystate A 3 Synthetic Benchmark SetupWe implemented the synthetic workload in Section 6 asfollows   Data  All  elds are randomly generated integers orstrings   Partition  We hash partition the output of each op erator on a di erent attribute  We use the randomlygenerated value for that attribute to determine whereto route each tuple   Select  We send a tuple to the output if the randomvalue of the select  eld  of type double and taking val ues from 0 through 1  is less than the given selectivity   Aggregate  We vary the state size by controlling thenumber of groups to which the input tuples are aggre gated   Join  Given the join selectivity    we join two inputtuples when the join attribute  for the two tuples  iscongruent modulo d 1e A 4 Ensuring Operator DeterminismOur framework requires that the operator partitions bedeterministic  In particular  rule 4 2 requires that  in re sponse to a valid request  a partition must always return the same sequence of tuples  irrespective of any failures itexperiences Most relational operators  and hence their partitions  canbe made deterministic as long as when they restart  theyprocess the same tuples in the same order across all their in puts  The challenge is that these inputs come from di erentmachines in the cluster and may thus arrive with di erentlatencies when they are replayed  One approach to ensurea deterministic input data order is to bu er and interleavetuples using a pre de ned rule  2  39   These techniques however  can impose signi cant memory and latency over heads due to tuple bu ering Instead  we adopt the approach of logging determi nants  11   As the operator receives input tuples  it accu mulates them into small batches  with one batch per inputrelation partition  For example  consider an operator withtwo inputs coming from parent operators p1and p2  Tuplesarrive on these inputs starting with tuple id p11 from p1andp21 from p2  Tuples arrive in interleaved order and the op erator accumulates them into batches  bu ering these twobatches separately in memory while maintaining the tuplearrival order within a batch  Whenever a particular batchexceeds a pre de ned size or receives an end of stream sig nal  the operator writes a log entry to disk that contains the identi er of the stream for this batch  the identi er ofthe  rst tuple in the batch  and the number of tuples inthe batch  Each log entry also has an implicit log sequencenumber  lsn  that is not written to disk  The logging isdone before processing a batch  The operator processes thebatches in the same order in which it writes their log en tries to disk  In our example  if we use a batch size of2500  and the operator receives 3500 tuples from p1and4000 tuples from p2  the logged entries might look as follows h2  p21  2500i h1  p11  2500i h1  p12501  1000i h2  p22501  1500i Log entries are force written to stable storage but  as weshow below  this logging overhead is negligible even for batchsizes as small as 512 tuples per batch  If the operator needsto reprocess its input  it uses the log to ensure the reprocess ing occurs in the same order as before  To avoid expensivedisk IOs when possible  i e   when the operator itself doesnot fail but its downstream neighbor fails   recent determi nants are cached in memory Before processing an input tuple  the operator tags it withhlsn  psni  where lsn corresponds to the log entry sequencenumber of the corresponding batch and psn is the tupleorder within that batch  This information is used to assignunique tuple identi ers to output tuples  Note that all logentries are of a constant size and a lsn is enough to index alog entry Output tuple identi ers consist of three integer  elds hlsn  psn  seqi  The  rst two  elds identify one of the inputtuples that contributed to this output tuple  A sequencenumber  seq  is added since one input tuple can contributeto many output tuples  as in the case of joins  As an example  we show how we use this mechanism togenerate unique identi ers for tuples produced by the fol lowing operators   Select  Our select always has a selectivity less than orequal to one and can thus propagate the input tupleidenti er onto the output tuple  setting seq to zero   Join  The latest tuple that led to the creation of thistuple is used to populate the  rst two  elds  The third01020304050601 3 6 12Runtime  s Number of ProducersWith input log Without input logFigure 13  Each pair of bars represents the time tocomplete processing  with and without logs  with adi erent number of upstream producers for a selectoperator  There is virtually no overhead even for 12input streams 0510152025512 1024 2048 4096 8192Runtime  s Batch SizeWith input log Without input logFigure 14  Each pair of bars represents the timeto complete processing  with and without logs  withdi erent batch sizes for a join operator  The mini mum overhead occurs with a batch size of 2048  eld is a count of the number of matches for any giventuple   Aggregate  Since aggregates are blocking operators they do not need a log  In case we use CHCKPT  wecan store the last tuple identi ers received from each ofthe upstream partitions when we make the checkpoint To validate that logging overhead is negligible  we executea select operator on a single machine with an input of size2 5   106tuples  or 1 19 GB  and we vary the number ofupstream producers while keeping the batch size  xed at512 tuples  Figure 13 shows the time to process all tupleswith and without logging enabled  The results show that thelogging mechanism scales well with the number of upstreamproducers  The average runtimes of three runs rounded tothe nearest second are identical To select the optimal log batch size we execute a joinoperator that processes 1 million tuples from each of its twoinputs  It is a 1x1 foreign key join and produces 1 millionoutput tuples  We have a total of four producers generatingall the data and we vary the log batch size from 512 to 8192 As Figure 14 shows  the smallest runtime overhead was 3 for a batch size of 2048 tuples  As expected the runtime withno logs for smaller batch sizes remains the same as that for2048 while the runtime with logging increases since we writemore log entries if batch sizes are smaller and more cputime is spent in writing the log entries to disk  It should benoted that the runtime with and without logs increases forbatch sizes of 4096 and 8192  This is because of an increasedbu ering delay for each input batch  In all our experiments we use a batch size of 2048 and a tuple size of 0 5 KB A 5 Resource Allocation in FTOptIn addition to fault tolerance strategies  FTOpt also pro duces an allocation of resources to operators because re source allocation and choice of fault tolerance strategy aretightly interconnected  Resource allocation is expressed asa fraction of all available CPU and network bandwidth re sources  Bandwidth is further constrained by network topol ogy In this paper  we make several simplifying assumptionsto implement and test our proof of concept optimizer  Weassume a simple setting where the set of compute nodesare connected through a single switch  The current versionof our optimizer abstracts out the resource allocation byassuming that the time to process each tuple and the disk IOcosts scale linearly with the amount of resources allocated toan operator  Thus  if a single operator partition of operatori takes tcputime to process a tuple  then with ni partitionseach assigned exclusively to a machine  the e ective time theoperator  i e   all the operator partitions together  takes toprocess a single tuple istcpunitime  Similarly the time takento write a tuple to disk is taken to betioni  Our optimizerhandles fractional resource assignments Given a resource allocation  operators can either be co scheduled on the same physical nodes  i e   all nodes exe cute all operators  or separated onto di erent nodes  i e  each node executes a subset of all operators   In the lattercase  resource allocation must be rounded o  to the gran ularity of machines  which can lead to lower performance In the former case  operators may end up writing their logsand checkpoints to the same disks for a more complex per formance curve for these interleaved IO operations  Whileour optimizer handles both strategies and computes frac tional resource assignments  in our experiments  we pinnedeach operator partition to its own core and its own disk oneach node to keep our models simple B  OPERATOR MODELING STRATEGYWe refer the reader to Section 5 3 1 for an overview ofwhat information about each operator in the pipeline we re quire so as to automatically infer the runtimes for the entirepipeline  To recap  We only require  a  the expected num ber of output tuples produced given the number of inputtuples received  and  b  the average cpu time required toproduce each output tuple We remind the reader that we have abstracted away thedi erent number of partitions of each operator by dividingthe time to process each tuple and the time to read or writeeach tuple by the number of machines allocated to the op erator We now restate the equations and explain how the equa tions are derived me    xIN kktk1f 2 me    tcpua  1 3 me   1k  xIN 1kjIj1 1k  4  1  k1 tf   jIjm1e   xNjIj  5 In the above equations  jIj is the output cardinality  and k come from the NBout nin  function  me is the num ber of output tuples produced per second at the instant theprocessing ends and tf is the  rst time at which the outputproduces tuples at the rate me Equality 2 realizes this relationship between me and tf  Speci cally  the rate at which output tuples are producedby the operator after time tf isme  ddtNBout nin t tf ddtnkint tf ddt txIN kt tf   xIN kktk1fInequality 3  me    tcpua  1  states that the operator cannot take less than tcpua time to produce an output tuple since this is the least amount of time the processor needs pertuple  given the resources it has  The inequality becomes anequality when the operator reaches a stage in its processingwhen the processor is working at its full capacity and cannot keep up with the theoretically maximum rate at whichoutput tuples could be produced given the rate at which theinput tuples are being received For inequality 4  me   1k  xIN 1kjIj1 1k   its right handside is the maximum rate at which output could be pro duced if the only bottleneck was the rate of arrival of inputtuples  Note that  since we require the NBout    function tohave a non negative rate of change  i e   the fastest outputproduction rate w</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory"/>
        <doc>On Finding Skylines in External Memory ### Cheng Sheng Yufei Tao CUHK CUHK Hong Kong Hong Kong csheng cse cuhk edu hk taoyf cse cuhk edu hk ABSTRACT We consider the skyline problem  a k a  the maxima problem   which has been extensively studied in the database community  The input is a set P of d dimensional points  A point dominates another if the former has a lower coordinate than the latter on every dimension  The goal is to    nd the skyline  which is the set of points p     P such that p is not dominated by any other data point  In the external memory model  the 2 d version of the problem is known to be solvable in O  N B  logM B N B   I Os  where N is the cardinality of P  B the size of a disk block  and M the capacity of main memory  For    xed d     3  we present an algorithm with I O complexity O  N B  log d   2 M B  N B    Previously  the best solution was adapted from an in memory algorithm  and requires O  N B  log d   2 2  N M   I Os  Categories and Subject Descriptors F2 2  Analysis of algorithms and problem complexity   Nonnumerical algorithms and problems   geometric problems and computations General Terms Algorithms  theory Keywords Skyline  admission point  pareto set  maxima set ### 1  INTRODUCTION This paper studies the skyline problem  The input is a set P of d dimensional points in general position  i e   no two points of P share the same coordinate on any dimension  Given a point p     R d   denote its i th  1     i     d  coordinate as p i   A point p1 is said to dominate another point p2  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  PODS   11  June 13   15  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0660 7 11 06     10 00  represented as p1     p2  if p1 has a smaller coordinate on all d dimensions  namely     i   1       d  p1 i    p2 i   The goal is to compute the skyline of P  denoted as SK Y  P   which includes all the points in P that are not dominated by any other point  namely  SK Y  P     p     P    p       P s t  p       p    1  The skyline is also known under other names such as the pareto set  the set of admission points  and the set of maximal vectors  see  24    price rating 1 2 3 4 5 6 7 8 Figure 1  The skyline is  1  5  7  Ever since its debut in the database literature a decade ago  7   skyline computation has generated considerable interests in the database area  see  24  for a brief survey   This is  at least in part  due to the relevance of skylines to multicriteria optimization  Consider  for example  a hotel recommendation scenario  where each hotel has two attributes price and rating  a smaller rating means better quality   Figure 1 illustrates an example with 8 hotels  of which the skyline is  1  5  7   Every hotel not in the skyline is worse than at least one hotel in the skyline on both dimensions  i e   more expensive and rated worse at the same time  In general  for any scoring function that is monotone on all dimensions  the skyline always contains the best  top 1  point that minimizes the function  This property is useful when it is di   cult  if not impossible  for a user to specify a suitable scoring function that accurately re   ects her his preferences on the relative importance of various dimensions  In Figure 1  for instance   1  5  7  de   nitely includes the besthotel  no matter the user emphasizes more  and how much more  on price or rating  1 1 Computation model Our complexity analysis is under the standard external memory  EM  model  2   which has been successful in capturing the characteristics of algorithms dealing with massive data that do not    t in memory  see  27  for a broad summary of results in this model   Speci   cally  in this model  a computer has a main memory that is able to accommodate M words  and a disk of unbounded size  The disk is formatted into disjoint blocks  each of which contains B consecutive words  The memory should have at least two blocks  i e   M     2B  An I O operation reads a block of data from the disk into memory  or conversely  writes a block of memory information into the disk  The time complexity is measured in the number of I Os performed by an algorithm  CPU calculation is free  In EM  linear cost should be interpreted as O N B  for a dataset of size N  as opposed to O N  in a memoryresident model such as RAM  In this paper  poly logarithmic should be understood as O polylogM B N B    instead of O polylog N   namely  it is important to achieve base M B  In this paper  a function is said to be near linear if it is in O  N B  polylogM B N B   but not in O N B   1 2 Previous results In internal memory  Matousek  21  showed how to    nd the skyline in O N 2 688   time when the dimensionality d of the dataset P is as high as the number N of points in P  In 2 d space  Kung et al   19  proposed an O N log N  time algorithm  For any    xed dimensionality d     3  they also gave an algorithm with time complexity O N log d   2 N   Bentley  4  developed an alternative algorithm achieving the same bounds as those of  19   Kirkpatrick and Seidel  16  presented algorithms whose running time is sensitive to the result size  and has the same complexity as the algorithms in  4  19  when the skyline has     N  points   It can be shown that any algorithm in the comparison class 1 must incur     N log N  time  implying that the solutions of  4  19  are already optimal in this class for d   2 and 3  see also some recent results on instance optimality due to Afshani et al   1    For d     4  Gabow et al   11  discovered an algorithm terminating in O N log d   3 N log log N  time  which still remains the best result up to this day  Note  however  that the solution of  11  does not belong to the comparison class  due to its reliance on the van Emde Boas structure  26  that uses features of the RAM model  Faster algorithms have been developed in some special circumstances where  for example  the data follow special distributions  5  6  10   or each dimension has a small domain  22   All the RAM algorithms can be applied in the EM model directly by treating the disk as virtual memory  Such a brute force approach  however  can be expensive in practice because it fails to take into account the e   ects of blocking  which do not exist in RAM but are inherent in external memory  For example  running the solution of  11  in EM naively 1 A skyline algorithm is comparison based if it can infer the dominance relation by only comparing pairs of points  The comparison class includes all such algorithms  would entail O N log d   3 N log log N  I Os   which amounts to reading the entire dataset O B log d   3 N log log N  times  B is at the order of thousands in practice   Hence  there is a genuine need to design I O oriented algorithms  For d   2  such an algorithm can be easily found  as Kung et al   19  showed that the problem can be settled by sorting the data followed by a single scan  we will come back to this in Section 2   which takes O  N B  logM B N B   I Os in total  To our knowledge  for general d  the RAM algorithm that can be most e   ciently adapted to EM is the one by Bentley  4   which performs O  N B  log d   2 2  N M   I Os     note that the base of the log is 2  instead of M B  We are not aware of any algorithm that can achieve near linear  or better  running time  For d     3   the skyline of  a  dataset P can be trivially obtained by computing the cartesian product P    P  i e   by comparing all pairs of points in P  which  in turn  can be produced by a blocked nested loop  BNL  in    N 2   M B   I Os  It has been observed  7  that such a quadratic complexity is too slow in practice for large N  In the past decade  several algorithms  as we survey below  have been designed to alleviate the de   ciency  typically by leveraging the transitivity of the dominance relation  i e   p1     p2 and p2     p3 imply p1     p3   Although empirical evaluation has con     rmed their e   ectiveness on selected datasets  none of those algorithms has been proved to be asymptotically faster than BNL in the worst case  We say that they are captured by the quadratic trap  Borzsonyi et al   7  presented a divide and conquer  DC  method that partitions P into disjoint groups P1       Ps where the number s of groups is large enough so that each Pi  i     s     ts in memory  DC proceeds by invoking an in memory algorithm to    nd the skyline SK Y  Pi  of each Pi  and then  deleting those points of SK Y  Pi  dominated by some point in the skyline of another group  Although divide and conquer is a promising paradigm for attacking the skyline problem  it is also employed in our solutions   its application in DC is heuristic and does not lead to any interesting performance bound  The sort    rst skyline  SFS  algorithm by Chomicki et al   9  works by sorting the points p     P in ascending order of score p   where score can be any function R d     R that is monotonically increasing on all dimensions  The monotonicity ensures that  p1     p2 implies score p1    score p2   but the opposite is not true   As a result  a point p     P cannot be dominated by any point that ranks behind it in the ordering  Following this rationale  SFS scans P in its sorted order  and maintains the skyline    of the points already seen so far  note that        SK Y  P  at any time   As expected  the choice of score is crucial to the e   ciency of the algorithm  No choice  unfortunately  is known to be able to escape the quadratic trap in the worst case  In SFS  sorting is carried out with the standard external sort  Intuitively  if mutual comparisons are carried out among the data that ever co exist in memory  during the external sort   many points may get discarded right away once con   rmed to be dominated  at no extra I O cost at all  Based on this idea  Godfrey et al   12  developed the linear elimination sort for skyline  LESS  algorithm  LESS has the property that  it terminates in linear expected I Os under the independent and uniform assumption  i e   all di method I O complexity remark Kung et al   19  O N log d   2 N  Gabow et al   11  O N log d   3 N log log N  not in the comparison class  Bentley  4  O  N B  log d   2 2  N M   adapted from Bentley   s O N log d   2 N  algorithm in RAM BNL    N 2   M B   also applies to the BNL variant of Borzsonyi et al   7  DC  7      N 2   M B   SFS  9  O N 2   M B   LESS  12  O N 2   M B   RAND  24  O   N  M B   expected    is the number of points in the skyline  which can be     N   this paper O  N B  log d   2 M B N B   optimal for d   3 in the comparison class Table 1  Comparison of skyline algorithms for    xed d     3 mensions are independent  and the points of P distribute uniformly in the data space   provided that the memory size M is not too small  12   When the assumption does not hold  however  it remains unknown whether the cost of LESS is o N 2   M B    Sarma et al   24  described an output sensitive randomized algorithm RAND  which continuously shrinks P with repetitive iterations  each of which performs a three time scan on the current P as follows  The    rst scan takes a random sample set S     P with size    M   The second pass replaces  if possible  some samples in S with other points that have stronger pruning power  All samples at the end of this scan are guaranteed to be in the skyline  and thus removed from P  The last scan further reduces  P   by eliminating all the points that are dominated by some sample  At this point  another iteration sets o    as long as P         RAND is e   cient when the result has a small size  Speci   cally  if the skyline has    points  RAND entails O   N  M B   I Os in expectation  When          N   however  the time complexity falls back in the quadratic trap  There is another line of research that concerns preprocessing a dataset P into a structure that supports fast retrieval of the skyline  as well as insertions and deletions on P  see  14  15  18  20  23  and the references therein   In our context  such pre computation based methods do not have a notable advantage over the algorithms mentioned earlier  1 3 Our results Our main result is  Theorem 1 1  The skyline of N points in R d can be computed in O  N B  log d   2 M B  N B   I Os for any    xed d     3  The theorem concerns only d     3 because  as mentioned before  the skyline problem is known to be solvable in O  N B  logM B N B   I Os in 2 d space  Unlike the result of Godfrey et al   12   we make no assumption on the data distribution  Our algorithm is the    rst one that beats the quadratic trap and  at the same time  achieves near linear running time  In 3 d space  our I O complexity O  N B  logM B N B   is asymptotically optimal in the class of comparison based algorithms  For any    xed d  Theorem 1 1 shows that the skyline problem can be settled in O N B  I Os  when N B    M B  c for some constant c  a situation that is likely to happen in practice   No previous algorithm is known to have such a property  See Table 1 for a comparison of our and existing results  The core of our technique is a distribution sweep 2 algorithm for solving the skyline merge problem   where we are given s skylines   1         s that are separated by s   1 hyperplanes orthogonal to a dimension  and the goal is to return the skyline of the union of all the skylines  namely  SK Y    1               s   It is not hard to imagine that this problem lies at the heart of computing the skyline using a divide and conquer approach  Indeed  the lack of a fast solution to skyline merging has been the obstacle in breaking the curse of quadratic trap  as can be seen from the divide andconquer attempt of Borzsonyi et al   7   We overcome the obstacle by lowering the dimensionality to 3 gradually  and then settling the resulting 3 d problem in linear I Os  Our solution can also be regarded as the counterpart of Bentley   s algorithm  4  in external memory  Remark  We have de   ned our problem by assuming that P is in general position  The skyline SK Y  P  is still well de   ned without this assumption  Speci   cally  let P be a set of points in R d  implying that P has no duplicates   Given two points p  p   in P  we have p     p   if p i      p    i  for all 1     i     d  Note that since p    p     the equality does not hold for at least one i  Then  SK Y  P  is still given by Equation 1  Our algorithms can be extended to solve this version of the skyline problem  retaining the same performance guarantee as in Theorem 1 1  Details can be found in Section 3 3  2  PRELIMINARIES In this section  we review some skyline algorithms designed for memory resident data  The purposes of the review are three fold  First  we will show that the 2 d solution of Kung et al   19  can be easily adapted to work in the EM model  Second  our discussion of their algorithm and Bentley   s algorithm  4  for d     3 not only clari   es some characteristics of the underlying problems  but also sheds light on some obstacles preventing a direct extension to achieve near linear time complexity in external memory  Finally  we brie   y explain the cost lower bound established in  19  and why a similar bound also holds in the I O context  Let us    rst agree on some terminologies  We refer to the 2 An algorithm paradigm proposed by Goodrich et al   13  that can be regarded as the counterpart of plane sweep in external memory p ymin x y 1 2 3 y z 1 2 3 4 p 5  a   b  Figure 2  Illustration of algorithms by Kung et al   19    a  2 d   b  3 d    rst  second  and third coordinate of a point as its x   y   and z coordinate  respectively  Sometimes  it will be convenient to extend the de   nition of dominance to subspaces in a natural manner  For example  in case p1 has smaller x  and y coordinates than p2  we say that p1 dominates p2 in the x y plane  No ambiguity can arise as long as the subspace concerning the dominance is always mentioned  2 d  The skyline SK Y  P  of  a  set P of 2 d points can be extracted by a single scan  provided that the points of P have been sorted in ascending order of their x coordinates  To understand the rationale  consider any point p     P  and  let P   be the set of points of P that rank before p in the sorted order  Apparently  p cannot be dominated by any point that ranks after p  because p has a smaller x coordinate than any of those points  On the other hand  p is dominated by some point in P   if and only if the y coordinate of p is greater than ymin  where ymin is the smallest y coordinate of all the points in P     See Figure 2a where P   includes points 1  2  3  and that no point in P   dominates p can be inferred from the fact that p has a lower y coordinate than ymin  Therefore  to    nd SK Y  P   it su   ces to read P in its sorted order  and at any time  keep the smallest y coordinate ymin of all the points already seen  The next point p scanned is added to SK Y  P  if its y coordinate is below ymin  in which case ymin is updated accordingly  In the EM model  this algorithm performs O  N B  logM B N B   I Os  which is the time complexity of sorting N elements in external memory  3 d  Suppose that we have sorted P in ascending order of their x coordinates  Similar to the 2 d case  consider any point p     P  with P   being the set of points before p  It is clear that p cannot be dominated by any point that ranks after p  Judging whether p is dominated by a point of P     however  is more complex than the 2 d scenario  The general observation is that  since all points of P   have smaller x coordinates than p  we only have to check whether p is dominated by some point of P   in the y z plane  Imagine that we project all the points of P   onto the y z plane  which yields a 2 d point set P      Let    be the  2d  skyline of P      It is su   cient to decide whether a point in    dominates p in the y z plane  It turns out that such a dominance check can be done e   ciently  In general  a 2 d skyline is a    staircase     In the y z plane  if we walk along the skyline in the direction of growing y coordinates  the points encountered must have descending z coordinates  Figure 2b illustrates this with a    that consists of points 1       5  To    nd out whether p is dominated by any point of    in the y z plane  we only need to    nd the predecessor o of p along the y dimension among the points of     and give a    yes    answer if and only if o has a lower z coordinate than p  In Figure 2b  the answer is    no    because the predecessor of p  i e   point 2  actually has a greater z coordinate than p  Returning to the earlier context with P     a    no    indicates that p is not dominated by any point in P     and therefore  p belongs to the skyline SK Y  P   Based on the above reasoning  the algorithm of  19  maintains    while scanning P in its sorted order  To    nd predecessors quickly  the points of    are indexed by a binary tree T on their y coordinates  The next point p is added to SK Y  P  upon a    no    answer as explained before  which takes O log N  time with the aid of T   Furthermore  a    no    also necessitates the deletion from    of all the points that are dominated by p in the y z plane  e g   points 3  4 in Figure 2b   Using T   this requires only O log N  time per point removed  As each point of P is deleted at most once  the entire algorithm    nishes in O N log N  time  A straightforward attempt of externalizing the algorithm is to implement T as a B tree  This will result in the total execution time of    N logB N   which is higher than the cost O  N B  logM B N B   of our solution by a factor of     B logB M   The de   ciency is due to the fact  see  13   that plane sweep  which is the methodology behind the above algorithm  is ill    tted in external memory  because it issues a large number of queries  often     N    rendering it di   cult to control the overall cost to be at the order of N B  Following a di   erent rationale  Bentley  4  gave another algorithm of O N log N  time  We will not elaborate his solution here because our algorithm in the next section degenerates into Bentley   s  when M and B are set to constants satisfying M B   2  Dimensionalities at least 4  Kung et al   19  and Bentley  4  deal with a general d dimensional  d     4  dataset P by divide and conquer  More speci   cally  their algorithms divide P into P1 and P2 of roughly the same size by a hyperplane perpendicular to dimension 1  Assume that the points of P1 have smaller coordinates on dimension 1 than those of P2  Let   1 be the skyline SK Y  P1  of P1  and similarly    2   SK Y  P2   All points of   1 immediately belong to SK Y  P   but a point p       2 is in SK Y  P  if and only if no point in   1 dominates p  Hence  after obtaining   1 and   2 from recursion  a skyline merge is carried out to evict such points as p z x y    2     3  1 2 3 4 5 6 8    1  7 Figure 3  Illustration of 3 d skyline merge  The value of s is 3  Only the points already encountered are shown  Points are labeled in ascending order of their y coordinates  which is also the order they are fetched   Point 8 is the last one seen  Each cross is the projection of a point in the x y plane     1  contains points 2  3  7     2  includes 4  6  8  and    3  has 5  1     1      2      3  equal the z coordinate of point 2  8  5  respectively  Point 8 does not belong to SK Y  P   because its z coordinate is larger than    1   i e   it violates Inequality 2 on j   1   Externalization of the algorithms of Kung et al   19  and Bentley  4  is made di   cult by a common obstacle  That is  the partitioning in the divide and conquer is binary  causing a recursion depth of     polylog N M    To obtain the performance claimed in Theorem 1 1  we must limit the depth to O polylogM B N B    This cannot be achieved by simply dividing P into a greater number s   2 of partitions P1       Ps   because  doing so may compromise the e   ciency of merging skylines  To illustrate  let   i   SK Y  Pi  for each 1     i     s  A point p     Si must be compared to SK Y  Pj   for all j   i  Applying the skyline merge strategy of  4  or  19  would blow up the cost by a factor of     s 2    which would o   set all the gains of a large s  Remedying the drawback calls for a new skyline merge algorithm  which we give in the next section  Cost lower bound  Kung et al   19  proved that any 2 d skyline algorithm in the comparison class must incur     N log N  execution time  To describe the core of their argument  let us de   ne the rank permutation of a sequence S of distinct numbers  x1       xN   as the sequence  r1       rN  where ri  1     i     N  is the number of values of S that are at most xi  For example  the rank permutation of  9  20  3  is  2  3  1   Kung et al   19  identi   ed a series of hard datasets  where each dataset P has N points p1       pN whose x coordinates can be any integers  They showed that  any algorithm that correctly    nds the skyline of P must have determined the rank permutation of the sequence formed by the x coordinates of p1       pN  In the EM model  it is known  2  that deciding the rank permutation of a set of N integers demands      N B  logM B N B   I Os in the worst case for any comparison based algorithm  It thus follows that this is also a lower bound for computing 2 d skylines in external memory  Note that the same bound also holds in higherdimensional space where the problem is no easier than in the 2 d space  It is worth mentioning that the I O lower bound      N B  logM B N B   is also a direct corollary of a result due to Arge et al   3   3  OUR SKYLINE ALGORITHM We will present the proposed solution in a step by step manner  Section 3 1    rst explains the overall divide andconquer framework underpinning the algorithm by clarifying how it works in 3 d space  To tackle higher dimensionalities d  there is another layer of divide and conquer inside the framework  as elaborated in Section 3 2 for d   4   The  4 d description of our algorithm can then be easily extended to general d  which is the topic of Section 3 3  3 1 3 d Our algorithm accepts as input a dataset P whose points are sorted in ascending order of x coordinates  If the size N of P is at most M  i e   the memory capacity   we simply    nd the skyline of P using a memory resident algorithm  The I O cost incurred is O N B   In case N   M  we divide P into s      M B  partitions P 1        P s  with roughly the same size  such that each point in P i  has a smaller x coordinate than all points in P j  for any 1     i    j     s  As P is already sorted on the xdimension  the partitioning can be done in linear cost  while leaving the points of each P i  sorted  in the same  way  The next step is to obtain the skyline    i  of each P i   i e      i    SK Y  P i    Since this is identical to solving the original problem  only on a smaller dataset   we recursively invoke our algorithm on P i   Now consider the moment when all    i  have been returned from recursion  Our algorithm proceeds by performing a skyline merge  which    nds the skyline of the union of all    i   that is  SK Y     1                 s    which is exactly SK Y  P   We enforce an invariant that  SK Y  P  be returned in a disk    le where the points are sorted in ascending order of y coordinates  to be used by the upper level of recursion  if any   Due to recursion  the invariant implies that  the same ordering has been applied to all the    i  at hand  We now elaborate the details of the skyline merge  SK Y  P  is empty in the outset     1           s  are scanned synchronously in ascending order of y coordinates  In other words  the next point fetched is guaranteed to have the lowest y coordinate among the points of all    i  that have not been encountered yet  As s      M B   the synchronization can be achieved by assigning a block of memory as the input bu   er to each    i   We maintain a value    i   which equals the minimum z coordinate of all the points that have already been seen from    i   If no point from    i  has been read     i         We decide whether to include a point p in SK Y  P  when p is fetched by the synchronous scan  Suppose that p is from    i  for  some i  We add p to SK Y  P  if p 3       j      j   i  2  where p 3  denotes the z coordinate of p  See Figure 3 for an illustration  The lemma below shows the correctness of this rule  Lemma 3 1  p     SK Y  P  if and only if Inequality 2 holds  Proof  Clearly  p cannot be dominated by any point in    i   1           s  because p has a smaller x coordinate than all those points  Let S be the set of points in    j  already scanned before p  for any j   i  No point p          j    S can possibly dominate p  as p has a lower y coordinate than p     On the other hand  all points in S dominate p in the x y plane  Thus  some point in S dominates p in R 3 if and only if Inequality 2 holds  We complete the algorithm description with a note that a single memory block can be used as an output bu   er  so that the points of SK Y  P  can be written to the disk in linear I Os  by the same order they entered SK Y  P   namely  in ascending order of their y coordinates  Overall  the skyline merge    nishes in O N B  I Os  Running time  Denote by F N  the I O cost of our algorithm on a dataset with cardinality N  It is clear from the above discussion that F N      O N B  if N     M s    F N s    G N  otherwise  3  where G N    O N B  is the cost of a skyline merge  Solving the recurrence gives F N    O  N B  logM B N B    3 2 4 d To    nd the skyline of a 4 d dataset P  we proceed as in the 3 d algorithm by using a possibly smaller s      min      M    M  B    The only di   erence lies in the way that a skyline merge is performed  Formally  the problem we face in a skyline merge can be described as follows  Consider a partition P 1        P s  of P such that each point in P i  has a smaller coordinate on dimension 1 than all points in P j   for any 1     i   j     s  Equivalently  the data space is divided into s slabs   1 1          1 s  by s     1 hyper planes orthogonal to dimension 1 such that P i    P       1 i  for all 1     i     s  We are given the skyline   1 i  of each P i   where the points of   1 i  are sorted in ascending order of their 2nd coordinates  The goal is to compute SK Y    1 1                1 s    which is equivalent to SK Y  P   Further  the output order is important  for backtracking to the upper level of recursion   we want the points of SK Y  P  to be returned in ascending order of their 2nd coordinates  The previous subsection solved the problem in 3 d space with O N B  I Os where N    P   In 4 d space  our objective is to pay only an extra factor of O logM B N B   in the cost  We ful   ll the purpose with an algorithm called preMerge 4d  the input of which includes     slabs   1 1          1 s      a set    of points sorted in ascending order of their 2nd coordinates     has the property that  if two points p1  p2        fall in the same slab  they do not dominate each other  preMerge 4d returns the points of SK Y      in ascending order of their 3rd coordinates  Although stated somewhat di   erently  the problem settled by preMerge 4d is  almost  the same as skyline merge  Notice that    can be obtained by merging   1 1          1 s  in O N B  I Os  Moreover  we can sort the points of SK Y       output by preMerge 4d  ascendingly on dimension 2 to ful     ll the order requirement of skyline merge  which demands another O  N B  logM B N B   I Os  Algorithm preMerge 4d  In case    has at most M points  preMerge 4d solves the problem in memory  Otherwise  in O      B  I Os the algorithm divides    into s partitions    1           s  of roughly the same size  with the points of    i  having smaller 2nd coordinates than those of    j  for any 1     i    j     s  We then invoke preMerge 4d recursively on each    i   feeding the same    1 1          1 s    to calculate   2 i    SK Y     i    Apparently  SK Y      is equivalent to the skyline of the union of all   2 i   namely  SK Y        SK Y    2 1                2 s    It may appear that we are back to where we started     this is another skyline merge  The crucial di   erence  however  is that only two dimensions remain    unprocessed     i e   dimensions 3 and 4   In this case  the problem can be solved directly in linear I Os  by a synchronous scan similar to the one in Section 3 1  By recursion  the points of each   2 i  have been sorted ascendingly on dimension 3  This allows us to enumerate the points of   2 1                2 s  in ascending order of their 3rd coordinates  by synchronously reading the   2 i  of all i      1  s   In the meantime  we keep track of s 2 values    i1  i2  for every pair of i1  i2      1  s   Speci   cally     i1  i2  equals the lowest 4th coordinate of all the points in   1 i1        2 i2  that have been scanned so far  or    i1  i2        if no such point exists  Note that the choice of s makes it possible to maintain all    i1  i2  in memory  and meanwhile  allocate an input bu   er to each   2 i  so that the synchronous scan can be completed in linear I Os  SK Y      is empty at the beginning of the synchronous scan  Let p be the next point fetched  Suppose that p fallsin   1 i1   and is from   2 i2   for some i1  i2  We insert p in SK Y      if p 4       j1  j2      j1   i1  j2   i2  4  where p 4  is the coordinate of p on dimension 4  An argument similar to the proof of Lemma 3 1 shows that p     SK Y      if and only if the above inequality holds  Note that checking the inequality happens in memory  and incurs no I O cost  Finally  as the points of SK Y      enter SK Y      in ascending order of their 3rd coordinates  they can be written to the disk in the same order  Running time  Let H K  be  the I O cost  of preMerge 4d when    has K points  We have H K      O K B  if K     M s    H K s    O K B  otherwise where s          M B   This recurrence gives H K    O  K B  logM B K B    Following the notations in Section 3 1  denote by G N  the cost of a skyline merge when the dataset P has size N  and by F N  the cost of our 4 d skyline algorithm  G N  equals H N  plus the overhead of sorting SK Y  P   Hence  G N    O  N B  logM B N B    With the above  we solve the recurrence in Equation 3 as F N    O  N B  log 2 M B N B    3 3 Higher dimensionalities We are now ready to extend our technique to dimensionality d     5  th</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory"/>
        <doc>Beyond Simple Aggregates  Indexing for Summary Queries ### Zhewei Wei Ke Yi Hong Kong University of Science and Technology Clear Water Bay  Hong Kong  China  wzxac  yike  cse ust hk ABSTRACT Database queries can be broadly classi   ed into two categories  reporting queries and aggregation queries  The former retrieves a collection of records from the database that match the query   s conditions  while the latter returns an aggregate  such as count  sum  average  or max  min   of a particular attribute of these records  Aggregation queries are especially useful in business intelligence and data analysis applications where users are interested not in the actual records  but some statistics of them  They can also be executed much more e   ciently than reporting queries  by embedding properly precomputed aggregates into an index  However  reporting and aggregation queries provide only two extremes for exploring the data  Data analysts often need more insight into the data distribution than what those simple aggregates provide  and yet certainly do not want the sheer volume of data returned by reporting queries  In this paper  we design indexing techniques that allow for extracting a statistical summary of all the records in the query  The summaries we support include frequent items  quantiles  various sketches  and wavelets  all of which are of central importance in massive data analysis  Our indexes require linear space and extract a summary with the optimal or near optimal query cost  Categories and Subject Descriptors E 1  Data   Data structures  F 2 2  Analysis of algorithms and problem complexity   Nonnumerical algorithms and problems General Terms Algorithms  theory Keywords Indexing  summary queries Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  PODS   11  June 13   15  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0660 7 11 06     10 00 ###  1  INTRODUCTION A database system   s primary function is to answer users    queries  These queries can be broadly classi   ed into two categories  reporting queries and aggregation queries  The former retrieves a collection of records from the database that match the query   s conditions  while the latter only produces an aggregate  such as count  sum  average or max  min   of a particular attribute of these records  With reporting queries  the database is simply used as a data storage retrieval tool  Many modern business intelligence applications  however  require ad hoc analytical queries with a rapid execution time  Users issuing these analytical queries are interested not in the actual records  but some statistics of them  This has therefore led to extensive research on how to perform aggregation queries e   ciently  By augmenting a database index  very often a B tree  with properly precomputed aggregates  aggregation queries can be answered e   ciently at query time without going through the actual data records  However  reporting and aggregation queries provide only two extremes for analyzing the data  by returning either all the records matching the query condition or one  or a few  single valued aggregates  These simple aggregates are not expressive enough  and data analysts often need more insight into the data distribution  Consider the following queries   Q1  In a company   s database  What is the distribution of salaries of all employees aged between 30 and 40   Q2  In a search engine   s query logs  What are the most frequently queried keywords between May 1 and July 1  2010  The analyst issuing the query is perhaps not interested in listing all the records in the query range one by one  while probably not happy with a simple aggregate such as average or max   either   What would be nice is some summary on the data  which is more complex than the simple aggregates  yet still much smaller than the raw query results  Some useful summaries include the frequent items  the     quantiles for  say       0 1  0 2          0 9  a sketch  e g   the Count Min sketch  8  or the AMS sketch  4    or some compressed data representations like wavelets  All these summaries are of central importance in massive data analysis  and have been extensively studied for o   ine and streaming data  Yet  to use the existing algorithms  one still has to    rst issue a reporting query to retrieve all query results  and then construct the desired summary afterward  This is clearly time consuming and wasteful  In this paper  we propose to add a native support for summary queries in a database index  such that a summary canbe returned in time proportional to the size of the summary itself  not the size of the raw query results  The problem we consider can be de   ned more precisely as follows  Let D be a database containing N records  Each record p     D is associated with a query attribute Aq p  and a summary attribute As p   drawing values possibly from di   erent domains  A summary query speci   es a range constraint  q1  q2  on Aq and the database returns a summary on the As attribute of all records whose Aq attribute is within the range  For example  in the query  Q1  above  Aq is    age    and As is    salary     Note that As and Aq could be the same attribute  but it is more useful when they are di   erent  as the analyst is exploring the relationship between two attributes  Our goal is to build an index on D so that a summary query can be answered e   ciently  As with any indexing problem  the primary measures are the query time and the space the index uses  The index should also work in external memory  where it is stored in blocks of size B  and the query cost is measured in terms of the number of blocks accessed  I Os   Finally  we also would like the index to support updates  i e   insertion and deletion of records  1 1 Related work on indexing for aggregation queries In one dimension  most aggregates can be supported easily using a binary tree  a B tree in external memory   At each internal node of the binary tree  we simply store the aggregate of all the records below the node  This way an aggregation query can be answered in O log N  time  O logB N  I Os in external memory   In higher dimensions  the problem becomes more di   cult and has been extensively studied in both the computational geometry and the database communities  Solutions are typically based on space partitioning hierarchies  like partition trees  quadtrees and R trees  where an internal node stores the aggregate for its subtree  There is a large body of work on spatial data structures  please refer to the survey by Agarwal and Erickson  2  and the book by Samet  26   When the data space forms an array  the data cube  13  is an e     cient structure for answering aggregation queries  However  all the past research  whether in computational geometry or databases  has only considered queries that return simple aggregates like count  sum  max  min   and very recently top k  1  and median  7  18   The problem of returning complex summaries has not been addressed  1 2 Related work on  non indexed  summaries There is also a vast literature on various summaries in both the database and algorithms communities  motivated by the fact that simple aggregates cannot well capture the data distribution  These summaries  depending on the context and community  are also called synopses  sketches  or compressed representations  However  all past research has focused on how to construct a summary  either o   ine or in a streaming fashion  on the entire data set  No one has considered the indexing problem where the focus is to intelligently compute and store auxiliary information in the index at pre computation time  so that a summary on a requested subset of the records in the database can be built quickly at query time  Since we cannot a   ord to look at all the requested records to build the summary at query time  this poses new challenges that past research cannot address  All existing construction algorithms need to at least read the data records once  The problem of how to maintain a summary as the underlying data changes  namely under insertions and deletions of records  has also been extensively studied  But this should not be confused with our dynamic index problem  The former maintains a single summary for the entire dynamic data set  while the latter aims at maintaining a dynamic structure from which a summary for any queried subset can be extracted  which is more general than the former  Of course for the former there often exist smallspace solutions  while for the indexing problem  we cannot hope for sublinear space  as a query range may be small enough so that the summary degenerates to the raw query results  Below we review some of the most fundamental and most studied summaries in the literature  Let D be a bag of items  and let fD x  be the frequency of x in D  Heavy hitters  A heavy hitters summary allows one to extract all frequent items approximately  i e   for a userspeci   ed 0        1  it returns all items x with fD x       D  and no items with fD x                D   while an item x with             D      fD x         D  may or may not be returned  A heavy hitters summary of size O 1     can be constructed in one pass over D  using the MG algorithm  23  or the SpaceSaving algorithm  22   Sketches  Various sketches have been developed as a useful tool for summarizing massive data  In this paper  we consider the two most widely used ones  the Count Min sketch  8  and the AMS sketch  4   They summarize important information about D and can be used for a variety of purposes  Most notably  they can be used to estimate the join size of two data sets  with self join size being a special case  Given the Count Min sketches  resp  AMS sketches  of two data sets D1 and D2  we can estimate  D1   D2  within an additive error of   F1 D1 F1 D2   resp     p F2 D1 F2 D2   with probability at least 1         3  8   where Fk is the kth frequency moment of D  Fk D    P x f k D x   Note that p F2 D      F1 D   so the error of the AMS sketch is no larger  However  its size is O  1    2   log 1       which is larger than the Count Min sketch   s size O  1     log 1       so they are not strictly comparable  Which one is better will depend on the skewness of the data sets  In particular  since F1 D     D   the error of the Count Min sketch does not depend on the skewness of the data  but F2 D  could range from  D  for uniform data to  D  2 for highly skewed data  Quantiles  The quantiles  a k a  the order statistics   which generalize the median  are important statistics about the data distribution  Recall that the    quantile  for 0        1  of a set D of items from a totally ordered universe is the one ranked at    D  in D  for convenience  for the quantile problem it is usually assumed that there are no duplicates in D   A quantile summary contains enough information so that for any 0        1  an    approximate    quantile can be extracted  i e   the summary returns a       quantile where                                 A quantile summary has size O 1      and can be easily computed by sorting D  and then taking the items ranked at    D   2   D   3   D            D   Wavel ets  Wavelet representations  or just wavelets for short  take a di   erent approach to approximating the data distribution by borrowing ideas from signal processing  Suppose the records in D are drawn from an ordered universe u     1          u  and let fD    fD 1           fD u   be the frequency vector of D  Brie   y speaking  in wavelet transformation we take u inner products si    fD  wi  where wi  i   1          u are the wavelet basis vectors  please refer to  12  20  for details on wavelet basis vectors   The si   s are called the wavelet coe   cients of fD  If we kept all u wavelet coe   cients  we would be able to reconstruct fD exactly  but this would not be a    summary     The observation is that  for most real world distributions  fD yields few wavelet coe   cients with large absolute values  Thus for a parameter k  even if we keep the k coe   cients with the largest absolute values  and assume all the other coe   cients are zero  we can still reconstruct fD reasonably well  In fact  it is well known that among all the choices  retaining the k largest  in absolute value  coe   cients minimizes the  2 error between the original fD and the reconstructed one  Matias et al   20  were the    rst to apply wavelet transformation to approximating data distributions  After that  wavelets have been extensively studied  9  11  12  15  21  29   and have been shown to be highly e   ective at summarizing many real world data distributions  All the aforementioned work studies how to construct or maintain the summary on the given D  In our case  D is the As attributes of all records whose Aq attributes are within the query range  Our goal is to design an index so that the desired summary on D can be constructed e   ciently without actually going through the elements of D  1 3 Other related work A few other lines of work also head to the general direction of addressing the gap between reporting all query results and returning some simple aggregates  Lin et al   19  and Tao et al   27  propose returning only a subset of the query results  called    representatives     But the    representatives    do not summarize the data as we do  They also only consider skyline queries  The line of work on online aggregation  16  17  aims at producing a random sample of the query results at early stages of long running queries  in particular  joins  A random sample indeed gives a rough approximation of the data distribution  but it is much less accurate than the summaries we consider  For heavy hitters and quantiles  a random sample of size    1    2   is needed  28  to achieve the same accuracy as the O 1     sized summaries we mentioned earlier  for estimating join sizes  a random sample of size         N  is required to achieve a constant approximation  which is much worse than using the sketches  3   Furthermore  the key di   erence is that they focus on query processing techniques for joins rather than indexing issues  Correlated aggregates  10  aim at exploring the relationship between two attributes  They are computed on one attribute subject to a certain condition on the other  However  this condition has to be speci   ed in advance and the goal is to compute the aggregate in the streaming setting  thus the problem is fundamentally di   erent from ours  1 4 Our results To take a uni   ed approach we classify all the summaries mentioned in Section 1 2 into F1 based ones and F2 based ones  The former includes heavy hitters  the Count Min sketch  and quantiles  all of which provide an error guarantee of the form   F1 D   note that an    approximate quantile is a value with a rank that is o    by   F1 D  from the correct rank   The latter includes the AMS sketch and wavelets  both of which provide an error guarantee related to F2 D   In Section 2 we    rst design a baseline solution that works for all decomposable summaries  A summary is decomposable if given the summaries for t data sets  bags of elements  D1          Dt with error parameter     we can combine them together into a summary on D1             Dt with error parameter O      where   denotes multiset addition  All the F1 and F2 based summaries have this property and thus can be plugged into this solution  Assuming that we can combine the summaries with cost linear to their total size  the resulting index has linear size and answers a summary query in O s   log N  time  where s   is the size of the summary returned  It also works in external memory  with the query cost being O  s   B log N  I Os if s       B and O  log N  log B s     I Os if s     B  Note that this decomposable property has been exploited in many other works on maintaining summaries in the streaming context  5  6  8   In Section 3 we improve upon this baseline solution by identifying another  stronger decomposable property of the F1 based summaries  which we call exponentially decomposable  The size of the index remains linear  while its query cost improves to O log N   s     In external memory  the query cost is O logB N   s   B  I Os  This resembles the classical B tree query cost  which includes an O logB N  search cost and an    output    cost of O s   B   whereas the output in our case is a summary of size s    This is clearly optimal  in the comparison model   For not too large summaries s     O B   the query cost becomes just O logB N   the same as that for a simple aggregation query or a lookup on a B tree  In Section 4  we demonstrate how various summaries have the desired decomposable or exponentially decomposable property and thus can be plugged into our indexes  Finally we show how to support updates in Section 5  2  A BASELINE SOLUTION In this and the next section  we will describe our structures without instantiating with any particular summary  Instead we just use       summary    as a placeholder for any summary with error parameter     Let S     D  denote an    summary on data set D  We use s   to denote the size of an    summary 1   Internal memory structure  Based on the decomposable property of a summary  a baseline solution can be designed using standard techniques  We    rst describe the internal memory structure  Sort all the N data records in the database on the Aq attribute and partition them into N s   groups  each of size s    Then we build a binary tree T on top of these groups  where each leaf  called a fat leaf  stores a group of s   records  For each internal node u of T   let Tu denote the subtree of T rooted at u  We attach to u an    summary on the As attribute of all records stored in the subtree below u  Since each    summary has size s   and the number of internal nodes is O N s     the total size of the structure is O N   To answer a query  q1  q2   we do a search on T   It is well known that any range  q1  q2  can be decomposed into O log N s     disjoint canonical subtrees Tu  plus at most two fat leaves that may partially overlap  We retrieve the    summaries attached to the roots of these 1 Strictly speaking we should write s   D  But as most     summaries have sizes independent of D  we drop the subscript D for brevity subtrees  For each of the fat leaves  we simply read all the s   records stored there  Then we combine all of them into an O     summary for the entire query using the decomposable property  We can adjust    by a constant factor in the construction to ensure that the output is an    summary  The total query time is thus the time required to combine the O log N s     summaries  For the Count Min sketch and AMS sketch  the combining time is linear in the total size of the summaries  so the query time is O s   log N   For the quantile summary and heavy hitters summary the query time becomes O s   log N log log N  2 as we need to merge O log N s     sorted lists  Details in Section 4   External memory index  The baseline solution easily extends to external memory  If s       B  then each internal node and fat leaf occupies    s   B  blocks  so we can simply store each of them separately  The space is still linear and we load O log N  nodes on each query  The query cost becomes O  s   B log N  I Os for the Count Min and AMS sketch and O  s   B log N logM B log N  I Os for the quantile and heavy hitters summary  For s     B  each node occupies a fraction of a block  and we can pack multiple nodes in one block  We use a standard B tree blocking of the tree T where each block contains    B s    nodes  except possibly the root block  Thus each block stores a subtree of height    log B s     of T   Then standard analysis shows that the nodes we need to access are stored in O log N  log B s     blocks  This implies a query cost of O logB s   N  I Os for the Count Min and AMS sketch and O logB s   N logM B logB s   N  I Os for the quantile and heavy hitters summary  3  OPTIMAL INDEXING FOR F1 BASED SUMMARIES The baseline solution of the previous section is not that impressive  Its    output    term has an extra O log N  factor  in external memory  we are missing the ideal O logB N  term which is the main bene   t of block accesses  The main bottleneck in the baseline solution is not the search cost  but the fact that we need to assemble O log N  summaries  each of size s    In the absence of additional properties of the summary  it is impossible to make further improvement  Fortunately  we observe that many of the F1 based summaries have what we call the exponentially decomposable property  which allows us to assemble summaries of exponentially decreasing sizes  This turns out to be the key to optimality for indexing these summaries  Definition 1  Exponentially decomposable   For 0        1  a summary S is    exponentially decomposable if there exists a constant c   1  such that for any t multisets D1          Dt with their sizes satisfying F1 Di         i   1 F1 D1  for i   1          t  given S     D1   S c    D2         S c t   1     Dt    1  we can construct an O     summary for D1                Dt   2  the total size of S     D1           S c t   1     Dt  is O s    and they can be combined in O s    time  and  3  for any multiset D  the total size of S     D           S c t   1     D  is O s     Intuitively  since an F1 based summary S     D  provides an error bound of    D   the total error from S     D1   S c    D2   2 In fact  an alternative solution achieves query time O s   log N  log log N  by issuing s   range quantile queries to the data structure in  7   but this solution does not work in external memory          S c t   1     Dt  is    D1    c   D2               c t   1    Dt         D1     c      D1                c    t   1    D1   If we choose c such that c     1  then the error is bounded by O    D1    satisfying  1   Meanwhile  the F1 based summaries usually have size s        1      so  2  and  3  can be satis   ed  too  In Section 4 we will formally prove the    exponentially decomposable property for all the F1 based summaries mentioned in Section 1 2  3 1 Optimal internal memory structure Let T be the binary tree built on the Aq attribute as in the previous section  Without loss of generality we assume T is a complete balanced binary tree  otherwise we can always add at most N dummy records to make N s   a power of 2 so that T is complete  We    rst de   ne some notation on T   We use S     u  to denote the    summary on the As attribute of all records stored in u   s subtree  Fix an internal node u and a descendant v of u  let P u  v  to be the set of nodes on the path from u to v  excluding u  De   ne the left sibling set of P u  v  to be L u  v     w   w is a left child and has a right sibling     P u  v   and similarly the right sibling set of P u  v  to be R u  v     w   w is a right child and has a left sibling     P u  v    To answer a query  q1  q2   we    rst locate the two fat leaves a and b in T that contain q1 and q2  respectively  Let u be the lowest common ancestor of a and b  We call P u  a  and P u  b  the left and respectively the right query path  We observe that the subtrees rooted at the nodes in R u  a      L u  b  make up the canonical set for the query range  q1  q2   Focusing on R u  a   let w1          wt be the nodes of R u  a  and let d1              dt denote their depths in T  the root of T is said to be at depth 0   Since T is a balanced binary tree  we have F1 wi       1 2  di   d1 F1 w1  for i   1          t  Here we use F1 w  to denote the    rst frequency moment  i e   size  of the point set rooted at node w  Thus  if the summary is  1 2  exponentially decomposable  and we have S c di   d1     wi  for i   1          t at our disposal  we can combine them and form an O     summary for all the data covered by w1          wt  We do the same for L u  b   Finally  the two fat leaves can always supply the exact data  it is a summary with no error  of size O s    in the query range  Plus the initial O log N  search cost for locating R u  a  and L u  b   the query time now improves to the optimal O log N   s     It only remains to show how to supply S c di   d1     wi  for each of the wi   s  In fact  we can a   ord to attach to each node u     T all the summaries  S     u   S c    u         S c q     u  where q is an integer such that scq      O 1   Nicely  these summaries still have total size O s    by the exponentially decomposable property  thus the space required by each node is still O s    as in the previous section  and the total space remains linear  A schematic illustration of the overall structure is shown in Figure 1  Theorem 1  For any  1 2  exponentially decomposable summary  a database D of N records can be stored in an internal memory structure of linear size so that a summary query can be answered in O log N   s    time             summary   3 2     summary    3 2   2     summary                                                                                                                               Query range Figure 1  A schematic illustration of our internal memory structure  The grayed nodes form the canonical decomposition of the query range  and the grayed summaries are those we combine into the      nal summary for the queried data  In this example we use c   3 2   3 2 Optimal external memory indexing In this section we show how to achieve the O logB N   s   B  I O query cost in external memory still with linear space  Here  the di   culty that we need to assemble O log N  summaries lingers  In internal memory  we managed to get around it by the exponentially decomposable property so that the total size of these summaries is O s     However  they still reside in O log N  separate nodes  If we still use a standard B tree blocking  for s       B we need to access    log N  blocks  for s     B  we need to access    log N  log B s     blocks  neither of which is optimal  Below we    rst show how to achieve the optimal query cost by increasing the space to super linear  then propose a packed structure to reduce the space back to linear  Consider an internal node u and one of its descendants v  Let the sibling sets R u  v  and L u  v  be as previously de   ned  In the following we only describe how to handle the R u  v    s  we will do the same for the L u  v    s  Suppose R u  v  contains nodes w1          wt at depths d1          dt  We de   ne the summary set for R u  v  with error parameter    to be RS u  v         S     w1   S c d2   d1     w2           S c dt   d1     wt    The following two facts easily follow from the exponentially decomposable property  Fact 1  The total size of the summaries in RS u  v      is O s     Fact 2  The total size of all the summary sets RS u  v       RS u  v  c            RS u  v  c t     is O s     The indexing structure  We    rst build the binary tree T as before with a fat leaf size of s    Before attaching any summaries  we block T in a standard B tree fashion so that each block stores a subtree of T of size    B   except possibly the root block which may contain 2 to B nodes of T   The resulting blocked tree is essentially a B tree where each leaf occupies O s   B  blocks and each internal node occupies 1 Leaf size  s      B  O log B  Figure 2  The standard B tree blocking of a binary tree  rB v2 RS rB  v2      RS rB  v2  c    u v1 RS u  v1      Figure 3  The summaries we store for an internal block B  block  Please see Figure 2 for an example of the standard B tree blocking  Consider an internal block B in the B tree  Below we describe the additional structures we attach to B  Let TB be the binary subtree of T stored in B and let rB be the root of TB  To achieve the optimal query cost  the summaries attached to the nodes of TB that we need to retrieve for answering any query must be stored consecutively  or in at most O 1  consecutive chunks  Therefore  the idea is to store all the summaries for a query path in TB together  which is the reason we introduced the summary set RS u  v       The detailed structures that we attach to B are as follows  1  For each internal node u     TB and each leaf v in u   s subtree in TB  we store all summaries in RS u  v      sequentially  2  For each leaf v  we store the summaries in RS rB  v  c j     sequentially  for all j   0          q  Recall that q is an integer such that scq      O 1   3  For the root rB  we store S c j     rB  for j   0          q  An illustration of the    rst and the second type of structures is shown in Figure 3  The size of the structure can be determined as follow 1  For each leaf v     TB  there are at most O log B  ancestors of v  so there are in total O B log B  such pairs  u  v   For each pair we use O s    space  so the space usage is O s  B log B   2  For each leaf v     TB we use O s    space  so the space usage is O s  B   3  For the root rB  the space usage is O s     Summing up the above cases  the space for storing the summaries of any internal block B is O s  B log B   Note that each internal block has fanout    B   and each leaf has size    s     so there are in total at most O N  Bs     internal blocks  and thus the total space usage is O N log B   Next we show that this structure can indeed be used to answer queries in the optimal O logB N   s   B  I Os  Query procedure  Given a query range  q1  q2   let a and b be the two leaves containing q1 and q2  respectively  We focus on how to retrieve the necessary summaries for the right sibling set R u  a   where u is the lowest common ancestor of a and b  the left sibling set L u  b  can be handled symmetrically  By the previous analysis  we need exactly the summaries in RS u  a       Recall that R u  a  are the right siblings of the left query path P u  a   Let B0          Bl be the blocks that P u  a  intersects from u to a  The path P u  a  is partitioned into l   1 segments by these l   1  blocks   Let P u  v0   P r1  v1           P rl  vl   a  be the l   1 segments  with ri being the root of the binary tree TBi in block Bi and vi being a leaf of TBi   i   0          l  Let w1          wt be the nodes in R u  a   at depths d1          dt of T   We claim that wi is either a node of TBk for some k      0          l   or a right sibling of rk for some k      0          l   which makes wi a root of some other block  This is because by the definition of R u  a   we know that wi is a right child whose left sibling is in some Bk  If wi is not in Bk  it must be the root of some other block  Recall that we need to retrieve S c di   d1     wi  for i   1          t  Below we show how this can be done e   ciently using our structure  For the wi   s in the    rst block B0  since we have stored all summaries in RS u  v0      sequentially for B0  case 1    they can be retrieved in O 1   s   B  I Os  For any wi being the root of some other block B   not on the path B0          Bl  since we have stored the summaries S c j     wi  for j   0          q for every block  case 3    the required summary S c di   d1     wi  can be retrieved in O 1   s c di   d1     B  I Os  Note that the number of such wi   s is bounded by O logB N   so the total cost for retrieving summaries for these nodes is at most O logB N   s   B  I Os  The rest of the wi   s are in B1          Bl  Consider each Bk  k   1          l  Recall that the segment of the path P u  a  in Bk is P rk  vk   and the wi   s in Bk are exactly R rk  vk   We have stored RS rk  vk  c j     for Bk for all j  case 2    so no matter at which relative depths di     d1 the nodes in R rk  vk  start and end  we can always    nd the required summary set  Retrieving the desired summary set takes O   1   s cd    d1     B    I Os  where d   is the depth of the highest node in R rk  vk   Summing over all blocks B1          Bl  the total cost is O logB N   s   B  I Os  Reducing the size to linear  The structure above has a super linear size O N log B   Next we show how to reduce its size back to O N  while not a   ecting the optimal query cost  u ul ur   w1 u   kh h w2 w3 S c    w S     w1  S c 2  2     w3  One summary for each node in u      s subtree Figure 4  A schematic illustration of our packed structure  Observe that the log B factor comes from case 1   where we store RS u  v      for each internal node u and each leaf v in u   s subtree in u   s block B  Focus on B and the binary tree TB stored in it  Abusing notation  we use Tu to denote the subtree rooted at u in TB  Assume Tu has height h  in TB   Our idea is to pack the RS u  v        s for some leaves v     Tu to reduce the space usage  Let ul and ur be the left and right child of u  respectively  The    rst observation is that we only need to store RS u  v      for each leaf v in ul   s subtree  This is because for any leaf v in ur   s subtree  the sibling set R u  v  is the same as R ur  v   so RS u  v        RS ur  v       which will be stored when considering ur in place of u  For any leaf v in ul   s subtree  observe that the highest node in R u  v  is ur  This means for a node w     R u  v  with height i in tree Tu  the summary for w in RS u  v      is S c h   i   1     w   Let u   be an internal node in ul   s subtree  and suppose u   has kh leaves below it  We will decide later the value of kh and  thus  the height log kh at which u   is chosen  the leaves are de   ned to be at height 0   We do the following for each u   at height log kh in ul   s subtree  Instead of storing the summary set RS u  v      for each leaf v in u      s subtree  we store RS u  u          which is the common pre   x of all the RS u  v        s  together with a summary for each of the nodes in u      s subtree  More precisely  for each node w in u      s subtree  if its height is i  we store a summary S c h   i   1     w   All these summaries below u   are stored sequentially  A schematic illustration of our packed structure is shown in Figure 4  Recall that all the summary sets we store in case 1  are used to cover the top portion of the query path P u  v0  in block B0  i e   RS u  v0       Clearly the packed structure still serves this purpose  We    rst    nd the u   which has v0 as one of its descendants  Then we load RS u  u          followed by the summaries S c h   i   1   w  required in RS u  v0       Loading RS u  u         still takes O 1 s   B  I Os  but loading the remaining individual summaries may incur many I Os since they may not be stored sequentially  Nevertheless  if we ensure that all the individual summaries below u   have total size O s     then loading any subset of them does not take more than O 1   s   B  I Os  Note that there are kh 2 i nodes at height i in u   s subtree  the total size of all sum maries below u   is logXkh i 0 kh 2 i sch   i   1       1  Thus it is su   cient to choose kh such that  1  is    s     Note that such a kh always exists 3   When kh   1    1  is sch   1      O s     when kh takes the maximum possible value kh   2 h   1   the last term  when i   h  in the summation of  1  is s    so  1  is at least    s     every time kh doubles   1  increases by at most O s     It only remains to show that by employing the packed structure  the space usage for a block is indeed O Bs     For a node u at height h in TB  the number of u      s at height log kh under u is 2 h  kh  For each such u     storing RS u  u          as well as all the individual summaries below u     takes O s    space  So the space required for node u is O 2 h s   kh   There are O B 2 h   nodes u at height h  Thus the total space required is O   logXB h 1 2 h s   kh    B 2 h     O   logXB h 1 Bs   kh     Note that the choice of kh implies that s   kh   O  logXkh i 0 1 2 i sch   i   1        O   hX   1 i 0 1 2 i sch   i   1        so the total size of the packed structures in B is bounded by logXB h 1 Bs   kh     B logXB h 0 hX   1 i 0 1 2 i sch   i   1      B logXB h 0 hX   1 i 0 1 2h   i   1 sc i        B logXB i 0 sc i    logXB h i 1 2h   i   1     2B logXB i 0 sc i      O Bs     Theorem 2  For any  1 2  exponentially decomposable summary  a database D of N records can be stored in an external memory index of linear size so that a summary query can be answered in O logB N   s   B  I Os  Remark  One technical subtlety is that the O s    combining time in internal memory does not guarantee that we can combine the O log N  summaries in O s   B  I Os in external memory  However if the merging algorithm only makes linear scans on the summaries  then this is not a problem  as we shall see in Section 4  4  SUMMARIES In this section we demonstrate the decomposable or exponentially decomposable properties for the summaries mentioned in Section 1 2  Thus  they can be used in our indexes in Section 2 and 3  3 We de   ne kh in this implicit way for its generality  When instantiating into speci   c summaries  there are often closed forms for kh  For example when s        1     and 1   c   2  kh      c h    4 1 Heavy hitters Given a multiset D  let fD x  be the frequency of x in D  The MG summary  23  with error parameter    consists of s     1    items and their associated counters  For any item x in the counter set  the MG summary maintains an estimated count    fD x  such that fD x        F1 D         fD x      fD x   for any item x not in the counter set  it is guaranteed that fD x        F1 D   Thus in either case  the MG summary provides an additive   F1 D  error  fD x        F1 D         fD x      fD x  for any x  The SpaceSaving summary is very similar to the MG summary except that the SpaceSaving summary provides an    fD x  overestimating fD x   fD x         fD x    fD x      F1 D   Thus they clearly solve the heavy hitters problem  The MG summary is clearly decomposable  Below we show that it is also    exponentially decomposable for any 0        1  The same proof also works for the SpaceSaving summary  Consider t multisets D1          Dt with F1 Di         i   1 F1 D1  for i   1          t  We set c   1           1  Given a series of MG summaries S     D1   S c    D2           S c t   1     Dt   we combine them by adding up the counters for the same item  Note that the total size of these summaries is bounded by Xt   1 j 0 scj      Xt   1 j 0 1 c j      O 1       O s     In order to analyze the error in the combined summary  let fj  x  denote the true frequency of item x in Dj and    fj  x  be the estimator of fj  x  in S c j   1     Dj    The combined summary uses Pt j 1    fj  x  to estimate the true frequency of x  which is Pt j 1 fj  x   Note that fj  x         fj  x      fj  x      c j   1   F1 Dj   for j   1          t  Summing up the    rst inequality over all j yields Pt j 1 fj  x      Pt j 1    fj  x   For the second inequality  we have Xt j 1    fj  x      Xt j 1 fj  x      Xt j 1 c j   1   F1 Dj       Xt j 1 fj  x      Xt j 1               j   1   F1 D1      Xt j 1 fj  x        F1 D1  Xt j 1           j   1   Xt j 1 fj  x      O   F1 D1    Therefore the error bound is O   F1 D1     O    F1 D1               Dt    To combine the summaries we require that each summary maintains its  item  counter  pairs in the increasing order of items  we impose an arbitrary ordering if the items are from an unordered domain   In this case each summary can be viewed as a sorted list and we can merge the t sorted lists into a single list  where the counters for the same item are added up  Note that if each summary is of size s    then we need to employ a t way merging algorithm and it takes O s  tlog t  time in internal memory and O  s  t B logM B t  I Os in external memory  However  when the sizes of the t summariesform a geometrically decreasing sequence  we can repeatedly perform two way merges in a bottom up fashion with linear total cost  The merging algorithm starts with an empty list  at step i  it merges the current list with the summary S   t 1   i  Dt 1   i   Note that in this process every counter of S   j   Dj   is merged j times  but since the size of S   j   Dj   is 1 cj   1      the total running time is bounded by Xt j 1 j c j   1      O     1         O s     In external memory we can perform the same trick and achieve the O s   B  I O bound if the smallest summary S c t   1     Dt  has size 1 c t   1      B  otherwise we can take the smallest k summaries  where k is the maximum number such that the smallest k summaries can    t in one block  and merge them in the main memory  In either case  we can merge the t summaries in s   B I Os  4 2 Quantiles Recall that in the    approximate quantile problem  we are given a set D of N items from a totally ordered universe  and the goal is to have a summary S     D  from which for any 0        1  a record with rank in             N          N  can be extracted  It is easy to obtain a quantile summary of size O 1      We simply sort D and take an item every   N consecutive items  Given any rank r     N  there is always an element within rank  r       N  r     N   Below we show that quantile summaries are    exponentially decomposable  Suppose we are given a series of such quantile summaries S   1  D1   S   2  D2           S   t  Dt   for data sets D1          Dt  We combine them by sorting all the items in these summaries  We claim this forms an approximate quantile summary for D   D1                    Dt with error at most Pt j 1   jF1 Dj    that is  given a rank r  we can    nd Pan item in the combined summary whose rank is in  r     t j 1   jF1 Dj    r   Pt j 1   jF1 Dj    in D  For an element x in the combined summary  let yj and zj be the two consecutive elements in S   j   Dj   such that yj     x     zj   We de   ne rmin j  x  to be the rank of yj in Dj and rmax j  x  to be rank of zj in Dj   In other words  rmin j  x   resp  rmax j  x   is the minimum  resp  maximum  possible rank of x in Dj   We state the following lemma that describes the properties of rmin j  x  and rmax j  x   Lemma 1   1  For an element x in the combined summary  Xt j 1 rmax j  x      Xt j 1 rmin j  x      Xt j 1   jF1 Dj     2  For two consecutive elements x1     x2 in the combined summary  Xt j 1 rmin j  x2      Xt j 1 rmin j  x1      Xt j 1   jF1 Dj    Proof  Since rmax j  x  and rmin j  x  are the local ranks of two consecutive elements in S   j   Dj    we have rmax j  x      rmin j  x        jF1 Dj    Taking summation over all j  part  1  of the lemma follows  We also note that if x1 and x2 are consecutive in the combined summary  rmin j  x1  and rmin j  x2  are local ranks of either the same element or two consecutive elements of S   j   Dj     In either case we have rmin j  x2      rmin j  x1        jF1 Dj    Summing over all j proves part  2  of the lemma  Now for each element x in the combined summary  we compute the global minimum rank rmin  x    Pt j 1 rmin j  x   Note that all these global ranks can be computed by scanning the combined summary in sorted order  Given a query rank r  we    nd the smallest element x with rmin P  x      r     t j 1   jF1 Dj    We claim that the actual rank of x in D is in the range  r     Pt j 1   jF1 Dj    r   Pt j 1   jF1 Dj     Indeed  we observe that the actual rank of x in set D is in the range   Pt j 1 rmin j  x   Pt j 1 rmax j  x   so we only need to prove that this range is contained by  r    Pt j 1   jF1 Dj    r  Pt j 1   jF1 Dj     The left side trivially follows from the choice of x  For the right side  let x   be the largest element in the new summary such that x       x  By the choice of x  we have Pt j 1 rmin j  x       r     Pt j 1   jF1 Dj    By Lemma 1 we have Pt j 1 rmin j  x      Pt j 1 rmin j  x         Pt j 1   jF1 Dj   and Pt j 1 rmax j  x      Pt j 1 rmin j  x      Pt j 1   jF1 Pj    Summing up these three inequalities yields Pt j 1 rmax j  x      r   Pt j 1   jF1 Dj    so the claim follows  For    exponentially decomposability  the t data sets have F1 Di         i   1 F1 D1  for i   1          t  We choose c   1           1  The summaries S   1  D1   S   2  D2           S   t  Dt  have   i   c i   1     Therefore we can combine them with error Xt j 1 c j   1   F1 Dj       Xt j 1                 j   1   F1 D1      F1 D1  Xt j 1           j   1   O   F1 D1     O   F1 D1                    Dt    To combine the t summaries  we notice that we are essentially merging k sorted lists with geometrically decreasing sizes  so we can adapt the algorithm in Section 4 1  The cost of merging the t summaries is therefore O s    in internal memory and O s   B  I Os in external memory  The size of combined summary is Xt j 1 1 c j   1      O     1         O s     4 3 The Count Min sketch Given a multiset D where the items are drawn from a universe  u     1          u   Let fD x  be the frequency of x in D  The Count Min sketch makes use of a 2 universal hash function h    u       1     and a collection of 1    counters C 1           C 1      Then it computes C j    P h x  j fD x  for j   1          1     A single collection of 1    counters achieve a constant success probability for a variety of estimation purposes  and the probability can be boosted to 1        by using O log 1      copies with independent hash functions  Here we only show the decomposability of a single copy  the same result also holds for O log 1      copies  Given multiple Count Min sketches with the same h  hence the same number of counters   they can be easily combined by adding up the corresponding counters  So the Count Minsketch is decomposable  However  for exponentially decomposability we are dealing with t Count Min sketches with exponentially increasing      s  hence di   erent hash functions  so they cannot be easily combined  Thus we simply put them together without combining any counters  Although the resulting summary is not a true Count Min sketch  we argue that it can be used to serve all the purposes a CountMin is supposed to serve  More precisely  for t data sets D1          Dt with F1 Di         i   1 F1 D1   we have t Count Min sketches S     D1           S c t   1     Dt   The i th sketch S c j   1     Dt  uses a hash function hi    u       1 c j   1      Again we set c   1          Note that the total size of all the sketches is O 1      1 c     1 c 2                  O 1       O s     so we only need to show that the error is the same as what a Count Min sketch S     D1          Dt  would provide  Below we consider the problem of estimating inner products  join sizes   which has other applications  such as point queries and self join sizes  as special cases  PLet fi denote the frequency vector of Di  and let f   t i 1 fi be the frequency vector of D   D1           Dt  The goal is to estimate inner product  f  g  where g is the frequency vector of some other data set  Note that when g is a standard basic vector  i e   containing only one    1       f  g  becomes a point query  when g   f   f  g  is the self join size of f  We distinguish between two cases   1  g is given explicitly  and  2  g is also represented by a summary returned by our index  i e   a collection of t Count Min sketches S     G1           S c t   1     Gt   where g   Pt i 1 gi and gi is the frequency vector of Gi  Recall that the Count Min sketch estimates  f  g  with an additive error of   F1 f F1 g   and we will show that we can do the same when f is represented by the collection of t Count Min sketches  Inner product with an explicit vector  For a g given explicitly  we can construct a Count Min sketch S c i   1     g  for g with hash function hi  for i   1          t  We observe that  f  g  can be expressed as Pt i 1  fi  g   and  fi  g  can be estimated using S c i   1     Di  and S c i   1     g  as described in  8  since they use the same hash function  The error is c i   1     fi  1  g  1      c    i   1     f1  1  g  1  For c   1          the total error is bounded by Xt i 1     i   1  2     f1  1  g  1   O     f1  1  g  1    O    F1 f F1 g    as desired  Inner product with a vector returned by a summary query  Next we consider the case where g is also represented by a series 4 of t Count Min sketches S     G1           S c t   1     Gt  with F1 Gi         i   1 F1 G1   We will show how to estimate  f  g  using the two series of sketches  This will allow the user to estimate the join size between the results of two queries  Note that this includes the special case of estimating the self join size of f  In this case we will inevitably face the problem of pairing two sketches of di   erent sizes  To do so we need more insight into the hash functions used  Suppose 1    is a power of 2  4 More precisely  g is represented by two such series  one from the left query path and one from the right query path  and so is f  But we can decompose  f  g  into 4 subproblems by considering the cross product of these series  where each subproblem involves only a single series of</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pssp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pssp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling"/>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_query_processing_on_semi-structured_data"/>
        <doc>Cost Based Plan Selection for XPath  ###  Haris Georgiadis  AUEB  harisgeo aueb gr  Minas Charalambides  AUEB  minchar86 gmail com  Vasilis Vassalos  AUEB  vassalos aueb gr  ABSTRACT We present a complete XPath cost based optimization and  execution framework and demonstrate its effectiveness and  efficiency for a variety of queries and datasets  The framework is  based on a logical XPath algebra with novel features and  operators and a comprehensive set of rewriting rules that together  enable us to algebraically capture many existing and novel  processing strategies for XPath queries  An important part of the  framework is PSA  a very efficient cost based plan selection  algorithm for XPath queries  In the presented experimental  evaluation  PSA picked the cheapest estimated query plan in  100  of the cases  Our cost based query optimizer independent  of the underlying physical data model and storage system and of  the available logical operator implementations  depending on a set  of well defined APIs  We also present an implementation of those  APIs  including primitive access methods  a large pool of physical  operators  statistics estimators and cost models  and  experimentally demonstrate the  effectiveness of our end to end  query optimization system   Categories and Subject Descriptors H 2 1   Information Systems   Logical Design   Data models   H 2 1  Information Systems   Physical Design  Access methods   H 2 3   Information Systems   Languages  Query languages   H 2 4  Information Systems   Systems  Query processing  D 2 9   Computing Methodologies   Algorithms  Algebraic algorithms  General Terms   Algorithms  Measurement  Performance  Experimentation   Languages  Keywords   XPath  XML  Cost based Optimization   Algebraic rewritings  Cost Models  Cardinality Estimation  1  ###   INTRODUCTION  There is considerable research and commercial interest in  processing semantically rich  flexibly structured data  and XML  processing more specifically  3  6  12   At the same time  the  benefits of developing ad hoc processing techniques applicable to  specific situations are reaching a plateau  and their number makes  it difficult  to exploit  each to the  fullest  A cost based optimizer  can    unlock    the benefit of existing  and future  processing  techniques by allowing  them to  be  utilized in the  best possible  way in an automatic fashion  It can also enable a significant leap  for XML performance  as progress in individual pieces of the  query processing framework  such as physical operator  implementation  cost modeling or statistics accuracy will translate  directly into improved query performance  The difficulty of costbased optimization for XPath XQuery lies in the same  characteristics that make XML query languages powerful and  expressive  as well as in the rapid progress in the field  A variety  of access paths into XML databases  structural joins  twig joins   staircase joins  e g     varied physical organization for storing  XML  e g   structural encoding schemes  index organized tables   etc   and challenging data model features  e g   hierarchy  flexible  schema   These characteristics    break    assumptions that held in  cost based optimization for relational DBMSs  such as the  commutative property of query operators  thus invalidating  existing cost models and plan selection algorithms    Overcoming these difficulties requires original research  as  simply extending existing techniques have not proven fruitful   Existing XML query cost based optimization attempts are not  generic  3  4  5  6  10  16   since they have been created for  specific XPath or XQuery execution engines  XPath algebras on  which they are based  1  2  5  18  19      most of them tailored for  the needs of specific XML database systems and ill fitting for  generic XPath optimizers     do not focus on the navigational  semantics of XPath and as a result  rewriting rules  when defined   cannot capture the different approaches for XPath query  processing   We present a complete XPath cost based optimizer that is  independent of the underlying XML storage system and the  techniques and algorithms used for XPath processing  It is based  on a navigation focused XPath algebra with novel operators and a  comprehensive set of rewriting rules  The optimizer does not  assume anything about the existing query processing operators  available or the underlying physical data model  Physical operator  and access method cost models can be plugged in  A core  component of the optimizer is  PSA  an efficient algorithm for  selecting the best physical plan     Apart from the query optimization framework  we present an  integrated query execution framework that allows the deployment  of physical operator implementations and cost models in a manner  independent to physical storage   We describe four families of  deployed physical operators along with their cost models  Three  of them are inspired by well known XPath processing techniques   Staircase join  12   PathStack  13  and PPFS  11    The main contributions of this work are the following       We propose a novel XPath logical algebra that constitutes a  good fit with many XPath processing techniques while being  completely agnostic about them  Sec 3        We define a rich set of rewriting rules for the algebra which  allow the generation of logical plans capturing a wide variety  of different XPath processing strategies  Sec 5     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page  To copy otherwise  or  republish  to post on servers or to redistribute to lists  requires prior specific  permission and or a fee   SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA   Copyright 2009 ACM  978 1 60558 551 2 09 06    5 00      We present an efficient and effective algorithm for cost based  selection of the estimated best physical plan  Sec 6 4        We provide an extensible framework for physical operator  cost estimation  Sec 6 2   as well as for XPath query  execution that supports different physical operator  implementations and XML storage engines in a way that is  agnostic to the physical data model and underlying XML  physical storage methods       We provide extensive experiments to evaluate the efficiency  and effectiveness of our system  Sec 7    2  ARCHITECTURE  The optimization engine consists of three basic components  the  Query parser  the Physical Plan Selector and the Physical Plan  Executor  as illustrated in Figure 1  The independence of the  optimizer from the XML Storage System implementation is  achieved via an API called XML Primitive Access  XPA API    The query parser parses the input XPath expression and generates  a logical plan  namely its algebraic representation in XPAlgebra   The Physical Plan Selector takes as input the initial logical plan  and generates the best physical plan using an efficient algorithm  for searching the space of possible query execution strategies  It  makes use of the available  Rewriting Rules for logical  transformations  while accessing all needed statistics from the  Database Statistics interface of the XPA API to estimate  cardinalities of logical operators  The physical plan selector gets  the costs of candidate physical operators from their corresponding  Descriptors  All components except the Physical Plan Executor  described so far constitute the Query Optimization Framework      Figure 1  Architecture of our XPath Optimizer  The second framework is the Query Execution framework which  offers natural support for different XPath processing and XML  storage methods  The Physical Plan Executor takes as input a  physical plan  invokes the actual Physical Operators  executes the  query and returns the result  To use a different XML Storage  System only the implementation of an  XPA driver is required  without the need for modifying  either the existing Physical  Operators or their Descriptors  This is due to the fact that Physical  Operators do not have direct access to the underlying  XML  Storage System  Instead  they make use of a series of primitive  access methods available through the  Primitive Access Methods interface of the XPA API  A cost model implemented in a  physical operator Descriptor finds the costs of the primitive  access methods calls it makes by asking the XPA API  Hence  use  of a new XML Storage System requires implementing the  Primitive Access Methods and defining their cost models in an  XPA driver   3  THE XPALGEBRA ALGEBRA  XPAlgebra is our generic sequence based logical algebra for a  large subset of XPath that includes forward and backward axes  and non positional predicates involving conjunctive Boolean  expressions that don   t involve  comparisons between paths   XPAlgebra follows XPath 2 0 semantics  The basic features of  XPAlgebra that make it more suitable  than  e g    1  19   for an  extensible  rewriting based algebraic optimizer and execution  engine are that       it maintains the navigation nature of XPath  and because of  that provides a straightforward correspondence between an  XPath expression and its algebraic representation and      it admits a number of rewritings that allow us to capture a  large number of XPath query processing strategies proposed  in the literature  12  13  11  14    Extending the algebra to the remaining axes  other Boolean  connectives and to value based joins  i e   comparisons involving  paths on both sides  is the object of current work    3 1 Logical Data Model  The core components of XPAlgebra   s logical data model are  element and  sequence  An XML  element has the following  descriptors  an abstract ID which uniquely identifies the element   the tag name  a  possible  text node value  and a map which  associates attribute names to attribute values  To keep the data  model abstract so as to support different physical data models and  storage techniques  an element is not required to carry references  to parent or child elements  though concrete implementations may  explicitly include such information and more   Such element  access methods may be provided by the XPA Driver  A sequence is a duplicate free list of elements in document order    3 2 Logical Operators  XPalgebra operators return either a sequence of nodes  sequence  operators  or a boolean value  boolean operators   We can think  of sequence operators as capturing navigation steps of the main  navigation of an XPath expression  called backbone path in  11   and  selection path in  17  and elsewhere  Boolean operators  capture predicates  Indeed  that    correspondence    is the basis for  the default translation of Sec  3 3  A  backbone operator is a   sequence  operator appearing in the  algebraic translation of the   backbone path    3 2 1 Sequence Operators  Both the input and the output of a Sequence operator are  sequences of nodes  The input sequence is called  context  sequence  and can be the output of another sequence operator   Table 1 includes all sequence operators of XPAlgebra  The first  seven perform navigation into an XML document  Their input and  output are  duplicate free sequences of XML elements in  document order  The ca  operator  corresponding to the child axis  of XPath  takes an input sequence S and returns the union of all a children for each element of S  in document order  Operators d  p and a correspond to the descendant  parent and ancestor axes and  are defined similarly  fpp  takes an input sequence S and returns the  union of all descendants of elements in S that are under relative path p  where p is a forward XPath expression with no predicates   bpp is defined similarly  with  p a relative backward XPath  expression consisting of parent  and or ancestor steps only  In  what follows  parent   and  ancestor   are abbreviated to   and      respectively  The novel cousin operator  csp1 p2 takes an input  sequence S and for each element e returns    cousin    elements of e that are reachable by fist navigating backwards on the backward  path  p1  then navigating from that ancestor on forward path  p2    Remaining sequence operators filter a given sequence according to some criteria and return a subsequence  The path filter operator  pfp rejects nodes whose root to node path doesn   t match path  p   where p is an absolute forward path with no predicates  Similarly   the value filter operator vfn op v  keeps only those nodes that have a  text node  if  n   text    or an attribute node  if  n    a  whose  value satisfies condition op v  where op is a comparison operator  and  v is an atomic value  The filter operator  f takes as input a  sequence S and a boolean expression BoolExpr  which is either a  constant or a conjunction of one or more boolean operators    BoolExpr  const      1   2         n   where    i    Boolean Operator   For each element n in S  n is passed as an argument to the boolean  operators of  BoolExpr  If the result of these calls is true   n is  included into the result    Table 1  Logical Operators  3 2 2 Boolean Operators  Boolean operators are applied to a single node and return boolean  values  Each boolean operator has a    matching    sequence  operator  A Boolean operator has an optional argument BE of type  BoolExp  It takes as input an element  n  and returns true if the  matching sequence operator has a nonempty result on  n  For  example  the boolean child operator    ca  searches for an a child  of n and returns true if one exists  If BE exists  it is evaluated on  each result of the    matching    sequence operator and filters it  depending on its result     ca  BE  for example would return true  only if for some a child of n BE returns true   3 2 3 Classification of Logical Operators  It is useful to arrange logical operators in a generalization  hierarchy  according to their common features  The  c   d and  fp operators can be considered specializations of the abstract  operator Fop  Likewise  the p  a  bp operators are specializations  of the abstract operator Bop  All Fop and Bop operators  together  with the  cs operator are grouped under the abstract navigation  operator Nop  Nop along with the remaining sequence operators  are abstracted under  s  Boolean operators are organized in a  similar hierarchy  Operators   c    d    fp are grouped under   Fop     p     a     bp under    Bop  Operators    Fop and    Bop are  specializations of the abstract operator   Nop    Nop along with  the remaining operators are abstracted under         3 2 4 Graphical Representation of XPAlgebra  An XPAlgebra expression can be represented graphically as a tree  read bottom up  Sequence operators are linked with simple lines   lines crossed by arrows show attached boolean operators to filter or to other boolean operators  Figure 2 a  and  b  illustrates the  graphical representations of the XPAlgebra expressions     dk  f fp s r   it  root    fp mb m to   vf text   x     and  dk  f cde  f fp s r   it  root    fp mb m to     fp p li p li  b e     respectively   Paths of  fp and  bp operators  as well as of their boolean  counterparts  are always relative paths starting with         or           for  forward paths  or with         or           for backward paths  An  absolute forward path is represented by a forward path whose  context sequence is the root singleton  e g  fp s r   it  root    Figure 2  XAlgerbra graphical representations   3 3 Translating XPath to XPAlgebra  The default translation of an XPath expression into an XPAlgebra  plan presented here is the starting point for our rewriting and costbased plan selection algorithm   Sec 6 4   Parsing the query  backbone from left to right until we reach the first predicate  we  generate one or more Nop operators  the result of each being an  input to the next one  Forward navigation steps give c  a d or fp  in  case there are more than one forward consecutive steps  Backward  steps give operators p  or a  or bp if there are multiple consecutive  backward steps  An  f operator is generated upon arriving at a  predicate  whose input is the  algebraic representation of the  backbone prefix read so far  One or more     operators are  generated from the predicate expression  in a similar manner  If  predicates are nested  a branch subtree is formed  with each  subsequent nested predicate generating the  BoolExpr  argument  for the previous    operator  The f operator is the input to the next  Nop generated when parsing the backbone path after the  predicate  and so on  The default translation never involves  operators  cs   vf   pf     pf  They appear in plans only due to the  application of rewriting rules     Example 1  The XPath  s r   it mb m to x   k  Q1  is translated  into the following algebraic expression                   dk  f fp s r   it  root     fp mb m to   vf text   x          L1Q1   whose graphical representation is shown in Figure 2 a       Example 2  The XPath  s r na it mb m to  de p li p li  b e   k  Q2   is translated into the following algebraic expression   dk  f cde  f fp s r   it  root    fp mb m to     fp p li p li  b e      L1Q2   whose graphical representation is shown in Figure 2  b         4  XPA API AND PHYSICAL OPERATORS  4 1 Primitive Access Methods  Logical operations  as described in the previous Sec are  performed by physical operators  As described in Sec 2  our  system can accommodate different XML storage engines  with  different implementations  and costs  of primitive access  methods  as well as different physical operator implementations   We describe next the relevant system modules   XPA API  and  Driver     4 1 1 The XPA API  The  XPA API consists of five interfaces   Element   Sequence   AccessMethods  AccMCostModel and DBStatistics  Figure 3   The  first two interfaces correspond to the two components of the  logical data model of the XPAlgebra described in Sec 3 1    Logical Operator  Descriptive Name  ca  S sequence  child  da  S sequence  descendant  fpp S sequence  forward path  pa  S sequence  parent  aa  S sequence  ancestor  bppath S sequence  backward path  csp1  p2  S sequence  cousin  f  S sequence  BE  BoolExpr   filter  Sequence      Operators  vf n op v   S sequence  value filter  pf p   S sequence  path filter    ca  BE  BoolExpr  boolean child    da  BE  BoolExpr  boolean descendant    fpp BE  BoolExpr   boolean forward path    pa  BE  BoolExpr  boolean parent    aa  BE  BoolExpr  boolean ancestor    bpp BE  BoolExpr  boolean backward path    pf n op v boolean path filter  Boolean Operators   vf n op v  boolean value filter Method  getRTNPath   of the Element interface returns the rootto node path  RTN path  of the element  The Sequence interface is  virtually a control structure for the successive traversal of  elements  similar to non scrollable  also known as forward only   cursors  The AccessMethod interface provides a pool of primitive  access methods that are used by the physical operators to access  XML data  For example  method Descendants   takes as input an  element e and a tag name a and returns a sequence of a elements  that are descendents of  e  The  AccMCostModel provides  information about the cost of the abovementioned access  methods  This information is used in the definition of physical  operator cost models  which is then agnostic to the actual  implementation of the access methods  see Sec 6 2  For example   CostForDescLookup   returns the average cost for retrieving the  first descendant of an element  provided that its tag name is  specified  The average cost for shifting to  next  descendant  element provided that the first one has been already retrieved is  given by the  CostForNextDesc   method  The average cost for  retrieving the  RTN path of an element is given by  costForRTNPathRetrieval     Figure 3  The main interfaces of the XPA API  Interface  DBStatistics provides basic statistical information for  the stored XML documents  These methods can be implemented  using any XML cardinality and selectivity estimation technique   such as  7  8  9   as long as the underlying statistics required are  maintained by the XML Storage System  Using more  sophisticated cardinality estimation techniques for the  implementation of these basic interface methods may increase the  precision and or the performance of these methods  Cardinality    returns the cardinality estimate for a given non predicated abs fp   Selectivity   returns the probability that an element conforming to  the base path  that has to be a non predicated abs fp     survives     an existential filter with the given navigational path  which must  be a non predicated relative fp or bp   Occurence basepath path  returns the estimated average number of elements e such that  for  each element  e  conforming to  basepath  e  is in the result of  following  path from  e   Finally   DistinctValues   returns the  number of all distinct values of attributes or text nodes belonging  to an element with a given tag name    4 1 2 An XPA implementation  Our cost based optimization framework can work with any  storage engine that implements the XPA API  We have developed  such a native XML storage system and implemented its XPA API   We also describe additional storage engine implementations in   32   The physical implementation of element specializes element  ID as a binary string that implements the dewey encoding  a wellknown prefix based labelling scheme for XML elements that  maps both element structural positions and ordering   Alternatively  other labelling schemes  such as region encoding   14  or ORDPATH  15  can be used  All methods of the Element interface that check for the existence of a particular structural  relationship between two elements are implemented by  performing lexicographical comparisons between their dewey  IDs  as described in  11   Our  Element implementation  additionally stores a reference to the RTN path of the element  All  distinct RTN paths are assigned a unique number and stored in a  B Tree structure  which also keeps cardinalities for  RTN paths   Therefore the implementation of the  getRTNPath   involves a  simple look up on the root to node paths B Tree  XML elements  are stored into B Tree structures with the dewey ID as key  Our  system is  schema aware  a separate B Tree structure is  maintained for each distinct element tag name  Secondary B Tree  indices can be created on demand for indexing text or attribute  values of elements of specific tag names  value indices   Most  methods of the AccessMethod interface involve range lookups on  appropriate element B Trees     Cost Models  The cost functions of the  AccMCostModels interface follow the cost model of B Trees  For example  the cost  for descendant lookups depends on the average height of element  B Trees  whereas finding the    next    descendant is cheap due to  the linked list of B Tree leaves  Due to  RTN path storage  the  costForRTNPathRetrieval   is very low  Finally  the  implementation of  DBStatistics uses the  RTN path B tree and  statistical information regarding  attribute and text node values   For more detailed description of the cost and cardinality  estimation functions for our primitive access method  implementations please see  32     4 2 Physical Operator Families  Our framework allows the extensible deployment of physical  operators  To deploy a physical operator two interfaces must be  implemented  the Descriptor interface and the Physical Operator interface  The first defines the cost model of the physical operator  in terms of the cost of the primitive access method it invokes in its  implementation  The second is the interface for the actual  implementation  It extends the  Sequence interface and uses the  appropriate primitive access methods to access XML data  Note  that the input sequence can be any instance as long as it  implements the Sequence interface   this includes other Physical Operator instances  In our effort to capture a large variety of  existing XPath query processing  techniques  we implemented a  large number of physical operators  divided roughly into four     families     each inspired by a proposed XPath processing  technique  SortMerge  Staircase Join  12   PPFS  11   and  PathStack 13   Each family contains physical operators for most  navigational logical operators of XPAlgebra  some of which  incorporate novel processing algorithms  Moreover  we  implemented two physical operators for the vf operator  one that  simply filters the input sequence and another that retrieves  elements using a value index   pf and    pf physical operators  perform regular expression matching check on the RTN paths of  the input elements  Operators have a pipelined implementation  that outputs elements in document order without performing  explicit sorting     for that  we exploit the fact that our XPA access  methods allow fetching of elements in document order   SortMerge   The basic processing strategy is to traverse two  sequences of XML elements  left and  right  similarly to  SortMerge join  left is the sequence of elements given as input to  the algorithm  while right is a sequence in which output elements  need to be found  The algorithm keeps track of the current  elements on the  left and the right  attempting to find pairs that match the necessary structural condition  For the fp  bp and their  boolean counterparts  there are two versions of the operators  one  that evaluates the result sequence in    one step    making use of the  getRTNPath   method of the  Element interface and one that  doesn   t  If there exists an RTN path index  meaning that access to  the  RTN path of an element is very cheap  the former versions   pathSortMerge  will be very cheap  as in  11   The latter versions   nopathSortMerge  split the path into steps and for each step the  corresponding single step Nop or   Nop SM physical operator is  called  For a detailed description of the SM operators see  32     StairCase   Processing proceeds similarly to the previous case   only here searching takes place inside a window that is formed  according to the technique in  12   These algorithms perform  more lookups than those of the  SortMerge family  but search a  more narrow space of elements  PPFS  The basic strategy is to search a minimum window of  elements for each context element  similarly to indexed nested  loops join  11   This window is fetched from the underlying  storage as a result of the appropriate primitive access method call  that corresponds to the XPath axis under evaluation  As in SM   for fp  bp  and their Boolean counterparts there are two version of  PPFS physical operators  pathPPFS and no pathPPFS  one    holistic      and the other    step by step     Direct implementation of this  strategy does not guarantee that the result sequence will be sorted  in document order and free of duplicates  Document order and  duplicate elimination can be preserved without the need for a  blocking pre fetching phase  Depending on the axes  we either  skip context elements that fulfill certain requirements with respect  to previous context elements  cautious strategy  or we skip a  candidate result element just before it is output if it is known that  it has already been returned  The former strategy increases the  performance of the operators since it prevents redundant window  searching  For further details please see  32    PathStack   For  Fop  operators we use an implementation of  PathStack  13   We also use a novel algorithm for backward path  evaluation inspired by PathStack  PathStack is an algorithm used  for identifying all those elements of a given document which can  be reached via a given path  For example  it can return the  sequence of C elements that lie on the path   A B C  D C  To do  so  it maintains a stack and a stream for each one of the elements  on the given path  5 stacks and 5 streams in our example   Of  course  in our implementation a  stream is just a sequence of  elements   4 3 From Logical to Physical Plans  A logical plan is converted into a physical one by picking a  physical operator for each logical operator op from the available  pool of physical operators corresponding to op    Figure 4  Physical plan L1Q2   b   Example 3  Let   s return to Q2 of Example 2  The logical plan into  which  Q2 is directly translated is  L1Q2 shown in Figure 2 a     According to  12    Q2 could be evaluated with 13 staircase  structural joins  The physical plan that corresponds to that  approach derives directly from  L1Q2  by replacing the  Nop operators  fp   c   d  and    fp with the corresponding Staircase  physical operators  The physical plan that corresponds to the  PPFS approach is derived by replacing operators fp  c  d and   fp with the corresponding PPFS physical operators  pathPPFS fp will  be assigned to fp   etc  The best plan for L1Q2 is one that uses a  combination of the available physical operators  Indeed  the best  physical plan for  L1Q2  without applying any rewritings  as  produced during our experiments  Sec 7   is shown in Figure 4   For the tested XML datasets  the execution time of that physical  plan is more than 16  smaller than the time of the cheapest plan  adopting a single technique      5  PLAN GENERATOR  XPath has fairly complex semantics  which translate into a variety  of interactions between operators  In particular  simple properties  such as commutativity or associativity of operations are difficult  to systematically exploit in XPath  On the other hand  one can  describe    from scratch    a large number of processing strategies for  an XPath query  and can directly program such strategies  using  the available access methods  as provided by the XPA  To  algebraically capture the large variety of plans  we adopt a  rewriting based approach  We present next a comprehensive set  of rewritings that can produce  for each XPath query  a large set  of plans capturing virtually all the important existing processing  strategies for XPath  as well as plans corresponding to novel  processing strategies  The Rewriting Rules module is extensible  and can accommodate additional rules  as in  18       We first define some abbreviations and terms to help presentation  of the rules   S X   is a sequence of sequence operators  the  innermost of which is applied to sequence X  B stands for a plan  tree branch consisting of boolean operators  as shown in Figure  2 a    For a path p  lastTag p  is the last tag name appearing in p   A  context node of a sequence operator is a node of its input  sequence  while for a boolean operator it is its input node  The  context operator c ContextO s  of a sequence operator s  is the  input operator to  s  If  s is a  Nop  or  pf  then the last tag name  lastTag s  of s is lastTag p  where p is the path of s  If  s is not a  Nop  then lastTag s   lastTag ContextO s    The context operator  for a boolean operator  and its last tag name are defined similarly       Definition 1  The inverse path of a path p of an operator s that is  fp   bp    fp or   bp  denoted as    p  is defined as follows  a  set  path   lastTag ContextO s   concat  p minus lastTag p    and b   invert path  by starting from the right and switching each         and           to         and          and vice versa    Example 4  For expression fp  c   d ca root   the inverse path of the  path of fp  denoted as       c   d    is  invert a  c         c  a      Definition 2   The  inverse counterpart of a logical operator  op   denoted as    op is defined as follows      ca     pb   where b   lastTag ContextO ca      da     ab   where b  lastTag ContextO ca      fppath     bp   path     bppath     fp   path    vf n op v      vf n op v        pf path      pf path          ca   pb   where b  lastTag ContextO ca        da   ab   where b  lastTag ContextO ca        fppath   bp   path      bppath   fp   path      vf n op v    vf n op v          pf path    pf path     Example  For fp  c   d  ca  root        fp  c   d  ca  root        bp   c  a     For f ca root     fp  c   d         fp  c   d    bp   c  a       5 1 Rewriting Rules  5 1 1 Basic Rewriting Rules  The following algebraic relationships result from the definition of  the logical operators  The         is used for string concatenation    fpp S1 root   fpp2 fpp1 S1 root     p p1 p2   1   fpp S1 root    ca fpp1 S1 root      p   p1 a  a  tag name   2   fpp S1 root   da fpp1 S1 root     p p1   a  a  tag name   3   bpp S1 root   bpp2 bpp1 S1 root     p p1 p2   4   bpp S1 root   pa bpp1 S1 root      p  p1  a  a  tag name   5   bpp S1 root   aa bpp1 S1 root      p  p1   a  a  tag name   6   f f S1 root  BE1  BE2    f S1 root   BE1 BE2    7   f  S1 root   true    f S1 root     S1 root    8   pfp dm root     fpp root   where m  lastTag p    9   csp1  p2   S1 root    fpp2 bpp1 S1 root      10     bpp TRUE      pf   p   TRUE      11   f S1 root    pf   p BE1  BE2  pf   p f S1 root   BE1 BE2     12   5 1 2 Pushing Back Path Filter  When a pf operator takes as input a Bop operator  we can push pf before the Bop under certain circumstances  Its path becomes the  concatenation of the path of the initial  pf path with the inverse  path of the Bop  A similar rule holds for the boolean counterparts  of  pf and Bop    pf and   Bop  The condition  cond for applying  these rules is presented below    pfp2 Bopp1 S root      Bopp1 pfp2   p1 S root     if cond is TRUE   13     Bop p1     pfp2 BE       pf p2   p1     Bop p1   BE    if cond is TRUE   14   cond is TRUE if p1 contains no ancestor axes or all of the following hold   a  p1 contains a single ancestor axis and it   s located on the last step  b  p2 contains a single descendant axis and it   s located on the last step   c  p2 contains lastTag p2  once and replacing           in p2 with lastTag p2  results in a path that does not exist in the database  5 1 3 Starting from an Arbitrary Operator  Recall query  Q1 of Example 1 whose logical plan is shown in  Figure 2 a   This logical plan can be described as  retieve all    it     elements under the absolute path     s r   it     keep those having at  least one    to    descendant under relative path     mb m to    with text  value    x     For the    it    elements left  return their    k    descendants   We could reverse the execution of the query  return all    k     elements with at least one    it    ancestor  which in turn  i  has a    to     descendant under relative path     s r   it    with text value    x     and   ii  has a    s    document element ancestor via relative path       r s      This approach corresponds to plan  L2Q1 shown in Figure 5 a    There is a third alternative that corresponds to L3Q1 of Figure 5 b     Figure 5  Logical plans L2Q1  and L3Q1  The next rule allows the transformation of a plan so that  navigation starts from an arbitrary sequence operator of the  backbone  To present it  we need to define   The reverse         transformation    maps a sequence operator s to  a BoolExpr  as follows   s root    TRUE   s S root        s  S root     and  f S root   BE    BE  S root      can also be applied to a boolean operator subtree  B  starting  from a leaf     The result  denoted     B  is a  chain of sequence  operators derived by substituting boolean operators with their  inverse counterparts starting from    up until the root of B  filter operators are also added when branching points of B are met  A  detailed discussion of    B is omitted due to lack of space    Starting from an arbitrary Sequence Operator rule   S2 Nop S1 root      S2 f da root     Nop S1 root           where a  lastTag Nop   15   S2 f S1 root   B BE   S2 f    B  da root   BE  S1 root      where    is a leaf of B  and a  lastTag      16   Example 5  The logical plan  L2Q1  Figure 5 a   is derived by  applying rule 15 on the plan of Figure 2 a  starting from the last  sequence operator  Nop dk     L3Q1   Figure 5  b   is derived by  applying rule 16 starting from the leaf boolean operator    vf of  the filter operator  i e   B    fp mb m to   vf text   x            vf text   x   no BE  The logical plan in Figure 6 a  is generated from logical plan  L2Q1 via rule 11  By applying rule 14  since     s r  it    does not  contain           we get the plan of Figure 6 b   Finally  rules 12 and 9  give us the plan shown in Figure 6 d    Figure 6  Applying rules 11 a   13 b   12 c   9 d   sequentially  5 1 4 Flattening a filter  Under certain circumstances a filter can be replaced by a chain of  non filter sequence operators  A filter is flatten able if a  the plan  B of its BE argument is a chain of operators  b  all operators    of  B are   Nop except the last one that may be a   vf operator and c   none of the   Nop of B are recursive  i e   no   a or   d  and the  path for   fp  and   bp does not contains          or            Then     S2 f S1 root     Nop1      NopN   vfn op v         S2    Nop1       NopN vfn op v NopN    Nop2 Nop1 S1 root            17   Example 6  Rule application   f ca  root      fp c   d     vf text   x       bp   c a vf text   x  fp c   d  ca root           Figure 7  Logical Plan L2Q1  a  and physical Plan P2Q1  b   5 2 Generating Equivalent Physical Plans  The application of the rewriting rules leads to many equivalent  logical plans and  subsequently   physical plans  Application of  rules 15 and 16 generates as many logical plans as there are ways  to    start    the navigation     we call these basic logical plans  Rules  1 14 and 17 can be applied to each of these basic logical plans   As shown in Sec  4 3  for each such plan there are many physical  plans  Efficiently finding the best physical plan in this large  search space is discussed in the next Sec    Figure 8  Physical plans for Q3  a  and Q4  b   Example 7  Recall query Q2 from Examples 2 and 3  By applying  exhaustively the rewriting rules  the space of plans expands from  just  L1Q2 to 58 logical plans  Among these  consider plan  L2Q2 shown in Figure 7 a   Plan L2Q2 follows a significantly different  execution strategy than L1Q2  The best physical plan for Q2  out  of hundreds  for a specific XPA implementation  physical  operator pool  and dataset  is derived from  L2Q2 by an optimal  selection of physical operators  Figure 7 b    Its execution time is  38  less than the execution time of the best plan for L1Q2  see  Sec 7     Existing XPath XQuery algebras do not have rewritings that can  produce a logical plan that follows the basic navigation steps of  plan L2Q2  Note that a plan similar to L2Q2 is not always the best  solution for queries    similar    to Q2  For the two similar queries    s r na it mb m to  de    e   k    Q3  and  s r na it mb m to  de    k  p li p li  b e   Q4   the best physical  plans turn out to be the ones shown in Figure 8 a  and  b    respectively  These two plans cannot be produced by existing  XPath XQuery algebras and rewriting rules      6  SELECTING THE BEST PLAN  Our cost based plan selection algorithm  PSA  is a hybrid  its  core is a rewrite based algorithm  as in  19   that applies the  rewritings of Sec  5 1 in a targeted way to generate only the  cheapest plan  The targeted application of rewritings involves a  stepwise    traversal    of the backbone path of the query  The  algorithm evaluates a number of plans in every such step  and  uses memoization to avoid recomputing their cost if it needs to  consider them again to build plans for the next step of the  traversal  Before presenting PSA we present briefly our approach  to cardinality and cost estimation    6 1 Logical Operator Cardinality Estimation  The cardinality estimate for each sequence operator equals   roughly  the cardinality of the input sequence multiplied by a  factor whose computation is based on statistics provided by the  DBStatistics interface  For Nop operators this factor is returned by  the Occurrence   method  For the filter operator this factor is the  probability that an element of the input sequence survives the  filtering  This probability is the product of the selectivities of all  boolean operators directly attached to the filter   The selectivity of  a boolean operator is computed  recursively  it equals the  probability that an input element passes the filtering criteria of the  boolean operator  called  partial selectivity  multiplied by the  selectivities of all boolean operators attached    underneath    in the  operator tree  For    Nop operators the partial selectivity is  computed by the Selectivity   method of the  DBStatistics   Cardinality and partial selectivity estimation for  vf and    vf respectively involve the  DistinctValues   method  similarly to  selectivity estimation of selection operators in relational systems    Example 8  The estimated cardinalities of the logical operators of  L1Q1  shown in Figure 2  a   as derived by the Physical Operator  Descriptors during the experiments presented in Sec 7  are   OUT fp s r   it     1 Occurrence     s r   it  21750   OUT f   OUT fp s r   it      SEL   fp mb m to     21750  Selectivity  s r   it    mb m to  SEL   vftext   x   21750 0 96  1 DistinctValues to text     3 8   OUT dk  OUT f   Occurrence  s r   it   k   3 8 4 15 2     6 2 Physical Operator Cost Estimation  Each physical operator has its cost model implemented in its  descriptor  which it applies when asked by the optimizer to return  its cost  All physical operators compute their cost using the cost  methods of the primitive access methods they invoke in their  implementation  The cost model of a physical operator may also  use database statistics from the  DatabaseStatistics interface  as  well as information taken by the logical plan it belongs to  for  example  cardinality of the context operator or cardinality of the  logical operator it implements  Let   s consider the cost models of  two physical operators of the  fp  the  pathPPFS and the  path SortMerge  In what follows the following variables are used   pathPPFS fp cost model  The    holistic    version of the PPFS based  fpp  makes a descendant lookup on  lastTag p  elements for each  element of the input sequence  For each lookup  it consumes  lastTag p  descendants  checking whether they conform to path p 1  thus  their  RTN path is retrieved and subjected to a regular  expression matching check   Cost c is computed as follows   c OUT cop   c1 Occurence cp   lastTag p     c2 c3    pathSortMerge fp cost model  The    holistic    version of the  SortMerge based  fpp  reads all elements of tag name  lastTag p    When such an element is found to be a descendant for some  context element  it is checked whether it conforms to path p  The  cost formula is the following   c c1 Cardinality   lastTag p   c2                          OUT cop  Occurence cp   lastTag p   c3  For specific XPA driver implementations  some physical operator  implementations are  cost dominant  For example   pathPPFS fp  dominates no pathPPFS fp for the driver described in Sec  4 1 2   6 3 Marking Rules As Heuristics  Even though the algorithm can apply cost comparisons to all  rewritings derived by the rules   some rule applications can be  shown to always lead to better plans  Such rules are called  heuristics and can be marked as such in the Plan Selector  which  means they are applied always  see Algorithm 2   Rules 1 8 fall in  this category  For the specific XPA Driver and API  implementation presented  rules 9  11  13 and 14 are also always  beneficial  In general  rules will be marked as heuristics if they  are  almost  always beneficial given the physical data model and  storage and the implementation of the primitive access methods   Rules not marked as heuristics are applied only if their resulting  plan has a lower cost estimate than the current plan   6 4 Physical Plan Selection Algorithm  The overall goal of the PSA algorithm is to pick the cheapest plan  without generating examining a large number of plans  It achieves  this goal with very targeted incremental plan generation  and the                                                                   1 The implementation guarantees that the result is duplicate free and in doc orderuse of memoization of some subplans and their costs  We start by  defining some terms  Given a logical plan p  it can be represented  as p   Sp ip   where we call ip the inner plan of p and Sp the body of p  Clearly  the choice of Sp and ip is not unique  Moreover  p  can be divided into as many  levels as the number of sequence  operators in the backbone path  starting from level 0  We call  prefix of level i  denoted as pi   the part of the query  and  by abuse  of terminology  of the default plan  up to level i   The operation of PSA is shown in Algorithm 1  The function  takes as input an XPath expression and returns the estimated  cheapest physical plan  The XPath expression is parsed and  translated into a logical plan  as described in Sec 3 3  line 2   The  algorithm reads the sequence operators of the backbone of the  default plan one at a time  do while loop in lines 6 22   After each  iteration  a set of physical plans that are equivalent to the query  prefix of the current level is computed  denoted as  equivalentPlansi    and  best i  points to the cheapest plan in  equivalentPlansi   For a query of k levels  after k iterations bestk  is  returned  line 23   Each iteration creates a prefix pi of the current  level  i  We divide  pi into a body SPi  that includes only the  currently read sequence operator s and an inner plan iPi that is the  estimated best plan of the previous level  line 9    SPi   which is  equal to s  is optimized by the function optimize    Algorithm 2     Optimize   takes as input a chain of sequence operators  S and  applies rewriting rules 1 14 greedily until no more rules apply   Each  heuristic application creates a new plan that replaces  S   otherwise the new plan is kept if its cost estimate is lower  For  each logical operator of the result  S  PSA assigns the physical  operator that returns the lowest estimated cost  Note that  additional rewritings rules  apart from 1 14  that consume a chain  of sequence operators and transform them into another operator or  chain of operators could be deployed affecting only Optimize      After Optimize  Algorithm 1 calls startFromOperator    line 12    shown in Algorithm 3   This function creates one or more  equivalent plans according to rewriting rule 16 or 15 depending or  whether the sequence operator s corresponding to the current level  is a  Filter or not  Algorithm 1 then checks whether rule 17 is  applicable to  pi   if s is a Filter that can be flattened  rule 17 is  applied  see Sec 5 1 4  and a new plan is generated in addition to  the existing plan  i e  plan cloning   Rule 17 is not applied if the  current level is 1  i e   if  filter is the second operator in the  backbone of the default logical plan  because the result of the rule  will coincide with a plan already generated by rule 16   Algorithm 1 then calls interactsWithInnerOrAnEquivalent    lines  15 and 18 respectively  with the plan generated by the application  of rule 17  p  and pi   This function  whose pseudocode appears in  Algorithm 5  identifies possible    interactions    between the first  operator of its input plan p and the last sequence operator of the  inner plan of  p  ip that can lead to cheaper plans  Note that  Algorithm 1 has already set inner plan ip and body Sp for the plan p that it passes to the function  Moreover  note that ip is the best  plan for some level l of the query  line 9 of Algorithm 1   and it  has associated with it a set of equivalent plans  equivalentPlanl   Function  interactsWithInnerOrAnEquivalent   decides whether  the first operator of  Sp  and the last operator of  ip or some  p  in  equivalentPlanl can be profitably combined using one of rules 1  14  lines 6 15  and whether that results in a cheaper plan for  p   line 16   This function may introduce new operators in the plan   such as cousin  cs    Algorithm 3 applies rewriting rules 15 and 16 starting from a  particular operator  If the operator is not a filter   lines 22 to 29 of  Algorithm 3   a single plan is generated starting with a d operator  with tag name lastTag s  and followed by a filter operator whose  BoolExpr is the result of the reverse         transformation  see Sec  5 1 1  applied on the current query prefix  where  s is the  first outermost operator   The  reverse         transformation is  returned by the Reverse   method  shown is Algorithm 4  which  returns the reverse         of the  prefix of the given level  it  computes the reverse of a level only the first time it is required    Returning to Algorithm 3  if s is a filter operation  lines 3 to 20    as many plans are generated as is the number of leaf operators    of the  BoolExpr of  filter s  For each such     the corresponding  plan starts with a d operator with tag name lastTag     followed  by    B  This is the reverse of the subtree B to which    belongs   Recall that    B  is a chain of sequence operators  Sec 5 1 3   A  new  Filter operation follows    B  lines 14 17   Its  BoolExpr is  the conjunction of the set of boolean operators returned by the  union of the root boolean operators of Filter s  without the root of  B   with the result of the reverse transformation applied on the  prefix of the previous level   pi 1   as returned by the  Reverse   method  line 13    Figure 9  Initial logical  a  and best physical plan  b  for Q5  Example 8  Suppose XPath query     s r   it m mb to x   k ancestor  li parent  p       Q5   The initial logical plan of Q5 is shown in Figure 9 a   The query  is separated into four levels  from level 0 up to level 3   Therefore  Algorithm 1 will execute the following four iterations   Iteration 0  Prefix  p0 is generated with no inner plan  The  optimize   function assigns  path SM fp physical operator  and   because neither startFromOperator   nor filter flattening generate  equivalent plans   best0 points to  p0   In Figure 10   p0 is shown  within the first iteration frame  although  in fact  is created when  first required during next iteration    Iteration 1  Prefix p1 has as inner plan what  best0 points to   namely p0   and a Filter as its body  We name B the single filter  branch  Function  startFromOperator    generates a plan which  through rewriting rules 11 and 12 ends up to  p1 2    p1 is also  created since it was asked by method  startFromOperator     Flattening does not occur because the Filter operator corresponds  to level 1  Among the two equivalent plans of that level  p1 2 has  the lowest cost and therefore is what best1 points to     Iteration 2  Prefix  p2 has as inner plan what  best1 points to   namely p1 2   and a d as its body  When p2  passes as argument to  interactsWithInnerOrAnEquivalent    it turns out that  d interacts  with the  bp of the  p1 2 giving a  cs operator  This transformation  proves to be worthy and thus is applied  Function  startFromOperator    generates a plan which is transformed into  p2 2 through rewritings 11  12 and 9 and after having applied 14 to   p2  At the end of the iteration  best2 points to p2    Iteration 3  Prefix p3 is created with p2 as its inner plan and a Filter as body  It is subjected to the following transformations     bp  switches to    pf   rewriting rule 11      pf  is pushed out of Filter   rewriting rule 11  and the    empty    Filter is eliminated  rewriting  rule 8   The  startFromOperator    generated an equivalent plan  which after transformation ends up to p3 2  The cost of this plan is  higher than the cost of p3 and therefore best3 points to p3   Since this  is the last iteration  p3  will be the plan that the algorithm returns as  the best physical plan for Q5         Figure 10  Selecting best plan  example  8  7  EXPERIMENTAL EVALUATION  Experiments were conducted using three XMark datasets  20   of  size 116MB  570MB  1 2GB and the ProteinDB dataset  21  of size  700MB  They were run on an Intel Core 2 Duo 2 67GHz with 2GB  of RAM  running MS Windows XP SP3  Our XPath query  optimization and processing system are implemented in Java  JDK  1 6   When execution times are reported  every query plan was  executed three times  and the last two measurements were averaged   Hence  query execution used a warm cache  With a cold cache all  the results were substantially the same and are not presented due to  lack of space    Table 2  Full Pool of Physical Operators  Table 3  Limited Pool of Physical Operators  Our plan selection algorithm PSA is shown in Sec  6 4  We also  implemented an algorithm that generates all possible physical plans  and computes their cost estimate  called GAPH  Generating All  Physical Plans  according to the method described in Sec 5 2  The  XPA API implementation  driver  used is the one described in Sec   4 1 2  For experiments presented in Sec  7 1  we deployed a total of  42 physical operators   4 per  Nop and  2 per    Nop  operator    illustrated in Table 2   For experiments of Sec  7 1 1 and 7 1 2 we  limited the number of operators to 28   2 per  Nop  and    Nop  operator   illustrated in Table 3  by omitting  cost dominated implementations  for the specific XPA driver   in order to decrease  the total number of physical plans generated by GAPH making their  execution feasible  The queries of the workload along with the  number of basic plans  see Sec 5 2   the number of logical plans and  the total number of physical plans  generated by GAPH with all the  42 physical operators deployed  for each query appear in Table 4   We created two value indices  see Sec  4 1 2 and 4 2   for nodes       from text      and      item  id     for all the three XMark databases   The experimental evaluation illustrates the efficiency of PSA and  the effectiveness of both PSA and our XPath query optimization and  processing system  and the benefit of our rewriting rules     Table 4  Queries for XMark Q1 19  and Protein P1 7  Dataset  7 1 Evaluating the Plan Selection Algorithm  7 1 1 Efficiency  We measured a  the overhead PSA adds to query execution time  and b  the performance difference between PSA and GAPH  brute  force  in picking the cheapest estimated plan    Figure  11  Overhead of PSA  Figure 11  a  and  b  show in the y axis the   ratio of the execution  time of PSA to the execution time of the chosen plan  Our query set  for Figure 11 a  includes queries  Q1 Q5 that have increasing  numbers of predicates  while in  b  queries Q6 Q10 have increasing  number of    switchbacks    between forward and backward  navigation  as shown in the x axis  Number of predicates and  number of switchbacks of navigation direction are used as  indications of query complexity  For the 570MB XMark database  PSA takes less than 1  of execution time even for the most  complex queries of our query set    Table 5 shows the PSA and GAPH execution times  for the 570MB  XMark dataset  as the number of predicates and    switchbacks     between forward and backward navigation increase  Q1 Q5 and Q7  Q10  respectively   PSA is the plan selection algorithm of choice  even for queries with small complexity  and clearly dominates  GAPH for more complex queries  The ratio of execution time of  PSA to execution time of GAPH ranges from 0 94  for non predicated  fp query  Q1  to 0 14  Q3 with three predicates  and  0 0001  Q5 with four path predicates     Table 5  Comparing GAPH with PSA  7 1 2 Effectiveness  The effectiveness of PSA depends on whether it picks the cheapest  estimated plan in the large plan space defined by the rewriting rules  and the physical operators  To evaluate effectiveness  we use GAPH  to generate all physical plans and their cost estimates and compare  that to the cost estimate of the PSA selected plan  For all queries in  our query sets  and all datasets   PSA picked the plan with the  lowest estimated cost   0 200 400 600 800 1000 1200 1400 Q1 Q2 Q3 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 msec  a  XMark 116MB  0 1000 2000 3000 4000 5000 Q1 Q2 Q3 Q6 Q7 Q8 Q9 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 msec  b  XMark 570MB   c  XMark 1 2 GB  0 10000 20000 30000 40000 50000 60000 70000 P1 P2 P3 P4 P5 msec  d  Protein Figure 12  Evaluating effectiveness of PSA  We also measured the effectiveness of our end to end cost based  optimization and execution engine  including the robustness and  precision of cost models and statistics estimation algorithms for  access methods and physical operators  For each query  we run all plans generated by GAPH and plot a  the execution time of the plan  selected by PSA b  the execution time of the best plan c  the  execution time of the best plan among those corresponding to the  default logical plan  and the execution time of the worst plan  As  illustrated in Figure 12  for the vast majority  over 80   of queries  PSA chooses a plan whose cost is less than 5  above the cost of the  actual best plan  Often  the selected plan is the fastest  Q18 is the  one exception  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_query_processing_on_semi-structured_data"/>
        <doc>ROX  The Robustness of a Run time XQuery Optimizer Against Correlated Data  ###  Riham Abdel Kader  1   Peter Boncz    2   Stefan Manegold    3   Maurice van Keulen  4   University of Twente Enschede  The Netherlands 1 r abdelkader utwente nl 4 m vankeulen utwente nl     CWI Amsterdam  The Netherlands 2 P Boncz cwi nl 3 Stefan Manegold cwi nl Abstract    We demonstrate ROX  a run time optimizer of XQueries  that focuses on    nding the best execution order of XPath steps and relational joins in an XQuery  The problem of join ordering has been extensively researched  but the proposed techniques are still unsatisfying  These either rely on a cost model which might result in inaccurate estimations  or explore only a restrictive number of plans from the search space  ROX is developed to tackle these problems  ROX does not need any cost model  and defers query optimization to run time intertwining optimization and execution steps  In every optimization step  sampling techniques are used to estimate the cardinality of unexecuted steps and joins to make a  ###  decision which sequence of operators to process next  Consequently  each execution step will provide updated and accurate knowledge about intermediate results  which will be used during the next optimization round  This demonstration will focus on   i  illustrating the steps that ROX follows and the decisions it makes to choose a good join order   ii  showing ROX   s robustness in the face of data with different degree of correlation   iii  comparing the performance of the plan chosen by ROX to different plans picked from the search space   iv  proving that the run time overhead needed by ROX is restricted to a small fraction of the execution time  I  MOTIVATION FOR ROX In their search for a good execution plan  relational compiletime optimizers rely on pre collected statistics and an accurate cost model to estimate the selectivity of operators  The accuracy of these estimations  although satisfying for a single operator  exponentially degrades through the plan resulting  in the case of big queries  in the execution of bad plans  Additionally  to simplify the cardinality estimation problem  these optimizers assume the attribute value independence heuristic  which does not hold in real life data  resulting in wide errors in estimations and rendering optimizers helpless in the face of correlation  Some databases solve this problem by creating indeces on multiple columns or collecting group column statistics  however  the challenge remains in knowing beforehand which columns are correlated and in storing and maitaining the statistics  In the XML context  data and query operators are more complex than relational ones because of the structural nature and expressiveness of the language  therefore cost models developed for XML are simply not available or  if present  by far less accurate than their relational counterparts  turning the    eld of cardinality estimation in the context of XQuery into deadly optimizer quicksand  Moreover XQuery accesses data through the fn doc url   thus potentially with a    table    name that is computed at runtime  In such cases  static information to guide a query optimizer cannot be available  To overcome the shortcomings of compile time optimizers  adaptive query processing techniques have been developed  Some of these apporaches  like  1      rst generate a potential good plan and re optimizes it when the observed cost exceeds static estimations  The quality of choices made by these techniques still highly depend on the quality of collected statistics and cost model  and therefore can not spot  early enough  opportunities where the existence of correlations can speed up query evaluation  To overcome this problem  some approaches monitor the performance of query execution and feedback the observations to the optimizer to adjust the cost model and statistics  2   however such optimizers are very complex modules  Routing based techniques  3  optimize a query by routing each tuple to the most ef   cient sequence of operators based on observed statistics  The drawback of such techniques is that they require the presence of symmetric operators  can only cover a restrictive number of alternative plans  and suffer from the overhead of maitaining query execution states  The ROX approach does not depend on any collected statistics or cost model  It defers optimization to run time  interleaving sampling based optimization decisions and execution steps  gaining at each iteration better knowledge about intermediate results characteristics and consequently which execution strategy to follow  II  THE RUN TIME XQUERY OPTIMIZER A  Join Graph ROX  4  fully integrates plan optimization with processing by iteratively switching between optimization and execution steps  It takes as input a join graph which is a representation of the joins and XPath steps in the XQuery without any implication on their order of execution  Therefore  the application of ROX is preceded by a compilation phase whichroot conference1 xml root conference2 xml root conference3 xml root conference4 xml author author author author text   text   text   text                                   Fig  1  Join Graph of the 4 way join XQuery consists of XQuery parsing  normalization  compilation  peephole driven optimization and    nally join graph extraction  The    rst four steps  described in  5   result in a DAG shaped plan of relational operators  The join graph isolation process  6  aims at separating joins from blocking operators by pushing the latter above the joins creating a plan tail whose execution ensures XQuery semantics  duplicate free and required order   This results in a cluster of joins  selections  and projections forming the to be optimized join graph  Fig  1 shows the join graph of the following XQuery  for  a1 in doc    conference1 xml      author   a2 in doc    conference2 xml      author   a3 in doc    conference3 xml      author   a4 in doc    conference4 xml      author where  a1 text      a2 text   and  a1 text      a3 text   and  a1 text      a4 text   return  a1 The vertices in a join graph represent index selectable sets of element  text  and attribute nodes  The edges specify all XPath step and join relationships between the nodes  A step join between two relations s1 and s2 is depicted by an edge s1      ax s2 where the label ax de   nes the axis of the step  The circle           denotes the direction of the step  i e   which of the two relations represents the context node sequence of the step  Note that the direction is only a representational issue  the algorithm may very well decide to execute it in the reverse direction  A relational join between s1 and s2 is depicted as s1     s2  The dotted edges in Fig  1 represent join equivalences  and are added by ROX to broaden the search space of plans allowing for more    exibility to    nd a good plan  B  Operators and Index Structures As an XML database backend  we use the open source system MonetDB XQuery  7   In MonetDB  XML documents are shredded into relational tables using a range based pre post encoding representation  That is  every XML node is stored in a separate relational tuple  and is referred to using the node identi   er pre  An advantage of the adopted range encoding is that XPath axes can be evaluated with only standard relational operators  It has been proved  however  that performance gain is possible if a tree aware operator is used  7   As a consequence  the XQuery module of MonetDB has extended its relational algebra with the staircase join operator  a structural join capable of exploiting the tree properties of the pre post plane  The staircase join can evaluate with linear complexity all XPath axes by making at most a single sequential pass over the document  returning duplicate free  in document order results  Note that the ideas in ROX can be used with other operators and do not require the presence of a staircase join  In addition  MonetDB XQuery implements an element index and a value index that covers all text and attribute values  All index lookup operations provide a list of node identi   ers  duplicate free and in do</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#qpp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_query_processing_on_semi-structured_data"/>
        <doc>Matrix  Bit loaded  A Scalable Lightweight Join Query Processor for RDF Data  ###  Medha Atre       Vineet Chaoji       Mohammed  J  Zaki       and James A  Hendler         Dept  of Computer Science  Rensselaer Polytechnic Institute  Troy NY  USA  atrem  zaki  hendler  cs rpi edu     Yahoo  Labs  Bangalore  India chaojv yahoo inc com ABSTRACT The Semantic Web community  until now  has used traditional database systems for the storage and querying of RDF data  The SPARQL query language also closely follows SQL syntax  As a natural consequence  most of the SPARQL query processing techniques are based on database query processing and optimization techniques  For SPARQL join query optimization  previous works like RDF 3X and Hexastore have proposed to use 6 way indexes on the RDF data  Although these indexes speed up merge joins by orders of magnitude  for complex join queries generating large intermediate join results  the scalability of the query processor still remains a challenge  In this paper  we introduce  i  BitMat     a compressed bit matrix structure for storing huge RDF graphs  and  ii  a novel  light weight SPARQL join query processing method that employs an initial pruning technique  followed by a variable binding matching algorithm on BitMats to produce the    nal results  Our query processing method does not build intermediate join tables and works directly on the compressed data  We have demonstrated our method against RDF graphs of upto 1 33 billion triples     the largest among results published until now  single node  non parallel systems   and have compared our method with the state ofthe art RDF stores     RDF 3X and MonetDB  Our results show that the competing methods are most e   ective with highly selective queries  On the other hand  BitMat can deliver 2 3 orders of magnitude better performance on complex  low selectivity queries over massive data  Categories and Subject Descriptors H 2 4  Systems   Query Processing General Terms Algorithms  Performance  Experimentation 1  ###   INTRODUCTION Resource Description Framework  RDF  1   a  W3C standard for representing any information  and SPARQL 2   a query language for RDF  are gaining importance as semantic 1 http   www w3 org TR rdf syntax grammar  2 http   www w3 org TR rdf sparql query  Copyright is held by the International World Wide Web Conference Committee  IW3C2   Distribution of these papers is limited to classroom use  and personal use by others  WWW 2010  April 26   30  2010  Raleigh  North Carolina  USA  ACM 978 1 60558 799 8 10 04  data is increasingly becoming available in the RDF format  RDF data consists of triples where each triple presents a relationship between its subject  S  and object  O   the name of the relationship is given by the predicate  P   and the triple is represented as  S P O   Such an RDF data can be represented as a labeled directed graph and it can be serialized and stored in a relational database simply as a 3 column table where each tuple in that table represents a triple in the original RDF graph  RDF is extensively being used for representing data from the    elds of bioinformatics  life sciences  social networks  and Wikipedia as well  Since disk space is getting cheaper  storing this huge RDF data does not pose as big a problem as executing queries on them  Querying these huge graphs needs scanning the stored data and indexes created over it  reading that data inside memory  executing query algorithms on it  and building the    nal results of the query  Hence  a desired query processing algorithm is one which  i  keeps the underlying size of the data small  using compression techniques    ii  can work on the compressed data without uncompressing it  and  iii  doesn   t build large intermediate results  A lot of work has already gone in using compression techniques in storing the data in a column store database as well as trying to work on the compressed data without uncompressing it by lazy materialization  2  4   In this paper  we go one step further and propose a compressed bitcube of RDF data and a novel SPARQL join query processing approach which always works on the compressed data by producing the    nal results in a streaming fashion without building intermediate join tables  A SPARQL join query  which can also be viewed as a Basic Graph Pattern Matching  BGP  query  or a conjunctive triple pattern query  resembles closely to an SQL join query  in fact any SPARQL join query can be systematically translated into an SQL join query  9    A typical SPARQL join query looks like the one shown in Figure 1  This query shows a join between three triple patterns  Such join queries can be broadly classi   ed into three categories  The    rst type is     queries having highly selective triple patterns 3   E g   consider the query   s  residesIn USA   s  hasSSN    123 45 6789      Since SSN is a unique attribute of a person  the second triple pattern has only one triple associated with it thereby being highly selective  The second type is     queries having triple patterns with low selectivity but which generate few results  i e   highly selective join results  E g   consider a multi national orga  3 Selectivity of a triple pattern is low if there are more number of triples associated with it and vice versa SELECT   WHERE      m rdf type  movie    n rdf type  movie   SPARQL join query  m  similar to  n Equivalent SQL join query Note  RDF graph stored as tripletable WHERE A subject   B subject tripletable AS A  tripletable AS B  tripletable AS C SELECT   FROM AND A object   C subject AND B predicate    rdf type  AND A object     movie  AND A predicate     similar to  AND B object     movie  AND C predicate    rdf type   Figure 1  An example of SPARQL join query nization BigOrg  having employees on the order of few millions all over the world  Now consider a densely populated country like India having population close to 1 2 billion and consider a query like   s  residesIn India   s  worksFor BigOrg   Although the number of triples associated with the two triple patterns is quite high  their join will produce much fewer number of results as there are only a few employees of BigOrg in India  The third type of queries are the ones having low selectivity triple patterns and low selectivity join results  i e   the ones generating a lot of results  For instance  a modi   cation of the    rst query given above  to    nd SSNs of all the people   s  residesIn USA   s  hasSSN  y   Most of the systems which generate various indexes on the data do well on the    rst type of queries     highly selective triple patterns  Especially having 6 way indexes helps in picking the right set of triples at the beginning  avoiding scanning a large amount of data  For the second type of queries     low selectivity triple patterns  but highly selective join results     systems using join selectivity estimation or pre computed join tables indexes get bene   ted to a certain extent  Although as shown in our experiments  join selectivity estimation does not always help in improving the query performance in case of complex joins involving lowselectivity intermediate join results  For the third type of queries     low selectivity triple patterns generating a large number of results     even the state of the art systems run into problems  as shown by our evaluation   Although disk space is growing at a much faster speed  the available main memory still remains very small compared to it  This creates the main bottleneck while executing queries of the second and third type mentioned above  Hence our goal is to build a scalable query algorithm which operates on the compressed data  Second our goal is to not generate intermediate join tables  thereby keeping the memory footprint of the system small  hence light weight   and cope well with the second and third type of queries mentioned above  Our key contributions in this work are  1  A compressed data structure for RDF data     called BitMat     to increase the size of the data that can    t in memory  2  A novel algorithm that performs e   cient pruning of the triples during the    rst phase of SPARQL join query execution and in the second phase  performs variable binding matching across the triple patterns to obtain the    nal results  both phases use compressed BitMats without any join table construction   3  Procedures and algorithms implemented to work on the compressed data directly  4  Experiments on very large RDF graphs     845 million and    1 33 billion  using a set of queries published on the web by the owners of the datasets  showing a comparison with the state of the art RDF storage systems     RDF 3X and MonetDB  Our results indicate that competing methods are much better on high selectivity queries  whereas our method outperforms them by 2  3 orders of magnitude on complex queries with lowselectivity intermediate join results  Work presented in this paper is a considerable extension of our preliminary work outlined previously in  5   2  RELATED WORK RDF data can be serialized and stored in a database and a SPARQL join query can be executed as an SQL join  hence recently a lot of database join query optimization techniques have been applied to improve the performance of SPARQL join queries  Notably  in the past couple of years  C Store  3   RDF 3X  16   MonetDB  21   and Hexastore  24  systems have proposed ways of optimizing SPARQL join queries  Out of these systems  C Store and MonetDB exploit the fact that typically RDF data has much less number of properties  predicates   thereby vertically partitioning the data for each unique predicate and sorting each partition  predicate table  on subject  object order  creating a subject object index on each property table   Hexastore and RDF 3X make use of the fact that an RDF triple is a    xed 3 dimensional entity and hence they create all 6 way indexes  SPO  SOP  PSO  POS  OPS  OSP   Although Hexastore does share common indexes within these 6 indexes  e g   SPO and PSO share the    O    index  without any compression  it su   ers from 5 fold increase in the space required to store these indexes  RDF 3X goes one step further and compresses these indexes as described in their paper  15   RDF 3X also implements several other join optimization techniques like RDF speci   c Sideways Information Passing  selectivity estimation  merge joins  and using bloom    lters for hash joins  Along with these systems  there are other systems being developed for RDF data storage and querying  such as  JenaTDB  1  and Virtuoso  10   Jena TDB faces scalability issues while executing queries on very large datasets  Along with these  BRAHMS  11  and GRIN  22  focus more on path like queries on RDF data  typically which cannot be expressed using existing SPARQL syntax  Most systems built to store and query RDF data typically use a left deep join tree which requires materialization of the intermediate join results in case of a complex join query involving several join variables  Merge joins cannot always be used while performing later joins  especially when the join column of an intermediate result is not sorted  In contrast to these  in our system instead of using sophisticated join optimization techniques  we have followed a simple rule of keeping the data compressed without materializing the intermediate join results  This helps to keep a large amount of required data in memory  We execute the join by following a novel algorithm  which propagates the constraints on the join variable bindings among di   erent join variables in the query  Our technique is reminiscent of the concept of semijoins  7  6  as discussed further in Section 4 1  We consider our query processing engine light weight     light weight on runtime memory consumption as well as optimization techniques  We have shown results by analyzing where our system outperforms the state of the art systems like RDF 3X and MonetDB S   dimension 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 O   dimension P   dimension 0 0 1 0 0 0 0     0 0     0 1     1 0     0 0     0 0     0 0     0 1     1 0     0 1     0 0     1 0     0 0     1     0    0 0     1     0    0 0     0     0    1 1     0     0    0 1 2 2 1 3 3  1999   the thirteenth floor  1999  Subject Predicate  the matrix  releasedIn  releasedIn  the thirteenth floor  the matrix rdf type rdf type  movie  movie Object  the thirteenth floor  similar to  the matrix  the matrix  similar to  the matrix reloaded a b c d  the matrix  the thirteenth floor S   O and O   S BitMats for Ps Note  a    the matrix  b    1999   c    movie  d    the matrix reloaded  releasedIn a c     d b  similar to a c     d b 0     0     1    0 0     0     1    0 rdf type a c     d b Transpose S   O O   S S   O O   S S   O O   S Figure 2  Example of S O  O S BitMat construction procedure for each P 3  BITMAT CONSTRUCTION Figure 2 shows sample RDF data and gives a pictorial representation of constructing a BitMat  Let Vs  Vp and Vo denote the sets of distinct subjects  predicates  and objects  respectively  in the RDF data  This RDF data can be represented by a 3D bit cube  where each dimension of the bitcube represents subjects  S   predicates  P   and objects  O   The volume of this bitcube is Vs    Vp    Vo  Each cell in the bitcube represents a unique RDF triple that can be formed by the combination of S  P  O positions which are the coordinates of that bit  A bit set to 1 denotes presence of that triple in the given RDF data  This 3D bit cube is sliced along a dimension to get 2D matrices  Figure 2 shows slicing along the P dimension  which gives S O bit matrices  BitMats   Inverting an S O BitMat gives an O S BitMat  We store these S O and O S BitMats for each P value  In all we get  Vp  such S O and O S matrices  Additionally  we slice the bitcube along S and O dimensions which gives P O and P S BitMats respectively  Note that we do not store inverted O P and S P BitMats  since based on our experience  usage of those BitMats is rare  and even if needed their construction from the corresponding PO or P S BitMat is easier due to the relatively few number of predicates in typical RDF data  In all we have  Vs  P O BitMats and  Vo  P S BitMats  To summarize  for each P value we have a S O and an  O S BitMat   for  each  S  value      a P O BitMat  and for each O value     a P S BitMat  in all 2 Vp     Vs     Vo  such BitMats   There are total  Vs       Vp      Vo  possible triples with the given Vs  Vp  and Vo sets  But it is observed that typically RDF data contains much fewer number of triples  hence the S O  O S  P S  P O BitMats are very sparse  We make use of this fact by applying gap compression on each bit row of these four types of BitMats  In gap compression scheme  a bit row of    0011000    will be represented as     0  2 2 3     That is  starting with the    rst bit value  we record alternating run lengths of 0s and 1s  We also store the number of triples in each compressed BitMat  this statistics is useful while executing our query algorithm as described later   Along with this  we store two bitarrays     row and column bitarray     which give a condensed representation of all the non empty row and column values in the given BitMat  For example  in Figure 2  for the S O BitMat of     similar to    predicate  marked by BitMat    2    in Figure 2   we store row bitarray    1 1    and a column bitarray    1 0 0 1     and for the O S BitMat we store row bitarray    1 0 0 1      and column  bitarray    1 1    respectively   These bitarrays are useful while performing    star join    queries  as elaborated in the Evaluation section   We store the compressed S O  O S  P S  P O BitMats in one    le on the disk and maintain a meta    le which gives the o   set of each BitMat inside the BitMat    le  Due to this  addition or deletion of the triples might require moving a large amount of data  but if bulk updates are expected on the RDF data  all the BitMats can be rebuilt at once since the BitMat construction time even for very large data is very small  as shown at the end of the Evaluation section   The above construction reveals that each unique S  P  and O is mapped to a unique position along each dimension and this position can be represented as an integer ID  We decide this mapping with the following procedure  Let Vso represent the Vs     Vo set  Each element in Vso  along with the elements in Vs  Vp and Vo  is assigned an integer ID as follows      Common subjects and objects  Set Vso is mapped to a sequence of integers  1 to  Vso       Subjects  Set Vs     Vso is mapped to a sequence of integers   Vso    1  to  Vs       Predicates  Set Vp is mapped to a sequence of integers  1 to  Vp       Objects  Set Vo     Vso is mapped to a sequence of integers   Vso    1  to  Vo   The common subject object identi   er assignment facilitates the bitwise operations in join queries wherein an S position in one triple pattern is joined over an O position in another triple pattern  e g   n in the query in Figure 1   For the present considerations  we do not handle joins across S P and P O dimensions  Such queries are rare in the context of assertional RDF data  None of the benchmark queries published for the large RDF datasets have queries having joins over S P or P O dimensions  Hence overlapping S  P  O IDs except for the common S and Os do not pose a problem while processing a query  With respect to the construction described above  the RDFCube  14  system is conceptually closest to BitMat  RDFCube also builds a 3D cube of S  P  and O dimensions  However  RDFCube   s design approximates the mapping of a triple to a cell by treating each  cell as a hash  bucket containing multiple triples  They primarily used this as a distributed structure in a peer to peer setup  RDFPeers  8   to reduce the network tra   c for processing join queries in a conventional manner  In contrast  BitMat   s compressed structure maintains unique mapping of a triple to a single bit  and also employs a di   erent query processing algorithm  Further  RDFCube has demonstrated their results on a bitcube of only up to 100 000 triples  whereas we have used more than 1 33 billion triples in this paper  3 1 BitMat Operations In this section we de   ne two basic operations fold and unfold which are used by our join query algorithm  Foldand unfold operate on the S O  O S  P O  P S compressed BitMats constructed while storing the original RDF data   1  Fol d  fold operation represented as    fold BitMat  RetainDimension  returns bitArray    folds the input BitMat by retaining the RetainDimension  For example  if in an S O BitMat of a given predicate P  RetainDimension is set to    columns     then BitMat is folded along the subject    rows    resulting into a single bitarray  i e   all the subject bit rows are ORed together to give an    object    bit array  Intuitively  a bit set to 1 in this array indicates the presence of at least one triple with the    object    corresponding to that position in the given S O BitMat  Without loss of generalization  this procedure can be applied to any of the S O  O S  P O  P S BitMats with    rows    or    columns    as RetainDimension   2  Unfold  Speci   ed as    unfold BitMat  MaskBitArray  RetainDimension      unfolds the MaskBitArray on the BitMat  Intuitively  in the unfold operation  for every bit set to 0 in the MaskBitArray all the bits corresponding to that position of the RetainDimension in the BitMat are cleared  For example  unfold BitMat     011000        columns     would result in a bitwise AND of     0  1 2 3     gap compressed representation of    011000     and each row of the BitMat  Note that fold and unfold operations are implemented to operate directly on a compressed BitMat  For example  a bitwise AND of compressed arrays     arr1 as   0   2 3 4 and arr2 as   1   3 4 2     can  be performed  by sequentially looking at their    gap values     E g   AND the    rst gap of arr1     2 0s and arr2     3 1s  which gives the    rst gap of 2 0s in the result  Since the two gaps were of uneven length  there is a leftover 1 from the    rst gap of arr2  Now AND the second gap of arr1     3 1s  and leftover    rst gap of 1 1s from arr2  which gives second gap in the result     1 1s  so on and so forth  Bitwise OR on the compressed bitarrays can be done with AND using simple Boolean logic  a OR b    NOT NOT a  AND NOT b    A bitwise NOT operation on a compressed bitarray is simply     NOT  0  2 3 4     1  2 3 4  4  JOIN PROCESSING ALGORITHM Before describing our join processing algorithm  we would like to note some facts about the join process  Property 1  Each triple pattern in a given join query has a set of RDF triples associated with it which satisfy that triple pattern  These triples generate bindings for the variables in that triple pattern  If the triples associated with another triple pattern containing the same variable cannot generate a particular binding  then that binding should be dropped  In that case  all the triples having that binding value should be dropped from the triple patterns which contain that variable  Property 2  If two join variables in a given query appear in the same triple pattern  then any change in the bindings of one join variable can change the bindings of the other join variable as well  Property 3  A join between two or more triple patterns over a join variable indicates an intersection between bindings of that join variable generated by the triples associated with the respective triple patterns  To elaborate the use of these properties  consider the query given in Figure 1   m and  n appear in the same triple pattern   m  similar to  n   A position marked with         in the  m  n  n rdf type  movie SS  m rdf type  movie  m  similar to  n SO BitMat BitMat BitMat 2 3 1 G G jvar tp Figure 3  Graph G for the query in Figure 1 triple pattern is variable  If we perform a join of   m  similar to  n    m rdf type  movie     rst  we get two bindings for the variable  m viz   the matrix and  the thirteenth     oor and two for  n  the matrix reloaded and  the matrix corresponding to  m   s bindings  When we do the join between   n rdf type  movie   m  similar to  n   we consider bindings generated for  m and  n after the    rst join  After the join on  n  binding  the matrix reloaded for  n gets dropped  hence the triple   the matrix  similar to  the matrix reloaded  gets dropped from the triples associated with   m  similar to  n  which in turn drops the the binding  the matrix for  m  Properties 1  2 and 3 together establish the basis of our pruning algorithm  We propagate the constraints on the bindings of each join variable in a given triple pattern to all other triple patterns and do aggressive pruning of the RDF triples associated with them  4 1 Step 1     Pruning the RDF Triples First we construct a constraint graph 4 G out of a given join query  The constraint graph is built as follows  1  Each triple pattern in the join query is denoted by a tp node in G  Hence forth we use the terms    tp node    and    triple pattern    interchangeably  A jvar node in G corresponds to a join variable in the query  Hence forth we use terms    jvar node    and    join variable    interchangeably  2  An undirected  unlabeled edge between a jvar node and a tp node exists in G if that join variable appears in the triple pattern represented by the tp node  This edge represents the dependency between triples associated with the tp node and the join variable bindings  ref  Property 1   3  An edge exists between two jvar nodes if the two join variables appear in the same triple pattern  This undirected  unlabeled edge represents the dependency between their bindings  ref  Property 2   4  An edge between two tp nodes exists if they share a join variable between them  This is an undirected  labeled edge with potentially multiple labels  Multiple labels can appear if the two triple patterns share more than one join variables  The labels denote the type of join between the two triple patterns     SS denotes subject subject join  SO denotes subject object join etc  For a query having no Cartesian joins 5   constraint graph 4 This graph is reminiscent of similar terminology used in the constraint satisfaction literature  5 A Cartesian join is where there is no shared variable in a set of triple patterns  and hence the result of the query is a full Cartesian product of all triples associated with each triple pattern G is always connected  Figure 3 shows the constraint graph for the join query given in Figure 1  Before starting the pruning algorithm  we initialize each tp node by loading the triples which match that triple pattern  In Section 3 we elaborated the construction of four types of BitMats viz  S O and O S for each P value  P S for each O value and P O for each S value  Assuming that a given query does not have any triple pattern with all variable positions  e g   x  y  z  we initialize the BitMats associated with each triple pattern using the four types of stored BitMats  case of all variable triple pattern is discussed at the end of this section   E g   if the triple pattern in the query is of type   s  p2  o321  then we load only one row corresponding to     p2    from the P S BitMat created for     o321     If the triple pattern is of type   s  p6  o  then we load either the S O or O S BitMat created for     p6     If  s is a join variable and  o is not  we load S O BitMat and vice versa  If both   s and  o  are join variables  then the decision depends on whether  s will be processed before  o  If a join over  s is processed before  o  we load S O BitMat and vice versa  If we have a triple pattern of type   s2  p  o6   then    rst we decide whether P S BitMat for     o6    has less number of triples or P O BitMat for     s2    has less number of triples  If P S BitMat has less number of triples  then we load the P S BitMat by keeping only the bit corresponding to     s2    in each row and mask out all other bits  Note that all these operations are done directly on the compressed BitMats  Thus at the end of initialization  each tp node has a BitMat associated with it which contains only the triples matching that triple pattern  For example  BitM at1 associated with   m rdf type  movie  has just a single row corresponding to    rdf type    loaded from the P S BitMat created for the object value     movie     Now we start the pruning algorithm  First  we consider an induced subgraph Gj var of G containing only jvar nodes  By the construction of graph G  Gj var is also always connected  see Figure 3   Gj var can be cyclic or acyclic  Next  we embed a tree on Gj var discarding any cyclic edges  To propagate the constraints on join variable bindings  Property 2   we walk over this tree from root to the leaves and backwards in breadth    rst search manner  At every jvar node  we take intersection of bindings generated by its adjacent tp nodes and after the intersection  drop the triples from tp node BitMats as a result of the dropped bindings  It can be seen that by the construction of graph G and following the tree over Gj var  constraints on the join variable bindings get propagated to other jvar nodes through the tpnode BitMats  when the triples get dropped   and this propagation follows an alternating path between jvar nodes and tp nodes  This procedure is elaborated in Algorithm 1   A topological sort of an undirected tree is nothing but enumerating all the nodes from root to leaves in a breadth    rstsearch fashion  For each node in the topological sorted list of join variables  we call prune for jvar  Lines  2     4 in  Algorithm 1    A topological sort ensures that a child jvar node always gets processed after all of its ancestors  The bitwise AND between folded bitarrays in prune for jvar J  computes the intersection of all the bindings generated by the tp nodes which contain J  Lines 2     5 in Algorithm 2    According to Property 1  for any binding dropped in the intersection  the respective triples are removed from the BitMats associated with the tp nodes which contain J using the unfold operation  Lines 6     9 in Algorithm 2    getDimension returns the position of J in the BitMat of the triple pattern  For instance  getDimension  n    m  similar to  n   can return column or row depending on whether it is an S O or O S BitMat  Algorithm 1 Pruning Step 1  queue q   topological sort V  Gjvar    2  for each J in q do 3  prune for jvar J  4  end for 5  queue q rev   q reverse     leaves Gjvar  6  for each K in q rev do 7  prune for jvar K  8  end for Algorithm 2 prune for jvar jvar node J  1  M askB itArrJ   a bit array containing all 1 bits  2  for each tp node T adjacent to J do 3  dim   getDimension J  T   4  M askB itArrJ   M askB itArrJ AND fold BitMatT   dim  5  end for 6  for each tp node T adjacent to J do 7  dim   getDimension J  T   8  unfold BitMatT   M askB itArrJ   dim  9  end for One such pass over all the jvar nodes ensures that the constraints are propagated to the adjacent jvar nodes from root to leaves of the tree  For a complete propagation of constraints  we traverse jvar nodes second time by following the reverse order of the    rst pass  Lines 5     8 in Algorithm 1    The leaves of the tree embedded on Gj var appear last in queue q  Since they are processed last in the    rst traversal over the tree  in the second traversal  we directly start with the parent nodes of these leaves  Line 5 in Algorithm 1    Notably  since we take intersection of the bindings in each pass  the number of triples in the tp node   s BitMat decrease monotonically as the constraints are propagated  At the end of Algorithm 1   each tp node contains a much reduced set of triples adhering to the constraints on join variable bindings  Typically  when Gj var is acyclic  this set of triples is also minimal  i e   each triple in the BitMat of a triple pattern is necessary to generate one or more    nal results  If Gj var is cyclic  then the set of triples is not guarantied to be minimal  But in any case  the unwanted set of triples get dropped in the following phase of    nal result set generation  Our pruning method closely resembles the idea of semijoins  7  6   Semi joins also build a query graph  QG  where the nodes of the graph are relations  tables  and an edge between the two nodes indicate a join between the two relations  A semi join QG for a SPARQL join query can be reduced to Gj var in BitMat   s constraint graph by following simple transformation  each edge in the QG is a node with the join variable name in Gj var  two nodes in Gj var have an edge between them if the corresponding edges in the QG are incident on the same node in the QG  If a QG is proper cyclic  6   Gj var is cyclic and if QG is a tree query then Gj var is acyclic  Bernstein et al  have proved in  7  6  that for the tree queries  semi joins can fully reduce the database for a given query  i e   at the end of a semi join the database has minimal tuples  whereas cyclic queries cannot be guarantied to have full reducers  A formal proof of minimal triple set generation in case of an acyclic Gj var in our method is rem iniscent of this proof  It is not included in this paper due to space constraints  In our current implementation  we load the BitMat associated with each triple pattern at the beginning of query processing and then never seek a disk access in the entire lifespan of the query  This necessitates that for a query having n triple patterns it needs Pn i 0 size BitM ati  amount of memory at the beginning  This poses limitations for queries having triple patterns with all variable positions   x  y  z   as it is not feasible to load a BitMat for the all variable tpnode containing the entire dataset in memory  Also  due to this condition  it can happen that for certain queries with highly selective triple patterns  the memory requirements of the conventional query processors are lesser than BitMat as they do not need to load entire indexes in memory to perform joins  e g  RDF 3X   Notably  BitMat   s memory requirement remains linear in terms of the triples associated with the triple patterns in the query  whereas for conventional query processors it can degrade polynomially for low selectivity multi join queries  Simple optimizations While performing the pruning step as elaborated before  we use some simple statistical optimization techniques  Tree root selecti on  After initialization  in a join query with n triple patterns  we sort all the triple patterns    rst in the order of increasing number of triples associated with them  If the    rst triple pattern in this list has only one join variable  we pick this join variable as the root of the tree embedded on the graph Gj var as described before  If it has more than one join variables  then we scan through the sorted list of triple patterns and    nd another triple pattern such that it shares a join variable with the    rst triple pattern  since constraint graph G is always connected for the queries without Cartesian joins  we are sure to    nd such a triple pattern   We then assign this shared join variable as the root of the tree embedded on Gj var  This method is similar to choosing tables with least number of triples to be joined    rst in the SQL joins  Early stopping condition  While performing the pruning at each jvar node  at any point if the M askB itArrJ contains all 0 bits  that is a direct evidence of the query generating empty set of results  If such a condition occurs we exit the query processing at that point telling that the query has 0 results  This avoids unnecessary further processing of other join variables and fold unfold operations over BitMats  4 2 Step 2     Generating Final Results After the pruning phase  we are left with a much reduced set of triples associated with each triple pattern  Intuitively  each BitMat of a triple pattern can be viewed as a compressed table in a relational database  Hence  one way of producing the results could have been to simply materialize these BitMats into tables and perform standard joins over them  But our goal is to avoid building intermediate join results by building a left deep join tree  which precludes a 2 way sequence of joins as done in a typical SQL query processor  In our method  we build and output an entire resulting row of variable bindings  which is similar to multi way joins  Notably for this process  we use at most k size additional memory bu   er  where k is the number of variables in the query  and hence the additional bu   er size is negligible   We keep a map of bindings for all k variables at a time  output one result when all k variables are mapped  and proceed to generate the next result  hence we call these    streaming results      Let us assume that a query has n triple patterns and N is the maximum number of triples in any of the n BitMats associated with the triple patterns  For simplicity  we denote BitM ati as the BitMat associated with the i th tp node  tpnodei   A simple brute force approach can be as follows  Choose say BitM at1  pick a triple from it  This triple will generate bindings for the variables in tpnode1  Store these bindings in the map  Next pick the    rst triple from BitM at2 and generate bindings for the variables in tpnode2  If tpnode2 and tpnode1 share one or more join variables  check the map if the variable bindings generated by both of them are the same  if not  pick a second triple from BitM at2  Repeat this procedure until you get the variable bindings consistent with the ones stored in the map  Then consider BitM at3 and repeat the same procedure as described above  Repeat this procedure until the last BitM atn in the query  If a triple in BitM atn generates valid variable bindings  such that all k variables are mapped to the bindings  output one result  Now start with BitM at1 again and choose the second triple  store the bindings for the variables in tpnode1  and repeat the same process  In general  while generating variable bindings from any BitM ati  check all the variable bindings stored in the map  We can quickly see that the worst case complexity of such a brute force approach is O N n    Since BitMat is a fully inverted index structure  in practice we devise following method which speeds up the above procedure by several orders of magnitude  In general a BitMat having lesser number of triples generates lesser number of unique bindings for the variables in its tp node  This means that in the    nal results of the query  these bindings will get repeated more often in di   erent result rows than other bindings  just like the product of two columns where    rst column has lesser number of rows than the other     values from the    rst column get repeated more often in the product   Making use of this fact  we choose a BitMat as BitM at1  which has the least number of triples  to be processed    rst  similar to the way of choosing the table having least number of triples to join    rst   generate bindings for the variables in tpnode1  and store them in the map  Next instead of picking BitM at2 randomly  we pick a tpnode2 which shares a join variable with tpnode1  Depending on the variable bindings stored in the map  we directly locate the triples which can satisfy these bindings inside BitM at2  Recall that BitMat being a completely inverted index structure  it is easy to locate speci   c triples  If no such triple exists in BitM at2  we discard the variable bindings in the map  go back to BitM at1  and pick the second triple from it to generate new bindings  this can happen only in case of a cyclic Gj var   If BitM at2 generates variable bindings consistent with BitM at1  pick tpnode3 which shares join variables either with tpnode1 or tpnode2  Considering the constraint graph given in Figure 3  let Gtp be an induced subgraph of G having only tp nodes and edges between them  We make use of Gtp to make the choice of the next tp node at every step  Hence it can be seen that after one walk over all the tp nodes of Gtp  if the map has all k variables mapped to bindings  we output one result  The procedure is repeated again until all the triples in BitM at1 are exhausted Presently the    nal phase  Step 2  always projects out bindings of all variables in the query unless it is a    star join     However  in the future  depending on the nature of the constraint graph G for a given query and the variable bindings asked by the SELECT clause  it might not be required to traverse Gj var twice  ref  Section 4 1  thereby further improving the overall query processing time  Note that all the procedures described previously work on a compressed BitMat  Since the pruning phase only reduces the number of triples in the BitMat monotonically  the memory requirement of the query processor goes on reducing as the pruning progresses and the    nal phase of result generation doesn   t build join tables  To conclude the description of our procedure  we would like to point out certain key di   erences of our query processing algorithm from the typical bitmap index joins  BitMat   s structure is similar to the idea of compressed bitmap indexes which are widely used to improve joins in OLAP data warehousing techniques  17  12  20   But an SQL join between multiple tables over di   erent columns cannot always make use of the bitmap indexes for the later joins  This is due to the fact that after the    rst level of join  a relational query processor has to materialize the results of the previous join to carry out the next join and this materialized table does not always have indexes formed on it  unless join indexes are precomputed based on heuristics   As opposed to that  BitMat   s pruning and    nal result generation steps always use compressed BitMats  without materializing the intermediate join results  5  EVALUATION BitMat structure and query algorithm is developed in C and is compiled using g   with  O3 optimization    ag  For the experiments we used a Dell Optiplex 755 PC having 3 0 GHz Intel E6850 Core 2 Duo Processor  4 GB of memory  running 64 bit 2 6 28 15 Linux Kernel  Ubuntu 9 04 distribution   with 7 GB of swap space on a 7200 rpm disk with 1 TB capacity  5 1 Choice of competitive RDF stores We had a wide choice to select the systems for competitive evaluation due to the availability of numerous RDF triplestores  We experimented with Hexastore 6   Jena TDB  RDF 3X  and MonetDB  Out of these we chose RDF 3X  v0 3 3  and MonetDB  v5 14 2      latest versions     for our evaluation  as they could load a large amount of RDF data  gave better performance than others  and are open source systems used by the research community for performance intensive RDF query execution  Like BitMat  RDF 3X maps strings URIs in RDF data to integer IDs and mainly operates on these IDs  building the entire result of a query in the integer ID format  It converts the IDs to strings using their dictionary mapping just before outputting the results in a user readable format  We observed that RDF 3X was taking signi   cant amount of time to convert IDs into strings  in certain cases it took even more time for this conversion than the time taken for the core query execution  Current BitMat system doesn   t support a formal SPARQL query parser interface and the interface to output the results in the string format is still under preliminary development  Hence for a fair compari  6 We obtained compiled binaries of Hexastore from the authors  son  we disabled the ID to string mapping in RDF 3X  which improved their query times a lot  All the RDF 3X query times reported in this paper are without their ID to string mapping  For a fair comparison  we loaded MonetDB 7 by inserting the integer IDs generated out of BitMat dictionary mapping  ref  Section 3   Hence essentially all the MonetDB queries were performed on S  P  Os as integer IDs  We created separate predicate tables in MonetDB by inserting the respective triples by ordering on S O values  21  and used these predicate tables in the query whenever there is a bound predicate in the triple pattern instead of the giant triple table containing all the triples  5 2 Choice of datasets and queries We chose UniProt dataset with 845 074 885 triples  147 524 984 subjects  95 predicates  and 128 321 926 objects  23   which is a protein dataset  We also generated a dataset using LUBM  13      a synthetic data generator provided by Lehigh University     with over 10 000 universities which gave 1 335 081 176 unique triples with 217 206 845 subjects  18 predicates  and 161 413 042 objects  LUBM is widely used by the Semantic Web community for benchmarking triplestores  For UniProt dataset we used 6 out of 8 queries published by RDF 3X in  16   Q7 Q10  Q12  Q13 in our list  leaving out 2 queries which have    all variable    triple patterns   To increase the diversity  we also included 5 more queries  out of 9  published by the UniProt dataset owners  19   Q1  Q4  Q6  Q11 in our list   We removed the FILTER condition in the original Q1 as currently it is not supported by our query processor and had to modify a    bound position    in Q5  Q11 as that value did not exist in the dataset  We modi   ed two of the RDF 3X queries by removing some bound positions to reduce the selectivity of triple patterns  Q2  Q3 in our list   For the LUBM dataset  OpenRDF has published a list of queries  18   But many of these queries are simple 2  triple pattern queries or they are quite similar to each other  Hence we chose 7 representative queries out of this list  All the queries are listed in Appendix A  5 3 Discussion For the evaluation  we measured the following parameters   i  query execution times  cold and warm cache   This is an end to end time counted from the time of query submission to the time including outputting the    nal results  For cold cache we dropped the    le systems caches using  bin sync and echo 3    proc sys vm drop caches   ii  initial number of triples     the sum of triples matching each triple pattern in the query  and  iii  the number of results  The evaluation is given in Tables 1  2  and 3  Query times were averaged over 10 consecutive runs  Geometric mean  is the geometric mean of the query times excluding the ones on which RDF 3X failed to complete processing  Note that our current BitMat query processing system does not use any sophisticated cache management  like MonetDB  and also does not mmap data   les into the memory  like RDF 3X   Due to this  as opposed to RDF 3X and MonetDB  in most of the queries the di   erence between our cold and warm cache times was not very high  After the evaluation  we could classify the queries into 3 categories     queries where BitMat clearly excelled over 7 MonetDB was compiled using      enable optimization       ag Table 1  Evaluation     UniProt 845 million triples  time in seconds  best times are boldfaced  Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Cold cache BitMat 451 365 269 526 173 324 9 396 78 35 1 34 9 33 13 06 MonetDB 548 21 303 2134 124 3563 9 63 97 28 11 28 9 91 15 93 RDF 3X Aborted 525 105 244 58 1 38 4 636 0 902 0 892 1 353 War m cache BitMat 440 868 263 071 168 6735 8 305 77 442 0 448 8 36 10 87 MonetDB 495 64 267 532 113 818 0 584 96 02 0 822 0 861 0 362 RDF 3X Aborted 487 1815 226 050 0 077 1 008 0 0064 0 003 0 0299  Results 160 198 689 90 981 843 50 192 929 0 179 316 0 0 19  Initial triples 92 965 468 73 618 481 78 840 372 16 626 073 60 260 006 15 408 126 16 625 901 53 677 336 Table 2  Evaluation     UniProt 845 million triples  time in seconds  best times are boldfaced  Q9 Q10 Q11 Q12 Q13 Geom  Mean Geom  Mean   without Q1  Cold cache BitMat 11 43 10 49 15 56 26 98 17 37 25 775 20 304 MonetDB 21 37 21 39 12 33 2 468 12 884 27 891 21 761 RDF 3X 1 718 1 549 3 268 2 804 1 765 N A 4 268 War m cache BitMat 9 78 8 69 14 13 25 19 15 77 21 754 16 929 MonetDB 0 611 0 563 0 71 0 744 1 02 3 845 2 565 RDF 3X 0 047 0 0469 0 547 0 295 0 0486 N A 0 255  Results 2 28 8893 2495 9  Initial triples 19 312 584 20 594 986 20 951 969 38 141 013 38 064 279 Table 3  Evaluation     LUBM 1 33 billion triples  time in seconds  best times are boldfaced  Q1 Q2 Q3 Q4 Q5 Q6 Geom  Mean Geom  Mean   without Q1  Cold cache BitMat 51 21 2 71 6 56 2 45 0 503 3 81 4 0285 2 4227 MonetDB 109 35 27 17 455 23 34 12 18 89 14 6 48 3195 41 0377 RDF 3X Aborted 34 868 2328 753 0 588 0 425 1 129 N A 7 4474 War m cache BitMat 48 57 2 11 1 94 0 686 0 27 2 85 2 1719 1 1666 MonetDB 96 65 6 56 398 46 3 209 0 566 0 542 7 9301 4 8094 RDF 3X Aborted 29 033 2028 6855 0 0024 0 0029 0 1814 N A 0 5947  Results 2528 10 799 863 0 10 10 125  Initial triples 165 397 764 224 805 759 219 416 877 438 912 513 3 000 966 9 100 649 both RDF 3X and MonetDB  in both cold and warm cache times   queries where BitMat did better than one of the systems or the reported query times were comparable to the other systems  and queries where BitMat   s performance was worse than both RDF 3X and MonetDB  Queries of the    rst type are     Q1  Q2 of UniProt and Q1  Q2  Q3 of LUBM  Notably  UniProt Q1  Q2 had a high number of initial triples and the join results were quite unselective  As was our initial conjecture  BitMat did much better on such queries than RDF 3X and MonetDB  On the other hand  LUBM Q1 and Q3 were more complex queries having a high number of initial triples associated with the triple patterns  but the    nal number of results were quite small  2528 and 0 respectively   For these queries  the initial selectivity of the triple patterns and selectivity of the intermediate join results were quite low  but together they gave highly selective results  these queries have cyclic dependency among join variables     ref  Section 4 1   For these queries BitMat was upto 3 orders of magnitude faster than RDF  3X and MonetDB due to its way of producing join results without materializing the intermediate join tables  RDF  3X aborted while executing UniProt Q1 and LUBM Q1 as the system ran out of its physical memory and swap space  We executed the same queries on RDF 3X on a higher con     guration server having 16 GB physical memory  RDF 3X processed UniProt Q1 in 858 464 sec  was observed to consume    11 GB resident memory  For LUBM Q1  RDF 3X took 1613 178 sec and the peak memory consumption was    11 GB  For both queries BitMat took 448 169 and 50 70 sec  and consumed    2 6 GB and    3 GB respectively on the same server  A    star join    query was the one where many triple patterns joined on one variable  the query had only one join variable  and that variable got projected in the    nal results  LUBM Q2  Q4  Q5 were star join queries  For star joins BitMat worked much better  because our query processor doesn   t need to load the BitMats of all the triple patterns in memory  It just loads the pre computed row or column bitarrays of each BitMat associated with the triple pattern  ref  Section 3   The    nal result generation phase consists of just listing out the 1 bit positions from the bitwise AND of the loaded bitarrays  similar to the bit</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09d01 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09d01">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_database_optimization"/>
        <doc>Polynomial Heuristics for Query Optimization ### Nicolas Bruno  Cesar Galindo Legaria  Milind Joshi    Microsoft Corp   USA  nicolasb cesarg milindj  microsoft com Abstract   Research on query optimization has traditionally focused on exhaustive enumeration of an exponential number of candidate plans  Alternatively  heuristics for query optimization are restricted in several ways  such as by either focusing on join predicates only  ignoring the availability of indexes  or in general having high degree polynomial complexity  In this paper we propose a heuristic approach to very e   ciently obtain execution plans for complex queries  which takes into account the presence of indexes and goes beyond simple join reordering  We also introduce a realistic workload generator and validate our approach using both synthetic and real data  I   ### Introduction Research in query optimization has quickly acknowledged the exponential nature of the problem  While certain special cases can be solved in polynomial time  e g   chain queries with no cross products  1  or acyclic join queries under ASI cost models  2    the general case is NP hard  see  3    2    Despite the inherent complexity of query optimization  algorithmic research has traditionally focused on exhaustive enumeration of alternatives  see  4  for the classical dynamic programming approach and  5    6  for a transformationbased approach   As queries become more complex  exhaustive algorithms simply cannot enumerate all alternatives in any reasonable amount of time  For instance  enumerating all join orders for a 15 table star query takes several minutes in commercial systems  but we have seen scenarios with queries that join more than 50 tables together   To be able to cope with such complex queries  several heuristics have been proposed in the literature  see Section II for more details   However  previous work is limited to joins operators  i e   they do not consider other relational operators like group by clauses   do not consider the presence of indexes  which  as we will see  can drastically change the landscape of optimal plans   and can still be ine   cient or inadequate in certain scenarios  This is illustrated by the following examples  Language integrated queries  New advances in programming languages  7  allow expressing declarative queries  similar to those using SQL  in procedural languages like C   Extensions to compilers and the language themselves make possible to optimize such queries at runtime  In contrast to traditional DBMSs  such declarative queries might be executed in main memory  are dynamically generated  and usually operate on relatively small amounts of data  For that reason  the latency for optimizing such queries needs to be minimal  and only very e   cient optimization strategies are allowed  In fact  any heuristic with more than a quadratic complexity factor risks being longer than a na    ve execution of an un optimized query  At the same time  availability of indexes and large join graphs present the opportunity for some amount of optimization  Transformation based optimization  Some optimization frameworks  like Volcano  6  and Cascades  5   perform a signi   cant amount of exploration  where logical alternatives are discovered  before obtaining physical execution plans that could be evaluated  Note that if we decide to abort optimization  due to time limits or memory pressure  before implementation is well underway  we might not have a single plan to evaluate  To mitigate this problem with complex queries  some implementations of Cascades  e g   Microsoft SQL Server  use optimization stages  which are performed sequentially  In a    rst stage  a heuristic join reordering is done  followed by very restricted exploration and implementation phases  e g   the exploration phase does not consider join reordering  or only uses it in a very restricted manner   A later stage performs the exhaustive enumeration  In this case  if the later  and time consuming  stage is aborted  the optimizer can always return the result of an earlier phase  which would always be available   It is therefore crucial that the plan found by the    rst phase is of reasonably good quality  since it will be returned if later phases are aborted   but also very quickly produced  since it is just an initial component of the whole optimization process   As illustrated in the previous examples  new applications require e   cient optimization heuristics that go beyond the traditional scenario that exclusively handles join predicates with no indexes  The main contributions of the paper are    We introduce the Enumerate Rank Merge framework  or ERM for short   that generalizes and extends previous join reordering heuristics in the literature  Section III     We extend previous work on heuristic join reordering to take into account other operators  Section IV     We design a workload generator that goes beyond previous proposals  and use it to comprehensively evaluate our techniques on synthetic as well as real data  Section V   II  Related Work There has been signi   cant research on techniques to exhaustively enumerate search spaces for relational query optimization  The pioneering work in  4  presents a dynamic programming algorithm to enumerate joins of a given query  This result was extended several times  culminating in a technique in  8  that e   ciently enumerates all bushy trees with no cross products  and additionally considers outer joins  A di   erent approach is considered in  9   which presentsa top down approach to exhaustively enumerate bushy trees with no cross products  Finally  reference  10  introduces an exhaustive algorithm to enumerate all bushy trees including cross products  Reference  11  extends the dynamic programming technique to additionally consider group by operators  A di   erent line of work in  6    5  exhaustively enumerates joins by using transformations  and can be more easily extended to explore other relational operators  Whenever certain query properties hold  there are polynomial algorithms that guarantee optimality  For instance  for chain queries  it is known that the classical dynamic programming algorithm works in polynomial time  1   Also  if the join graph is acyclic and the cost model satis   es the ASI property  2   the IKKBZ technique in  12  returns the optimal plan in O N 2   where N is the number of tables  The join reordering problem has also received signi   cant attention in the context of heuristic algorithms  One line of work adapts randomized techniques and combinatorial heuristics to address this problem  These techniques consider the space of plans as points in a high dimensional space  that can be    traversed    via transformations  e g   join commutativity and associativity   Reference  13  surveys di   erent such strategies  including iterative improvement  simulated annealing  and genetic algorithms  These techniques can be seen as heuristic variations of transformation based exhaustive enumeration algorithms  Another line of work implements heuristic variations of dynamic programming  These approaches include reference  14   which performs dynamic programming for a subset of tables  picks the best k table join  replaces it with a new    virtual    table  and repeats the procedure until all tables are part of the    nal plan   reference  15   which simpli   es an initial join graph by disallowing non promising join edges and then exhaustively searches the resulting  simpler problem using  8    and references  16    17   which greedily build join trees one table at a time   Except for  16    17   the above heuristics do not naturally    t the scenarios described in the introduction  Although more e   cient than exhaustive search  these approaches are still expensive for low latency scenarios  do not consider other relational operators beyond joins  and are unaware of indexes  III  Enumerate Rank Merge Approach In this section we introduce our main approach  which can be seen as a generalization of earlier greedy techniques  e g   see  17    16    When trying to understand the di   erent heuristics  it is useful to refer to Figure 1  which presents a generic pseudocode that is used  in one form or another  by virtually all greedy approaches  The input to the algorithm is a set of tables T and a join graph J  Each table is associated with a base cardinality and  optionally  a selectivity of all singletable predicates applied to it  The join graph J has associated a selectivity value to each join edge  In line 1 we initialize a pool of plans P  which consists of a Scan operator for each table t     T  We then modify the pool of plans P in lines 2 6  until there is a single plan remaining  which we return in line 7  Each iteration of the main loop starts by enumerating all ERM  T  set of tables  J  join graph  01 P     Scan t    t     T   02 while  P      1 03 E     P1  P2      P    P   valid P1  P2   04 find  P1  P2     E that maximizes R P1  P2  05 Pn   merge P1  P2  06 P   P      P1  P2       Pn  07 return p     P Fig  1  Generic greedy technique to enumerate joins  valid ways to combine two plans in P  enumerate step   Line 4 chooses the pair with the best ranking value  ranking step   Line 5 then combines the highest ranked pair of plans into a new plan Pn  merge step   Finally  line 6 removes the original plans in the highest ranked pair from P and adds the new plan Pn into the pool  The invariant at all times is that any plan p in P is valid for the sub query that contains all tables of the plans that were merged to obtain p  Therefore  at the end  when  P    1  the only plan in P is valid for the original query  The generic algorithm ERM is parameterized by three components  First  we need to determine which pairs of plans in P are valid  enumerate   Second  we need to rank each valid pair of plans  rank   Third  we need to combine a pair of plans into a new plan  merge   The complexity of ERM is then O N 2 R   N M   where N is the number of tables  and R and M are  respectively  the complexities of a assigning a score to a single alternative  and merging a pair of plans  if R is constant and M is at most linear  the complexity of ERM is quadratic in the number of tables   Ranking has a factor N 2 because we can arrange the computation so that the    rst time we evaluate N 2 pairs  and then at each iteration we only compute the rank of each element in P and the newly added Pn  other ranks stay the same   which is linear per iteration 1   The following sections explain the components of ERM in detail  A  Enumerate Step The enumeration step determines the fundamental characteristics of plans that we consider  If we are only interested in linear trees  the valid function in line 3 only allows a pair  P1  P2  if  after merging  there will be exactly one plan in the pool de   ned over more than one table  It is easy to see that this condition always results in linear trees  Similarly  if we do not want to consider cross products  the valid function would reject any pair  P1  P2  for which there is no join predicate in J between a table in P1 and a table in P2  If the valid function succeeds for every possible combination  we end up considering bushy trees with cross products  Previously proposed approaches use di   erent versions of Enumerate  minSel  16  considers linear trees with no cross products  and Goo  17  considers bushy trees and cross products  Linear vs  Bushy Trees Since bushy trees include linear trees  the optimal bushy tree is never worse than the optimal linear tree  However  1 In this analysis we assume that the ranking of pairs is done by assigning a score to each pair  and then sorting by such score exhaustively enumerating all bushy trees is signi   cantly more expensive than doing so for just the subset of linear trees  For that reason  query engines commonly restrict the search space to consider only linear trees  In the context of our greedy heuristics  the real advantage of bushy trees comes from the additional    exibility and potential to recover from mistakes  A problem with greedy techniques that only consider linear trees is that they are fragile with respect to initial bad choices  Example 1  Suppose that we have a chain query R1   R2   R3           Rk  and suppose that R1   R2 is the highest ranked join  Furthermore  suppose that R2   R3 is a really bad choice  since it increases 100x the number of tuples in R3  Suppose that we initially choose R1   R2  Next  we have no choice but choosing  R1   R2    R3 to respect linear trees and no cross products  even though  R1   R2  R3  is ranked badly  it is the only valid pair   The net e   ect is an explosion of intermediate results  Alternatively  if bushy trees are allowed  we can still    rst join R1 and R2  but we would have the option to choose other join sub trees before joining the plans containing R2 and R3  the    nal plan could be something like  R1   R2     R3           Rk   therefore delaying the    bad join    as much as possible  As we show experimentally  including bushy trees in a greedy search strategy greatly improves the quality of results  B  Ranking Step Consider a pair of plans  P1  P2   and denote T P  the  set of tables referenced in plan P  Further assume that J is the set of join predicates in the query  There are several ways to assign a score to  P1  P2  to enable ranking of pairs  In this section we review some alternatives  we assume that smaller scores are better ranked   and in the next section we re   ne the ranking function so that it is aware of merging  We    rst present the rationale behind a good ranking function  The total cost of an execution plan p is the sum of costs of all operators in p  Consider for simplicity a join tree composed exclusively of hash based algorithms  Under a simple cost model  the cost of each hash join is proportional to the sizes of both its inputs and output  Since all plans would read the same amount of data from base tables  minimizing the cost of the query is very well aligned with minimizing the total sizes of intermediate results  The di   erent metrics discussed below try to minimize the sum of intermediate results in di   erent ways    MinSel  The score of pair  P1  P2  is de   ned as  P1 P2   P1    P2    where the join predicate is the conjunction of all join predicates in J for which one table is in T P1  and the other table in T P2   In other words  it is the selectivity of the join predicates between T P1  and T P2   This ranking function is used in minSel  16     MinCard  The score of pair  P1  P2  is de   ned as  P1   P2   where the join predicate is the same as above  In other words  it is the cardinality of the intermediate result that joins all tables in T P1      T P2   This ranking function is used in Goo  17     MinSize  The score of pair  P1  P2  is de   ned as  P1   P2    rowLen P1   P2   where the join predicate is de   ned as before  and rowLen P  is the size  in bytes  of a row produced by plan P  We slightly generalized MinCard into MinSize  which additionally takes into account the width of tuples produced by di   erent operators  and more closely satis   es the original intent of minimizing intermediate result sizes  We experimentally validated that MinSize is consistently better and more robust than the alternatives  and this is the ranking function we assume for the rest of the paper  It is important to note  though  that we can construct scenarios for which MinSel results in better results  but this rarely happens in practice  especially when considering bushy trees   The following example illustrates this fact  Example 2  Consider a chain query A   B   C   D  where  A     B    1K   C     D    100   A   B    1K  selectivity is 1 1K    B   C    x  selectivity is x 100K   and  C   D    200  selectivity is 1 50   Further assume  for simplicity  that all tuples have the same size  For x   300  selectivity is 3 1K   minSel would choose the join order   A   B    C    D  and the sum of intermediate results would be 1K 300 600   1 9K  In turn minSize would choose the order   D   C    B    A  and the sum of intermediate results would be smaller at 200 600 600   1 4K  Now  if we change x   900  selectivity is 9 1K   minSel would still choose the same join order  and the sum of intermediate results would be 1K 900   1 8K  3 7K  When considering minSize  it will    rst choose C   D as before  After the    rst iteration  we have  C   D    200 and thus   C   D    B    1 8K  which is larger than  A   B    900  If we are restricted to linear trees  minSize would still end up choosing the same plan as before  and the sum of intermediate results would be 200 1 8K 1 8K   3 8K  which is worse than that of minSel   If bushy trees are allowed  however  minSize would choose A   B in the second iteration  and would return the    nal plan  D   C     B   A   with a sum of intermediate results of 200   1K 1 8K  3K  better than minSel  which returns the same plan for either linear or bushy trees   C  Merging Step Once we decide in line 4 in Figure 1 the highest ranked pair of plans  P1  P2   we have to merge them together in line 5 and replace the P1 and P2 with the merged plan Pn in line 6  Traditionally  the merge operation simply combines both execution sub plans with a join alternative  i e   Pn P1   P2   which always results in a valid plan  Note however that we can consider alternative execution plans  as long as we do it e   ciently  as we discussed earlier  since there is a quadratic number of  constant time  ranking invocations  and a linear number of merges  we have a linear budget to implement better versions of merging without increasing the complexity of the algorithm   An interesting aspect of merging is that it is very    exible  If we think of a new way of merging two execution plans  we can add the new alternative and pick the best optionoverall  We next introduce our merging alternatives  assuming that the ranking function is minSize  Switch HJ This is a very simple merge  motivated by our ranking functions not distinguishing between left and right inputs  In reality  it is better to have the smaller relation in the build side of a hash join  So when merging P1 and P2  we choose the alternative that uses the smaller input as the build side  Switch Idx If we are about to merge P1 and P2  where P2 refers to a single table T2  and there is an index on T2 that enables an index join strategy between P1 and T2  we consider this alternative in addition to the one that does not use indexes  and pick the one expected to be cheaper  While this is an interesting merge that consider indexes  it still depends on the ranking function to choose the appropriate pair P1 and P2  In other words  we would only use an index alternative    reactively     if by chance there was an available index for the highest ranked pair  A more robust alternative would be to bias the ranking function so that it takes into account the presence of indexes    natively     At the same time  since ranking is done for each pair of plans  this modi   cation to the ranking function needs to be very cheap to compute  Suppose that we are considering a pair of plans  P1  P2  where P2 covers a single table T2  Further suppose that P1 has cardinality C  An index join alternative needs to perform an index lookup for each of the C outer tuples  and seek the index for matches  if the index is not covering  for each one of these matches there should be a clustered index lookup to obtain the remaining columns  2   Suppose that each index lookup fetches LT2 tuples that    t in LP2 pages from T2  Then  the index join would read C    LP2 pages  plus C    LT2 pages from the clustered index if the index is not covering   for the bene   t of not reading T2 altogether  The value PI  de   ned as  PI   max   0  pages T2      C     LP2     cov I     F    LT2    approximates the number of pages we would save  if any  by using an index join alternative with index I  where pages T2  is the number of pages of table T2    cov I  is zero if the index is covering  and one otherwise   and F is the ratio of a random page read versus a sequential page read  to normalize PI values to    sequential page reads      The value of F can be obtained by either using the optimizer   s cost model  which we did  or by a calibration phase  We then re   ne the minSize ranking function as follows  minSize    P1  P2    minSize P1  P2      pageSize    PI This function is still an approximation based on some simplifying assumptions  such as not modeling duplicate outer values or bu   er pool e   ects  While all these could be accommodated by further re   ning the ranking function  our simple metric already provides good quality results  2 The details are slightly more complex since we need to take into account the single table predicates on table T2  which could be applied before the clustered index lookup  We omit such details for simplicity  Push The previous merging alternatives are simple and can be implemented in constant time  they only change the root of the merged tree   Push is a merging alternative that sometimes is able to correct early    mistakes    that result from using a greedy heuristic  Consider merging  P1  P2  and suppose that J is the set of join predicates between tables in P1 and tables in P2  As hinted by its name  the Push alternative tries to push one plan inside the other  Consider pushing P2 into P1  Without considering cross products  we can push P2 to any subtree in P1 that has a non empty join predicate between the subtree and P2  Figure 2 a  shows the positions where we could push P2   R5   R6 if the join predicate between P1 and P2 is J   R3   R5   For each such sub tree p of P1  we replace p with an additional join operator that joins p and P2  see Figure 2 b  for an example   Once we determine all subtrees for pushing P2  we select the one with the highest aggregated rank  i e   the sum of sizes for all intermediate results   Note that pushing P2 into a subtree p in P1 only changes the result size of all ancestors of p in P1  This enables strategies that share most of the work for calculating aggregated ranks of all push alternatives  For instance  if we assume independence among join predicates  each pushing of P2 into a subtree can be calculated in constant time  by precomputing cardinality information on each plan in the pool incrementally  There are two extensions to this basic approach  First  note that for each push  we can apply the Switch HJ and Switch Idx merges to each intermediate join in the path from the root to the pushed element  This is useful as changes in cardinality might make a index join alternative much more useful than before the push  Note that  since P2 into the root of P1 is the same as joining both trees  the Push strategy is e   ectively a generalization of the previous SwitchHJ and Switch Idx  At the same time  the complexity of this alternative is larger than that of the original approach  The bene   t of the Push technique is twofold  First  it might help correct early mistakes done by the greedy technique  For instance  suppose that  A   B    C is better than  A   C    B overall  but A   B is more expensive than A   C  this is the canonical adversarial scenario for a greedy technique   Our greedy heuristic would pick A   C    rst  obtaining a suboptimal plan  However  when later joining with B  we can push B towards A and obtain the better plan again  The second  related  bene   t of Push has to do with join cycles  or residual predicates that operate over a subset of tables  Greedy heuristics are not aware that certain joins are bene   cial because they are part of a larger join cycle  or a multi table predicate  that would signi   cantly reduce cardinality values  Therefore  greedy heuristics sometimes do not close cycles early enough  Whenever the join cycle is eventually closed  the Push heuristic will attempt to push it down into the tree  possibly making a substantial reduction in intermediate cardinality appear earlier R1 R2 R3 R4 Join is R5    R3 R5 R6 P2 R2 R3 R2 R3 Push P2 R3  R5 R6          a  Candidate subtrees for pushing P2   b  Applying a speci   c push operation  Fig  2  Merging two plans using Push  Pull This strategy tries to correct missed opportunities of indexbased joins  After merging P1 and P2  the resulting cardinality can decrease signi   cantly  either as a result of the join or because of an additional residual predicate or join cycle   Then  some early decisions that discarded index joins because of large outer cardinalities can become interesting again  The Pull strategy operates on P1   P2 as follows  It    rst considers each leaf node in the plan that is not already the inner side of an index join alternative  and tries to    pull    the leaf node all the way up to the root of the plan to be used as the inner of an index join  Figure 3 illustrates how R3 is pulled up to the root of the tree  it also shows that sometimes earlier joins that involve the pulled table must be converted in cross products   As before  every pull operation of node p changes the cardinality of all operators in the path from root to p  By carefully maintaining cardinality information for all leaf nodes in the pool of plans  the pulling of each node can be done in constant time  and thus the Pull strategy is linear in the size of the tree  we omit details due to space constraints   R1 R2 R3 R4 R1 R2 R3 R4 Pull R3  HJ HJ HJ HJ IJ Fig  3  Pulling table R3  D  BSizePP  Heuristic join reordering Figure 4 shows an instance of the ERM algorithm of Figure 1  the name BSizePP is explained in Section V   We consider bushy trees and cross products  use the minSize ranking function with the extensions in Section III C to natively consider indexes  considers all valid pull and push merges and picks the one that minimizes the aggregated minSize value  We will validate each of this choices experimentally in Section V  IV  Beyond Join Reordering In this section we extend our strategies  which heuristically reorder join graphs  to additionally handle other operators  BSizePP  T  set of tables  J  join graph  01 P     Scan t    t     T   02 while  P      1 03 E      P1  P2      P    P      consider bushy trees 04 find  P1  P2      E minimizing minS ize P1  P2  05 Pn   best of Push P1  P2  and Pull P1  P2     Includes Switch HJ and Switch Idx 06 P   P      P1  P2       Pn  07 return p     P Fig  4  Generic greedy technique to enumerate joins  A  Group by Clauses After joins  group by clauses are the most commonly used operator in SQL  A group by clause is de   ned by  i  a set of grouping columns c   ii  a set of aggregate functions a  and  iii  the relational input expression R  We concisely write a group by expression as GB c a R   or simply GB c R  if the aggregate functions are not relevant for the discussion  Group by clauses can be done before or after joins in certain situations  Speci   cally  a group by clause can be pulled up above a join  as long as  i  we add a key of the other join relational input to the set of grouping columns  and  ii  the join predicate is not de   ned over an aggregated column  otherwise the resulting join is not well formed   Figure 5 illustrates this transformation  note that b2 must include b1 so that the original join is well formed   A group by clause can be pushed below a join R   S  down to S   whenever  i  the grouping columns include a key of R   ii  the columns from S in the join predicate are included in the grouping columns or derived from them via functional dependencies  and  iii  all aggregates are de   ned in terms of columns in S  11    18   A GB b2  B a b1 GB  b2  key A   A B a b1 Fig  5  Pulling a group by clause above a join  Canonical representation Join graphs are well known canonical representations of SPJ queries  We now describe how to extend this concept for queries that additionally contain group by clauses  The canonical representation of a generic query with joins and group byclauses is a join graph augmented with a set of canonical group by clauses  Each canonical group by clause is obtained from an original group by in the query  and consists of  i  a set of grouping columns   ii  a set of aggregate functions  and  iii  an input signature  instead of the relational input expression in regular group by clauses   An input signature consists of a set of tables and optionally other canonical group by clauses  Consider a logical operator tree representing a query with joins and group by clauses  We    rst push all group by clauses down joins as much as possible  reordering joins below group by clauses if required   Since group by clauses in the same query are either de   ned over disjoint relational inputs or else one is part of the other   s input  the result of this step is always unique and well de   ned  We then consider each group by clause in the resulting tree  and convert it into a canonical group by  The grouping columns and aggregate functions in the canonical group by clause are identical to those in the original group by clause  The input signature contains all tables and group by clauses that are descendants of the original group by clause and can be reached from it using a path in the query tree that does not go through another group by clause  The join graph  in turn  is constructed as if no group by clauses were present  for simplicity  cases which de   ne joins between aggregated columns are discussed later in this section   Example 3  Consider the query in Figure 6 a  and the corresponding operator tree representation in Figure 6 b   The inner group by clause initially is de   ned by  i  columns key A   B 1  D 3   ii  the COUNT    aggregate  and  iii  the input relation A   B   C   D  Note that we can push the group by below A  A 1 B 1  B   C   D  since k A  and B 1 are part of the grouping columns  Then  the group by is canonically de   ned as  i  the set of grouping columns  B 1  D 3    ii  the aggregate  and  iii  the input signature  B  C  D   We concisely write this as GB1   GB  B 1 D 3   B C D    Using a similar argument  the canonical representation of the outer group by clause is GB2   GB  E 3   A  GB1 E    Figure 6 c  shows the canonical representation of the query above  It is important to note that we were able to obtain a single join graph even though the original query had two join components    separated    by a group by clause  This would allow to reorder joins across group by clauses  which is not possible in previous approaches  The canonical representation of a query allows identifying the locations in a join tree where each group by clause can be inserted  Speci   cally  we can insert a group by G on top of a sub tree T whenever T contains at least all tables and group by clauses in G   s input signature  In such case  the set of grouping columns in the resulting group by clause would additionally include keys for any table or group by clause in T that is not present in the    transitive closure    of the input signature of G  i e   not present in G   s input signature  in input signatures in G   s input signature  and so on   Example 3   continued  Consider a sub tree T   A   B   C   D   E  We can insert a root group by G1  see Figure 6  on top of T because T includes all tables in G1   s input signature  We have to include in G1   s grouping columns a key for A and E  which are not in the input signature of G1   Group by G2 can now be placed on top of the resulting subtree because G1  A and E are present in the sub tree  The grouping columns in G2 do not need to be augmented  since all tables and group by clauses in the sub tree appear in either G2   s input signature  or in G1   s input signature  which belongs to G2   s input signature   Selectivity Estimation of Group By Clauses When considering a query that only contains joins  we can associate a selectivity value with each join because this value is independent of the order of the join in an execution tree  No matter where a join is positioned in an execution plan  the selectivity of the join would always be the same 3   We now extend this idea to queries that also contain group by clauses  We de   ne the selectivity of a group by G as the number of output tuples divided by the number of input tuples  In this work we do not take a position on how to calculate this selectivity value  in general it would involve using statistical formulas and exploiting histogram information   Instead  we show that the selectivity of a group by clause is independent to its position in an execution plan  Suppose that there are  A  tuples in A and  B  tuples in B  Furthermore  assume that there are dvA distinct values of a in A  each one repeating DA times  so  A    dvA     DA  this is a simpli   cation  but is the information that we have per histogram bucket on A a  and we can extend the discussion to hold over all buckets   Similarly  there are dvB distinct values of b in B  each one repeating DB times  so  B    dvB     DB  Additionally  each distinct value of b in B  i e   these DB tuples  contain distinct values for b    recall that the columns in b   includes those in b  so it tuples that share b values generally vary in columns b       b   Let   s say that there are dv   B distinct values of b   in these DB tuples  each one repeating D   B times  so DB   dv   B    D   B  again  there is an implicit assumption about independence of column distributions here  which is typically used during cardinality estimation   With this notation  we can calculate cardinality and selectivity values in presence of both joins and group by clauses  Speci   cally  1   GB b     B     dvB    dv   B  the group by clause removes all DB    copies   2  S el GB b     B      GB b     B    B    1 D   B   3   A   B    min dvA  dvB    DA    DB  using the containment assumption for joins   4   A    GB b     B      min dvA  dvB     DA    dv   B  using the containment assumption again  but now each group of DB tuples was reduced to only dvB    tuples due to the group by clause   5   A    GB b     B       GB b     key A   A   B    since both expressions are the same when pulling up a group by   6  S el GB b     key A   A   B     GB b     key A   A   B    A   B    1 D   B   3 This statement leverages the standard independence assumption commonly used for cardinality estimation SELECT E 3  COUNT    FROM E  F   SELECT A 1  B 1  D 3  COUNT    FROM A B C D WHERE A 1 B 1 AND B 2 C 2 AND C 3 D 3 GROUP BY key A   B 1  D 3  T1 WHERE E 1 F 1 AND E 2 T1 A 1 GROUP BY key F   E 3 GB  key F  E 3  E F GB  key A  B 1  D 3  A B C D A F D E B C GB1   GB  B 1  D 3    B  C  D   GB2   GB  E 3    A  GB1  E    a  Original Query   b  Operator tree representation   c  Canonical Representation  Fig  6  Canonical representation of joins and group by clauses  By 2 and 6 above  the selectivity of the group by is the same independent of placement of additional joins  Join selectivities are also una   ected by additional group by clauses  since  7  S el A   B    min dvA  dvB     DA    DB  A      B    min dvA  dvB   dvA    dvB    1  max dvA  dvB   8  S el A   GB b     B      A   GB b     B     A      GB b     B      by 4 and 1  min dvA  dvB   DA   dv   B    A    dvB    dv   B     min dvA  dvB   dvA    dvB    1  max dvA  dvB   This means that joins and group by clauses  considered together  are not that di   erent from joins alone in terms of having a single selectivity value that is independent of the position of the operator in the    nal tree  The main di   erence is that group by clauses cannot be placed arbitrarily in an execution plan  but can only be placed on top of a plan that contains all elements in its input signature  Reordering Joins and Group by Clauses To summarize the previous discussion  we showed  i  how to extend canonical representations of queries so that they also handle group by clauses  which allows us to understand when a group by clause can be    merged    with an existing subtree   and  ii  how to assign a selectivity value to a group by clause independent on the position in the execution tree  An interesting consequence of these properties is that group by clauses can now be naturally incorporated into the EnumerateRank Merge approach  We now describe the extensions to the approach in Section III by explaining how each component in the main algorithm of Figure 1 needs to be extended  Initial pool of plans P  line 1   In addition to one Scan operator for each table in the query  we include one virtual group by node for each canonical group by in the query  These virtual group by nodes are not executable leaf nodes  as Scan nodes are   but instead only make sense when they are put on top of a valid execution sub plan  Enumerate  line 3   We disallow pairing two virtual group by nodes together  as explained before  it does not result in a well formed execution plan   Additionally  we only consider pairing a virtual group by G with a non virtual groupby plan P2 if this would results in a valid placement of the group by as per de   nition of canonical representation  i e   if the tables and group by clauses in P2 include those in the signature of G   All other pairings of non virtual group by nodes are handled as before  note that these input plans can have a group by operator as the root node  but the pairing would result  as before  in a join or cross product   Ranking  line 4   The ranking functions introduced in Section III B are easily extended to deal with group by clauses  For instance  the value minSize G  P1  is the output size of applying the group by operator G on top of P1  This value can easily be computed based on the cardinality of P1 and the selectivity of G  which  as we saw earlier  does not depend on the operator placement in the tree  Merging  line 5   The na    ve merging operation is trivially extended so that when combining a virtual group by G with a non virtual plan P  the generated plan consists of the group by G  augmenting the grouping columns appropriately  on top of P  Switch HJ and Switch Idx only operate on the root of the tree  so they are not applicable when one of the plans in the pair is a virtual group by operator  Push and Pull operations require a bit more care  but are also simple to extend  The main challenge is to prevent invalid trees from being generated through merges  Consider the pull operation of a table T that is in the input signature of some intermediate group by  In this case  simply pulling the table to the root of the tree would generate an invalid plan with a group by that does not operate on its input signature  Instead  when the pulled table appears in the input signature of an intermediate group by G  we pull  along with the table  any dependent group by operator as well to the top of the tree  Similarly  consider the push operation when applied to a virtual group by node G  In addition to generating the group by at the root of the new tree  we try to push the group by down whenever possible  and pick the best alternative  At a high level  the main extensions require us to re evaluate the ranking function of alternative plans  and pick the one with the best ranking as the resulting merging plan  Additional Details For simplicity  we have not discussed a special case when handling join predicates de   ned on aggregate columns of group by clauses  as illustrated in the example below  SELECT   FROM T1   SELECT SUM x  AS SX FROM TX  T2 WHERE T1 X   T2 SX We cannot join T1 and T2 until after doing the aggregation on T2  otherwise  the join is not well de   ned   In general  this situation results in a dependency graph between joins andgroup by nodes  where the application of some operator is blocked until other operators unlock it  Earlier in this section  we discussed the case of group by operators being blocked by their input signatures  The same ideas and solutions can be applied in this case  where group by clauses block joins  A suitable extension of the enumerate step in line 3 of Figure 1 is required  but we omit such details due to space constraints  B  Semi join operators Although semi joins     are not directly speci   ed in SQL queries  they are very useful relational operators because  i  semi joins can be used to handle nested sub queries with optional correlation   ii  there exist e   cient set  and tupleoriented implementations of semi joins  and  iii  algebraically they can be expressed as group by clauses and joins  which enables additional optimization alternatives  To make the last item concrete  the traditional algebraic equivalence that relates semi joins with joins and group by clauses is as follows  A   B   GB key A   A   B  We use this equivalence to produce a canonical representation of a query that contains joins  group by clauses and semi joins  Suppose that the semi join predicate in the equality above is A  a b B  This equality results in the functional dependency  key A      b  and thus in GB  key A   b   A   B   which can then be rewritten by pushing the group by clause down the join as in A  a b  G B b  B    Now consider a slightly di   erent example without an equi join  namely A  a b B  This is equivalent to the expression GB key A   A  a b B  but we cannot push the group by down the join since column b in the join predicate is not a grouping column  As a more complex example  consider A  a b  B  b2 c2 C   In this case  the transformation results in GB key A   A   B   C   which  by a similar reasoning as in the    rst case above  can be pushed down through the    rst join into A  a b G B b  B  b2 c2 C   In general  to obtain a canonical representation for a subplan like R   T we replace the   with a   and add a group by with key R  as the grouping columns  We then proceed as before  by pushing the group by as much as possible and obtaining the join graph and canonical group by clauses  As a    nal example consider expression A  a key B   B  b c C   We translate it to GB key A   A   B   C   but key A      a  which is equal to key B   and key B      b which is equal to c  so this is actually GB  key A   key B   c   A   B   C   and now we can push the group by below joins to result in A   B   G B c C   The canonical representation contains the join graph A   B   C and the canonical group by clause GB c C   Integrating Semi joins Since we transform semi joins into joins and group by clauses  our algorithms can handle semi joins with no modi   cations  While this approach is correct  it has a drawback  group by clauses obtained from semi joins have sometimes special properties  For instance  often there is a single table in the right side of the semi join  and if there are indexes  we can use e   cient index based execution plans without actually performing the group by and join  Below we outline some suggestions to address this issue  Leveraging that a semi join results into a join plus a groupby  the ranking function can have certain limited look ahead capabilities  In general  if there are no aggregate functions de   ned on a group by clause over a join  we can attempt to apply the join and the corresponding group by together using an index based strategy when ranking the pair that contains the join  Of course  merging should be made aware of the look ahead mechanism and implement the optimized plans whenever possible  The main challenge in this approach is that the merge operators  e g   Pull and Push  need to be aware of new physical operators that we might introduce in specialized plans  We believe that this is not a straightforward task  and we consider this form of tighter integration of semi joins with our Enumerate Rank Merge framework as part of future work  C  Other operators In addition to joins  semi joins and group by clauses  there are other operators in SQL that need to be handled  In this section we brie   y comment on how we can approach such operators 4   In general  these operators are handled independently of the join semi join group by components  either using preor post processing  as explained next  Anti Joins  As far as we know  we cannot freely reorder left and right children of anti joins as we did for joins and semijoins  Conceptually  R  a b S is evaluated by considering each row in R and processing it according to S  in the example  it would return the tuple from R if there is no tuple in S that satis   es predicate a b   This procedure can be seen as evaluating an expensive predicate for each row of R  In fact  the cost per tuple in R depends on S and  according to cardinality estimation  only a fraction of tuples from R would pass the anti join  To deal with queries that include anti joins  we    rst obtain a heuristic execution plan for the query that omits the anti join  We then identify the most speci   c point in the query plan in which the anti join can be evaluated  and use the heuristic techniques in  20    21  to obtain the most appropriate place to position the anti join in the tree  Outer Joins  We pre process a query with outer joins  denoted as      by delaying them as much as possible  and therefore obtaining larger join components   We do this using the algebraic equivalence R    S     T     R   S       T and pull outer joins above joins as much as possible before applying the ERM approach to the join component  A full integration of outer join reordering is part of future work  Other operators  We do not speci   cally handle other operators  such as unions or full outer joins   and leave them as in the original query  D  Putting it All Together Out technique to heuristically optimize queries is as follows  4 An alternative  more systematic approach that adapts work on anti join and outer join reordering  e g    8    19   is part of future work 1  Analyze the query and transform it into a set of join semi join group by canonical components  with associated anti join expensive predicates   and connected among each other via  delayed  outer joins  unions  and any other un handled operators  2  Apply ERM to each component  3  Greedily place anti joins in each component  4  Use default implementations for outer joins  unions  and other un handled operators  V  Experimental Evaluation We now report an experimental evaluation of the techniques described in this paper  We implemented a C  prototype that enables us to compare di   erent optimization variants  Each experiment is speci   ed by three components  a query workload  an index con   guration  and a set of optimizers to evaluate  We next detail each component in detail  A  Query Workloads A query is speci   ed by a set of tables  with selectivity values given by single table predicates   a join graph with associated selectivity values  and a set of group by clauses  Tables  We generate table cardinalities by following Zip   an distributions  The parameters of the table generation routine are number of tables N T and the Zip   an parameter Z  A value Z   0 generates tables with the same cardinality  while a value Z   5 generates 10 tables with the following cardinalities  for a total 100M tuples    96440632  3013770  396875  94180  30861  12403  5738  2943  1633  964  Alternatively  we used two additional data generation routines  random  which generates tables with random cardinality  and strati   ed  which was previously proposed in the literature  13  and generates tables of size 10 100 with 15  probability  size 100 1K with 30  probability  size 1K 10K with 25  probability  and size 10K  100K with 30  probability   Finally  a boolean parameter  sorted  speci   es whether the tables are sorted by cardinality or are instead shu   ed during query construction  this is important due to join and    lter generation  explained next   Join Graphs  These are generated using the following parameters    J  Number of joins  The    rst N T   1 joins form a spanning tree  and remaining joins introduce cycles    0     J L     JR     1  which control the graph topology  The i th join  for i   N T  is done between the i th table  in the order produced during table generation  and a random table between JL  i 1  and JR  i 1   This procedure generalizes previous approaches in the literature to generate interesting join topologies  For instance  for J L JR 0 we obtain star topologies  for J L JR 1 we obtain chain topologies  and for other values  such as J L 0 and JR 0 3 we obtain snow   ake like topologies  After N T     1 joins  we randomly join pairs of tables generating cycles    JoinType  which is a categorical variable which determines the cardinality of the join  It could be random  which assigns a random selectivity value to the join   Min Max MinMax  which assigns the cardinality of the smallest largest table  or a number in between   or N M  which models n m joins by picking a number of distinct values in each table and having the join selectivity be 1 max dv1  dv2  as in  13    We can specify a di   erent distribution for the    rst N T     1 joins and the rest  Table    lters  So far each table is joined without applying any    lter on it  which is    ne if no indexes are present  since we always need to read the full table   When indexes are present  tables with    lters can result in big di   erences  We specify table    lters by a pair of values  0     lo     hi     1  specifying the selectivity of the    lter  and a choice of tables to    lter  which could be either a random fraction of the tables  the value First  which    lters the fact table in a snow   ake topology   or Leaves  which    lters dimension tables in snow   ake topologies   Group by clauses  Group by clauses are generated by parameters G  the number of group by clauses  and T  the number of random tables that are required in the group by input signature   T0 T1 T2 T3 T4 T5 T6 T7 T8 T9  92M  filtered to 320K   5M   1M   320K   62K   130K   33K   19K   11K   7K  3  2 M 3 5  M 4 2 5 K 5 5 0 K 2 3 0 K 35M 6 0 M 5 5M 72M 121K  a  Snow   ake topology  T3 15M  T4  13M  T2  17M  T0  30M  T1  21M  GB T2 T3  sel 0 08  b  Chain topology  Fig  7  Synthetically generated queries  An Example The generation procedure described above is rather    exible  and many combinations are possible  We can produce star  linear  and snow   ake topologies respecting foreign key constraints  as well as cycles simulating M N joins  Figure 7 shows examples generated with this approach  We generated the snow   ake query in Figure 7 a  using N T   10 tables with Z   4 tuple distribution  sorted by cardinality   a join graph with 10 joins  the    rst 9 using J L   0  JR   0 5 and MinMax selectivity values  and the last one introducing a cycle with N M selectivity   and    lter predicates on the fact table  Figure 7 b  shows a chain query generated with N T   5 tables with Z   0 5 tuple distribution  shu   ed tables   a join graph with 4 joins using J L   JR   1 and Max selectivity values  and a group by clause over two tables B  Index Con   gurations A con   guration is de   ned a set of indexes  Each index consists of a leading column and a boolean value which speci   es whether the index is covering during an index lookup  A con   guration is generated by two parameters  ProbIndex  which gives the probability that an join column or single table predicate has a backing index  and ProbCover  which gives the probability of an index being covering   C  Optimizers To simplify the presentation  we encode an optimizer by a three part name E R M  where    E identi   es the search space  which can be L  linear trees with no cross products  or B  bushy trees with cross products     R identi   es the ranking function and can be Sel  rank by minSel   Size  rank by minSize   and ST  rank by smallestTable 5      M identi   es the merging approach  It could be empty if only the trivial merge is used    if Switch HJ and SwitchIdx are used  and PP if additionally Pull Push is used  Using this notation  our experiments use subsets of the following optimizers  LSel  LSel   LST  LST   BSel  BSel   BSize  BSize   BSizePP  We additionally implemented the following exhaustive optimizers    LEx  Exhaustive optimizer that considers linear trees and no cross products    BEx  Exhaustive optimizer that considers bushy trees and cross products    LIKKBZ  LIKKBZ   Optimizer that returns optimal linear trees for acyclic queries and no indexes  augmented with minimum spanning trees for cyclic queries  12   D  Metrics For a given workload  index con   guration  and set of optimizers  we proceed as follows  We optimize each query by all optimizers  We cost each resulting execution plan using Microsoft SQL Server   s cost model  We then divide the cost of each plan by the cost of the best plan found by any optimizer in our set  The value that we assign to a given optimizer is this ratio  which goes from one  optimal  upwards  and quanti   es how much worse the solution is compared to the optimal one  After repeating this procedure for all queries in a workload  we report the minimum  maximum  average  median  and the 95  quantile of the ratio for each optimizer  These quality numbers are precise if an exhaustive optimizer is in the mix  since the optimal execution plan would be found  While we can evaluate exhaustively for small number of tables   15   it is not possible to do so for a large number of tables  In that case we do not know what the optimal plan is  but we always include LIKKBZ in the set of optimizers  which is guaranteed to produce optimal linear trees for acyclic queries  and thus is a baseline   5 This is another heuristic found in the literature  16   where at each iteration we choose that smallest table that can be joined  E  Summary of Results The sections below show a sample of experiments performed on a 3GHz dual core machine  We focused on general trends and insights  and omit several additional variants that we tried due to space constraints  Execution times We    rst report execution times for the techniques discussed in this paper  Note that our prototype is not optimized for speed  instead  we tried to reuse as much code as possible to implement all variants  which results in performance limitations   Therefore  performance numbers should be interpreted as trends rather than in absolute terms  Figure 8 shows elapsed times for all techniques  note the logarithmic scale   All heuristic implementations show the same trend and    nish in less than 200 milliseconds for 60 tables  and in tens of milliseconds for up to 30 tables  query graphs were generated randomly in this experiment  as we just wanted to measure average performance   In contrast  exhaustive techniques quickly become too slow over 10 seconds for as little as 13 tables  Also note that exhaustively exploring bushy trees can be an order of magnitude slower than just exploring linear trees  The results in Figure 8 do not consider indexes  We ran the experiment again but adding 50  of relevant indexes to the physical design  and the results  while slower than the ones reported in the    gure  showed the same trends                                                                                                                                                                           Fig  8  Performance of di   erent optimization alternatives  Quality results We    rst generated a workload with N T   10 tables with Z   4 cardinalities  not sorted by cardinality   and a chain join topology  J L   JR   1  where each join has a cardinality given by MinMax  We did not consider any index  Figure 9 shows the quality metrics for all optimizers  ordered by average ratio   We make the following observations  1  BEx is  obviously  the best strategy in all cases  2  With the exception of BSel  all bushy strategies are better than linear strategies  This is a direct consequence of the join topology  which is very restrictive and does not give    exibility to recover from large intermediate results  3  LEx is on average 4 2 times  up to 35 2 times  worse than BEx  which shows that bushy trees can be substantially better than optimal linear trees  It is interesting to note that most bushy heuristics result in better plans than that of the exhaustive linear tree                          4  LIKKBZ  has almost the same quality as LEx  the slight di   erence comes from small variations in the cost model used by IKKBZ  12    5  Simple merging variants     generally pay o     resulting in better quality than the corresponding technique with na    ve merging   6  Our technique BSizePP performs on average 6  worse than the optimal bushy strategy  and 32  worse in 95  of cases   The advanced merging operations Push and Pull did not additionally improve quality over BSize   Avg Min Median 95  Max BEx 1 000 1 000 1 000 1 000 1 000 BSizePP  1 060 1 000 1 001 1 328 1 880 BSize  1 060 1 000 1 002 1 328 1 880 BSize 1 367 1 000 1 131 2 462 4 096 BSel  3 764 1 006 1 509 15 184 69 812 LEx 4 288 1 000 1 533 17 345 35 278 LIKKBZ 4 295 1 000 1 533 17 345 35 278 LIKKBZ  4 295 1 000 1 533 17 345 35 278 BSel 11 241 1 033 3 156 51 432 234 573 LSel  12 074 1 005 4 169 43 504 129 153 LST  12 331 1 001 3 797 50 236 193 288 LST 13 061 1 037 4 779 50 371 193 604 LSel 45 256 1 012 12 420 177 877 536 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09do2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09do2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_database_optimization"/>
        <doc>Ef   cient Algorithms for Recommending Top k Items and Packages ### Mohammad Khabbaz  Min Xie  Laks V S  Lakshmanan Department of Computer Science  University of British Columbia  mkhabbaz minxie laks  cs ubc ca ABSTRACT Recommender systems use methods such as collaborative    ltering in order to make personalized recommendations to users  Collaborative    ltering has become a prominent approach for making predictions and personalized ranking  It forms the core business of companies such as Amazon and Net   ix  Many sophisticated algorithms have been proposed and much effort has been devoted to improving the accuracy of predictions  Most of this research has been concerned with what we regard as    rst generation recommender systems  Ever since the database community got interested in recommender systems  people have begun researching questions related to    exibility and functionality  In this paper  we propose an integrated framework to review and explain some of the recent work conducted by our group to address some of these questions  We provide ef   cient methods for updating recommendation models and computing top k recommendations  In addition to recommending individual items  we propose methods to recommend packages subject to user de   ned constraints  We refer to this integrated framework as TopRecs   Our goal is to design an ef   cient  scalable and    exible recommender system for the next generation suite of applications ###  1  INTRODUCTION Recommender systems use methods such as collaborative    ltering in order to make personalized recommendations to users  Collaborative    ltering has become a prominent approach for making predictions and personalized ranking  It forms the core business of companies such as Amazon and Net   ix  Many sophisticated algorithms have been proposed and much effort has been devoted to improving the accuracy of predictions 1   Most of this research has been concerned with what we regard as    rst generation recommender systems  Ever since the database community got interested in recommender systems  people have begun investigating questions related to ef     ciency  scalability     exibility and functionality  6  17   Following this line of work  our group in UBC has initiated several projects to push the envelope on recommender systems by considering    exible recommender systems which can ef   ciently compute top     items Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Copyright 20XX ACM X XXXXX XX X XX XX     10 00  within their framework  16  and using recommender systems to design packages subject to user speci   ed constraints  9  10   Many of the recommendation algorithms  while highly accurate  have scalability issues  The number of items managed by modern information systems is growing rapidly  Therefore  scalability is one of the serious issues for future generation recommender systems  Recommendation methods try to capture personalized patterns in user feedback data by making assumptions and keeping dense summaries of data  User feedback is typically represented in the form of a sparse matrix that stores existing ratings of users on items  Any item recommendation process has two steps   1  an off line training phase that captures personalized pro   les  either as a model or as a similarity matrix    2  an online recommendation generation process that uses the latest up to date model or similarity matrix to return top     recommendations for a user  Any approach for improving scalability of item recommendation needs to pay attention to both pro   le building and recommendation generation  Section 2   In addition to ef   ciency and scalability  an important limitation of classical recommender systems is that they only provide recommendations consisting of single items  It has been recognized that several applications call for package recommendations consisting of sets  lists or other collections  E g   in trip planning  9  10   a user is interested in suggestions for a set of places  POIs  to visit  If the recommender system only provides a ranked list of POIs  the user has to manually    gure out the most suitable set of POIs  which is often non trivial as there may be a cost to visiting each POI  time  price  etc    and the user may want the overall cost of all POIs to be less than a cost budget  Furthermore  some additional constraints such as    no more than 3 museums in a package        not more than two parks        the total distance covered in visiting all POIs in a package should be     10 km     might render the manual con   guration process even more tedious and time consuming  Another application which needs package recommendation is music list generation  15   where the system needs to recommend users with lists of songs called playlists  and users may have a constraint on the overall time for listening to these songs  and possibly constraints on the diversity of songs in the list  So in these applications  there is a natural need for identifying top     packages for recommending the users with high quality packages which satisfy all the constraints  1 1 TopRecs  We believe that the next generation recommender systems should be ef   cient  scalable  and    exible enough to be tailed to different applications and users    customization requests such as the ability to compose packages and other collections  and enforce user speci   ed constraints  In Figure 1  we show the architecture of our envisioned next generation recommender system that we call TopRecs     In TopRecs     the recommendation engine can choose to recom Rating Matrix Item Recommendation Process Item  Metadata Package Recommendation Process Users Similarity Matrix Top k Item  Recommendation  Algorithm Top k Items Top k Packages TopRecs   Figure 1  Next Generation Recommender System mend either top items or top packages depending on the application and user requests  The item recommendation engine can leverage the user item rating matrix to ef   ciently maintain the item item similarity matrix  then based on this similarity matrix  an ef   cient and scalable top     item recommendation algorithm can provide users with the set of     most interesting items  16   On the other hand  based on the items generated by the item recommendation engine  combined with metadata information  such as price  type and etc   associated with each item  the top     package recommendation engine can return top     packages that users will be interested in  9  10   2  TOP      ITEM RECOMMENDATION Predicting personalized scores of individual items for users is the core task of most recommendation algorithms  We follow itembased collaborative    ltering  CF   4   which is used widely in research and in practice  13  5   In CF  input data is typically represented as a sparse            matrix       of user ratings on items  An entry           shows the existing rating of user        on item          The main task is to predict the unknown ratings using the existing ones  Itembased CF computes and maintains an item item similarity matrix using the existing ratings in       Pearson correlation coef   cient  11   is one of the popular choices for calculating item similarities  In item based CF  the unknown rating                  of        on          is predicted by taking the weighted average of ratings of        on     most similar items to          Equation 1 shows how existing ratings are aggregated to calculate                where                      denotes the set of     items that are most similar to        and are rated by                                          1                                                                                     1  A unique challenge here in providing a score sorted list of items  is the fact that the individual scores to be aggregated for calculating                come from different lists for different items  Candidate item can have a different set of     nearest neighbors among those rated by        and this makes adaptations of classical top     algorithms inef   cient  Our theoretical results in  16  show that adapting classic TA NRA  12  algorithms which are known to be instance optimal in the classical setting  leads to unpredictable performance due to the above challenge  Therefore  we identify the problem of discovering                      for all candidate items to be the costly step in score prediction  For this purpose we propose a novel algorithm called the Two Phase Algorithm  TPH  to overcome this challenge  2 1 Two Phase Algorithm A naive approach for    nding     nearest neighbors of a candidate item        in          s pro   le is to retrieve similarities of        to all           items rated by         Going over all         similarity values and    nding the     highest ones can be done in                           for one item  Thus  the total cost is                               for all items which can be costly in practice if         is large  In order to design a more ef   cient process  we propose a new global data structure       in place of the similarity matrix  Every column of      corresponds to one of the items  Items in each column are sorted using their similarities with respect to the item indexing the column  Thus  the            entry in the            column of      is a pair  item id  sim   where item id is the id of the            most similar item to the            item  The second element of the pair is the similarity between these two items  The main intuition behind the two phase algorithm is the following  Assuming that some                   nearest neighbors of a candidate item        are known     nding     nearest neighbors can be done more ef   ciently  This is regardless of whether         is greater or smaller than      Suppose we know               nearest neighbors of          then    nding the remaining ones can be done in                                           If                no further processing is needed  For                it again takes                            to    nd the     nearest neighbors  All of the required similarity values for    nding     nearest neighbors are in the         columns of      that correspond to rated items  Therefore  we propose our two phase algorithm as follows  In the    rst phase  we choose a similarity threshold and read only those values from these columns that are above the threshold  This leads to discovering a variable number of nearest neighbors for every candidate item  Depending on the number of neighbors found for each item  in the second phase we    nd the exact     nearest neighbors  For those items that we have managed to    nd some neighbors for  the process will be more ef   cient as described earlier  Figure 2 illustrates the process using a threshold value        0 72  In the    rst phase all of the entries above the threshold are read as shown on the left side  In this example           3  Notice that for both cases of       1 or 2  the process can be done more ef   ciently for three out of four candidate items  2 2 Optimal Threshold      The cost of the two phase algorithm              can be written as the sum of three main components   1  Cost of the    rst phase      1          2  Cost of    nding     nearest neighbors when                    2          3  Cost of    nding     nearest neighbors when                    3         For instance  assuming the example in Figure 2 and       2     5 falls in the second category and    6 falls in the third category  While for    4 and    7  we have already found their 2 nearest neighbors  Using smaller      results in greater     1 and     3 and smaller     2  This is due to the fact that more similarity values will pass the    lter and make it to the second phase  On the other hand      2 increases if we use a larger threshold and the other two components decrease  Therefore  there is a tradeoff between     1 and     3 on one hand and     2 on the other  Optimal threshold value is one that minimizes the total cost of all components put together  We perform a probabilistic cost based analysis in  16   in order to    nd the optimal threshold value  In  16   we    t a Gaussian probability distribution to the similarity values in the similarity matrix  Using the cumulative density function  we calculate the probability that a particular similarity value can result in one of the     nearest neighbors of another item  Our cost function provides an upper bound on the expected cost of both phases together  We    nd the optimal similarity threshold that minimizes             Moreover  we theoretically prove that due to the tradeoff between cost components             always has one and only one minimum  We refer the reader to  16  for more details  where we also empirically evaluate our algorithm  Our empirical results con   rm the reliability of our theoretical probabilistic process for    nding the optimal threshold value which in turn leads to a consistently ef   cient performance of the top     recommender algorithm  Figure 3  shows part of our experimental results that shows the effectiveness of the threshold chosen using our method on the performance of the two phase algorithm                takes as input a similarity threshold and performsFigure 2  An example of running the two phase probe step using a prob threshold of        0 72 and comparing it to naive algorithms its two steps according to this threshold as illustrated by Figure 2  In this experiment we vary this threshold from 0 2 to 1 increasing by 0 1 in each step  We also measure the performance using our theoretically found optimal threshold circled in the    gure  We    nd that our theoretically found value for all sizes results in a reliable performance  Particularly  in one case           1000                performed more ef   ciently using our value compared to the other tested values  The shape of    gures obtained by this experiment also highlights the correctness of our results regarding uniqueness of the minimum  We refer the reader to  16  for more details  Figure 3  Performance of                using different values of       2 3 Updating the Similarity Matrix Several measures have been proposed for calculating item similarities the most popular of which is Pearson correlation  It is possible to provide guidelines in order to keep the similarity matrix updated for most of the measures  Here we show the process for Pearson correlation  Equation 2 shows how similarity between two items        and        is calculated using Pearson correlation coef   cient  It measures the similarity between two items using only ratings of users who have rated both items                                                                                                                                                                                                        2                                                                2                                     2  When a new rating becomes available by         there are         entries in the similarity matrix that need to be updated  Computing all of these values from scratch can be costly  It is more ef   cient to break down the Equation 2 into a number of components that can be partially updated  Figure 4  shows these partially updatable components  More speci   cally  four matrices are required which have the same size as the similarity matrix and one vector maintaining average rating of each item  Figure 4  shows how we can rewrite an entry of the similarity matrix using these components  Intermediate Matrices r i j     New rating Update statistics Rewriting similarity matrix using intermediate matrices  Update similarities Figure 4  Updating the similarity matrix with                      As an example it is well known how to update the average of a list of numbers iteratively  Given that we know the previous average rating           of a user  When the new rating becomes available we can upd</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09do3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09do3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_database_optimization"/>
        <doc>Incremental Maintenance of Length Normalized Indexes for Approximate String Matching ### Marios Hadjieleftheriou AT T Labs     Research Florham Park  NJ  USA marioh research att com Nick Koudas University of Toronto Toronto  ON  Canada koudas cs toronto edu Divesh Srivastava AT T Labs     Research Florham Park  NJ  USA divesh research att com ABSTRACT Approximate string matching is a problem that has received a lot of attention recently  Existing work on information retrieval has concentrated on a variety of similarity measures  TF IDF  BM25  HMM  etc   speci   cally tailored for document retrieval purposes  As new applications that depend on retrieving short strings are becoming popular  e g   local search engines like YellowPages com  Yahoo Local  and Google Maps  new indexing methods are needed  tailored for short strings  For that purpose  a number of indexing techniques and related algorithms have been proposed based on length normalized similarity measures  A common denominator of indexes for length normalized measures is that maintaining the underlying structures in the presence of incremental updates is ine   cient  mainly due to data dependent  precomputed weights associated with each distinct token and string  Incorporating updates usually is accomplished by rebuilding the indexes at regular time intervals  In this paper we present a framework that advocates lazy update propagation with the following key feature  E   cient  incremental updates that immediately re   ect the new data in the indexes in a way that gives strict guarantees on the quality of subsequent query answers  More speci   cally  our techniques guarantee against false negatives and limit the number of false positives produced  We implement a fully working prototype and illustrate that the proposed ideas work really well in practice for real datasets  Categories and Subject Descriptors H 2  Information Systems   Database Management General Terms Algorithms ### 1  INTRODUCTION A variety of applications deal with short strings  Examples include directory search that retrieves business listings relevant to a short query string  e g   YellowPages com  Yahoo Local  Google Maps  and data cleaning and record Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  linkage applications on relational data that try to match records across tables and databases  e g   employee names and addresses   16  8   Given that queries often contain spelling mistakes and other errors  and stored data have inconsistencies as well  e   ectively dealing with short strings requires the use of specialized approximate string matching indexes and algorithms  Previous work on information retrieval has focused mostly on document retrieval  e g   Google  11   Lucene  1   FAST  10    Although fundamentally documents are long strings  these approaches make assumptions that in general are not true when dealing with shorter strings  For example  the frequency of a term in a document might suggest that the document is related to a particular query or topic with high probability  while the frequency of a given token or word in a string does not imply that a longer string  containing more tokens  is more similar to the query than a shorter string  Or the fact that shorter documents are preferred over longer documents  for parsimony  con   icts with the fact that in practice for short queries the vast majority of the time users expect almost exact answers  answers of length similar to the length of the query   This is compounded by the fact that for short strings length does not vary as much as for documents in the    rst place  making some length normalization strategies ine   ective  Moreover  certain other properties of short strings enable us to design very fast specialized approximate string matching indexes in practice  2  22  13  18   In many applications it is not uncommon to have to execute multiple types of searches in parallel in order to retrieve the best candidate results to a particular query  and use a    nal ranking step to combine the results  e g   almost exact search versus sub string search  ignore special characters search  full string search or per word search  2 grams or 3  grams or 4 grams  edit distance versus TF IDF search   From our experience  when dealing with short string queries  the majority of users are looking for almost exact matches  e g   the queries    wallmart        wal mart     and    wal mart    should all return the correct listing    walmart     instead of    walgreens wallpaper mart      In that respect  we would like to build indexes that can retrieve almost exact matches as fast as possible  and revert to more   fuzzy     and hence slower  searches as a subsequent step  in order to be able to provide interactive responses for the majority of queries  e g   in the form of a text completion box as the user is typing the query   Recently  the work in  13  showed that using L2 length normalization when building the inverted indexes enables us to retrieve almost exact matches almost for free by using very aggressive pruning strategies on the inverted lists  Nev ertheless  the drawback of this approach is that the indexes are expensive to construct and they do not support incremental updates  Generally speaking  even though various types of length normalization strategies have been proposed in the past  approaches that have strict properties that can enable aggressive index pruning are hard to maintain incrementally  while simpler normalization methods are easier to maintain but su   er in terms of query e   ciency and result quality  yielding slower answers and signi   cantly larger  i e   fuzzier  candidate sets  A key issue that we have to deal with in a real system is that data is continuously updated  A small number of updates to the dataset would necessitate near complete recomputation of an L2 normalized index  since L2 is sensitive to the total number of records in the dataset  and the distribution of tokens  n grams  words  etc   within the strings  Given that datasets tend to contain tens of millions of strings and that strings could be updated on an hourly basis  recomputation of the indexes can be prohibitively expensive  In most practical cases  updates are bu   ered and the indexes are rebuilt on a weekly basis  Index recomputation typically takes up to a few hours to complete  However  the online nature of some applications necessitates re   ecting updates to the data as soon as possible  Hence  being able to support incremental updates as well as very e   cient query evaluation are critical requirements  In  15  two techniques were proposed for enabling propagation of updates to the inverted indexes  The    rst was blocking the updates and processing them in batch  The second was thresholding updates and performing propagation in multiple stages down the index  depending on the update cost one is willing to tolerate  That work presented heuristics that perform well in practice  based on various observations about the distribution of tokens in real data  but it did not provide any theoretical guarantees with respect to answer accuracy while updates have not been propagated fully  In our work we develop an update propagation framework on length normalized inverted indexes that incorporates updates very e   ciently and at the same time provides strict guarantees on the precision of query answers on the updated index  We show that our algorithms can propagate a batch of daily updates in a matter of minutes  rather than the few hours that it would take to rebuild the index  while the updated index can be used to answer queries very e     ciently with no false negatives and a small number of false positives with respect to the answers that would be provided by fully propagated updates  We study the properties of the proposed updating framework theoretically using a rigorous analysis  and illustrate its e   ciency on real datasets and updates  through a comprehensive experimental study  Section 2 discusses length normalized inverted indexes in detail  Section 3 presents a high level description of the proposed update propagation framework  In Section 4 we conduct a detailed theoretical analysis of the approach  Section 5 discusses the update propagation algorithm in detail  Section 6 presents a thorough experimental evaluation  Related work is discussed in Section 7  Finally  Section 8 concludes the paper  2  PRELIMINARIES Indexes for approximate string matching are mostly based on token decomposition of strings  e g   into n grams or words  and building inverted lists over these tokens  Then  similarity of strings is measured in terms of similarity of the respective token sets  e g   by using the vector space model to compute cosine similarity  or positional tokens to estimate edit distance   Consider strings    Walmart    and    Wal mart     We can decompose the two strings in 3 gram sets     Wal    1     alm    2     lma    3     mar    4     art    5  and     Wal    1     al     2     lm    3      ma    4     mar    5     art    6   The two sets have three 3 grams in common  one of which matches in position exactly  while the other two match the position within distance one  Using the two sets we can compute both TF IDF based cosine similarity scores and an upper bound on the edit distance between the two strings  Formally  consider a collection of strings D  where every string consists of a number of tokens from universe U  For example  let string s    t 1           t n    t i     U  Let df t i   be the total number of strings in D containing token t i and N be the total number of strings  Then  idf t i     log2  1   N df t i      1  Another popular de   nition of idf is based on the Okapi BM25  20  formula  idf t i     log2 N     df t i     0 5 df t i     0 5    2  The L2 length of string s is computed as L2 s    sX t i   s idf t i   2    3  Pand one can also compute simpler lengths based on L1 s    t i   s idf t i P    or the number of tokens normalization L0 s    t i   s 1  De   ne the L2 normalized IDF  BM25 similarity of strings s1 and s2 as  S2 s1  s2    X t i   s1   s2 idf t i   2 L2 s1 L2 s2     4  assuming that for short strings the token frequency of the majority of tokens is equal to 1  L2 normalization forces similarity scores in the range  0  1   We can de   ne score functions with similar properties for L1  On the other hand  with L0 normalization the range of similarity scores is bounded only by the maximum string length in the dataset  Furthermore  for L2 an exact match to the query always has similarity equal to one  it is the best match   On the other hand  for L0 an exact match to the query is not always the best match  In other words  exact matches can have similarity with the query smaller than non exact matches  This is easy to see with an example  Referring to Figure 1  assume that q    t 1   t 2   t 3   with L0 q    3  Let similarity S0 q  s    P t i   q   s idf t i   2  L0 q L0 s   Let data strings s1    t 1   and s4    t 1   t 2   t 3    Then  S0 q  s1    100 3   S0 q  s4    168 9  It is easy to see that irrespective of the actual similarity function used we can always construct such examples  Given that we are interested in identifying almost exact matches fast  the bene   ts of using L2 normalization are evident  Consider an approximate string matching query that  given a query string q  retrieves from the dataset all strings s     D   S  q  s           It can be shown that by using S2 similarity  Theorem 1  Length Boundedness  13    Given query string q  string s and threshold      if S2 q  s         it follows that    L2 q      L2 s      L2 q      9 5 2 1 idfs  10 8 2      1  5  5  7 id 3 4 2  1      4  4          1  1  56 6 3 4 4 8 t 1 t 2 t 3 w  1  1 Figure 1  Inverted lists sorted by decreasing token contribution in the overall score  Clearly  by using Theorem 1 we can immediately prune all strings whose lengths fall outside the given bounds  hence enabling very aggressive pruning of indexed strings  Notice that given the length of the query string  the theorem determines which database strings have lengths either too short or too long to actually exceed the user de   ned similarity threshold  This is irrespective of the number of tokens these strings have in common with the query  or the magnitude of the idfs of the common tokens  On the other hand  it can be shown easily using counter examples  that no such property holds in the vector space model for L0  Hence  the bene   ts of using L2 in terms of query answering e   ciency are also evident  experimental evidence of which appears in Section 6   Typical approximate string matching indexes consist of inverted lists built on the tokens in U  Formally  let  w s  t i     idf t i   L  s    5  be the partial score contribution of token t i in S  q  s   independent of q  We construct one inverted list per token t i     U  that consists of one pair hs  w s  t i  i per string s containing t i   An example of inverted lists corresponding to three tokens t 1   t 2   t 3 appears in Figure 1  where data strings from the database are associated with a unique identi   er in the index  to save space  Let q    t 1           t n   with length L  q   We can    nd all strings that exceed similarity score    by scanning the inverted lists corresponding to t 1           t n and computing S  q  s  for all s  If lists are sorted by string id we can use multi way merge sort based algorithms to compute string scores very fast  Still  these algorithms need to read lists exhaustively in order to    nd all strings with similarity larger than      Alternatively  assume that lists are sorted in decreasing w s  t i   order  and secondarily in increasing string id order   Given monotonic similarity functions  e g   Equation  4    we can use TA NRA  9  13  algorithms to compute the scores incrementally  and terminate before exhaustively reading the lists  Moreover  when using L2 lengths to build the lists  we can make use of Theorem 1 to restrict the part of the lists we scan within the window    L2 q      L2 s      L2 q      since lists are implicitly sorted by increasing string lengths when they are sorted by decreasing partial weights w   dramatically limiting the total size of the lists we have to examine  hence improving e   ciency of query answering  An example is shown in Figure 1  Assume that query q consists of only three tokens t 1   t 2   t 3   In order to compute the similarity score of all database strings that have at least one token in common with the query we simply have to scan the three inverted lists corresponding to the query tokens and sum up the partial weights  multiplied by the query token weights as in Equation  4    Notice that only string id 4 is contained in all three lists  There are two cases here  Either string 4 is the same as the query  or string 4 is a super set of the query  By using the length of string 4 we can conclusively determine either case  Assuming that the partial weights in the lists have been computed using L2 lengths  we can easily see that the length of string 4 is equal to w 4  t 1     10 L2 4      L2 4    10 0 5   20  while the length of the query string is     10 2   8 2   2 2   12 96  Now  given a similarity threshold    and by using Theorem 1 we can compute tight partial weight ranges within each list that we have to examine  in order to deterministically    nd all strings with similarity larger than      The extra cost of building the length normalized inverted lists stems from the need to sort the lists in decreasing order of partial weights  The prohibitive cost of supporting incremental updates stems from the fact that L2 lengths depend on token idfs  idfs change as updates occur  and hence the length of any string may change even if the string itself did not change  In contrast  notice that with L0 normalization the length of a string does not depend on the idfs of the string tokens  hence maintaining the lists is easier and less expensive  On the other hand  L0 normalization is more relevant to document retrieval  where there is no focus on    nding almost exact matches to query strings  L0 does not o   er the query answering e   ciency advantages of L2  and    nally  the proposed lazy update propagation framework for L2 will result in incremental updates that are as fast as fully propagated updates for L0  and with an insigni   cant drop in query answer precision  3  PROPAGATING UPDATES 3 1 Building from scratch The easiest way to propagate a set of updates is to rebuild the inverted lists from scratch  For large datasets this is infeasible on a frequent basis since this is a very expensive operation  especially for length normalized indexes  In this section we give a detailed analysis of the construction process to clearly illustrate the costs associated with building these indexes  The    rst step of the construction process is a linear scan over the data strings  pre processing of the data to format it appropriately  make it case insensitive  remove special characters  stemming  etc    extraction of tokens  n grams or words  and computation of token frequencies  At the end of the    rst pass we can compute the idf of each token  After idfs have been computed  a second pass over the data is performed  after performing exactly the same pre processing on the strings that occurred during the    rst pass   1 During the second pass  the tokens of every string are generated once again  the normalized length of the string is computed  using the idfs from the previous step for L2 normalization   and the sorted inverted lists are created  Notice that for L0 normalization we only need to perform one pass over the data since the idfs are not needed for computing the length of the strings  The goal of the second pass is to prepare the tokens for building sorted inverted lists  It should be mentioned here that if we are not willing to tolerate the cost of sorting the lists  we can always use unsorted lists and exhaustive scan 1 Alternatively we can store the pre processed strings in an intermediate    le in case pre processing is expensive Algorithm 3 1  Construct D  for each s     D   do     Pre process s for each t     s   df t     1 for each s     D   do     Compute L2 s  for each t     s   appendhs  w s  t itolist t  for each t do     Sort list t  on w Build B tree Figure 2  Selection sort construction algorithm  querying strategies instead of TA NRA algorithms  with either L0 or L2   but for a steep penalty on query performance  as will be seen in Section 6   In the rest we concentrate only on sorted inverted lists  There are two ways to accomplish this task  The    rst is to prepare the tokens for external sorting  lexicographically by token and numerically by partial weight   After performing the external sort the    nal sorted    le can be split easily into one inverted list per token  e g   by bulk loading one B tree  7  per token   The second option is to use a version of selection sort directly to secondary storage as tokens are being generated  The idea is to create one empty inverted list per token  with the potential of having to manage thousands of    les at a time and thrashing the disk  and store tokens in their respective    les as they get generated  Then  after all inverted lists have been populated  we scan and sort each inverted list by partial weight independently  either in main memory if the list is small or using external sorting if the list is large   Alternatively  we can directly create one B tree per token and direct each token to the appropriate B tree as it is generated  again with the adverse e   ect of thrashing the B trees  A sample list construction algorithm is shown in Algorithm 3 1  At    rst it might appear that using external sorting is more e   cient than having to manage a very large number of open    les or B trees for the selection sort step  Nevertheless  there are several trade o   s that need to be taken into account  The cost of using bu   ered    les is that    les need to be    ushed  closed and reopened on demand as main memory becomes full  The cost of directly inserting into B trees is the random I O incurred per B tree insertion  The external sort approach has to read and write each token to disk multiple times  at least once in the beginning to create the initial buckets  a second time for the    rst merging step  and a third time for splitting the    nal sorted    le and creating the B trees   which can become prohibitive when dealing with tens of millions of short strings that get decomposed into several hundred million tokens  On the other hand  external sort will scale irrespective of the total available main memory size and dataset size  Our experiments show that when there are only a few lists  hundreds to a few thousand   selection sort directly to bu   ered    les and subsequent bulk loading of B trees outperforms external sorting on small data sets  up to tens of millions of tokens   However  when the number of lists is huge  or the data sets are very large  several hundred million tokens   external sorting is the fastest  For example  indexing the 2 5 million author names in the DBLP dataset  17  produces 40 million 3 grams and 42 million 4 grams  For this dataset we have to manage approximately 24 000 inverted lists when using 3 grams and ID Author Name   of papers 1 Michael Carrey 36 2 David DeWitt 33 3 Surajit Chaudhuri 31 4 Je   rey Naughton 29 5 Divesh Srivastava 28 6 Michael Stonebraker 28 7 Joseph Hellerstein 27 8 Hector Garcia Molina 26 9 Raghu Ramakrishnan 26 Table 1  Authors with more than 25 papers in ACM SIGMOD until 2007  161 000 for 4 grams  Clearly  as the number of lists explodes  it becomes more and more expensive to manage the bu   ered    les needed for selection sort  while external sort uses a    xed amount of memory irrespective of the number of lists  On the other hand  for the Business Listings dataset  see Section 6  that generates approximately 365 million n grams  selection sort is almost as e   cient as external sorting  3 2 Incremental updates  An example In the rest we focus on L2 normalized indexes  since L0 is easier to maintain  Consider Table 1 containing the names of authors who have more than 25 papers in ACM SIGMOD  In the following analyses and examples we will assume that all strings are decomposed into n grams  and speci   cally view the tokens in U as n grams  For example string    Michael Carrey     decomposed into 3 grams  becomes        M         MI        MIC                CA        CAR        ARR        RRE        REY        EY         Y        The table contains 153 distinct 3 grams  including the special beginning and end of word 3 grams and after ignoring case   Only fourteen of them appear in more than one string  The most frequent 3 grams are    CHA    and    N       with three appearances  Consider now that we build the inverted lists corresponding to the 153 3 grams  and that insertions  deletions and modi   cations arrive at regular time intervals  A single insertion or deletion of a string changes the total number of strings N in the table  and hence theoretically the idfs of all 3 grams  according to Equations  1  and  2   Complete propagation of the update would require recomputation of the length of each string  see Equation  5    and hence updating all 153 lists  Consider now a modi   cation of a single string  for example    xing the spelling mistake    Michael Carrey    to    Michael Carey     This modi   cation has three consequences  1  A change in the length of string 1  2  The deletion  and subsequent disappearance  of 3 grams    ARR    and    RRE     3  The insertion of 3 gram    ARE     which is a new 3 gram  A by product of consequence 1 is that the partial weight of string 1 has to be updated in all inverted lists corresponding to the    fteen 3 grams comprising string 1  Nevertheless  consequences 2 and 3 have minor e   ects  The idfs of existing 3 grams do not get a   ected  since the deleted 3 grams were contained only in the modi   ed string  and the newly inserted 3 gram did not exist in the table before the insertion  Alternatively  consider the modi   cation    Michael Carrey    to    Micael Carrey     essentially deleting 3 grams    ICH        CHA     and    HAE     The by product of deleting one occurrence of 3 gram    CHA     and hence changing the idf of this 3 gram  is that the lengths of all three strings containing this 3 gram change  This in turn means that the 46 lists correspondingt 7 s1 s2 s3 s4 s5 s6 s7 t 1 t 2 t 3 t 4 t 5 t 6 Figure 3  The inverted index represented conceptually by a sparse matrix M  with the actual ordering of strings within each list not being portrayed   to the 3 grams contained in these three strings need to be updated  since they contain partial weights computed using the old lengths of the strings  Propagating an update that changes the idf of a very frequent 3 gram necessitates updating a large fraction of the inverted lists  Clearly  in order to be able to support incremental updates we need to propagate updates more e   ciently  by relaxing the exact recomputation of idfs  but at the same time being able to deterministically compute query results with strict guarantees on the quality of answers  3 3 Detailed analysis First  we analyze the e   ects of fully propagating insertions  deletions and modi   cations to the inverted index  Then  we show how to propagate the updates in stages  in order to reduce the update cost in a way that limits the maximum loss in precision  when compared to a fully updated index  For ease of exposition  consider that the inverted index is represented conceptually by a sparse matrix M where each row corresponds to a string in the database  and each column to a token from U  Each cell Mi j contains a partial weight w si  t j   if string si contains token t j   and zero otherwise  An example is shown in Figure 3  partial weights omitted for clarity   3 3 1 Insertions  Consider the example in Figure 3  An insertion can have one or more of the following consequences  1  It can generate new tokens  and thus the creation of new columns in M  For example token t 7   after insertion of string s7  2  It might require adding new strings in existing inverted lists  as for columns t 2   t 5   t 6    hence a   ecting the idfs of existing tokens  3  Most importantly  after an insertion the total number of strings N increases by one  As a result the idf of every single token gets slightly a   ected  which a   ects the length of every string and hence all partial weights in matrix M  A fully propagated update would have to touch every single non empty cell in M  4  String entries in inverted lists that have no connection to the directly updated tokens might need to be updated  This happens when the length of a string changes due to an updated token  triggering an update to all other lists corresponding to the rest of the tokens contained in that string  5  The order of strings in a particular inverted list can change  This happens when a di   erent number of tokens between two strings gets updated  e g   three tokens in one string and only one token in another   hence a   ecting the length of one string more than the length of the other  Let us concentrate on inverted list t 1   containing strings s4  s5  s6  None of these strings contain any of the inserted tokens t 2   t 5   t 6   t 7   No matter how the lengths of these strings change  due to a change in N   the relative order of their partial weights will remain una   ected  Hence  updating the list requires only updating the partial weights of strings contained therein  Focus now on column t 4   containing strings s1  s2  s4  Notice that s1    t 4   t 5   t 6   and s2    t 2   t 4    Clearly  depending on the new idf of t 2   t 5   t 6   the relative order of the partial weights of these two strings might actually change  The important thing to notice in this example is that an insertion can a   ect the ordering of strings even in lists that are not directly related to the update  Order changes are particularly expensive  If  for example  inverted lists are maintained as B trees with partial weights as the key  simply modifying partial weights can be accomplished with a sequential scan of the leaf level and a bottom up rebuild of the B tree  Changing ordering  on the other hand  requires in addition re sorting the data    rst  Notice also that identifying the lists containing a particular string whose partial weight needs to be updated is an expensive operation  To accomplish this we need to retrieve the actual string and    nd the tokens it is composed of  There are two alternatives for retrieving the strings  First  we can store the exact string along with every partial weight in all lists  This solution of course will duplicate each string as many times as the number of tokens it is composed of  The second option is to store unique string identi   ers in the lists  and perform random accesses to the database to retrieve the strings  This solution will be very expensive if the total number of strings contained in a modi   ed list is too large  3 3 2 Deletions  A deletion has the opposite e   ects of an insertion  A token might disappear if the last string containing the token gets deleted  Various entries might have to be deleted from a number of inverted lists  thus changing the idfs of existing tokens  The number of strings N will decrease by one  Thus  the idf of all tokens  and hence  the lengths and partial weights of all strings will slightly change  causing a cascading e   ect similar to the one described for insertions  3 3 3 Modi   cations  A modi   cation does not change the total number of strings N  and hence does not a   ect the idf of tokens not contained in the strings being updated  Nevertheless due to a modi     cation  new tokens can be created and old tokens can disappear  In addition  a modi   cation can change the idf of existing tokens  with similar cascading e   ects  Going back to the example in Figure 3  assume that string s7 is modi   ed by a deletion of token t 2   The idf of t 2 changes and hence the lengths of all strings contained in list t 2 change  i e  strings s2  s7  This means that we need to modify the partial weights of these strings in every other inverted list that contains them  i e   lists t 4   t 5   t 6   t 7 in the example 3 4 Relaxed propagation Fully propagating updates for L2 normalization is infeasible for large datasets if updates arrive frequently  The alternative is to relax equations  1       4  in order to limit the cascading e   ect of a given update  We can partially propagate updates in a way that provides guarantees on the quality of query answers  3 4 1 Relaxation of N  What causes the need for a complete recomputation of the index during insertions and deletions is the change of token idfs due to the modi   cation of the total number of strings N  We can introduce a slack in how often we update N and keep idfs constant within a range of updates  Let Nb be the total number of strings when the inverted index was built  Then  Nb was used for computing the idfs of all tokens  Assume that we do not require a recomputation of idfs  unless if the current value of N diverges signi   cantly from Nb  we quantify this change in the next section   Given a query q we need to quantify the loss of precision in evaluating the relaxed similarity S     2  q  s  for all s     D  given idfs computed using Nb instead of N  Intuitively  we do not expect the idfs to be a   ected substantially given the log factor in Equations  1  and  2  for reasonable divergence of N  Relaxing N alleviates the need to recompute all idfs on a regular basis  Nevertheless  whenever N deviates outside some prede   ned bounds  we will have to rebuild the inverted index from scratch  The hope is that for balanced insertions and deletions  this will rarely occur  3 4 2 Relaxation of df  Assume now that we would also like to limit the e   ect of an update when the idf of a speci   c token changes  due to an insertion  deletion  or modi   cation  Remember that a single token idf modi   cation can have a dire cascading effect on a large number of inverted lists  as already discussed in Section 3 3 3  Assume that the idf of token t i has been computed using document frequency dfp t i   at the time the inverted index was built or the last time updates were propagated to the index  In this case we allow some slack in the recomputation of the exact idf of t i by allowing the current document frequency df t i   to vary within some prede   ned range  before actually updating idf t i    First  notice that the e   ect of a small number of updates to a particular token is insigni   cant due to the log factor in Equations  1  and  2   In addition  the hope is to amortize the cost of propagating changes of frequently updated tokens  We analyze the exact loss in precision in Section 4  Notice that the most severe cascading e   ects during updates are caused by the most frequent tokens  i e   the tokens with large document frequency df t i    and hence low inverse document frequency idf t i    The most frequent tokens are obviously the ones that have highly populated inverted lists  and hence the ones causing the biggest changes to the inverted index during updates  In expectation  these will also be the tokens that will be updated more frequently  It is hence critical to limit changes to these tokens as much as possible  In our favor  notice that low idf tokens are actually the ones that contribute the least in similarity scores S2 q  s   due to their small partial weights  Essentially  by delaying update propagation to low idf tokens  we limit the cost of updates signi   cantly  and at the same time marginally a   ect query answer precision  4  ANALYSIS OF LOSS IN PRECISION In this section we analyze the exact loss in precision from delayed propagation theoretically  Let Np  dfp t i    idfp t i   be the total number of strings  the document frequencies  and the inverse document frequencies of tokens in U at the time the inverted index is built or the last time updates were propagated  Let N  df t i   and idf t i   be the current  exact values of the same quantities  Given a fully updated inverted index and a query q  let the exact similarity score between q and any s     D be S2 q  s   Assuming now delayed propagation  let the approximate similarity score computed using quantities  p be S     2  q  s   Our goal is to precisely quantify the relation between S2 and S     2   To simplify our analysis assume that the total possible divergence in the idf of t i   by considering the divergence in both N and df t i    is given by  idfp t i          idf t i             idfp t i     6  for some value     We will analyze the loss of precision with respect to     and in doing so  we don   t have to worry about actual changes in N or whether all idfs have been computed using the same N value or not  as long as all idfs are within the relaxation bounds  Notice that our analysis is independent of the particular form of the idf Equations  1  and  2   and will also hold for all other idf alternatives  Consider query q and arbitrary string s     D  Their cosine similarity is equal to S2 q  s    P t i   q   s idf t i   2 qP t i   s idf t i   2 qP t i   q idf t i   2   Let x   P t i   q   s idf t i   2 be the contribution of the tokens common to both q and s to the score  Let y   P t i   s  q   s  idf t i   2 be the contribution of tokens in s that do not appear in q  and z   P t i   q  q   s  idf t i   2 the contributions of tokens in q that do not appear in s  Thus  S2   f x  y  z    x     x   y     x   z    7  Notice that if q   s  then y   z   0  Our derivation will be based on the fact that function  7  is monotone increasing in x  and monotone decreasing in y  z  for positive x  y  z  It is easy to see that the latter holds  We prove the former  Lemma 1  f x  y  z        x x y     x z is monotone increasing in x  for positive x  y  z  Proof  Consider the function g x  y  z    1 f x  y  z  2   f x  y  z  is monotone increasing in x i    g x  y  z  is monotone decreasing  g x  y  z     x   y  x   z  x2   1   y   z x   yz x2   Since 1 x and 1 x 2 are monotone decreasing in x  g x  y  z  is monotone decreasing in x  hence f x  y  z  is monotone increasing in x  Given the de   nition of x  y  z and relaxation factor     it holds that  xp    2     xc        2    xp  8  yp    2     yc        2    yp zp    2     zc        2    zp where xp  yp  zp are with respect to propagated idfs  and xc  yc  zc are the current  exact values of the same quantities  We are given an inverted index built using idfs idfp t i    and a query q with threshold      We need to retrieve all strings s     D   S2 q  s           What is a threshold    0      s t  retrieving all s     D   S     2  q  s         0 guarantees no false dismissals  Notice that for any s  given Lemma 1  the current score S2 q  s  can be either larger or smaller than S     2  q  s   depending on which tokens in x  y  z have been a   ected  If    s   S     2  q  s    S2 q  s   we need to introduce threshold    0      to avoid false dismissals  Hence         S2        2 xp p xp    2   yp    2 p xp    2   zp    2      4 S     2  9         0          4   Clearly this bound is very loose and may introduce a very large number of false positives in practice  even for small slack     We consider now a more involved analysis that shows that given a relaxation factor    the actual loss in precision is a much tighter function of     We want to quantify the divergence of S     2 from S2  constrained on inequalities  8  and S2 q  s           The query can be formulated as a constraint optimization problem  Minimize f xp  yp  zp  constrained upon  f xc  yc  zc          10  xc    2     xp        2    xc yc    2     yp        2    yc zc    2     zp        2    zc  where inequalities  8  have been re written after solving for xp  yp  zp  instead of xc  yc  zc  the exact same inequalities actually result in this case   Theorem 2     0           4  1       is the minimum value of f xp  yp  zp  that satis   es all of the constraints in  10   Proof  First we show that f x  y  z  is minimized for y   z  Let v    y     z  2 and w    y   z  2  f x  y  z  is minimized  when g x  y  z    f 2  x  y  z  is minimized  for positive x  y  z   g x  y  z    x 2  x   w  2     v 2   g x  y  z  is minimized when the denominator is maximized  i e   when v 2   0     y   z  given that y  z are independent of each other and v  w are only a rotational transformation of y  z   Now  f xp  yp  yp  is further minimized when xp is minimized and yp  or zp  is maximized  according to Lemma 1  Hence  f xp  yp  zp  is minimized at  f xc    2      2 yc     2 yc    xc xc      4 yc    11  Consequently  f xc  yc  zc              12  xc xc   yc            yc     xc 1             Substituting Equation  12  into  11  we get  f xc    2      2 yc     2 yc      xc xc   xc   4 1          13               4  1            Thus     0              4  1             14  satis   es all constraints  It is not hard to see that Equation  14  is always a tighter bound than  9  for all values of    and     Hence it is expected to yield signi   cantly fewer false positives in most practical cases  while guaranteeing no false dismissals  To see this  consider the following example  For      1 1 and a query threshold      0 9  we need to lower the threshold to    0   0 86  according to Equation  14   to guarantee no false dismissals  a very small decrease which in practice is expected to yield a small number of false positives  In contrast  by using the loose analysis and Equation  9   we would have to reduce the threshold to    0   0 61 to achieve the same guarantee  a signi   cant threshold decrease  4 1 Practical considerations We discuss here some practical considerations with respect to the lower bound presented in the previous section  Notice that our analysis assumed for simplicity the worst case scenario  where all token idfs take either their smallest or largest possible value  In practice  of course  the extreme values might not have been reached for all tokens  Notice that at query evaluation time we know the exact deviation of every token   s propagated idf from its correct value  Clearly  we can take this information into account to limit false positives even further  We propose the following  First  we maintain the global maximum deviation           among all token idfs in U  Then  at query time we compute the maximum deviation           among all token idfs in q  In deriving a lower bound for threshold    0   we use    as a relaxation factor for xp  zp  the tokens in q     s and q    q     s    and    for yp  the tokens in s    q     s    This lower bound in practice will be tighter than  14   Finally  we discuss the practical meaning of relaxation factor     Assuming modi   cations only  and hence a    xed value N  the basic intuition is that by allowing the idfs to deviate from their computed values by no more than factor     we are essentially allowing token dfs to increase or decrease by a certain amount  A compound increase or decrease of a token   s df will result in the need to propagate updates  Nevertheless  for low idf tokens  with very large df values  the probability of a compounded increase or decrease in the order of several thousands  is very small  On the other hand  for high idf tokens  with very small df values   the probability of a compounded increase or decrease by a small number is much higher  This works in our favor  since we would like low idf tokens  which have very small contributions to similarity scores  to be updated very infrequently  and high idf tokens  which have a very important contribution to similarity scores  to be updated more frequently 5  UPDATE PROPAGATION ALGORITHM We focus our attention to engineering issues related with supporting update propagation  We have an inverted index consisting of one inverted list per token in U  where every list is stored on secondary storage  List entries hs  w s  t i  i are stored in decreasing order of partial weights w  To support update propagation we will need to perform incremental updates on the sorted lists  Hence  we store each list as a B tree sorted on w  At index construction time we choose slack     Assume that we bu   er arriving updates and propagate them in batches at regular time intervals  e g   every 5 minutes   Let the updates be given in a relational table consisting of  1  The type of update  insertion  deletion  modi     cation   2  The new data in case of insertions  3  The old data in case of deletions  4  Both the old and new data in case of modi   cations  We also build an idf table consisting of  1  A token t  2  The propagated idf of the token idfp t   3  The current  exact frequency of the token df t   Before applying the batch update to the index  we load the idf table in main memory  In practice  the total number of tokens  U  for most datasets is fairly small  Hence  maintaining the idf table in main memory is inexpensive  In order to update the inverted lists e   ciently we need to localize updates to each list and execute them in batch  taking advantage of data locality and bu   ering  We would certainly like to take advantage of sequential I Os if the Btrees are clustered on disk  over thrashing multiple B trees one after the other  Notice that we are forced to process updates in order of arrival to avoid con   icts  e g   deletions on data that has not been inserted yet   But  localizing updates on a per list basis implies that we should not process updates one by one or in order of arrival  To circumvent this pitfall we use a journaling approach  We maintain a journal of updates on a per list basis in main memory  We process the updates in the update table in order of arrival  and record the necessary modi   cations to the in memory journals  We    ush all changes to the corresponding B trees all at once in the end  after performing certain allowable reorderings to improve performance  If any particular journal is becoming too large to    t in main memory  we    ush the changes to the corresponding B tree and clear the journal  After all journals have been populated  we process each journal in sequence  applying all the required changes to the respective B trees  Notice that the total memory required to store the journals is linear to the size of the update table  B trees use the partial weight of an entry as the key  This implies that    rst  in order to delete an entry  we have to compute its original partial weight  and thus the length of the respective string in order to locate the entry in the Btree  Furthermore  in order to modify an entry whose partial weight  and hence its length  has changed  we need to compute both the old and the new length of the respective string  For that purpose  the update table has to contain the old data for deletions and modi   cations  A modi   cation of a key in the B tree essentially translates into a deletion followed by an insertion  While populating the journals we also maintain the main memory idf table  If a token is inserted we increase its exact document frequency by one  If it is deleted we decrease it by one  Tokens with document frequency zero are removed from the table  New tokens  that did not appear before  are inserted in the table and their current exact idf is computed  After the updates have been propagated to the B trees we Algorithm 5 1  Propagate D  U  Let hash table H containing token df  idfp Let M          Journals J  N for each  op  s      U 8     8if op is an insertion     N    1 for each t     s do 8     if t is a new token do 8     Insert in H df t    1 idfp t    log2  1   N df t    else df t     1 Compute L p 2  s  for each t     s   Add insertion hs  wp s  t i to J t  if op is a deletion 8     N      1 Compute L p 2  s  for each t     s   do     df t       1 Add deletion hs  wp s  t i to J t  Commit journals for each t     H 8     if df t    0   Remove t from H if NOT idfp t         log2  1   N df t        idfp t    do     M     t idf t    log2  1   N df t    for each t     M 8     for each s     inverted list t do 8     Compute new L2 s  for each t 0     s   M do    Add modi   cation hs  w s  t 0  i to J t 0   Rebuild inverted list t Commit journals Figure 4  Update propagation algorithm  need to assess whether any token idfs have deviated more than the allowed slack    in any direction  We scan the idf table and identify the corresponding tokens and compute their idfs  Assuming that multiple token idfs have changed  we need to scan the B trees corresponding to these tokens  and retrieve all strings contained therein  which requires at least one random I O per string id for retrieving the actual strings from the database   Then  we compute the new lengths of the strings  given the updated token idfs  Finally     rst we rebuild the B trees corresponding to tokens whose idf has changed  and also update all other B trees that contain those strings  Every time we process a string we store the string id in a hash table and make sure that we do not process that string again  if it is subsequently encountered in another Btree  this will be the case for strings that contain multiple tokens whose idfs have changed   Pseudocode of the update propagation algorithm is shown in Algorithm 5 1  6  EXPERIMENTS We implemented a full featured prototype to test our update propagation framework with real datasets  The prototype is implemented in C   and we used the open source Oracle BerkeleyDB  19  as the underlying DBMS for the B tree indexes  For our experiments we used the publicly available DBLP citation database  17   and the Business Listing  BL  database from a major online website  YellowPages   All experiments were run on a two four core Intel R  minutes  2 grams 3 grams 4 grams S1 S3 S1 S3 S1 S3 Process strings 0 98 0 98 1 21 1 23 1 56 1 53 Sort n grams 1 58 4 96 6 51 5 61 63 7 6 58 Create B trees 4 8 3 7 97 2 96 69 3 16 8 Total 7 36 8 94 15 7 9 81 134 6 24 91 Table 2  Construction cost for DBLP  Xeon R  CPU 2 66 GHz  with 16 GB of main memory  We focused on a DBLP table containing associations between author names and publication ids for books  articles and proceedings  We downloaded daily snapshots over a 30 day period  February 11th  2008 to March 11th  2008  and computed di   s  which we then used to create update tables  The original table contains 2460433 author id pairs  5712041 total words and 269281 distinct words  The average author name size is 13 characters  and there are 2050 distinct 2  grams  23666 3 grams  and 161213 4 grams in the original table  There were 33461 total updates produced within a month  approximately 1000 updates per day   These consisted of 32121 insertions and 1340 deletions  In reality  deletions correspond to modi   cations of existing records  which we represent as deletions followed by insertions for simplicity  there were no actual deletions from the DBLP database  We also run experiments using the BL database  New business listings are added on a daily basis  hence every listing is associated with an insertion timestamp  We select 15 million entries as the dataset  and another 30 thousand entries as insertions  The dataset contained approximately 52 million words and 633 thousand distinct words  The average business name length is 20 characters  There are 37524 distinct 3 grams  6 1 Construction cost First  we build the inverted index on the    nal DBLP and BL tables  after incorporating all updates   There are three strategies we can use to generate n grams  S1  Selection sort and subsequent bulk loading of B trees  S2  Direct insertion into B trees  S3  External sort and subsequent bulk loading of B trees  We use 4096 byte page sizes for the B trees in all cases  and 2 grams  3 grams  4 grams for this experiment  We use 300MB as an aggregate    le bu   er for S1  S2 and as a main memory bu   er for external sorting  We omit results for S2 since it is not competitive  The time to run each step of the building process is shown in Table 2 and Figure 5  The construction cost increases super linearly to the number of n grams  due to the sorting step   and the number of n grams increases for larger n  making construction more expensive  from 40 million 3 grams to 42 million 4 grams  and from 23K to 161K lists    The most expensive step is the creation of B trees  but as the number of n grams increases  the cost of sorting n grams approaches that of creating Btrees  Notice the overwhelming cost of managing bu   ered    les for S1  both for writing during sorting and for reading during B tree creation  versus the comparative times for S3  From the construction cost of the BL dataset we can see that for large datasets strategy S1 performs as well as S3  for this dataset the n gram generation process for 15M strings creates approximately 365 million 3 grams  versus the 40 million 3 grams for the DBLP table   Increasing the dataset size even further will exacerbate the di   erence between S1 and S3  as the number of n grams increases linearly but the number of lists is almost constant  0 20 40 60 80 100 120 S1 S3 S1 S3 S1 S3 5M 10M 15M Minutes Create B trees Sort n grams Process strings Figure 5  Construction cost for BL  Next  we compare the construction cost of using L0 normalization  L0 normalized lists require only one pass of the data to compute idfs and lengths  Nevertheless  sorting the n grams and creating B trees cannot be avoided  Given that the string processing step is the most inexpensive  7 to 44 times cheaper than B tree creation in practice  it is clear that L0 normalization does not o   er any signi   cant advantages in terms of index construction  it is only marginally faster than L2 construction   At the same time it does not o   er the bene   ts of L2 normalization in terms of query e     ciency  see Section 2   We will also show in the next section that L0 normalization is not signi   cantly faster than the proposed lazy update propagation for L2 normalization in terms of incremental update cost  Given that the cost of extracting the n grams and creating B trees increases super linearly to the total number of strings in the base table  it is clear that for tables with tens of millions of entries  even for relatively short strings  the cost of rebuilding the inverted index quickly becomes infeasible on a daily basis  It should be stressed here that we could avoid building sorted inverted lists and hence having to maintain B trees  but that would result in slower query execution times based on merging strategies that need to exhaustively read lists  We pay the cost of slower update times for faster query execution  6 2 Update propagation cost Next  we construct the inverted index on the original DBLP table  before incorporating the updates   and measure the cost of subsequently updating the index using our propagation framework  Our goal is to be able to apply the updates in a semi real time fashion  where we bu   er updates and propagate in regular time intervals  e g   every 5 minutes   The hope is that propagating even the 1000 daily updates will require signi   cantly less time than 5 minutes  In the rest we concentrate mostly on the DBLP data which is publicly available  since results for the BL dataset followed similar trends  Figure 6 shows the time it takes to propagate a batch of updates for various batch sizes using our algorithm  For this experiment we use a relaxation factor of 3   First  we report the time it takes to parse one batch of updates  extract the 3 grams  retrieve their idfs and populate the journals with the appropriate changes for each B tree  Next  we report the time it takes to commit these changes to the B trees  Finally  we report the time it takes to commit all changes related to idfs that have exceed their relaxation bounds  The most important observation here is that such changes are indeed very small and a   ect only an extremely small fraction of the strings  The bulk of the cost is incurred by the B tree0 2 4 6 8 10 12 14 16 18 20 1000 10000 100000 1000 10000 100000 1000 10000 100000 2 grams 3 grams 4 grams Minutes Idf updates Commit journals Populate journals Figure 6  Cost of propagating one batch of updates for various batch sizes  DBLP  relaxation 3    Batch Size Statistic 1000 10000 100000   of update operations 539767 539767 539767   of out of bounds idfs 95 88 83   of a   ected ids 252 230 221   of a   ected lists 2138 1715 1321   idf related updates 4216 3744 3544 Table 3  Statistics of update propagation for various batch sizes  DBLP  3 grams  3    operations  that cannot be avoided if we want the updates to be re   ected in the index immediately  irrespective of the normalization used  The cost of incremental updating using L0 normalization consists of the    rst two steps  populating and committing the journals  and hence is only marginally faster than our relaxed propagation policy  In other words  we are able to provide the full bene   ts of using L2 normalization  for a small penalty in update cost  In particular  for the 1000 batch size there are a total of 34 batches  The average time to propagate a single batch when using 3 grams is 1 37 minutes for L2 and 1 34 minutes for L0  Similarly  for a 10000 update batch it takes on average 3 97 minutes for L2 and 3 88 minutes for L0  and for the full 33461 updates 7 35 minutes for L2 and 7 03 minutes for L0  Clearly  we are able to process more than 10000 updates on a per 5 minute update window  An indication of the cost of incremental updates is the total number of update operations incurred  Table 3 lists several statistics  here we use relaxation 3  and 3 grams   We list  1  The total number of update operations solely from executing insertions and deletions of strings  2  The number of idfs that fall out of bounds during the update operation  3  The number of string ids contained in the respective inverted lists whose new lengths will have to be propagated  4  The number of lists that will be a   ected by this propagation operation  5  The total number of update operations due to idf changes  The total number of list updates incurred from idf propagation is two orders of magnitude smaller than the total number of normal update operations  This veri   es our intuition that propagating updates for infrequent n grams only will have a very small impact in the overall cost  Notice that as the batch size increases the total number of idfs that falls out of bounds decreases since insertions and deletions average out in the end  before the updates nee</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nns1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nns1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search"/>
        <doc>E   cient and Accurate Nearest Neighbor and Closest Pair Search in High Dimensional Space  ### YUFEI TAO  Chinese University of Hong Kong  KE YI  Hong Kong University of Science and Technology  CHENG SHENG  Chinese University of Hong Kong  PANOS KALNIS  King Abdullah University of Science and Technology  Nearest neighbor  NN  search in high dimensional space is an important problem in many applications  From the database perspective  a good solution needs to have two properties   i  it can be easily incorporated in a relational database  and  ii  its query cost should increase sub linearly with the dataset size  regardless of the data and query distributions  Locality sensitive hashing  LSH  is a well known methodology ful   lling both requirements  but its current implementations either incur expensive space and query cost  or abandon its theoretical guarantee on the quality of query results  Motivated by this  we improve LSH by proposing an access method called the locality sensitive B tree  LSB tree  to enable fast  accurate  high dimensional NN search in relational databases  The combination of several LSB trees forms a LSB forest that has strong quality guarantees  but improves dramatically the e   ciency of the previous LSH implementation having the same guarantees  In practice  the LSB tree itself is also an e   ective index  which consumes linear space  supports e   cient updates  and provides accurate query results  In our experiments  the LSB tree was faster than  i  iDistance  a famous technique for exact NN search  by two orders of magnitude  and  ii  MedRank  a recent approximate method with non trivial quality guarantees  by one order of magnitude  and meanwhile returned much better results  As a second step  we extend our LSB technique to solve another classic problem  called closest pair  CP  search  in high dimensional space  The long term challenge for this problem has been to achieve sub quadratic running time at very high dimensionalities  which fails most of the existing solutions  We show that  using a LSB forest  CP search can be accomplished in  worst case  time signi   cantly lower than the quadratic complexity  yet still ensuring very good quality  In practice  accurate answers can be found using just two LSB trees  thus giving a substantial reduction in the space and running time  In our experiments  our technique was faster  i  than distance browsing  a well known method for solving the problem exactly  by several orders of magnitude  and  ii  than D shift  an approximate approach with theoretical guarantees in low dimensional space  by one order of magnitude  and at the same time  outputs better results  Categories and Subject Descriptors  H2 2  Database Management   Access Methods  H3 3  Information Storage and Retrieval   Information Search and Retrieval Author   s address  Y  Tao  taoyf cse cuhk edu hk   Department of Computer Science and Engineering  Chinese University of Hong Kong  Sha Tin  Hong Kong  K  Yi  yike cse ust hk   Department of Computer Science and Engineering  Hong Kong University of Science and Technology  Clear Water Bay  Hong Kong  C  Sheng  csheng cse cuhk edu hk   Department of Computer Science and Engineering  Chinese University of Hong Kong  Sha Tin  Hong Kong  P  Kalnis  panos kalnis kaust edu sa   Division of Mathematical and Computer Sciences and Engineering  King Abdullah University of Science and Technology  Thuwal  Saudi Arabia  Permission to make digital hard copy of all or part of this material without fee for personal or classroom use provided that the copies are not made or distributed for pro   t or commercial advantage  the ACM copyright server notice  the title of the publication  and its date appear  and notice is given that copying is by permission of the ACM  Inc  To copy otherwise  to republish  to post on servers  or to redistribute to lists requires prior speci   c permission and or a fee   c 20YY ACM 0000 0000 20YY 0000 0001  5 00 ACM Journal Name  Vol  V  No  N  Month 20YY  Pages 1   0   2    General Terms  Theory  Algorithms  Experimentation Additional Key Words and Phrases  Locality Sensitive Hashing  Nearest Neighbor Search  Closest Pair Search ### 1  INTRODUCTION Nearest neighbor  NN  search is a classic problem with tremendous impacts on arti   cial intelligence  pattern recognition  information retrieval  and so on  Let D be a set of points in d dimensional space  Given a query point q  its NN is the point o         D closest to q  Formally  there is no other point o     D satisfying ko  qk   ko       qk  where k  k denotes the distance of two points  In this paper  we consider high dimensional NN search  Some studies  Beyer et al  1999  argue that high dimensional NN queries may not be meaningful  On the other hand  there is also evidence  Bennett et al  1999  that such an argument is based on restrictive assumptions  Intuitively  a meaningful query is one where the query point q is much closer to its NN than to most data points  This is true in many applications involving high dimensional data  as supported by a large body of recent works  Andoni and Indyk 2006  Athitsos et al  2008  Ciaccia and Patella 2000  Datar et al  2004  Fagin et al  2003  Ferhatosmanoglu et al  2001  Gionis et al  1999  Goldstein and Ramakrishnan 2000  Har Peled 2001  Houle and Sakuma 2005  Indyk and Motwani 1998  Li et al  2002  Lv et al  2007  Panigrahy 2006   Sequential scan trivially solves a NN query by examining the entire dataset D  but its cost grows linearly with the cardinality of D  From the database perspective  a good solution should satisfy two requirements   i  it can be easily implemented in a relational database  and  ii  its query cost should increase sub linearly with the cardinality for all data and query distributions  Despite the bulk of NN literature  see Section 8   with a single exception to be explained shortly  we are not aware of any existing solution that is able to ful   ll both requirements at the same time  Speci   cally  a majority of them  e g   those based on new indexes  Arya et al  1998  Goldstein and Ramakrishnan 2000  Har Peled 2001  Houle and Sakuma 2005  Lin et al  1994   demand non relational features  and thus cannot be incorporated in a commercial system  There also exist relational solutions  such as iDistance  Jagadish et al  2005  and MedRank  Fagin et al  2003    which are experimentally shown to perform well for some datasets and queries  Their drawback is that they may incur expensive query cost on other datasets  Locality sensitive hashing  LSH  is the only known solution that satis   es both requirements  i  and  ii   It supports c approximate NN search  Formally  a point o is a c approximate NN of q if its distance to q is at most c times the distance from q to its exact NN o       namely  ko  qk     cko       qk  where c     1 is the approximation ratio  It is widely recognized that approximate NNs already ful   ll the needs of many applications  Andoni and Indyk 2006  Arya et al  1998  Athitsos et al  2008  Datar et al  2004  Ferhatosmanoglu et al  2001  Gionis et al  1999  Har Peled 2001  Houle and Sakuma 2005  Indyk and Motwani 1998  Krauthgamer and Lee 2004  Li et al  2002  Lv et al  2007  Panigrahy 2006   LSH was originally proposed as a theoretical method  Indyk and Motwani 1998  with attractive asymptotical space and query ACM Journal Name  Vol  V  No  N  Month 20YY    3 performance  As elaborated in Section 3  its practical implementation can be either rigorous or adhoc  Speci   cally  rigorous LSH ensures good quality of query results  i e   small approximation ratio c   but requires expensive space and query cost  Although adhoc LSH is more e   cient  it abandons quality control  i e   the neighbor it outputs can be arbitrarily bad  In other words  no LSH implementation is able to ensure both quality and e   ciency simultaneously  which is a serious problem severely limiting the applicability of LSH  Motivated by this  we propose an access method called locality sensitive B tree  LSB tree  that enables fast high dimensional NN search with excellent quality  The combination of several LSB trees leads to a structure called the LSB forest that combines the advantages of both rigorous  and adhoc LSH  without sharing their shortcomings  Speci   cally  the LSB forest has the following features  First  its space consumption is the same as adhoc LSH  and signi   cantly lower than rigorousLSH  typically by a factor over an order of magnitude  Second  it retains the approximation guarantee of rigorous LSH  recall that adhoc LSH has no such guarantee   Third  its query cost is substantially lower than adhoc LSH  and as an immediate corollary  sub linear to the dataset size  Finally  the LSB forest adopts purely relational technology  and hence  can be easily incorporated in a commercial system  All LSH implementations require replicating the database multiple times  and therefore  entail large space consumption and update overhead  Many applications prefer an index that consumes only linear space  and supports insertions deletions e   ciently  The LSB tree itself meets all these requirements  by storing every data point once in a conventional B tree  Based on real datasets  we experimentally compared the LSB tree to iDistance  Jagadish et al  2005   which is a famous technique for exact NN search  and to MedRank  Fagin et al  2003   which is a recent approximate method with non trivial quality guarantees  The LSB tree outperformed iDistance by two orders of magnitude  well con   rming the advantage of approximate retrieval  Compared to MedRank  our technique was consistently superior in both query e   ciency and result quality  Speci   cally  the LSB tree was faster by one order of magnitude  and at the same time  returned neighbors with much better quality  As a second step  we tackle another classic problem  called closest pair  CP  search  in high dimensional space  Here  given a set D of points  the goal is to    nd two points whose distance is the smallest among all pairs of points in D  This problem has abundant applications in geographic information systems  clustering  and numerous matching problems  such as stable marriage  Wong et al  2007    and has been very well solved in low dimensional space  Corral et al  2000  Hjaltason and Samet 1998  Lenhof and Smid 1992   When the dimensionality increases  the challenge has been to achieve sub quadratic running time  namely  faster than the naive approach that simply examines each pair of points in D  Algorithms that work well in low dimensional space generally see their computation cost quickly climb to quadratic even at a moderate dimensionality  The c approximate version of the CP problem is to return a pair of points with distance at most c times the distance of the closest pair  The dimensionality curse haunts this approximate version  too  For example  when the dimensionality can be viewed as a constant  Lopez and Liao ACM Journal Name  Vol  V  No  N  Month 20YY 4     Lopez and Liao 2000  propose an algorithm  which we call D shift  with a constant approximation ratio  i e   c   O 1    As the dimensionality grows  however  their approximation ratio increases super linearly  and thus  becomes unattractive very soon  We conquer the above challenge in this paper by giving an algorithm that runs in time signi   cantly lower than the quadratic complexity and meanwhile  gives a very good worst case guarantee on the quality of results  approximation ratio around 2   regardless of the dimensionality  As in the NN context  although such nice theoretical performance demands a full LSB forest  in practice only 2 LSB trees are already su   cient to return accurate results  thus substantially reducing the space and query cost  In the experiments  we compared the proposed algorithms against distance browsing  Corral et al  2000   which is a well cited exact solution  and the D shift algorithm mentioned earlier  Our technique was faster than distance browsing by several orders of magnitude  and than D shift by one order of magnitude  Moreover  our solutions returned much more accurate answers than D shift  The rest of the paper is organized as follows  Section 2 presents the problem settings and our objectives  Section 3 points out the defects of the existing LSH implementations  Section 4 explains the construction and NN search algorithms of the LSB tree  and Section 5 establishes its performance guarantees  Section 6 extends the LSB tree to provide additional tradeo   s between space query cost and the quality of query results  Section 7 explains how to use LSB trees for closest pair search  Section 8 reviews the previous work directly related to ours  Section 9 contains an extensive experimental evaluation  Finally  Section 10 concludes the paper with a summary of our    ndings  2  PROBLEM SETTINGS Without loss of generality  we assume that each dimension has a range  0  t   where t is an integer  Following the LSH literature  Datar et al  2004  Gionis et al  1999  Indyk and Motwani 1998   in analyzing the quality of query results  we assume that all coordinates are integers  so that we can put a lower bound of 1 on the distance between two di   erent points  In fact  this is not a harsh assumption because  with proper scaling  we can convert the real numbers in most applications to integers  In any case  this assumption is needed only in theoretical analysis  neither the proposed structure nor our query algorithms rely on it  We consider that distances are measured by  p norm  which has extensive applications in machine learning  physics  statistics     nance  and many other disciplines  Moreover  as  p norm generalizes or approximates several other metrics  our technique is directly applicable to those metrics as well  For example  in case all dimensions are binary  i e   having only 2 distinct values    1 norm is exactly Hamming distance  which is widely employed in text retrieval  time series databases  etc  Hence  our technique can be immediately applied in those applications  too  The main problem studied is c approximate NN search  where c is a positive integer  As mentioned in Section 1  given a point q  such a query returns a point o in the dataset D  such that the distance ko  qk between o and q is at most c times the distance between q and its real NN o       We assume that q is not in D  Otherwise  the NN problem becomes a lookup query  which can be easily solved ACM Journal Name  Vol  V  No  N  Month 20YY    5 by standard hashing  A direct extension of NN queries is kNN search  which    nds the k points in D closest to q  The c approximate version of kNN search aims at returning k points  where the i th  1     i     k  one is a c approximation of the real i th nearest neighbor  Formally  let o     1        o     k be the real k NNs in ascending order of their distances to q  Then  a set of points o1       ok  also sorted in the same way  is a c approximate answer if koi   qk     cko     i   qk for all i      1  k   We consider that the dataset D resides in external memory where each page has B words  Furthermore  we follow the convention that every integer or real number is represented with one word  Since a point has d coordinates  the entire D occupies totally dn B pages  where n is the cardinality of D  In other words  all algorithms  which do not have provable sub linear cost growth with n  incur I O complexity     dn B   Our objective is to design a relational solution beating this complexity  The second problem solved in this paper is c approximate CP search  Speci   cally  let us de   ne the closest pair in D to be the pair of points  o     1   o     2   having the minimum distance among all pairs of points in D  Then the goal of c approximate CP search is to return a pair of points  o1  o2  in D whose distance is at most c times the distance of the closest pair  namely  ko1  o2k     cko     1   o     2k  A naive solution examines all pairs of points  and thus  has time complexity quadratic to n  Note that the CP problem has a bichromatic counterpart  which includes two datasets D1 and D2  Here  the exact answer  o     1   o     2   is the one with the smallest distance in the cartesian product D1    D2  and a c approximate answer is a pair  o1  o2      D1    D2 such that ko1  o2k     cko     1   o     2k  These two CP problems can also be extended to kCP search  whose c approximate version can be de   ned in the same fashion as c approximate kNN  We denote by M the amount of available memory  measured in number of words  Unless speci   cally stated  M can be as small as 3B for our algorithms to work  i e   there are at least 3 memory pages   This  however  excludes the memory needed to store the query results  Speci   cally  a set of kNN or kCP result requires O kd  extra words in memory  which we assume can be a   orded  Our theoretical analysis assumes that a point can    t in a constant number of disk pages  i e   d   O B    which is almost always true in reality  For instance  we may set the constant to 10  thus comfortably supporting dimensionality up to 10B  Also  to simplify the resulting bounds  we assume that the dimensionality d is at least log n B   all the logarithms  unless explicitly stated  have base 2   This is reasonable because  for practical values of n and B  log n B  seldom exceeds 20  whereas d   20 is barely    high dimensional     3  THE PRELIMINARIES Our solutions leverage LSH as the building brick  In Sections 3 1 and 3 2  we discuss the drawbacks of the existing LSH implementations  and further motivate our methods  In Section 3 3  we present the technical details of LSH that are necessary for our discussion  3 1 Rigorous LSH and ball cover As a matter of fact  LSH does not solve c approximate NN queries directly  Instead  it is designed  Indyk and Motwani 1998  for a di   erent problem called c approximate ball cover  BC   Let D be a set of points in d dimensional space  Denote by B q  r  ACM Journal Name  Vol  V  No  N  Month 20YY 6    a ball that centers at the query point q and has radius r  A c approximate BC query returns the following result   1  If B q  r  covers at least one point in D  return a point whose distance to q is at most cr   2  If B q  cr  covers no point in D  return nothing   3  Otherwise  the result is unde   ned  Fig  1  Illustration of ball cover queries Figure 1 shows an example where D has two points o1 and o2  Consider    rst the 2 approximate BC query q1  the left black point   The two circles centering at q1 represent balls B q1  r  and B q1  2r  respectively  Since B q1  r  covers a data point o1  the query will have to return a point  but it can be either o1 or o2  as both of them fall in B q1  2r   Now  consider the 2 approximate BC query q2  Since B q2  2r  does not cover any data point  the query must return empty  Interestingly  an approximate NN query can be reduced to a number of approximate BC queries with di   erent radii r  Har Peled 2001  Indyk and Motwani 1998   The rationale is that  if ball B q  r  is empty but B q  cr  is not  then any point in B q  cr  is a c approximate NN of q  Consider the query point q in Figure 2  Here  ball B q  r  is empty  but B q  cr  is not  It follows that the NN of q must have a distance between r and cr to q  Hence  any point in B q  cr   i e   either o1 or o2  is a c approximate NN of q  B q r B q cr q o1 o2 Fig  2  The rationale of the reduction from nearest neighbor to ball cover queries Based on this idea  Indyk and Motwani  Indyk and Motwani 1998  propose a structure that supports c approximate BC queries at r   1  c  c 2   c 3        x respectively  where x is the smallest power of c that is larger than or equal to td  recall that t is the greatest coordinate on each dimension   They give an algorithm  Indyk and Motwani 1998  to guarantee an approximation ratio of c 2 for ACM Journal Name  Vol  V  No  N  Month 20YY    7 NN search  in other words  we need a structure for     c approximate BC queries to support c approximate NN retrieval   Their method  which we call rigorousLSH  consumes O  logc t   logc d      dn B  1 1 c   space  and answers a query in O  logc t   logc d    dn B  1 c   I Os  Note that t can be a large value  thus making the space and query cost potentially very expensive  Our LSB tree will eliminate the factor logc t   logc d completely  Finally  it is worth mentioning that there exist complicated NN to BC reductions  Har Peled 2001  Indyk and Motwani 1998  with better complexities  However  those reductions are highly theoretical  and are di   cult to implement in relational databases  3 2 Adhoc LSH Although rigorous LSH is theoretically sound  its space and query cost is prohibitively expensive in practice  The root of the problem is that it must support BC queries at too many  i e   logc t   logc d  radii  Gionis et al   Gionis et al  1999  remedy this drawback with a heuristic approach  which we refer to as adhoc LSH  Given a NN query q  they return directly the output of the BC query that is at location q and has radius rm  where rm is a    magic    radius pre determined by the system  Since only one radius needs to be supported  adhoc LSH improves rigorousLSH by requiring only O  dn B  1 1 c   space and O  dn B  1 c   query time  Unfortunately  the cost saving of adhoc LSH trades away the quality control on query results  To illustrate  consider Figure 3a  where the dataset D has 7 points o1  o2       o7  and the black point is a NN query q  Suppose that adhoc LSH is set to support 2 approximate BC queries at radius rm  Thus  it answers the NN query q by    nding a data point that satis   es the 2 approximate BC query located at q with radius rm  The two circles in Figure 3a represent B q  rm  and B q  2rm  respectively  As B q  rm  covers some data of D   by the de   nition stated in the previous subsection  the BC query q may return any of the 7 data points in B q  2rm   It is clear that no bounded approximation ratio can be ensured  as the real NN o1 of q can be arbitrarily close to q   a  rm too large  b  rm too small Fig  3  Drawbacks of adhoc LSH The above problem is caused by an excessively large rm  Conversely  if rm is too small  adhoc LSH may not return any result at all  To see this  consider Figure 3b  Again  the white points constitute the dataset D  and the two circles are B q  rm  ACM Journal Name  Vol  V  No  N  Month 20YY 8    and B q  2rm   As B q  2rm  is empty  the 2 approximate BC query q must not return anything  As a result  adhoc LSH reports nothing too  and is said to have missed the query  Gionis et al  1999   Adhoc LSH performs well if rm is roughly equivalent to the distance between q and its exact NN  which is why adhoc LSH can be e   ective when given the right rm  Unfortunately     nding such an rm is non trivial  Even worse  such rm may not exist at all because an rm good for some queries may be bad for others  Figure 4 presents a dataset with two clusters whose densities are drastically di   erent  Apparently  if a NN query q falls in cluster 1  the distance from q to its NN is signi   cantly smaller than if q falls in cluster 2  Hence  it is impossible to choose an rm that closely captures the NN distances of all queries  Note that clusters with di   erent densities are common in real datasets  Breunig et al  2000   Fig  4  No good rm exists if clusters have di   erent densities Recently   Lv et al  2007  present a variation of adhoc LSH with less space consumption  This variation  however  su   ers from the same drawback  i e   no quality control  as adhoc LSH  and entails higher query cost than adhoc LSH  In summary  currently a practitioner  who wants to apply LSH  faces a dilemma between space query e   ciency and approximation guarantee  If the quality of the retrieved neighbor is crucial  as in security systems such as    nger print veri   cation   a huge amount of space is needed  and large query cost must be paid  On the other hand  to meet a tight space budget or stringent query time requirement  one would have to sacri   ce the quality guarantee of LSH  which somewhat ironically is exactly the main strength of LSH  3 3 Details of hash functions Let h o  be a hash function that maps a d dimensional point o to a one dimensional value  It is locality sensitive if the chance of mapping two points o1  o2 to the same value grows as their distance ko1  o2k decreases  Formally  Definition 1  LSH   Given a distance r  approximation ratio c  probability values p1 and p2 such that p1   p2  a hash function h    is  r  cr  p1  p2  locality sensitive if it satis   es both conditions below  1  If ko1  o2k     r  then P r h o1    h o2       p1  2  If ko1  o2k   cr  then P r h o1    h o2       p2  ACM Journal Name  Vol  V  No  N  Month 20YY    9 LSH functions are known for many distance metrics  For  p norm  a popular LSH function is de   ned as follows  Datar et al  2004   h o       a     o   b w      1  Here   o represents the d dimensional vector representation of a point o   a is another d dimensional vector where each component is drawn independently from a so called p stable distribution  Datar et al  2004    a     o denotes the dot product of these two vectors  w is a su   ciently large constant  and    nally  b is uniformly drawn from  0  w   Equation 1 has a simple geometric interpretation  To illustrate  let us consider p   2  i e    p is Euclidean distance  In this case  a 2 stable distribution can be just a normal distribution  mean 0  variance 1   and it su   ces to set w   4  Datar et al  2004   Assuming dimensionality d   2  Figure 5 shows the line that crosses the origin  and its slope coincides with the direction of  a  For convenience  assume that  a has a unit norm  so that the dot product  a     o is the projection of point o onto line  a  namely  point A in the    gure  The e   ect of  a     o   b is to shift A by a distance b  along the line  to a point B  Finally  imagine we partition the line into intervals with length w  then  the hash value h o  is the ID of the interval covering B  Fig  5  Geometric interpretation of LSH The intuition behind such a hash function is that  if two points are close to each other  then with high probability their shifted projections  on line  a  will fall in the same interval  On the other hand  two faraway points are very likely to be projected into di   erent intervals  The following is proved in  Datar et al  2004   Lemma 1  Proved in  Datar et al  2004    Equation 1 is  1  c  p1  p2  locality sensitive  where p1 and p2 are two constants satisfying ln 1 p1 ln 1 p2     1 c   4  LSB TREE This section includes everything that a practitioner needs to know to apply LSBtrees  Speci   cally  Section 4 1 explains how to build a LSB tree  and Section 4 2 gives its NN algorithm  We will leave all the theoretical analysis to Section 5  ACM Journal Name  Vol  V  No  N  Month 20YY 10    including its space  query performance  and quality guarantee  For simplicity  we will assume  2 norm but the extension to arbitrary  p norms is straightforward  4 1 Building a LSB tree The construction of a LSB tree is very simple  Given a d dimensional dataset D  we    rst convert each point o     D to an m dimensional point G o   and then  obtain the Z order value z o  of G o   Note that z o  is just a simple number  Hence  we can index all the resulting Z order values with a conventional B tree  which is the LSB tree  The coordinates of o are stored along with its leaf entry  Next  we clarify the details of each step  From o to G o   We set the dimensionality m of G o  as m   log1 p2  dn B   2  where p2 is the constant given in Lemma 1 under c   2  n is the size of dataset D  and B is the page size  As explained in Section 5  this choice of m makes it rather unlikely that the G o1  and G o2  of two far away points o1  o2 are similar on all m dimensions  Note that  the choice of c   2 is not compulsory  and our technique can be adapted to any integer c     2  as discussed in Section 6  The derivation of G o  is based on a family of hash functions  H o     a     o   b        3  Here   a is a d dimensional vector where each component is drawn independently from the normal distribution  mean 0 and variance 1   Value b     is uniformly distributed in  0  2 f w   where w is any constant at least 4  and f   dlog d   log te   4  Recall that t is the largest coordinate on each dimension  Note that while  a and w are the same as in Equation 1  b     is di   erent  which is an important design underlying the e   ciency of the LSB tree  as elaborated in Section 5 with Lemma 2   We randomly select m functions H1          Hm    independently from the family described by Equation 3  Then  G o  is the m dimensional vector  G o    hH1 o   H2 o        Hm o i   5  From G o  to z o   Let U be the axis length of the m dimensional space G o  falls in  As explained shortly  we will choose a value of U such that U w is a power of 2  Computation of a Z order curve requires a hyper grid partitioning the space  We impose a grid where each cell is a hyper square with side length w  therefore  there are U w cells per dimension  and totally  U w  m cells in the whole grid  Given the grid  calculating the Z order value z o  of G o  is a standard process well known in the literature  Gaede and Gunther 1998   Let u   log U w   Each z o  is thus a binary string with um bits  Example  To illustrate the conversion  assume that the dataset D consists of 4 two dimensional points o1  o2       o4 as shown in Figure 6a  Suppose that we select m   2 hash functions H1    and H2     Let  a1   a2  be the     a vector    in function H1     H2      For simplicity  assume that both  a1 and  a2 have norm 1  ACM Journal Name  Vol  V  No  N  Month 20YY    11 o1 A a1 a2 o2 o3 o4 deciding H1 o1 b1  H1 o3 H1 o2 H1 o4 H2 o1 b2  B H2 o2 H2 o3 H2 o4 q  a  Computing hash values  b  Computing Z order values Fig  6  Illustration of data conversion In Figure 6a  we slightly abuse notations by also using  a1   a2  to denote the line that passes the origin  and coincides with the direction of vector  a1   a2   Let us take o1 as an example  The    rst step of our conversion is to obtain G o1   which is a 2 dimensional vector with components H1 o1  and H2 o2   The value of H1 o1  can be understood in the same way as explained in Figure 5  Speci   cally     rst project o1 onto line  a1  and then move the projected point A  along the line  by a distance b     1 to a point B  H1 o1  is the distance from B to the origin 1   H2 o2  is computed similarly on line  a2  note that the shifting distance is b     2    Treating H1 o1  and H2 o2  as coordinates  in the second step  we regard G o1  as a point in a data space as shown in Figure 6b  and derive z o1  as the Z order value of point G o1  in this space  In Figure 6b  the Z order calculation is based on a 8    8 grid  As G o1  falls in a cell whose  binary  horizontal and vertical labels are 010 and 110 respectively  z o1  equals 011100  in general  a Z order value interleaves the bits of the two labels  starting from the most signi   cant bits  Gaede and Gunther 1998    Choice of U  In practice  U can be any value making U w a su   ciently large power of 2  For theoretical reasoning  next we provide a speci   c choice for U  Besides U w being a power of 2  our choice ful   lls another two conditions   i  U w     2 f   and  ii   Hi o   is con   ned to at most U 2 for any i      1  m   In the form of Equation 3  for each i      1  m   write Hi o     ai     o   b     i   Denote by k aik1 the  1 norm2 of  ai   Remember that o distributes in space  0  t  d   where t is the largest coordinate on each dimension  Hence   Hi     is bounded by Hmax   m max i 1  k aik1    t   b     i     6  We thus determine U by setting U w to the smallest power of 2 that bounds both 1Precisely speaking  it is  H1 o1   that is equal to the distance  H1 o1  itself can be either positive or negative  depending on which side of the origin B lies on  2Given a d dimensional vector  a   ha 1   a 2        a d i  k ak1   Pd i 1  a i    ACM Journal Name  Vol  V  No  N  Month 20YY 12    2 f and 2Hmax w from above  4 2 Nearest neighbor algorithm In practice  a single LSB tree already produces query results with very good quality  as demonstrated in our experiments  To elevate the quality to a theoretical level  we may independently build a number l of trees  We choose l   p dn B   7  which  as analyzed in Section 5  ensures a high chance for nearby points o1  o2 to have close Z order values in at least one tree  Denote the l trees as T1  T2       Tl respectively  and call them collectively a LSBforest  Use zj  o  to represent the Z order value of o in tree Tj  1     j     l   Without ambiguity  we also let zj  o  refer to the leaf entry of o in Tj   Remember that the coordinates of o are stored in the leaf entry  Given a NN query q  we    rst get its Z order value zj  q  in each tree Tj  1     j     l   As with the Z order values of data points  zj  q  is a binary string with um bits  We denote by LLCP zj  o   zj  q   the length of the longest common pre   x  LLCP  of zj  o  and zj  q   For example  suppose zj  o    100101 and zj  q    100001  then LLCP zj  o   zj  q     3  When q is clear from the context  we may refer to LLCP zj  o   zj  q   simply as the LLCP of zj  o   Figure 7 presents our nearest neighbor algorithm at a high level  The main idea is to visit the leaf entries of all l trees in descending order of their LLCPs  until either enough points have been seen  or we have found a point that is close enough  Next  we explain the details of lines 2 and 3  Algorithm NN1 1  repeat 2  pick  from all the trees T1       Tl   the leaf entry with the next greatest LLCP 3  until condition E1 or E2 holds  the two conditions will be clari   ed later  4  return the nearest point found so far Fig  7  The NN algorithm Finding the next greatest LLCP  This can be done by a synchronous bidirectional expansion at the leaf levels of all trees  Speci   cally  recall that we have obtained the Z order value zj  q  in each tree Tj  1     j     l   Search Tj to locate the leaf entry ej  with the lowest Z order value at least zj  q   Let eja be the leaf entry immediately preceding ej   To illustrate  Figure 8 gives an example where each Z order value has um   6 bits  and l   3 LSB trees are used  The values of z1 q   z2 q   and z3 q  are given next to the corresponding trees  In T1  for instance  z1 o1    011100 is the lowest among all the Z order values at least z1 q    001110  Hence  e1  is z1 o1   and e1a is the entry z1 o3    001100 preceding z1 o1   The leaf entry with the greatest LLCP must be in the set S    e1   e1a       el   ela   Let e     S be this entry  To determine the leaf entry with the next greatest LLCP  we move e away from q by one position in the corresponding tree  and then repeat the process  For example  in Figure 8  the leaf entry with the maximum LLCP is e2a  whose LLCP is 5  as it shares the same    rst 5 bits with z2 q    Thus  ACM Journal Name  Vol  V  No  N  Month 20YY    13 T1 z1 q e1 e1 000100 001100 011100 110010 T2 z2 q e2 e2 010001 011110 110001 110100 T3 z3 q e3 e3 011110 100111 101100 101110 z2 o1   z2 o4   z2 o3   z2 o2   z3 o2   z3 o3   z3 o4   z3 o1   z1 o2   z1 o3   z1 o1   z1 o4   Fig  8  Bi directional expansion  um   6  l   3  we shift e2a to its left  i e   to z2 o1    010001  The entry with the next largest LLCP can be found again in  e1   e1a       e3   e3a   Terminating condition  Algorithm NN1 terminates when one of two events E1 and E2 happens  The    rst event is  E1  the total number of leaf entries accessed from all l LSB trees has reached 4Bl d  Event E2 is based on the LLCP of the leaf entry just retrieved from line 2  Denote the LLCP by v  which bounds from above the LLCP of all the leaf entries that have not been processed  E2  the nearest point found so far  from all the leaf entries already inspected  has distance to q at most 2 u   bv mc 1   Let us use again Figure 8 to illustrate algorithm NN1  Assume that the dataset consists of points o1  o2       o4 in Figure 6a  and the query is the black point q  Notice that the Z order values in tree T1 are obtained according to the transformation in Figure 6b with u   3 and m   2  Suppose that ko3  qk   3 and ko4  qk   5  As explained earlier  entry z2 o4  in Figure 8 has the largest LLCP v   5  and thus  is processed    rst  NN1 obtains the object o4 associated with z2 o4   and calculates its distance to q  Since ko4  qk   5   2 u   bv mc 1   4  condition E2 does not hold  Assuming E1 is also violated  i e   let 4Bl d   1   the algorithm processes the entry with the next largest LLCP  which is z1 o3  in Figure 8 whose LLCP v   4  In this entry  NN1    nds o3 which replaces o4 as the nearest point so far  As now ko3  qk   3     2 u   bv mc 1   4  E2 holds  and NN1 terminates by returning o3  Retrieving k neighbors  Algorithm NN1 can be easily adapted to answer kNN queries  Speci   cally  it su   ces to modify E1 to    the total number of leaf entries accessed from all l LSB trees has reached  4Bl d   k   1 l     and E2 to    q is within distance 2 u   bv mc 1 to the k nearest points found so far     Also  apparently line 4 should return the k nearest points  Finally  the value of l in Equation 7 needs to be increased by O log n   times  All these changes are to ensure strong quality guarantees in theory for any k  as will be analyzed in the next Section   In practice  as long as k is small  only the change to E2 is needed  and E1 and l can remain as they are for k   1  ACM Journal Name  Vol  V  No  N  Month 20YY 14    kNN search with a single tree  Maintaining a forest of l LSB trees incurs large space consumption and update overhead  In practice  we may prefer an index that has linear space and supports fast data insertions deletions  In this case  we can build only one LSB tree  and use it to process kNN queries  Accordingly  we slightly modify the algorithm NN1 by simply ignoring event E1 in the terminating condition  as this event is designed speci   cally for querying l trees   Condition E2  however  is retained  As a tradeo    for e   ciency  querying only a single tree loses the theoretical guarantees of the LSB forest  as established in the next section   Nevertheless  this approach is expected to return neighbors with high quality  because the converted Z order values adequately preserve the proximity of the data points in the original data space  5  THEORETICAL ANALYSIS We now proceed to study the theoretical characteristics of the LSB tree  Denote by D the original d dimensional space of the dataset D  Namely  D    0  t  d   where t is the maximum coordinate on each axis  Recall that  to construct a LSB tree  we convert each point o     D to an m dimensional point G o  as in Equation 5  Denote by G the space where G o  is distributed  By the way we select U in Section 4 1  G       U 2  U 2  m   5 1 Quality guarantee We begin with an observation on the basic LSH in Equation 1  Observation 1  Given any integer x     1  de   ne hash function h 0  o       a     o   bx w    8  where  a  b  and w are the same as in Equation 1  h 0     is  1  c  p1  p2  locality sensitive  and ln 1 p1 ln 1 p2     1 c  Proof  We    rst point out a useful fact  Imagine a line that has been partitioned into consecutive intervals of length w  Let A  B be two points on this line with distance y     w  Shift both points towards right by a distance uniformly drawn from  0  w     where    is any integer  After this  A and B fall in the same interval with probability 1     y w  which is irrelevant to     Consider the hash function h o  in Equation 1  Use  a to denote also the line passing the origin containing vector  a  As explained in Section 3 3   a     o decides a point in Line  a  and  a     o   b shifts the point away from the origin by distance b along the line  Call it the shifted projection of o  Let us partition line  a with intervals of length w  By Equation 1  two objects o1  o2 have the same hash value if and only if their shifted projections fall in the same interval  Now assume that we change the shifting distance from b to bx  Since b is uniformly distributed in  0  w   bx is uniformly distributed in  0  wx   Hence  the change does not alter the probability for the shifted projections of o1 and o2 to fall in the same interval  This means that Equation 8 is also  1  c  p1  p2  locality sensitive with the same p1 and p2 as Equation 1  ACM Journal Name  Vol  V  No  N  Month 20YY    15 For any s      0  f  with f given in Equation 4  de   ne  H     o  s       a     o   b     2 sw    9  where  a  b     and w follow those in Equation 3  We have  Lemma 2  H     o  s  is  2 s   2 s 1   p1  p2  locality sensitive  where p1 and p2 satisfy ln 1 p1 ln 1 p2     1 2  Proof  Create another space D 0 by dividing all coordinates of D by 2 s   It is easy to see that the distance of two points in D is 2 s times the distance of their converted points in D 0   Consider h 00  o 0        a    o 0    b      2 f w  2 f   s w  w    10  where o 0 is a point in D 0   As b       2 f w  is uniformly distributed in  0  w   by Observation 1  h 00     is  1  2  p1  p2  locality sensitive in D 0 with  ln 1 p1   ln 1 p2      1 2  Let o be the corresponding point of o 0 in D  Clearly   a    o 0     a     o  2 s   Hence  h 00  o 0     H     o  s   The lemma thus holds  As shown in Equation 5  G o  is composed of hash values H1 o        Hm o   In the way we obtain H     o  s   Equation 9  from H o   Equation 3   let H    i  o  s  be the hash function corresponding to Hi o   1     i     m   Also remember that z o  is the Z order value of G o  in space G  and function LLCP       returns the length of the longest common pre   x of two Z order values  Now we prove a crucial lemma that is the key to the design of the LSB tree  Lemma 3  Let o1  o2 be two arbitrary points in space D  A value s satis   es s     u     bLLCP z o1   z o2   mc if and only if H    i  o1  s    H    i  o2  s  for all i      1  m   Proof  Recall that  for Z order value calculation  we impose on G a grid with 2 u cells  each with side length w  per dimension  Refer to the entire G as a level u tile  In general  a level s  2     s     u  tile de   nes 2 m level  s     1  tiles  by cutting the level s tile in half on every dimension  Thus  each cell in the grid partitioning G is a level 0 tile  As a property of the Z order curve  G o1  and G o2  belong to a level s tile  if and only if their Z order values share at least m u   s  most signi   cant bits  Gaede and Gunther 1998   namely  LLCP z o1   z o2       m u     s   On the other hand  note that a level s tile is a hyper square with side length 2 s w  This means that G o1  and G o2  belong to a level s tile  if and only if H    i  o1  s    H    i  o2  s  for all i      1  m   Thus  the lemma follows  Lemmas 2 and 3 allow us to rephrase the probabilistic guarantees of LSH using LLCP  Corollary 1  Let r be any power of 2 at most 2 f   Given a query point q and a data point o  we have  1  If kq  ok     r  then LLCP z q   z o       m u   log r  with probability at least p m 1   ACM Journal Name  Vol  V  No  N  Month 20YY 16    2  If kq  ok   2r  then LLCP z q   z o       m u     log r  with probability at most p m 2   Furthermore  ln 1 p1 ln 1 p2     1 2  The above result holds for any LSB tree  Recall that  for NN search  we need a forest of l trees T1       Tl built independently  Next  we will explain an imperative property guaranteed by these trees  Let q be the query point  and r be any power of 2 up to 2 f such that there is a point o     in the ball B q  r   Consider events P1 and P2  P1  LLCP zj  q   zj  o            m u     log r  in at least one tree Tj  1     j         P2  There are less than 4Bl d leaf entries zj  o  from all trees Tj  1     j     l  such that  i  LLCP zj  q   zj  o       m u     log r   and  ii  o is outside B q  2r   The property guaranteed by the l trees is  Lemma 4  P1 and P2 hold at the same time with at least constant probability  Proof  Equipped with Corollary 1  this proof is analogous to the standard proof  Gionis et al  1999  of the correctness of LSH  Now we establish an approximation ratio of 4 for algorithm NN1  In the next section  we will extend the LSB tree to achieve better approximation ratios  Theorem 1  Algorithm NN1 returns a 4 approximate NN with at least constant probability  Proof  Let o     be the NN of query q  and r       ko       qk  Let r be the smallest power of 2 bounding r     from above  Obviously r   2r     and r     2 f  notice that r     is at most td     2 f under any  p norm   If when NN1    nishes  it has already found o     in any tree  apparently it will return o     which is optimal  Next  we assume NN1 has not seen o     at termination  We will show that when both P1 and P2 are true  the output of NN1 is de   nitely 4 approximate  Denote by j     the j stated in P1  Recall that NN1 may terminate due to the occurrence of either event E1 or E2  If it is due to E2  and given the fact that NN1 visits leaf entries in descending order of their LLCP  the LLCP v of the last fetched leaf entry is at least LLCP zj      q   zj      o            m u     log r   It follows that bv mc     u   log r  E2 ensures that we return a point o with ko  qk     2r   4r       In case the termination is due to E1  by P2  we know that NN1 has seen at least one point o inside B q  2r   Hence  the point returned has distance to q at most 2r   4r       Finally  Lemma 4 indicates that P1 and P2 are true with at least constant probability  thus completing the proof  Also  the proof of Theorem 1 actually shows  Corollary 2  Let r     be the distance from q to its real NN  With at least constant probability  NN1 returns a point within distance 2r to q  where r is the lowest power of 2 bounding r     from above  ACM Journal Name  Vol  V  No  N  Month 20YY    17 Remark 1  When de   ning the problem in Section 2  we restricted point coordinates to integers  In fact  the above analysis holds also for real coordinates as well  as long as the minimum distance between two points in D is at least 1  Remark 2  As a standard trick in probabilistic algorithms  by repeating our solution O log 1 p   times  we boost the success probability of algorithm NN1 from constant to at least 1   p  for any arbitrarily low p   0  In other words  by repeating O log n  times  namely  increasing l to O log n p dn B    the failure probability of NN1 can be lowered to at most 1 n  Using the Union Bound inequality  also called the Boole   s inequality   it is easy to show that the kNN algorithm described in Section 4 2 gives a 4 approximate answer with at least constant probability  5 2 Space and query time Theorem 2  We can build a forest of l LSB trees that consume totally O  dn B  1 5   space  Given these trees  algorithm NN1 answers a 4 approximate NN query in O E p dn B  I Os  where E is the height of a LSB tree  Proof  Each leaf entry of a LSB tree stores a Z order value z o  and the coordinates of o  z o  has um bits where u   O f    O log d   log t  and m   O log dn B    As log d   log t bits    t in 2 words  z o  occupies O log dn B   words  It takes d words to store the coordinates of o  Hence  overall a leaf entry is O d  words long  Hence  a LSB tree consumes O  dn B   pages  and l   p dn B of them require totally O  dn B  1 5   space  Algorithm NN1  i     rst accesses a single path in each LSB tree  and then  ii  fetches at most 4Bl d leaf entries  The cost of  i  is bounded by O lE   As a leaf entry consumes O d  words  4Bl d of them occupy at most O l  pages  By implementing each LSB tree as a string B tree  Ferragina and Grossi 1999   the height E is bounded by O logB n   resulting in query complexity O  p dn B logB n   5 3 Comparison with rigorous LSH As discussed in Section 3  for 4 approximate NN search  rigorous LSH consumes O  log d   log t  dn B  1 5   space  and answers a query in O  log d   log t  p dn B  I Os  Comparing these complexities with those in Theorem 2  it is clear that the LSB forest improves rigorous LSH signi   cantly in the following ways  First  the performance of the LSB forest is not sensitive to t  the greatest coordinate of a dimension  This is a crucial improvement because t can be very large in practice  As a result  rigorous LSH is suitable only when data are con   ned to a relatively small space  The LSB forest enjoys much higher applicability by retaining the same e   ciency regardless of the size of the data space  Second  the space consumption of a LSB forest is lower than that of rigorousLSH by a factor of log d   log t  For practical values of d and t  e g   d   50 and t   10000   the space of a LSB forest is lower than that of rigorous LSH by more than an order of magnitude  Furthermore  note that the LSB forest is as space e   cient as adhoc LSH  even though the latter does not guarantee the quality of query results at all  Third  the LSB forest promises higher query e   ciency than rigorous LSH  As mentioned earlier  the height E can be strictly con   ned to O logB n  by resorting ACM Journal Name  Vol  V  No  N  Month 20YY 18    Algorithm NN2  r  1  o   the output of algorithm NN1 on F 2  o 0   the output of algorithm NN1 on F 0 3  return the point between o and o 0 closer to q Fig  9  The 3 approximate algorithm to the string B tree  Even if we simply implement a LSB tree as a normal B tree  the height E never grows beyond 6 in our experiments  This is expected to be much smaller than log d   log t  rendering the query complexity of the LSB forest considerably lower than that of rigorous LSH  In summary  the LSB forest outperforms rigorous LSH signi   cantly in applicability  space and query e   ciency  It therefore eliminates the reason for resorting to the theoretically vulnerable approach of adhoc LSH  Finally  remember that the LSB tree achieves all of its nice characteristics by leveraging purely relational techniques  6  EXTENSIONS This section presents several interesting extensions to the LSB tree  which are easy to implement in a relational database  and extend the functionality of the LSB tree signi   cantly  Supporting ball cover  A LSB forest  which is a collection of l LSB trees as de   ned in Section 4 2  is able to support 2 approximate BC queries whose radius r is any power of 2  Speci   cally  given such a query q  we run algorithm NN1  Figure 7  using the query point  Let o by the output of NN1  If ko  qk     2r  we return o as the result of the BC query q  Otherwise  we return nothing  By an argument similar to the proof of Theorem 1  it is easy to prove that the above strategy succeeds with high probability   2      approximate nearest neighbors  A LSB forest ensures an approximation ratio of 4  Theorem 1   Next we will improve the ratio to 3 with only 2 LSBforests  As shown earlier  a LSB forest can answer 2 approximate BC queries with any r   1  2  2 2        2 f where f is given in Equation 4  We build another LSB forest to handle 2 approximate BC queries with any r   1 5  1 5    2  1 5    2 2        1 5    2 f   For this purpose  we can create another dataset D0 from D  by dividing all coordinates in D by 1 5  Then  a LSB forest on D0 is exactly what we need  noticing that the distance of two points in D0 is 1 5 times smaller than that of their original points in D  The only issue is that the distance of two points in D0 may drop below 1  while our technique requires a lower bound of 1  see Remark 1 in Section 5 1   This can be easily    xed by scaling up D    rst by a factor of 2  i e   doubling all the coordinates   Any two points in the new D have distance at least 2  so any two points in D0 now have distance at least 2 1 5   1  Denote by F and F 0 the LSB forest on D and D0 respectively  Given a NN query q  we answer it using simple the algorithm NN2 in Figure 9  Theorem 3  Algorithm NN2 returns a 3 approximate NN with at least constant probability  ACM Journal Name  Vol  V  No  N  Month 20YY    19 Proof  Let D be the d dimensional space of dataset D  and D 0 the space of D0   Denote by r     the distance between q and its real NN o       Apparently  r     must fall in either  2 x   1 5  2 x   or  1 5  2 x   2 x 1   for some x      0  f   Refer to these possibilities as Case 1 and 2  respectively  For Case 1  the distance r    0 between q and o     in space D 0 is between  2 x  1 5  2 x    Hence  by Corollary 2  with at least constant probability the distance between o 0 and q in D 0 is at most 2 x 1   where o 0 is the point output at line 2 of NN2  It thus follows that o 0 is within distance 1 5    2 x 1     3r     in D  Similarly  for Case 2  we can show that o  output at line 1  is a 3 approximate NN with at least constant probability  The above idea can be easily extended to  2      approximate NN search for any 0       2  Speci   cally  we can maintain 1  b1  log 1    2 c LSB forests  such that the i th forest  1     i     1  b1  log 1    2 c  supports 2 approximate BC queries at r       2    2 2          2 f     where       1     2  i   1   Given a query q  we run algorithm NN1 on all the forests  and return the nearest point found  By an argument similar to proving Theorem 3  we have  Theorem 4  For any 0       2  we can build O   1 log 1      LSB forests that consume totally O    dn B  1 5 1 log 1      space  and answer a  2      approximate NN query in O   E p dn B 1 log 1      I Os  where E is the height of a LSB tree   c      approximate nearest neighbors  In practice  an application may be able tolerate an approximation ratio higher than that of the basic LSB forest  In this case  it is possible to further reduce the space and query cost  In the sequel  we generalize the LSB tree to o   er any approximation ratio arbitrarily close to c  for any integer c     3  We make several changes in building a LSB tree     Recall that m equals log1 p2  dn B  in Section 4 1  For c     3  the expression for m remains identical  but p2 is the constant as given in the Lemma 1 for the value of c we are considering     In Equation 3  b     will be uniformly drawn from  0  c f w   where f  instead of following Equation 4  is set to dlogc d   logc te     We will decide U  i e   the axis length of the m dimensional space of G o   by setting U w to the smallest power of c that bounds both c f and 2Hmax w from above  where Hmax is given in Equation 5  The last change lies in the way a Z order value z o  is calculated from G o   Let us denote by G the m dimensional space where G o  is distributed  Impose a hypergrid over G where each cell is a hyper square with side length w  As mentioned earlier  U w is a power of c  therefore  the grid has totally x cm cells  for some integer x  Figure 10 shows an example where G has m   2 dimensions  c   3  and G is partitioned by a 3 2    3 2 grid  We utilize the grid to compute z o  as follows  Recall that the grid partitions each dimension of G into c x intervals  Number these intervals consecutively using ACM Journal Name  Vol  V  No  N  Month 20YY 20    3 ary numbers 00 01 02 10 11 12 20 21 22 00 01 02 10 11 12 20 21 22 0120 Fig  10  Computing Z order values for the order 3 LSB tree c ary values  For instance  in Figure 10  each dimension of G is cut into 3 2   9 intervals  which are numbered from 00 to 22  Then  the Z order value of each cell in the grid is obtained by interleaving its c ary digits on all dimensions  For example  the grey cell in Figure 10 is numbered 02 and 10 on the horizontal and vertical dimensions  respectively  Hence  its Z order value is 0120  taking the    rst digits of 02 and 10  then followed by their second digits  z o  equals the Z order value of the cell that G o  falls in  By the Z order values thus calculated  we impose an ordering of the cells as depicted by the zigzag line in the    gure  We call the adapted LSB tree an order c LSB tree  and build a forest of l    dn B  1 c such trees independently  Call it an order c LSB forest  The query algorithm NN1 in Section 4 2 can be deployed directly on the forest  except that the number 2 u   bv mc 1 in event E2 should be replaced by c u   bv mc 1   By an argument similar to the one in Section 5  we can show  Theorem 5  We can build a set of order c LSB trees that consume totally O  dn B  1 1 c   space  Given a query  NN1 returns a c 2  approximate NN in O E dn B  1 c   I Os  where E is the height of an order c LSB tree  Notice that the order c LSB forest captures the basic LSB forest as a special case with c   2  Recall that a basic LSB forest is able to answer 2 approximate BC queries with r being powers of 2  Likewise  an order c LSB forest is able to answer c approximate BC queries with r   1  c  c 2        To lower the approximation ratio to c      we can build 1   b c log 1   c  c order c LSB forests  Speci   cally  the i th  1     i     1   b c log 1   c  c  forest is responsible for c approximate BC queries with radius r       c    c 2          where       1     c  i   1   Following the way of establishing Theorem 4  we can prove  Theorem 6  For any 0       c 2     c  we can build O   c log 1   c    order c LSBtrees that consume totally O    dn B  1 1 c c log 1   c    space  and answer a  c       approximate NN query in O   E dn B  1 c c log 1   c    I Os  where E is the height ACM Journal Name  Vol  V  No  N  Month 20YY    21 of an order c LSB tree  Note that  for c     3  the complexities in the above theorem are strictly smaller than those in Theorem 4 because the polynomials in Theorem 6 have lower exponents  7  CLOSEST PAIR SEARCH In this section we will extend the LSB technique to solve the CP problem  There is a straightforward solution  Speci   cally  assume that a LSB forest has been built on dataset D  First  for every point o     D  run algorithm NN1  Figure 7  to    nd its NN o 0   Then  among all such pairs  o  o 0    report the one with the smallest distance  This will give us a 4 approximate answer with high probability  In main memory  the solution is quite e   cient  requiring only O n 1 5 log n   time  Datar et al  2004   In external memory where an access unit is a page of B words  the running time becomes O n p dn B logB n   which can be even worse than the trivial bound O  dn B  2    Next  we will propose a di   erent approach that requires only O  dn B  1 5   I Os  As will be clear shortly  the analysis of this approach   s running time is drastically di   erent from that in  Datar et al  2004   7 1 Ball pair search As explained in Section 3 1  LSH approaches NN search with ball cover  Similarly  we attack the CP problem with another problem we call ball pair  BP  search  which can be regarded as the counterpart of ball cover in the CP context  Formally  given a radius r  a c approximate BP query on D returns the following   1  If there is a pair of points in D with distance at most r  return a pair of points in D with distance at most cr   2  If no two points in D have distance at most cr  return nothing   3  Otherwise  the result is unde   ned  o1 o2 o4 o3 Fig  11  Illustration of the ball pair problem For example  consider Figure 11 where D has 4 points  Let r be the distance between o1 and o2  Then  a 2 approximate BP query must return a pair of points within distance 2r  In our example  there are two such pairs   o1  o2    o1  o3   either of which is a correct result  On the other hand  for any r   1 2 ko1  o2k  a 2 approximate BP query must not return anything at all  Our discussion will focus on radius r that is a power of 2 between 1 and 2 f   where f is given in Equation 4  In the sequel  let      log r  We will    rst target an approximation ratio of c   2  and then extend to other ratios later  For c   2  we need ACM Journal Name  Vol  V  No  N  Month 20YY 22    a LSB forest with l trees built in exactly the way described in Section 4 1  Next  we will    rst clarify the algorithm for BP search  and then analyze its theoretical properties  Algorithm  Let us    rst concentrate on a single LSB tree T  Remember that each leaf entry carries a Z order value  Two points o1  o2 are said to be in the same bucket if they share the    rst m u         bits in their Z order values  namely  LLCP z o1   z o2       m u             see the de   nitions of m  u  and LLCP    in Section 4  Intuitively  a bucket is essentially a hyper square with 2   m cells in the grid partitioning the space G  that is used to de   ne Z values   For example   same as Figure 6b  Figure 12a shows a space G with m   2 dimensions  the coordinates of which are encoded with u   3 bits  For      1  there are 16 buckets  each with 2   m   4 cells that share same    rst m u           4 bits in their Z order values  Figure 12b demonstrates the case of      2  where there are 4 buckets each with 16 cells  Note that a bucket of      2 encloses 4 buckets of      1  This is true in general  every time    grows by 1  i e   r doubles   a new bucket covers 2 m old buckets  Also notice that  in any case  the cells of a bucket always have continuous Z order values  000 001 010 011 100 101 110 111 000 001 010 011 100 101 111 110 000 001 010 011 100 101 110 111 000 001 010 011 100 101 111 110  a  16 buckets when      1  b  4 buckets when      2 Fig  12  Coverage of buckets in space G  m   2  u   3  Let us divide the leaf entries of tree T based on the buckets they belong to  Apparently  points of the same bucket must come together in adjacent leaf nodes  as illustrated in Figure 13  Note that a bucket may span multiple leaf nodes  but may also be so small that several buckets can    t in a single leaf  In any case  the important fact is that by scanning the leaf level from the leftmost node rightwards  we can easily determine the bucket boundaries  by comparing the Z order values of consecutive leaf entries  Now  let us take back the entire forest of l trees  since we are ready to elaborate the algorithm for BP search  which is fairly simple as presented in Figure 14  For each tree  we scan its leaf level from left to right  starting from the leftmost leaf  For every bucket I encountered during the scan  evaluate the distances of all the pairs of points in I bruteforcely in O dd I  Be 2   I Os  Meanwhile  we keep track ACM Journal Name  Vol  V  No  N  Month 20YY    23 leaf node     pointer between leaves bucket 1 bucket 2 bucket 3 bucket 4 bucket 5 Fig  13  Buckets at the leaf level of a LSB tree of the total number of pairs evaluated  from all trees  so far  namely  the count increases by  I   I      1  after processing a bucket I  The algorithm terminates as soon as the count reaches 2Bnl d  where n is the size of D      this includes even the duplicate pairs discovered from di   erent trees  At termination  we return the closest pair  among all the pairs evaluated  if its distance is at most 2r  Otherwise  we return nothing  Algorithm BP r     assume that the LSB forest has trees T1       Tl    1  for i   1 to l 2  scan from the leaf nodes of Ti rightwards  starting from the leftmost one 3  for each bucket I encountered 4  evaluate the O  I  2   pairs of points in I 5  break  as soon as 2Bnl d pairs have been evaluated  from all trees  6  if the closest pair found so far has distance at most 2r then return it 7  else return nothing Fig  14  The BP algorithm Analysis  Next we will    rst establish the quality guarantee of our algorithm BP  and then analyze its running time  Lemma 5  BP returns a 2 approximate answer with at least constant probability  Proof  The proof is an adaptation of the standard LSH analytical framework for NN search  we will focus on the di   erences in the CP context  Given two points in a bucket of a tree  we say that they form a bad pair if their distance is larger than 2r  In the sequel  we will assume the existence of a pair  o     1   o     2   within distance at most r  the proof is similar if such a pair does not exist   Observe that BP    nds a 2 approximate answer if both of the following hold  P 0 1   There are less than 2Bnl d bad pairs in all the l trees in the LSB forest  P 0 2    o     1   o     2   appear in at least one bucket of a LSB tree  The rest of the proof will show that they hold at the same time with at least constant probability  Recall that two points o1  o2 fall in the same bucket of some LSB tree Tj  1     j     l  if and only if LLCP z o1   z o2       m u          By Corollary 1  if ko1  o2k   2r  they form a bad pair in Tj with probability at most p m 2   Hence  in all l trees  the expected number of bad pairs is at most l  n n   1 p m 2   which is smaller than Bnl d with the choice of m in Equation 2  By Markov Inequality  the probability for the ACM Journal Name  Vol  V  No  N  Month 20YY 24    total number of bad pairs in all l trees to be at least 2Bnl d is at most 1 2  that is  P 0 1 fails with probability at most 1 2  By the same reasoning in the standard LSH framework  with the choice of l in Equation 7  P 0 2 fails with probability at most 1 e  Hence  the probability that at least one of P 0 1 and P 0 2 fails is bounded by 1 2   1 e   0 87 from above  implying that they hold with probability at least 0 13  Although the proof of quality generally follows the LSH framework  the running time analysis is substantially di   erent  Lemma 6  BP performs O  dn B  1 5   I Os  Proof  We consider only buckets with more than B d points  Each bucket with at most B d points    ts in at most 2 pages  so examining all pairs of points in each of these buckets takes linear I Os  namely O  dn B  1 5   I Os  Without loss of generality  assume that at the time BP    nishes  it has encountered J buckets in this order  I1  I2       IJ  they may come from di   erent trees   Note that  except the last one IJ   all the other buckets have been fully processed  Namely  if we denote by xi  1     i     J     1  the size of bucket Ii   Ii contributed xi xi     1  pairs to the count BP is maintaining  As for the last bucket IJ   assume that BP scanned xJ points in it  Hence  IJ contributed at least  xJ     1  xJ     2  to the count  It su   ces to consider xJ     3   B d  otherwise  we can ignore IJ but add only O 1  I Os to the overall cost   As totally BP evaluates no more than 2Bnl d pairs  we have   xJ     1  xJ     2    JX   1 i 1 xi xi     1      2Bnl d   11  Since xi     1   B d  1     i     J     1  and xJ     3   B d  Inequality 11 implies XJ i 1  xi    B d     xJ     1  xJ     2    JX   1 i 1 xi xi     1      2Bnl d  Hence  XJ i 1 xi     2Bnl d B d   O nl    12  From Inequalities 11 and 12  we have  XJ i 1 x 2 i     2Bnl d   3 XJ i 1 xi   O Bnl d   13  where the last equality is due to d   O B   Furthermore  J satis   es J     PJ i 1 xi B d   O dnl B    14  A bucket Ii  1     i     J     1  with size xi occupies at most O ddxi Be  pages  Hence  bruteforce examination of all pairs of points in Ii requires O ddxi Be 2   ACM Journal Name  Vol  V  No  N  Month 20YY    25 I Os  Likewise  examining the last bucket IJ takes O ddxJ  Be 2   I Os  Thus  the total cost on all buckets is bounded by O   XJ i 1 ddxi Be 2     O   XJ i 1  dxi B   1  2     O   J   d 2 B2 XJ i 1 x 2 i   d B XJ i 1 xi   which  by Inequalities 12 14  is bounded by O dnl B    O  dn B  1 5    7 2 Solving the closest pair problem The closest pair problem can be reduced to BP search  A simple approach is to invoke algorithm BP  Figure 14  with doubling radius r   1  2  4  and so on  until it returns a pair of points whose distance is at most twice the current r  This procedure  referred to as algorithm CP1  is formally presented in Figure 15  which can be easily shown to return a 4 approximate answer with at least constant probability  Algorithm CP1 1  r   1 2  repeat 3  call BP r  4  if the above returns a pair of points  o1  o2  and ko1  o2k     2r then return  o1  o2  5  else r   2r Fig  15  The    rst CP algorithm The drawback of CP1 is that its running time may be O  log d  log t  dn B  1 5   in the worst case  where t is the maximum coordinate of a dimension  In the sequel  we give an alternative algorithm that requires only O  dn B  1 5   time  i e   eliminating the log d   log t factor  An improved algorithm  We refer to our new algorithm as CP2  Unlike CP1 that performs multiple BP search  CP2    rst picks an appropriate value of r  denoted as rgood  and then  performs at most two BP search at r   rgood 2 and rgood  respectively  As shown later  rgood can be found in O  dn B  1 5   I Os  which is the same cost of one BP search  thus making the overall cost O  dn B  1 5   as well  CP2 is presented in Figure 16  Next we will focus on explaining Line 1  Recall that  given a particular r  algorithm BP examines a number of point pairs in D  Let us denote the number as C r   Obviously  C r  is always bounded from above by 2Bnl d  as it is the largest number of pairs evaluated by BP  It is fairly simple to obtain the exact C r  by reading  from left to right  the leaf levels of all LSB trees once as follows  First  set C r  to 0  and start reading the    rst tree  At any time  we keep a count x of how many points have been seen in the current bucket being scanned  When the boundary of the bucket is reached  we add x x   1  to C r   and then  reset x to 0 for the next bucket  At the time all trees have been ACM Journal Name  Vol  V  No  N  Month 20YY 26    Algorithm CP2 1     nd an appropriate radius rgood 2  call BP rgood 2  3  if the above returns a pair of points with distance greater than rgood 4  call BP rgood  5  return the closest of all pairs of points examined Fig  16  An improved CP algorithm scanned  C r  becomes    nal  Since every leaf node of each tree is read once  the total cost is O  dn B  1 5   I Os  A nice feature of the above strategy is that it needs to store only two values in memory at any time  C r  and x  There are  however  merely f   dlog d   log te di   erent values of r  Hence  we can compute the C r  of all possible r in a single pass  The memory size required is only one memory page  as the reading bu   er  plus 2f integers  Then  rgood is decided as rgood   min r   C r      2Bnl d   15  namely  rgood is the lowest r such that C r      2Bnl d  Theorem 7  Algorithm CP2 returns a 4 approximate answer with at least constant probability  Proof  We will make a claim X  every pair whose distance is evaluated by CP1 is also evaluated by CP2  Under the claim  CP2 never returns a worse answer than CP1  which will establish the theorem  Assume that when CP1    nishes  the value of r is r 0   Clearly  r 0     rgood because algorithm BP never evaluates more than 2Bnl d pairs  If r 0   rgood  it means that the best pair CP1 returns is found by the last BP search  namely  BP r 0    Then  X is true because CP2 also needs to perform the same BP search BP rgood   notice that Line 4 of CP2 will de   nitely be executed  i e   the if condition at Line 3 will fail   Now consider r 0   rgood  The crucial fact is that  for any r1   r2   rgood where r1 and r2 are powers of 2  the set of point pairs BP r1  evaluates is always a subset of the set BP r2  evaluates  due to the selection of rgood  Hence  whatever is evaluated by BP r 0   is also evaluated by BP rgood 2   Hence  X also holds  Thus  we arrive at  Theorem 8  Given a LSB forest  we can perform 4 approximate closest pair search in O  dn B  1 5   I Os as long as the memory size M is at least max 3B  B   2f  words  Note that the value of f is smaller than B in practice  As a reference  let d   1000 and t   10 10   in this case  f   44  which means 2f integers can easily    t in a page of B   1024 words  Thus  the memory size needed to run CP2 is only 3 pages  In case the LSB forest does not exist in advance  then the total time increases by a factor of logM B dn   because building all the leaf levels with external sort takes O  dn B  1 5 logM B dn   I Os  the non leaf levels are unnecessary   ACM Journal Name  Vol  V  No  N  Month 20YY    27 Extensions  In theory  kCP search can also be supported in a way similar to how kNN is handled  First  we need to increase l to O  p dn B log n   Second  the limit on how many point pairs are evaluated by algorithm BP should be raised to 2Bnl d    k     1 l  In practice when k is small  no change is required  and we can simply output the k closest pairs among all the pairs CP2 evaluates during its execution  So far we have been targeting an approximation ratio of 4  but the ratio can be improved to arbitrarily close to 2 with only a constant blowup in the computation cost  Conversely  one may also opt for a higher ratio as a tradeo    for lower running time  Using the methods explained in Section 6  for any integer c     2 and arbitrary   satisfying 0       c 2     c  we can    nd a  c      approximate answer in O    dn B  1 1 c c log 1   c    I Os  If the LSB forests need to be built on the    y  the cost is O logM B dn   times higher  All the results can be extended to bichromatic CP search as well  In particular  for k   1  a 4 approximate closest pair between D1 and D2 can be found with at least constant probability in O  q d     n1n2 B       dn1 B  logM B dn1     dn2 B  logM B dn2     I Os  where n1 and n2 are the cardinalities of the participating datasets D1 and D2  respectively  Note that when n1   n2   n  this complexity degenerates into the one obtained earlier for a single dataset  Using a single LSB tree  As mentioned in Section 4 2  a practical application may choose to maintain only a single LSB tree  because this consumes only linear space and allows logarithmic update time  Before    nishing this section  we give an algorithm  referred to as CP3  which performs approximate kCP search using only such a tree  The rationale behind CP3 is that a LSB tree generally captures the proximity of the points in the original space  Namely  if points o1 and o2 are close  they tend to have similar Z values Z o1  and Z o2   Hence  for each leaf entry  we will evaluate its distances only to its nearby leaf entries  More speci   cally  at any time  we pinpoint a leaf node N in memory  After computing the distances of all pairs of points in N  we use another memory page N0 to scan forward the subsequent leaf pages one by one  Every point in N0 has its distances to all points in N computed  This continues until the Z order value of an entry in N0 is    su   ciently faraway     to be elaborated shortly  from that of the rightmost entry in N  see Figure 17   When this happens  we move N to the leaf node on its right  and repeat the process  The    rst N pinpointed is the leftmost leaf node  It remains to clarify what we mean by    two entries Z o1  and Z o2  are faraway     We adopt a heuristic similar to the one used in Section 4 2 for kNN search  Specifically  let dist be the distance of the k th closest pair of points CP3 has discovered so far  We rule that Z o1  and Z o2  are faraway if dist     2 u   bLLCP  Z o1  Z o2   mc 1 where u  m  and LLCP       are as de   ned in Section 4  The algorithm CP3 is formally presented in Figure 18  ACM Journal Name  Vol  V  No  N  Month 20YY 28    pinpointed in memory         scan forward     until these two entries    Z values are sufficiently different Fig  17  CP search with only one LSB tree Algorithm CP3 1  N   the leftmost leaf node 2  repeat 3  compute the distances of all pairs of points in N 4  N0   the leaf node to the right of N 5  repeat 6  compute the distance of each point in N0 to each point in N 7  if an entry in N0 is su   ciently faraway from the rightmost entry of N 8  break 9  else N0   the leaf node to the right of N0 10  until N0       11  N   the leaf node to the right of N 12  until N       13  return the k closest pairs found so far Fig  18  A CP algorithm using only a single LSB tree 8  RELATED WORK NN search is well understood in low dimensional space  Hjaltason and Samet 1999  Roussopoulos et al  1995   This problem  however  becomes much more di   cult in high dimensional space  Many algorithms  e g   those based on data or space partitioning indexes  Gaede and Gunther 1998   that perform nicely on low dimensional data  deteriorate rapidly as the dimensionality increases  Bohm 2000  Weber et al  1998   and are eventually outperformed even by sequential scan  Research on high dimensional NN search can be divided into exact and approximate retrieval  In the exact category  Lin et al   Lin et al  1994  propose the TV tree which improves conventional R trees  Beckmann et al  1990  Guttman 1984  by creating MBRs only in selected subspaces  Weber et al   Weber et al  1998  design the VA    le  which compresses the dataset to minimize the cost of sequential scan  Also based on the idea of compression  Berchtold et al   Berchtold et al  2000  develop the IQ tree  combining features of the R tree and VA    le  Chaudhuri and Gravano  Chaudhuri and Gravano 1999  perform NN search by converting it to range queries  In  Berchtold et al  2000  Berchtold et al  provide a solution leveraging high dimensional Voronoi diagrams  whereas Korn et al   Korn et al  2001  tackle the problem by utilizing the fractal dimensionality of the dataset  Koudas et al   Koudas et al  2004  give a bitmap based approach  The state of the art is due to Jagadish et al   Jagadish et al  2005   They develop the iDistance technique that converts high dimensional points to 1D values  which are indexed using a B tree for NN processing  We will compare our solution to iDistance experimentally  ACM Journal Name  Vol  V  No  N  Month 20YY    29 In exact search  a majority of the query cost is spent on verifying a point as a real NN  Bennett et al  1999  Ciaccia and Patella 2000   Approximate retrieval improves e   ciency by relaxing the precision of veri   cation  Goldstein and Ramakrishnan  Goldstein and Ramakrishnan 2000  assume that the query distribution is known  and leverage the knowledge to balance the e   ciency and result quality  Ferhatosmanoglu et al   Ferhatosmanoglu et al  2001     nd NNs by examining only the interesting subspaces  Chen and Lin  Chen and Ling 2002  combine sampling with a reduction  Chaudhuri and Gravano 1999  to range search  Li et al   Li et al  2002     rst partition the dataset into clusters  and then prunes the irrelevant clusters according to their radii  Houle and Sakuma  Houle and Sakuma 2005  develop SASH which is designed for memory resident data  but is not suitable for diskoriented data due to severe I O thrashing  Fagin et al   Fagin et al  2003  develop the MedRank technique that converts the dataset to several sorted lists by projecting the data onto di   erent vectors  To answer a query  MedRank traverses these lists in a way similar to the threshold algorithm  Fagin et al  2001  for top k search  We will also evaluate MedRank in the experiments  None of the aforementioned solutions ensures sub linear growth of query cost in the worst case  How to achieve this has been carefully studied in the theory community  see  for example   Har Peled 2001  Krauthgamer and Lee 2004  and the references therein   Almost all the results there  however  are excessively complex for practical implementation  except LSH  This technique is invented by Indyk and Motwani  Indyk and Motwani 1998  for in memory data  Gionis et al   Gionis et al  1999  adapt it to external memory  but as discussed in Section 3 2  their method loses the guarantee on the approximation ratio  The locality sensitive hash functions for lp norms are discovered by Datar et al   Datar et al  2004   Bawa et al   Bawa et al  2005  propose a method to tune the parameters of LSH automatically  Their method  however  no longer ensures the same query performance as LSH unless the adopted hash function has a so called        f     property     Bawa et al  2005   Unfortunately  no existing hash function for  p norms is known to possess this property  Charikar  Charikar 2002  investigate LSH for several distance metrics di   erent from  p norms  LSH has also received other theoretical improvements  Andoni and Indyk 2006  Panigrahy 2006  which cannot be implemented in relational databases  Furthermore  several heuristic variations of LSH have also been suggested  For example  Lv et al   Lv et al  2007  reduce space consumption by probing more data in answering a query  while recently Athitsos et al   Athitsos et al  2008  introduce the notion of distance based hashing  The solutions of  Athitsos et al  2008  Lv et al  2007  guarantee neither sub linear cost nor good approximation ratios  CP search is also one of the oldest problems studied in computational geometry  In two dimensional space  Shamos and Hoey  Shamos and Hoey 1975  give an optimal algorithm that runs in O n log n  time  Interestingly  for any    xed dimensionality d  the problem can also be settled optimally in O n log n  time  as discovered by Lenhof and Smid  Lenhof and Smid 1992   The optimality of the above algorithms  however  assumes that d is a constant  when it is not  their running time increases exponentially with d  Avoiding such exponential growth turns out to be a hard problem  Recently  by resorting to matrix multiplication  Indyk et al   Indyk et al  2004  give several algorithms with non trivial bounds that are ACM Journal Name  Vol  V  No  N  Month 20YY 30    applicable to L1 and L    norms  but not to the other Lp norms  The methods mentioned earlier are rather theoretical  On the practical side  Hjaltason and Samet  Hjaltason and Samet 1998  give a solution  called distance browsing  that utilizes R trees to report point pairs in ascending order of their distances  Following the same idea  Corral et al   Corral et al  2000  propose an enhanced algorithm with smaller cost  which will be evaluated in the experiments  The above solutions aim at solving the CP problem exactly  There have been attempts to address the approximate version  but most of those algorithms require running time that is quadratic to the cardinality n  albeit faster than dn 2    see for example  Kleinberg 1997   Based on the LSH technique  Datar et al   Datar et al  2004  propose an algorithm with sub quadratic time  but their analysis targets internal memory only  Our discussion in Section 7 1 can in fact also be applied to LSH  and shows that the case of external memory requires a more elaborate reasoning approach  Furthermore  our result in Section 7 2 is actually better  by a logarithmic factor  than the obvious bound adapted from  Datar et al  2004   which corresponds to the performance of algorithm CP1 in Figure 15   Another algorithm worth mentioning is due to Lopez and Liao  Lopez and Liao 2000   When d is regarded as a constant  their algorithm  which we call D shift  guarantees an answer with constant approximation ratio  Their algorithm can be incorporated in relational databases  and will be compared to our solutions in the experiments  Finally  it is worth pointing out that this paper substantially extends its preliminary version  Tao et al  2009  in the following ways  First  in Section 6  we have shown how to modify our NN techniques to achieve approximation c     for any integer c     3  only c   2 is discussed in  Tao et al  2009    thus giving a stronger tradeo    between the result quality and the query space e   ciency  Second  while the preliminary work concentrates on NN search  the current version contains a full set of results on the CP problem  Section 7   together with the corresponding experiment in the next section  9  EXPERIMENTS Next we experimentally evaluate the performance of LSB trees  using the existing methods as benchmarks  Section 9 1 describes the datasets and queries  Sections 9 2 and 9 3 list the techniques to be examined for NN and CP search  respectively  Section 9 4 explains the computing environments as well as the assessment metrics  Section 9 5 demonstrates the superiority of the LSB forest over alternative LSH implementations  Then  Section 9 6  9 7  shows that our techniques signi   cantly outperform the previous methods  in both exact and approximate NN  CP  search  9 1 Data and queries We experimented with both synthetic and real datasets  Synthetic data were generated according to a varden distribution to be clari   ed shortly  As for real data  we deployed datasets color and mnist  which were also used in the papers  Fagin et al  2003  Jagadish et al  2005  where iDistance and MedRank are invented respectively  both methods were included in our experiments   For all datasets  the universe was normalized to have domain  0  10000  on each dimension  The distance metric employed was Euclidean distance  Each NN workload contained 50 query points that followed the same distribution as the underlying dataset  ACM Journal Name  Vol  V  No  N  Month 20YY    31 CP search takes no query point  it simply    nds the k closest pairs in a dataset  The details of varden  color  and mnist are as follows  Varden  This distribution contains two clusters with drastically di   erent densities  The sparse cluster has 10 points  whereas all the other points belong to the dense cluster  Furthermore  the dense cluster has the shape of a ring  whose radius is comparable to the average mutual distance of the points in the sparse cluster  The two clusters are well separated  Figure 19 illustrates the idea with a 2D example  We varied the cardinality of a varden dataset from 10k to 100k  and its dimensionality from 25 to 100  In the sequel  we will denote a d dimensional varden dataset with cardinality n by varden nd  The corresponding workload of a varden dataset had 10 and 40 query points that fell in the areas of the sparse and dense clusters  respectively  No query point coincided with any data point  Fig  19  The varden distribution Color  This is a 32 dimensional dataset 3 with 67 967 points  where each point describes the color histogram of an image in the Corel collection  Jagadish et al  2005   We randomly removed 50 points to form a query set  As a result  our color dataset has cardinality 67 917  Mnist  The original mnist dataset 4 is a set of 60 000 points  Each point is 784  dimensional  capturing the pixel values of a 28    28 image  Since  however  most pixels are insigni   cant  we reduced dimensionality by taking the 50 dimensions with the largest variances  After this  we got two identical points so one of them was removed  rendering the    nal cardinality to be 59 999  The mnist collection also contains a test set of 10 000 points  Fagin et al  2003   among which we randomly picked 50 to form our workload  Obviously  each query point was also projected onto the same 50 dimensions output by the dimensionality reduction  9 2 Competitors for nearest neighbor search Sequential scan  SeqScan   The bruteforce approach is included because it is known to be a strong competitor in high dimensional NN retrieval  Furthermore  the relative performance with respect to SeqScan serves as a reliable way to compare against methods that are reported elsewhere but not in our experiments  3 http   kdd ics uci edu databases CorelFeatures   4 http   yann lecun com exdb mnist  ACM Journal Name  Vol  V  No  N  Month 20YY 32    LSB forest  As discussed in Section 4 2  this method takes l LSB trees  l given by Equation 7   and applies algorithm NN1  Figure 7  for query processing  For kNN queries with k   1  LSB forest still uses the same l  i e   no increase in the number of trees  and query algorithm  except that the terminating condition E2 is modi   ed in the way explained in Section 4 2  LSB noE2  Same as LSB forest except that it disables condition E2 in algorithm NN1  In other words  LSB noE2 terminates on condition E1 only  LSB noE2 is applied only for k   1  LSB tree  This method deploys a single LSB tree  as opposed to l in LSB forest   and hence  requires only linear space and can be updated e   ciently  As mentioned at the end of Section 4 2  it disables condition E1  and terminates on E2 only  again  E2 needs to be modi   ed for k   1   Rigorous   Indyk and Motwani 1998  and adhoc LSH  Gionis et al  1999   They are the existing LSH implementations as reviewed in Sections 3 1 and 3 2  respectively  Recall that both methods are designed for c approximate BC search  We set c to 2 to match the guarantee of the LSB forest  see Section 6   Adhoc LSH requires a set of l hash tables  which is used to perform BC queries at a magic radius  to be tuned experimentally later   where l is the same as in Equation 7  Rigorous LSH can be regarded as combining multiple versions of adhoc LSH  one for every radius supported  iDistance  Jagadish et al  2005   A famous approach for exact NN search  As mentioned in Section 8  it indexes a dataset using a single B tree after converting all points to 1D values  As with LSB tree  it consumes linear space and supports data insertions and deletions e   ciently  MedRank  Fagin et al  2003   A recently proposed method for approximate NN search with a non trivial quality guarantee  Given a dataset  MedRank creates several sorted lists  such that every data point has an entry in each list  More speci   cally  an entry has the form  id  key   where id uniquely identi   es a point  and key is its sorting key  a point has various keys in di   erent lists   Each list is indexed by a B tree on the keys  Point coordinates are stored in a separate hash table to facilitate probing by id  The number of lists equals log n  following Theorem 4 in  Fagin et al  2003    where n is the dataset cardinality  It should be noted that MedRank is not friendly to updates  because a single point insertion deletion requires updating all the log n lists  9 3 Competitors for closest pair search Quadratic  The naive approach that examines all pairs of points  It serves as a benchmark for comparison with other solutions to the CP problem not included in our experiments  LSB forest  This method uses l LSB trees  where l is given by Equation 7  and applies algorithm CP2  Figure 16   The same l and algorithm are also used for kCP search with k   1  except that CP2 needs to report the k best pairs  instead ACM Journal Name  Vol  V  No  N  Month 20YY    33 of 1   2LSB tree  The method uses two LSB trees  it applies algorithm CP3  Figure 18  on each tree separately  and returns the k best pairs after combining the outputs from both trees  Here we are using one more tree than the LSB tree method in the previous subsection  in order to outperform the competitor D shift  to be introduced later  in result quality  Note that using 2 trees does not increase the space and update time complexities  Namely  2LSB tree still occupies linear space and can be updated in logarithmic time  Distance browsing  DistBrowsing   Corral et al  2000   An extensively cited solution to exact kCP search  Similar to  Hjaltason and Samet 1998   it leverages an R tree on the underlying dataset to enumerate point pairs in ascending order of distances  Diagonal shift  D shift   Lopez and Liao 2000   An approximate algorithm with a non trivial quality guarantee  Given a d dimensional dataset  it creates d copies of the dataset  where each copy is obtained by shifting the original dataset along the direction of the main diagonal by a di   erent o   set  hence the name Dshift   Then  the closest pairs are found by sorting and scanning each list once  9 4 Computing environments and assessment metrics The page size was    xed to 4 096 bytes  All the experiments were run on a computer equipped with a 3GHz CPU  A memory bu   er of 50 pages was adopted in all cases  Under such settings  the running time of all  NN and CP  algorithms was dominated by their I O overhead  Therefore  we will report the number of I Os as the computation cost  Quality of NN search  We evaluate the quality of a kNN result by how many times farther a reported neighbor is than the real NN  Formally  let o1  o2       ok be the k neighbors that a method retrieves for a query q  in ascending order of their distances to q  Let o     1   o     2      o     k be the actual    rst  second       k th NNs of q  respectively  For any i      1  k   we de   ne the rank i  approximation  ratio  denoted by Ri q   as Ri q    koi   qk ko     i   qk   16  The overall  approximation  ratio is the mean of the ratios of all ranks  namely    Pk i 1 Ri q   k  When a query result is exact  all ratios are 1  Given a workload W  de   ne its average overall ratio as the mean of the overall ratios of all queries in W  This metric re   ects the general quality of all k neighbors  and was used in most experiments  Sometimes we needed to scrutinize the quality of neighbors at individual ranks  In that case  we measured the average rank i ratio  1     i     k   which is the mean of the rank i ratios of all queries in W  namely    P    q   W Ri q    W   Quality of CP search  Also assessed based on rank i ratio and overall ratio  both of which are extended from the earlier de   nitions in a straightforward manner  ACM Journal Name  Vol  V  No  N  Month 20YY 34    average overall ratio     1 100 2 2 6 2 10 2 14 2 18 2 22 2 magic radius rm sparse  average overall ratio     1 100 dense 2 2 6 2 10 2 14 2 18 2 22 2 magic radius rm  a  Quality vs  rm  b  Quality of sparse and dense queries Fig  20  Magic radius tuning for adhoc LSH  varden 10k50d  9 5 Behavior of LSH implementations This section explores the characteristic behavior of LSB forest  LSB noE2  rigorousLSH  and adhoc LSH  For this purpose  we focused on k   1 because the LSH methodology was originally designed in the context of single NN retrieval  Note that  when k   1  the overall ratio of a query is identical to its rank 1 ratio  The data distribution examined was varden  as it allowed us to adjust the dimensionality and cardinality  Unless otherwise stated  a varden dataset had a default cardinality n   10k and dimensionality d   50  Recall that adhoc LSH answers a NN query by processing instead a BC query with a magic radius rm  As argued in Section 3 2  there may not exist an rm good for all NN queries  To demonstrate this  Figure 20a shows the average overall ratio of adhoc LSH as rm varied from 2 2 to 2 22   For small rm  the ratio is      implying that adhoc LSH missed at least one query in the workload  namely  returning nothing at all  The ratio improved suddenly to 66 when rm reached 2 14   and stabilized as rm grew further  It is thus clear that  given any rm  the result of adhoc LSH was at least 66 times worse than the real NN on average  As discussed in Section 3 2  if rm is considerably smaller than the NN distance of a query  adhoc LSH may return an empty result  Conversely  if rm is considerably larger  adhoc LSH may output a point much worse than the real NN  We performed an experiment to verify this  Recall that a workload for varden has queries in both the sparse and dense clusters  Let us call the former the sparse queries  and the latter the dense queries  We observed that the average NN distance of a sparse  dense  query was around 12 000  15   The phenomenon in Figure 20a occurred because the values of rm good for sparse queries were bad for dense queries  and vice versa  To support the claim  Figure 20b plots the average overall ratios of sparse and dense queries separately  When rm was smaller than or equal to 2 13   8 192  it was much lower than the NN distances of sparse queries  hence  adhoc LSH returned nothing for them  as is why the sparse curve in Figure 20b stays at     for all rm     2 13   As rm climbed to 2 12  adhoc LSH started to return bad results for many dense queries  The situation was worse for larger rm  so the dense curve ACM Journal Name  Vol  V  No  N  Month 20YY    35 d 25 50 75 100 rigorous LSH 1 adhoc LSH 43 66 4 87 104 2 LSB forest 1 02 1 02 1 02 1 01 LSB noE2 1  a  Average overall ratio vs  dimensionality d  n   50k  n 10k 25k 50k 75k 100k rigorous LSH 1 adhoc LSH 66 4 68 1 70 3 76 5 87 1 LSB forest 1 02 1 02 1 03 1 02 1 02 LSB noE2 1  b  Average overall ratio vs  cardinality n  d   50  Table I  Result quality on varden data rigorous LSH adhoc LSH LSB forest LSB noE2 I O cost 0 2 4 6 8 10 12 14 25 50 75 100 dimensionality d   u 100  0 5 10 15 20 25 30 35 10k 25k 50k 75k 100k cardinality n  I O cost   u 100   a  Cost vs  d  n   50k   b  Cost vs  n  d   50  Fig  21  Query e   ciency on varden data Figure 20b increases continuously from 2 12   In all the following experiments  we    xed rm to the optimal value 2 14   The next experiment compares the result quality of rigorous LSH  adhoc LSH  LSB forest  and LSB noE2  Table Ia  Ib  shows their average overall ratios under di   erent dimensionalities  cardinalities   Both rigorous LSH and LSB noE2 achieved perfect quality  namely  they successfully returned the exact NN for all queries  LSB forest incurred slightly higher error because in general it accesses fewer points than LSB noE2  and thus  has a lower chance of encountering the real NN  Adhoc LSH was by far the worst method  and its e   ectiveness deteriorated rapidly as the dimensionality or cardinality increased  To evaluate the query e   ciency of the four methods  Figure 21a  21b  plots their I O cost as a function of dimensionality d  cardinality n   LSB forest considerably outperformed its competitors in all cases  Notice that while LSB noE2 was slightly more costly than adhoc LSH  LSB forest entailed only a fraction of the overhead of adhoc LSH  This phenomenon reveals the importance of having terminating condiACM Journal Name  Vol  V  No  N  Month 20YY 36    tion E2 in the NN1 algorithm  Rigorous LSH was much more expensive than the other approaches  which is consistent with its vast asymptotical complexity  Tables IIa and IIb show the space consumption  in mega bytes  of each solution as a function of d and n  respectively  LSB noE2 is not included because it di   ers from LSB forest only in the query algorithm  and thus  had the same space cost as LSB forest  Furthermore  adhoc LSH also occupied as much space as LSB forest  because a hash table of the former stores the same information as a LSB tree of the latter  As predicted by their space complexities  rigorous LSH required more space than LSB forest by a factor of log d   log t  where t  the largest coordinate on each dimension  was 10 000 in our experiments  d 25 50 75 100 rigorous LSH 382 896 1 563 2 436 adhoc LSH 24 57 101 159 LSB forest 24 57 101 159  a  Space vs  dimensionality d  n   50k  n 10k 25k 50k 75k 100k rigorous LSH 895 3 624 10 323 18 892 29 016 adhoc LSH 57 231 660 1 208 1 855 LSB forest 57 231 660 1 208 1 855  b  Space vs  cardinality n  d   50  Table II  Space consumption on varden data in mega bytes It is evident that LSB forest is overall the best technique in the above experiments  It retains the query accuracy of rigorous LSH  consumes the same space as adhoc LSH  and incurs signi   cantly smaller query cost than both  9 6 Comparison of NN solutions Having veri   ed the correctness of our theoretical analysis  in the sequel we assess the practical performance of SeqScan  LSB tree  LSB forest  adhoc LSH  MedRank  and iDistance  Rigorous LSH and LSB noE2 are omitted because the former incurs gigantic space query cost  and the latter is merely an auxiliary method for demonstrating the importance of condition E2  Remember that SeqScan and iDistance return exact NNs  whereas the other methods are approximate  Only real datasets color and mnist were adopted in the subsequent evaluation  The workload on color  mnist  had an average NN distance of 833  11 422   We set the magic radius of adhoc LSH to the smallest power of 2 that bounds the average NN distance from above  namely  1 024 and 16 384 for color and mnist  respectively  The number k of retrieved neighbors varied from 1 to 100  Let us start with query e   ciency  Figure 22a  22b  illustrates the average cost of a kNN query on dataset color  mnist  as a function of k  LSB tree was by far the fastest method  and outperformed all the other approaches by a factor of at least an order of magnitude  In particular  on mnist  LSB tree even achieved a speedup of two orders of magnitude over iDistance  justifying the advantages of approximate retrieval  LSB forest was also much faster than iDistance  MedRank  and adhoc LSH  especially in returning a large number of neighbors  ACM Journal Name  Vol  V  No  N  Month 20YY    37 LSB forest LSB tree iDistance MedRank adhoc LSH I O cost number k of neighbors 1 20 40 60 80 10 100 10 3 10 2 10 1 I O cost number k of neighbors 1 20 40 60 80 10 100 10 3 1 10 10 4 10 2 Cost of SeqScan   2189 Cost of SeqScan   2989  a  Color  b  mnist Fig  22  E   ciency of kNN search The next experiment inspects the result quality of the approximate techniques  Focusing on color  mnist   Figure 23a  23b  plots the average overall ratios of MedRank  LSB forest  and LSB tree as a function of k  Since adhoc LSH may miss a query  i e   unable to return k neighbors   we present its results as a table in Figure 23c  where each cell contains two numbers  The number in the bracket indicates how many queries were missed  out of 50   and the number outside is the average overall ratio of the queries that were answered properly  No ratio is reported if adhoc LSH missed more than 30 queries  LSB forest incurred low error in all cases  maximum ratio below 1 5   owing to its nice theoretical properties  LSB tree also had good precision  maximum ratio 2   indicating that the proposed conversion  from a d dimensional point to a Z order value  adequately preserved the spatial proximity of data points  MedRank  in contrast  exhibited much worse precision than the proposed solutions  In particular  observe that MedRank was not e   ective in the important case of single NN search  k   1   for which its average overall ratio was over 4  Finally  adhoc LSH was clearly unreliable due to the large number of queries it missed  The average overall ratio re   ects the general quality of all k neighbors reported  It does not  however  indicate how good the neighbors are at individual ranks  To    nd out  we set k to 10  and measured the average rank i ratios at each i      1  10   Figures 24a and 24b demonstrate the results on color and mnist  respectively  adhoc LSH is not included because it missed many queries   Apparently  both LSB forest and LSB tree provided results signi   cantly better than MedRank at all ranks  Observe that the quality of MedRank deteriorated considerably at high ranks  whereas our solutions returned fairly good neighbors even at the highest rank  Note that the results in Figure 24 should not be confused with those of Figure 23  For example  the average rank 1 ratio  of k   10  is di   erent from the ACM Journal Name  Vol  V  No  N  Month 20YY 38    MedRank LSB forest LSB tree average overall ratio number k of neighbors 1 2 3 4 5 1 20 40 60 80 100 10 average overall ratio number k of neighbors 1 20 40 60 80 10 100 1 2 3 4 5  a  Color  b  Mnist k 1 10 20 40 60 80 100 color 1 2  0  1 3  30     42     46     46     47     48  mnist 1 2  0  1 3  13  1 3  19  1 4  28     37     39     41   c  Results of adhoc LSH  in each cell  the number inside the bracket is the number of missed queries  and the number outside is the average overall ratio of the queries answered properly  Fig  23  Average overall ratio vs  k MedRank LSB forest LSB tree average rank i ratio rank i 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 9 10 average rank i ratio rank i 1 2 3 4 5 6 7 8 9 1 2 3 4 5 6 7 8 9 10  a  Color  b  mnist Fig  24  Average ratios at individual ranks for 10NN queries overall average ratio of k   1 5   Table III compares the space consumption of di   erent methods  LSB tree required slightly less space than iDistance and MedRank  We  however  ought to point out that  at least in theory  LSB tree needs to store more information than 5The average rank 1 ratio is lower because processing a query with k   10 needs to access more data than a query with k   1  and therefore  has a better chance of encountering the nearest neighbor  ACM Journal Name  Vol  V  No  N  Month 20YY    39 iDistance  so the latter should be more space economical  However  the actual space consumption may contain some extra overhead depending on the concrete implementation  The implementation of iDistance we deployed was exactly the one written by the authors of  Jagadish et al  2005   Also note that our implementations of LSB tree  LSB forest  and Adhoc LSH have been improved compared to those in the preliminary version  Tao et al  2009   iDistance MedRank adhoc LSH LSB forest LSB tree color 14 17 573 573 13 mnist 18 19 874 874 16 Table III  Space consumption on real data in mega bytes Recall that LSB forest utilizes a large number l of LSB trees  where the number l was 47 and 55 for color and mnist  respectively  LSB tree represents the other extreme that uses only a single tree  Next  we explore the compromise of these two extremes  by using multiple  but less than l  trees  The query algorithm is the same as the one adopted by LSB tree  In general  leveraging x trees increases the query  space  and update cost by a factor of x  The bene   t  however  is that a larger x also improves the quality of results  To explore this tradeo     Figure 25 shows the average overall ratio of 10NN queries on the two real datasets  when x grew from 1 to the corresponding l of LSB forest  Interestingly  the precision improved dramatically with just a small number of trees  In other words  we can obtain much better results without increasing the space or query overhead considerably  which is especially appealing for datasets that are not updated frequently  color minst average overall ratio number of LSB trees 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 20 30 40 50 55 Fig  25  Bene   ts of using multiple LSB trees  k   10  In summary  our experiment results suggest that an exact solution such as iDistance often requires excessively long query response time in practice  con   rming the motivation of study approximate solutions  The most serious drawback of AdhocLSH is that it may fail to report enough neighbors for many queries  In any case  its query overhead is still too high to provide fast response time  MedRank is even more expensive than adhoc LSH  furthermore  its result quality is relatively low ACM Journal Name  Vol  V  No  N  Month 20YY 40    LSB forest 2LSB tree quadratic DistBrowsing D shift I O cost number k of p</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search"/>
        <doc>Continuous Obstructed Nearest Neighbor Queries   in Spatial Databases ###  Yunjun Gao   School of Information Systems   Singapore Management University   yjgao smu edu sg   Baihua Zheng   School of Information Systems   Singapore Management University   bhzheng smu edu sg       ABSTRACT In this paper  we study a novel form of continuous nearest  neighbor queries in the presence of obstacles  namely continuous  obstructed nearest neighbor  CONN  search  It considers the  impact of obstacles on the distance between objects  which is  ignored by most of spatial queries  Given a data set P  an obstacle  set O  and a query line segment q in a two dimensional space  a  CONN query retrieves the nearest neighbor of each point on  q according to the  obstructed distance  i e   the  shortest path between them without crossing any obstacle  We formulate  CONN search  analyze its unique properties  and develop  algorithms for exact CONN query processing  assuming that both  P and  O are indexed by conventional data partitioning indices   e g   R trees   Our methods tackle the CONN retrieval by  performing a single query for the entire query segment  and only  process the data points and obstacles relevant to the final result   via a novel concept of control points and an efficient quadraticbased split point computation algorithm  In addition  we extend  our solution to handle the  continuous obstructed k nearest  neighbor  COkNN  search  which finds the  k       1  nearest  neighbors to every point along q based on obstructed distances  A  comprehensive experimental evaluation using both real and  synthetic datasets has been conducted to demonstrate the  efficiency and effectiveness of our proposed algorithms    Categories and Subject Descriptors H 2 8  Database Management   Database Applications     Spatial  databases and GIS  H 2 4  Database Management   Systems     Query processing   General Terms Algorithms  Design  Experimentation  Performance  Theory   Keywords Nearest neighbor  Continuous nearest neighbor  Continuous  obstructed nearest neighbor  Spatial database  Obstacle ###  1  INTRODUCTION   With the growing popularity of smart mobile devices  e g    PDAs  cellular phones  and the rapid advance of wireless  communication and positioning technologies  e g   GPS   more  and more users issue queries even when they are moving   Consequently  the traditional snapshot query might not satisfy the  real requirements from mobile users  and  continuous query  processing that is based on a moving trajectory instead of a fixed  point has been investigated  For instance  the continuous nearest  neighbor  CNN  search is to answer the nearest neighbor query  issued by clients who are moving  Imagine a client who is driving  along the highway I 95 issues a CNN query to retrieve the nearest  gas station from his current location to his destination  as shown  in Figure 1 a   Here  the data set P contains six gas stations  i e   a   b   c   d   f   g  and the trajectory segment  q    S   E  represents a  segment of I 95  The output of the CNN search is     d   S  s1          b    s1   s2          g   s2   s3          c   s3   E       meaning that gas station d is the  nearest one when the client is travelling along the interval  S  s1     gas station b is the nearest one along the interval  s1  s2   and so  on  The points s1  s2  s3 along q are defined as split points  where  the nearest neighbor  NN  object is changed    s1 s2 s3 E a b c d f g split point S q     S E a b c d f g o3 o1 o2 o4 obstacle split point q s2  s3  m s1   a  CNN search                                     b  CONN search   Figure 1  Example of CNN and CONN queries   CNN search has been well studied  22  24   Based on the  distance metric used to measure the closeness of objects  the  existing work can be classified into two categories  i e   Euclidean  distance based CNN search  22  23  24  and  network distance based CNN search  4  7  14   The former assumes a  Euclidean  space where the objects enjoy totally free and unblocked  movement and employs the  Euclidean distance to indicate the  proximity of objects  while the latter considers a  network space where the movements of objects are restricted by the underlying  networks  e g   roads  railways  etc   and utilizes the  network  distance to measure the distance between objects    Although the existing work satisfies the requirements of a large  number of real applications  it does not consider the movement in  an open space constrained by the obstacles  i e   obstructed space    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full  citation on the first page  To copy  otherwise  or republish  to post on servers or to redistribute to lists   requires prior specific permission and or a fee   SIGMOD   09  June 29   July 2  2009  Providence  RI  USA    Copyright 2009 ACM 978 1 60558 551 2 09 06    5 00   For example  a battlefield usually does not have any fixed road  network structure and tanks soldiers can move totally free  as long  as the path is not blocked  Another example is that mobile robots  help rescue survivors after a  disaster  e g   a devastating  earthquake   The robots equipped with location sensing ability as  well as visual and other sensors can burrow into the rubble and try  to locate potential survivors  which can facilitate the excavation  without further injuring survivors  Theoretically  the robot  navigating the space can take any direction but the physical  obstacles  e g   rocks etc   affect the real distance that a robot has  to travel in order to reach its destination    Consequently  the obstructed space is different from both the  Euclidean space and the network space  Compared with the  Euclidean space  it considers the existence of obstacles that may  block the immediate path from one object to another and hence  the Euclidean distance between them does not always indicate the  real travelling distance  On the other hand  compared with the  network space  it does not assume any underlying fixed network  structure and still entitles the objects to free movements   Correspondingly  the distance between two objects in an  obstructed space is measured based on the  obstructed distance   i e   the shortest path connecting two objects without crossing any  obstacle  Take the objects  a and  g shown in Figure 1 b  as an  example  Their Euclidean distance is the length of segment  a  g    whereas their obstructed distance is the summation of the lengths  of segment  a  m  and segment  m  g   because of the obstruction  of obstacle o4     In this paper  we consider the CNN search in an obstructed  space  namely  continuous obstructed nearest neighbor  CONN   search  Given a data set  P  an obstacle set  O  and a query line  segment  q in a two dimensional  2D  space  a CONN query  retrieves the obstructed nearest neighbor  ONN  for every point  along  q according to the obstructed distance  Specifically  the  CONN retrieval aims at finding a set of    p  R    tuples  where p     P is the ONN for any point in the interval R     q  Continuing the  example in Figure 1 b  where the obstacle set  O contains four  rectangular obstacles 1 o1   o2   o3   and o4   the CONN query returns      a   S   s1              b   s1       s2              g   s2       s3              c   s3       E       which  indicates that object a is the ONN for each point along interval  S   s1      object b is the ONN for each point along interval  s1     s2      etc   Note that the split points s1  s2  and s3 defined by a CNN search  are different from the split points  s1      s2     and  s3    defined by a  CONN search  In addition  the answer objects vary as well  For  example  object d is the NN for S in a Euclidean space  whereas it  is not the ONN for S in an obstructed space  due to the blocking  of obstacle o3     CONN search is useful for many real life applications   Consider the example of robots rescuing survivors  Suppose that  the robots successfully locate some survivors and a 3D map has  been constructed based on the data collected by robots along their  way navigating the space  Based on the map information  we can  identify several routes that are not blocked and invoke CONN  search to locate the nearest survivors along the path  The number  of nearest survivors and the distance between the survivors to the  corresponding points along the path provide critical information  that can help the emergency personnel to plan the excavation   We  focus this paper on the CONN query  not only because the                                                                   1  Although an obstacle can be in any shape  e g   triangle   pentagon  etc    we assume it is a rectangle in this paper    problem is interesting and has a large application base  but also  because it poses some challenging research issues that are worth  investigating    The first issue is how to calculate the obstructed distance  efficiently   Based on the existing work related to robot motion  planning  the lower bound of the calculation is O   nlogn   with n as the total number of obstacle vertices  2   In practice  a popular  and practical method based on visibility graph VG  2  has O   n 2 logn  as the worst case time complexity  Compared with  Euclidean distance which can be derived in constant time  the  computation cost of the obstructed distance is much more  expensive  In addition  VG based approaches need to maintain a  visibility graph  which requires  O   n 2   space in the worst case   The high space complexity deteriorates its scalability  not to  mention its extremely high update cost    We try to tackle this issue from two aspects  i e   reducing the  number of obstructed distance calculations and simplifying the  obstructed distance calculations  The former objective is achieved  via effective pruning techniques that can filter out unqualified  objects as early as possible  As for the second target  we construct  a  local visibility graph to simplify the calculation process   Initially  the local visibility graph only contains two endpoints of  a given query line segment  As we process the query and evaluate  data points  we incrementally insert the obstacles that might affect  the query result into the local visibility graph  Due to the small  size of the local visibility graph  the insertion deletion update can  be efficiently supported    The second issue is how to efficiently answer a CONN query   A naive approach is to issue an  obstructed nearest neighbor  ONN  search  31  at every point of a specified query line  segment q  However  this approach is definitely infeasible as the  number of points on  q is infinite  Motivated by the fact that  nearby points along the query segment might share the same  ONN  we adopt an incremental approach to fine tune the result  upon the evaluation of each new data point  based on the concept  of split point  i e   the points along the query segment bounded by  two continuous split points share the same ONN   Nevertheless   due to the existence of obstacles  existing split point formation  algorithms developed for CNN search cannot be applied  In this  paper  we propose a novel concept  namely  control point  to  facilitate the computation of obstructed distances  and design a  quadratic based approach to form split points  In addition  several  pruning strategies have been proposed to further improve the  search performance    In summary  this paper has made five fold main contributions   summarized as follows    y We formalize CONN search  a new addition to the family of  spatial queries in an obstructed space  To the best of our  knowledge  this paper is the first attempt on this problem    y We introduce the concept of control point that significantly  simplifies the computation and comparison of the obstructed  distance between two objects    y We propose a quadratic based method to form split points   by solving quadratic inequalities    y We develop an efficient algorithm for processing CONN  search which can be extended to handle COkNN retrieval    y We conduct extensive experiments using both real and  synthetic datasets to demonstrate the efficiency and  effectiveness of the proposed algorithms   The rest of this paper is organized as follows  Section 2  overviews related work  Section 3 formulates the CONN problem   introduces the concept of control point  and presents the split  point computation approach  Section 4 elaborates an efficient  algorithm for CONN query  Section 5 reports experimental results  and our findings  Finally  Section 6 concludes the paper with  some directions for future work    2  RELATED WORK   In this section  we review the existing work related to CONN  queries  namely  point NN search in the Euclidean space   snapshot CNN CkNN queries  query processing with the existence  of obstacles  and main memory obstacle path search    2 1 Point NN Search   A conventional  i e   point  NN query finds the  k       1  data  point s  from a data set that are closest to a specified query point  q according to Euclidean distance  The algorithms for NN search  on R trees  1  11  19  follow the branch and bound paradigm and  utilize some metrics to prune the search space  For example  the  metric mindist q  N  corresponds to the minimal distance between  q and any point included by a node N  and thus it gives a lower  bound of the distance from any point of N to q    Existing algorithms for NN search usually follow either  bestfirst  BF  or depth first  DF  traversal paradigm  DF algorithms  3   18  start from the root  and visit recursively the node with the  smallest mindist to q until the leaf level where a potential NN is  reached  Subsequently  the algorithm conducts backtrackings  In  particular  during backtracking to the upper levels  DF only visits  those entries whose minimum distances to q are smaller than the  distance between the NN candidate retrieved so far and the query  point  As demonstrated in  16   the DF algorithm is suboptimal   i e   it accesses more nodes than necessary    BF algorithms  12  13  achieve the optimal I O performance by  visiting only the nodes necessary for obtaining the NN s    Towards this  BF maintains a heap H with the entries visited so  far  sorted in ascending order of their mindist to q  Starting from  the root node  BF recursively examines the top entry e of H  If e is  an intermediate node  i e   a non leaf node   its child entries are  en heaped for later examination  If e is a data point  it is reported  as an actual NN of a query point  Both DF and BF can be easily  extended to retrieve k    1  NNs  Furthermore  BF is incremental   i e   it returns the NNs in ascending order of their distances to the  query point  and hence k does not have to be known in advance   which allows different termination conditions to be applied    2 2 Snapshot CNN CkNN Queries   The CNN search has received considerable attention since it  was first introduced by Sistla  et al   21  in spatial temporal  databases  In the initial work  modeling methods and query  languages for the expression of CNN queries are presented  but  not the processing algorithms  The first algorithm for CNN query  processing  based on periodical sampling technique  is proposed  in  22   Due to the inherent shortcoming of sampling  its  performance highly depends on the number and positions of those  sampling points and its accuracy cannot be guaranteed  In order to  conduct  exact CNN search  two query processing algorithms are  proposed in  23  24   using R trees as the underlying data  structure  The first algorithm is based on the concept of  timeparameterized  TP  queries  23   which treats a query line  segment as the moving trajectory of a query point  Thus  the  nearest object to the moving query point is valid only for a limited  duration  and a new TP query is issued to retrieve the next nearest  object once the valid time of current query expires  i e   when a  split point is reached  Although the TP approach avoids the  drawbacks of sampling  it needs to issue m TP queries with m the  number of answer objects  In order to improve the performance   the second algorithm  24  retrieves all the answer objects for the  whole query line segment in a single round  Recently  Zheng et al   32  study CNN search in wireless data broadcast systems  where  mobile clients answer their own CNN search via listening to the  wireless broadcast channel    All the above work on CNN queries use Euclidean distances to  measure the proximity  of objects  As for network distance  the  first algorithm to process CNN queries in a road network is  proposed in  7   which tries to find the locations on a path that an  NN search must be performed  However  it does not support  CkNN search  Motivated by this  Kolahdouzan and Shahabi  14   present two methods  namely  Intersection Examination  IE  and  Upper Bound Algorithm  UBA   Compared with IE  UBA gains  better performance by restricting the evaluation of kNN queries to  only the locations where they are required  An alternative  approach is proposed in  4   It retrieves the kNN object sets of all  network nodes in the query path  and associates them with the  data objects located along the path    As mentioned in Section 1  all the aforementioned algorithms  do not take into consideration the existence of obstacles and they  cannot be used to deal with CONN queries efficiently  The main  difference between CNN search and CONN search has been  summarized in  31     2 3 Queries with Obstacles   In an obstructed space  the distance between objects is affected  by the existence of physical obstacles  e g   buildings  rivers  etc     Zhang  et al   31  propose several algorithms for processing  common spatial queries such as range queries  NN search   edistance join queries  closest pair queries and distance semi join  queries  in the obstructed space  Xia  et al   29  present a more  detailed study of the  obstructed nearest neighbor  ONN  query  which finds the k       1  NNs of a given query point according to  the obstructed distance  However  to the best knowledge of the  authors  the CONN search has not been studied before    More recently  the impact of obstacles on the object visibility  has been studied  Although it does not employ the obstructed  distance to measure the closeness between objects  it does  consider the existence of obstacles and two objects are visible to  each other iff the straight line segment connecting them does not  pass through any obstacle  Nutanong et al   15  explore the visible  k nearest neighbor  VkNN  search  which returns the k NNs that  are  visible to a specified query point  Further studies along this  line include  visible reverse k nearest neighbor search  8  and  continuous VkNN search  9     In addition  the problem of spatial clustering in the presence of  obstacles has attracted considerable attention in recent years  It  divides a set of 2D data points into smaller homogeneous groups   i e   clusters  by taking into account the influence of obstacles   Handling these constraints can lead to effective and fruitful data  mining by capturing application semantics  26   A large number  of clustering algorithms with obstacle constraints have been  proposed in the literature  including COD CLARANS  25    AUTOCLUST   6   DBCLuC  30   DBRS   28   DBRS O  27    and DBSCAN MDO  17   etc   2 4 Main Memory Obstacle Path Queries   Main memory based shortest path problem in the presence of  obstacles has been well studied in computational geometry  2    and the most common approach is based on the visibility graph  VG  A  VG is constructed based on an obstacle set  O and the  source destination point ps  pe   Its nodes correspond to the vertices  of the obstacles or source destination point  Two nodes ni   nj  are  connected iff the straight line segment between them does not  intersect any obstacle interior    ps pe obstacle the shortest path n1 n2 n3 n4 o1 o2 n5 n6 n8 n7 Figure 2  Visibility graph and obstacle path   An example  VG is illustrated in Figure 2  where shaded  polygons represent obstacles  Nodes n2 and n7  are not connected  as the corresponding straight line segment  n2   n7   intersects with  the obstacle o2   There are multiple paths available from the source  point ps  to the destination point pe   such as the path via nodes n1   n6 and the path via nodes n1  n8  and n7  Among all the available  paths  the one with the shortest distance is returned  i e   the path  via nodes  n4   n3   n5 and  n6 for this example  Since the shortest  path contains only the edges of VG  as proved in  2    a popular  and practical obstacle path  i e   shortest path  computation  approach proceeds in two steps  The first step constructs  VG   which takes O  n 2  logn  based on rotational plane sweep  20   and  can be optimized to  O   m    n logn  with an optimal outputsensitive algorithm  10   Here   n is the number of nodes in VG and m is the number of edges in VG  The second step computes  the shortest path in  VG using Dijkstra   s algorithm  5   which  incurs O   m   n logn   Thus  the time and space complexities of  the approach are O   n 2  logn  and O   n 2    respectively  Obviously   the algorithm has a poor scalability and cannot guarantee the  efficiency when a large number of obstacles are considered    3  PRELIMINARIES   In this section  we formally define CONN search  introduce the  concept of control points  and present the quadratic based split  point computation algorithm that is crucial to CONN query  processing  Table 1 summarizes the notations used in the rest of  this paper    Table 1  Symbols and descriptions   Notation  Description   P  the set of data points p in a 2D space   O  the set of obstacles o in a 2D space   Tp  the R tree on P   To the R tree on O   q the query line segment with q    S  E    VG  the visibility graph   RL the result list of a CONN query   dist pi   pj    the Euclidean distance between pi  and pj    H head  the top entry of a heap H   e key  the search key value of a heap entry e DEFINITION 1  VISIBILITY  8    Given p   p        P and O  p and p    are visible to each other iff the straight line connecting them does  not cut through any obstacle  i e       o     O   p  p         o                  DEFINITION  2  VISIBLE  REGION    Given p       P   O  and q  the  visible region of p over q  denoted by VRp q  is the set of intervals  R     q  such that p is visible to all the points along R                       In a Euclidean space  any two points are visible to each other as  there are no obstacles  However  this statement does not hold in  an obstructed space  As shown in Figure 3  the visible region of p over q is  S  s1    and the rest  i e   the segment  s1   E   is blocked  by obstacles o1 and or o2  Point s2 is not located inside the visible  region of p  i e   s2     VRp q    and hence it is invisible to point p   The visible region formation algorithm has been studied in  8  9     S s2 s3 s4 E p a b c d o1 o2 obstacle control point s1 q Figure 3  Example of control point list   DEFINITION 3  OBSTACLE FREE PATH   Given O and two points  p   p         P  a path P p  p        d1   d2        dn   connecting p with p    sequentially passes n nodes   i e   the vertices of obstacles   denoted as di   Let d0   p  dn 1   p     and assume P p  p     reaches di before di 1   P p  p     is an obstacle free path  path for short  iff     i       0  n   di and di 1 are visible to each other  Its distance  P p  p           i    0  n  dist di   di 1                                                                          DEFINITION  4  OBSTRUCTED  DISTANCE   25     The  obstructed  distance between two points p   p         P  denoted by   p   p       is the  length of the shortest obstacle free path  shortest path for short  from p to p     denoted as SP p  p      i e       P p  p       P p  p           SP p  p       Here    p  p         SP p  p                                                                 Given a set of obstacles  there are usually multiple obstaclefree paths from a given point p to another point p     As an example   in Figure 3  the path  P p   E     c   b  passes  c and  b before  reaching E  and P p  E     d  provides an alternative obstaclefree path from p to E  Among all the obstacle free paths from p to  E  the one with the minimal distance  i e    P p  E     d   is the  shortest path SP p  E   The obstructed distance between p and E is  the length of the corresponding shortest path  i e     p  E      SP p   E     dist p  d    dist d  E     DEFINITION 5  OBSTRUCTED NEAREST NEIGHBOR   For a given  query point p  point p         P is the  obstructed nearest neighbor   ONN  of p iff     p           P      p        p     p         p        p                                DEFINITION  6  CONTINUOUS  OBSTRUCTED  NEAREST  NEIGHBOR  QUERY    Given P   O   and q  a  continuous obstructed nearest  neighbor  CONN  query returns the result list RL that contains a  set of    pi   Ri      i      1  t   tuples  such that  i     i    1  t  Ri   q   ii      i  j  i      j       1  t   Ri      Rj        and  iii         pi   Ri         RL   pi  is the  ONN of any point along interval Ri                                                  In this paper  we focus on the processing of CONN search  As  pointed out in Section 1  a naive approach is to perform ONN  retrieval  31  at every single point of a specified query line  segment  q  However  it is not feasible due to the unlimited  number of points along q  It is observed that nearby points along q normally share the same ONN  Take a result list RL       i    1  t     pi    Ri      for a CONN query as an example  The object pi  is the ONN  for every point along  Ri   Consequently  it is only necessary to  issue ONN search at those points where ONN objects change  In  view of this  the concept of split point is introduced  24   as  defined in Definition 7    DEFINITION  7  SPLIT  POINT FOR CONN    Given q    a   b    O  and p1   p2      P  let p1 be the ONN to all the points along  a  m  and p2 be the ONN for all the points along  m  b   point m is a  split point where the ONN corresponding to q changes            Based on the concept of split point  the CONN search can be  conducted as follows  Initially  the result list RL           q     When  the first data point p is evaluated  p for sure is the ONN for any  point along the query segment q  i e   RL       p  q      As more  and  more points are processed  split points are generated and q will be  decomposed into smaller segments with each having its own ONN   In other words  the evaluation of a new data point  p    on the  current result list RL is converted to check whether the existence  of  p    introduces any new split point on a region interval  Ri included in RL  However  due to the existence of obstacles  the  computation of split points for CONN query is not a trivial issue   and it is different from that for CNN search  24   In this paper  we  introduce a novel concept  namely  control point that is formally  defined in Definition 8  to facilitate the formation of split points    DEFINITION  8  CONTROL  POINT    Given p       P   O  and an  interval R  a point cp is the control point of p over R  denoted by  CPp R  iff   i  the shortest path from p to any point on R passes  through cp  and  ii  cp is visible to every point on R                        As shown in Figure 3  point a is the control point for point p over segment  s1  s2   meaning that for any point p          s1  s2   the  shortest path from p to p    must pass a  and the obstructed distance  between p and p     i e     p  p       equals   p  a     dist a  p      Based on  the concept of control point  each point p has its control point list  over q  denoted as CPLp q  see Definition 9   Correspondingly  the  result list RL has to be decomposed further into    pi   cpi   Ri      which  indicates that point pi  is the ONN to any point along Ri   and the  shortest paths must pass point cpi   We leave the detailed detection  algorithm for control points to Section 4  and focus this section on  how control points can help to find out split points and to provide  pruning opportunity    DEFINITION  9  CONTROL  POINT LIST    Given p      P and q  the  control point list of p over q  denoted by CPLp q  contains a set of     cpi   Ri      i      1  n   tuples  such that  i     i    1  n  Ri   q   ii      i  j  i      j       1  n   Ri     Rj        and  iii         cpi   Ri         CPLp q   cpi  is the  control point for p over interval Ri                                                    Given a segment q and two points p  p     suppose point v is the  control point of p over q  point u is the control point of p    over q   and   p  v      p     u   are known with   p  v         p     u     d  We further  assume that p is the ONN of q before p    is accessed  and now we  are going to evaluate p     The locations of u  v and the value of d have a direct impact on the number position of the split point s   that are introduced by p    on q  In the following  we first prove that  the maximal number of split points introduced by p    is two  then  explain how to determine the locations of split points  and finally  present several pruning strategies    Y X S E u v y s n m z a b c     u  v  ab b c     2 2  2 2 a c b a       q control point     X Y Y dist u  v  Y a Y  a d   dist u  v  a d dist u  v   a d   a d    a S E y z  a  Example                           b  Y x    dist u  s      dist v  s   Figure 4  Properties of split points  THEOREM 1  Given two points p   p     a line segment q    S  E    together with corresponding control points v u of p p    over q  let d      p  v         p     u    There are at most two points along q with same  obstructed distance to p and p                                                            PROOF  Consider the illustrative example of Figure 4 a   in  which points  m and  n are the projections of  u and  v on  q respectively  point  y is the intersection between  q and the  extended line of segment  u   v   and point z is the intersection  between the perpendicular bisector      u   v  of  u   v  and  q  We  further assume that point  n is the origin of the  XY coordinate  system as shown in Figure 4 a   Let dist n  m    a    0   dist v  n     b  dist u  m    c and assume c   b  As we want to find point s such that   p  v     dist v   s      p     u     dist u   s    we  need  to  find  points s that satisfy  dist u  s      dist v   s      p   v         p      u      d  Assume point sp has coordinate  x  0   we need to solve following  quadratic polynomial    2 2 2 2 d   dist u sp      dist v sp     a     x    c     x  b         1   Let A   4a 2     4d 2   B      4aT  and C   T 2     4b 2 d 2   with T   a 2     c 2     b 2     d 2   the roots of Equation  1  can be derived as follows   i   if A   0  then x      C B  otherwise  ii   2 x                  4    2   B B  AC  A Hence  there are at most two points such that   p  v     dist v  sp       p      u      dist u  sp   The proof can be easily adjusted for other  cases that include  i  a   0  segment  u  v  is vertical to q   ii  if b   c   u  v  is parallel to q  and  iii  b   c                                           The above Theorem proves that there are at most two points sp such that they have the same obstructed distances from p and p      We can also prove that as q is decomposed into smaller segments  R by points sp  all the points along R must share the same ONN   either  p or  p      i e   points  sp are split point s   In order to  facilitate understanding  we transfer Equation  1  to Equation  2    and assume point  s is located at  x  0   The positions of split  points are corresponding to the x values such that Y x    d  Figure  4 b  plots the distribution of Y x  under different x values    2 2 2 2 Y x    dist u s      dist v s     a     x    c     x   b    2   Based on the derivative and the limit of Equation  2  w r t  a  variable  x  as shown in Equation  3   we can conclude that  i   when x   ab  b   c   Y x  is monotone increasing and Y      a  dist u   v     ii  when x   ab  b   c   it is monotone decreasing and Y       a   dist u   v    and  iii  when x   ab  b   c   Y x  reaches its maximal value 2 dist u   v   The positions of split points can be determined  as follows  according to the value of d     p  v         p     u   and Y x     2 2 2 2 2 2  2  2 2 2 2 2 1 1           1 1             0     1 1     0 1 1          x a c b x a  x x a  x Y x  x  a a x  c  x  b x c b a x  x                                                                                                      3   lim     x Y x  a                 and  lim     x Y x  a             Case 1  d     dist u  v   As Y x      dist u  v   it is for sure that for  any point s along q  Y x    dist u  s      dist v  s      d     p  v         p      u    In other words  it indicates   p     u     dist u  s        p  v     dist v   s   and thus new point p    will replace p as the ONN for any point  along q without introducing any new split point    Case 2  a   d   dist u  v   As depicted in Figure 4 b   there will  be two values  x1  and  x2  such that  Y x1      Y x2      d  with  x1     ab  b   c    x2   Let  x1   0  be s1  and  x2   0  be s2   For a given point s with coordinate  x  0   when  i  x   x1  or x   x2   Y x    d which  means dist u  s      dist v  s      p  v         p     u    i e     p     u     dist u   s      p   v      dist v  s   and hence point  p    is the ONN for each  point along the segments  S  s1   and  s2   E   and  ii  x1     x     x2    Y x      d which means dist u  s      dist v  s        p  v         p     u    i e      p     u     dist u  s        p  v     dist v  s   and thus point p still is the  ONN for all the points along the segment  s1  s2   In this case  p    introduces two split points s1   s2     Case 3   a    d     a  As depicted in Figure 4 b   there will be  only one value x1 such that Y x1    d  Let  x1  0  be s1  For a given  point s with coordinate  x  0   when  i   x    x1   Y x     d which  indicates  dist u  s       dist v  s      p   v          p      u    i e     p      u      dist u   s      p  v     dist v  s   and hence point p is still the ONN  for every point along the segment  S  s1   and  ii  x     x1  Y x      d which means dist u  s      dist v  s        p  v         p     u    i e     p     u      dist u  s        p  v     dist v  s   and thus point p    is the ONN for all  the points along the segment  s1   E   In this case  p    introduces one  split point s1    Case 4  d      a  As Y x     a  it is for sure that dist u  s      dist v   s      d  In other words  it indicates   p     u     dist u  s        p   v      dist v  s   Consequently  point p is still the ONN to any point on q    In the above discussion  we define a quadratic polynomial  whose roots can be used to derive the positions and number of  split points  However  some special case of Case 1 Case 4 can be  detected by Lemma 1  without any expensive calculation of the  quadratic polynomial  Its pruning power will be detailed in  Section 4 where we present the CONN search algorithm    LEMMA  1   Given two points p   p     a line segment q     S   E    together with corresponding control points v and u  let dist    cp  q                                                                   2  Note that the distribution of Y x  under other cases  e g   a   0   b     c  has different trend  i e   different inflexion points and  maximal minimal values    be the vertical distance from a control point cp to a line segment  q  and assume dist    u  q    dist    v  q   Point p for sure is closer to  any point along q compared with p     if it satisfies   i    p      u     dist u  S      p  v     dist v  S   and  ii    p     u     dist u  E      p  v     dist v  E                                                                                         PROOF  Without loss of generality  we assume that there is at  least one point s along the segment q such that   p     s       p  s    As  points  v and  u  are the control points of  p and  p    over  q respectively    p     s       p     u     dist u  s  and   p  s       p  v     dist v   s     p     s       p  s   indicates dist u  s      dist v  s      p  v         p     u      d  On the other hand  based on condition  i  and condition  ii   we  have dist u  S      dist v  S    d and dist u  E      dist v  E    d  Let  Y t    dist u  t      dist v  t  with t      S  E   As t varies from S to E   the value of Y t  first drops and then increases  which contradicts  the distribution of Y t  shown in Figure 4 b   Consequently  our  assumption that   p     s       p  s   is invalid  and point p for sure is  nearer to any point along q than p                                Based on Lemma 1  we introduce a pruning distance  namely  RLMAX   MAXi    1  t     pi   Ri  l      pi   Ri  r    3   Given a current result list   if all the unexamined objects have their minimal distances to the  query line segment larger than  RLMAX  it is guaranteed that the  current result list will not be changed by any unexamined object   as proved by Lemma 2   In other words  Lemma 2 provides a  search termination condition which will be utilized in our CONN  search algorithm that is to be presented in the next section    LEMMA 2  Given a result list RL     i    1  t     pi   cpi   Ri      a point p  and a segment q     S   E   p for sure cannot change RL if  mindist p  q    RLMAX                                      PROOF  Without loss of generality  we assume that there is at  least one point s     Ri  along q such that   p  s       pi   s    As s is a  point on Ri     p  s       dist p  s      mindist p  q   On the other hand     pi   s       pi   cpi      dist cpi   s   Since cpi  is the control point of pi over  Ri     q  it is  visible to any point along  Ri   Consequently   dist cpi   s      MAX dist cpi   Ri  l   dist cpi   Ri  r    i e     pi   s         pi    cpi       MAX dist cpi    Ri  l    dist cpi    Ri  r      MAX   pi    Ri  l      pi    Ri  r         RLMAX    mindist p   q   Hence    p  s   that is larger than  mindist p   q  for sure is larger than   pi    s    The assumption is  invalid and the proof completes                               4  CONN QUERY PROCESSING   In this section  we present the detailed CONN query processing  algorithm  The basic idea is to traverse the data set P in ascending  order of their Euclidean distances  mindist that is the lower bound  of the obstructed distance  to the query line segment q  assuming  that P and O are indexed by two separate R trees  For each data  point p     P visited  we first find out all the obstacles that might  affect the obstructed distances from p to any point along q  then  identify the control points of  p over  q  and finally evaluate the  impact of p on the current result list RL which is initialized to                q     In what follows  we elaborate these three steps  then  propose the complete CONN search algorithm  and finally discuss  the flexibility extension of the search algorithm  To simplify the                                                                   3  If        pi   cpi   Ri         RL with pi           pi   Ri  l       pi   Ri  r          and  MAX a   b  is a function to return  i   a if  a      b and  ii   b otherwise   discussion  we use line segments  but not rectangles  to represent  obstacles in our running examples  while the ideas can be easily  extended to rectangles that are sets of line segments    4 1 Obstacle Retrieval   As mentioned in Section 1  the existing  VG based approach  needs to maintain the visibility graph and its high space and time  complexities deteriorate its practicability  Actually  for a given  point p and a given query line segment  q    S  E   only a small  number of obstacles will affect the obstructed distances from p to  any point along  q  As demonstrated in Theorem 2  once the  shortest path from p to S and that from p to E are identified  the  search range for all the obstacles that may affect the obstructed  distance between  p and any point along  q  denoted by  SRp q  is  confirmed  and thus the obstacle retrieval can be safely terminated  after all the obstacles inside SRp q are retrieved    S E p a b c d f g s x o1 obstalce q shortest path from p to S shortest path from p to E SRp q Figure 5  The obstacle search range   THEOREM 2  Given a data point p  a query segment q    S  E    let SRp q be the range bounded by SP p  S   SP p  E   and q      s     q  SP p  s  only passes vertices of obstacles o     SRp q                      PROOF  We assume that there is a point  s     q such that its  shortest path  SP p  s  passes a vertex of at least one obstacle  o outside SRp q  As o     SRp q and s    SRp q  SP p  s  must intersect  the boundary of SRp q  and let point x be an intersection  Without  loss of generality  we assume  x is located at  SP p   S    Consequently  we have two paths from  p to  x  i e    P1 p   x   following  SP p   s  and  P2  p   x  following  SP p   S  but both  stopping at x instead of s S  Take Figure 5 as an example  SP p  s      f  g  and SP p  S     a  b   Correspondingly  P1 p  x     f  g   and P2 p  x     a  b   If  P1 p  x      P2 p  x     P1 p  x       x  S       P2 p  x       x  S       p  S   which contradicts the fact that   p  S   is  the minimal distance between  p and  S  Otherwise   P1 p   x         P2 p  x     P1 p  x       x  s        P2 p  x       x  s    and hence the path  from p to s passing o1 is not the shortest path  which contradicts  our assumption  Therefore  our assumption is invalid and the  proof completes                                                                                In order to utilize Theorem 2 to bound the search range for all  the obstacles affecting the obstructed distances from  p to any  point along  q    S   E   both  SP p   S  and  SP p   E  have to be  identified  Thus  Lemma 3 is developed    LEMMA 3  Given a point p  a point s along q  and a path P p  s  from p to s  suppose all the obstacles that have their minimal  Euclidean distances to q bounded by  P p  s   have been retrieved  and maintained in a set So  i e   So    o     O   mindist o  q       P p   s     Let P2 p  s  be the shortest path from p to s obtained based  on So  If  P2 p  s        P p  s    it is confirmed that P2 p  s  must be  the real shortest path from p to s  i e   P2 p  s    SP p  s             PROOF  If P2 p  s  is not the real shortest path from p to s  there  must be another one P3  p  s    SP p  s  with  P3  p  s       P2  p  s     As P2 p  s  is the shortest one among all the paths from p to s such  that they only pass the vertices of obstacles inside  So   P3 p  s   must pass at least one vertex  denoted as v  of some obstacle that  is not included in So  i e   located outside the circle cir s   P p  s     centered at s with  P p  s   as radius  We further decompose P3 p   s  into two paths via node v  P3  p  v  and P3  v  s   As  P3  p  s       P3 p  v      P3 v  s     P3 p  s      P3 v  s       dist v  s     P p  s    On  the other hand   P2 p  s        P p  s   holds  Hence   P3 p  s      P2 p   s   contradicts our assumption  and the proof completes                   Based on Theorem 2 and Lemma 3  our Incremental Obstacle  Retrieval Algorithm  IOR  is developed  with its pseudo code  depicted in Algorithm 1  The basic idea is to retrieve the obstacles  according to ascending order of their minimal distances to q  and  add them into the local visibility graph VG which initially only  includes the point  p currently processed and two endpoints of  q  i e   S and E   Based on local VG  a local shortest path from p to  endpoint  S E can be identified by Dijkstra   s algorithm  5    denoted as P1  p  S  and P2  p  E   Line 2   It then fetches all the  obstacles having their smallest distances to  q bounded by  P1  p   S   or  P2 p  E    and inserts them into local VG  Lines 6 12   Since  VG is changed  both P1 p   S  and P2 p  E  need to be validated   which may trigger the retrieval  of more obstacles  The process  proceeds until the new  P1 p   S  and  P2 p   E  do not trigger the  retrieval of any new obstacle  As stated in Lemma 3  P1  p  S  and  P2  p  E  must represent the real shortest path from p to S E  i e    P1 p   S     SP p   S  and  P2 p   E     SP p  E   Consequently  the  range bounded by  P1 p   S    P2 p   E  and  q corresponds to the  range  SRp q defined in Theorem 2  In other words  the fact that  IOR retrieves all the obstacles with their minimal distances to q not exceeding MAX  P1 p  S     P2 p  E    means that all the obstacles  located inside range SRp q  have been retrieved  as demonstrated in  Lemma 4  Therefore  the correctness of IOR is guaranteed    Algorithm 1 Incremental Obstacle Retrieval Algorithm  IOR    Input   obstacle R tree To  min heap Ho  query line segment q    S  E                  data point p  visibility graph VG  previous search distance d     1   while  1  do     2       P1 p  S    Dijkstra VG  p  S  and P2 p  E    Dijkstra VG  p  E      3       d      MAX  P1 p  S     P2 p  E        4       if  d      d  then   5           d   d         for the next loop     6           while Ho         and Ho head key     d do     7               de heap the top entry  e  key  of Ho     8               if e is an obstacle then     9                   add e to set So and their vertices to VG   10               else      e is a non leaf node   11                   for each child entry ei     e do   12                       insert  ei   mindist ei   q   into Ho 13       else break   LEMMA  4   Given a query line segment q     S   E   let d    MAX  SP p  S     SP p  E     All the obstacles that are inside range  SRp q  must have their minimal distances to q bounded by d              PROOF  Suppose there is an obstacle o that is inside the range SRp q with  mindist o   q     d  Let segment l    o  s  refer to the  shortest path from o to q in an Euclidean space which intersects q  at point s  i e   mindist o  q    dist o  s   Without loss of generality   we extend the segment l to l    and assume l    intersects SP p  S  or SP p  E  at point p     i e   l       p     s   Since point p    lies along SP p   S  or SP p  E     p     s       MAX  SP p  S     SP p  E       d  On the  other hand  dist o  s      dist p     s        p     s       d  Consequently  our  assumption that  mindist o   q     dist o  s     d is not valid  The  proof completes                                                                                In addition  we would like to highlight that the local visibility  graph VG formed by a point p can be reused by a point p    that is  accessed evaluated after  p  If  p    does not trigger the retrieval of  any new obstacle  i e   current VG has already covered the search  range SRp    q   IOR for point p    can be safely terminated by reusing  the current VG  Otherwise  it expands the local VG by adding new  obstacles until the search range  SRp    q has been fully covered   Therefore  the IOR for all the points in P will access the obstacle  set O at most once    4 2 Control Point List Computation   Once the local VG contains all the obstacles that may affect the  obstructed distances from a specified data point p to q  our next  step is to find out the control point list of p over q  i e   CPLp q  A  straightforward approach is to utilize the fact that a control point  over  R must be visible to  R and invoke Dijkstra   s algorithm to  form the shortest path from p to every node n that is within the  SRp q 4   For each n     SRp q  we get the visible region VRn q  and add  a new tuple     n   Rn    VRn q    to  CPLp q  assuming that  n is the  control point of p over VRn q  If Rn overlaps Rm that is associated  with some other control point m included in current CPLp q  i e          m   Rm        CPLp q with  Rn     Rm           an update is performed   Obviously  this method is expensive  especially when the number  of nodes inside  SRp q is large  In order to handle this issue and  improve the performance  we propose several Lemmas that can  simplify the evaluation cost of some nodes n     SRp q    LEMMA  5   Given a point p  a line segment q     S   E   and a  node v in VG  we assume the shortest path SP p  v  visits node u  right before it reaches v  Let VRu q and VRv q be the visible regions  of u and v over q respectively  v cannot be the control point of p  over any interval R      VRu q     VRv q                                                PROOF  As shown in Figure 6 a   suppose v is the control point  of p over at least one point x       VRu q     VRv q  and let P1 p  x  be  the shortest path from p to x via v  i e    P1 p  x       p  v     dist v   x      p   u      dist u   v     dist v   x      p   u      dist u   x    Consequently  P1  p  x  is not the shortest path from p to x  which  contradicts our assumption  The proof completes                             S E x s1 u v p q o1 o2 obstacle    S E s1 s2 q s3 x s4 o1 o2 obstacle y v u p R l R r o  a  v is not control point of  S  s1         b  v is not control point of  s3  s4    Figure 6  Optimizations for control point list computation                                                                    4  Note that the current local VG covers an area larger than SRp q   but only those nodes inside SRp q may have an impact on CPLp q    As Dijkstra   s algorithm gradually expands the search space  from  q  i e   it always reaches  u before  v if   p   u       p   v      Lemma 5 matches its traversal perfectly  Whenever a node  v is  examined  it must be reached by the shortest path from  p  and  hence the node u visited right before v along the path is known   As illustrated in Figure 6 a   the shortest path from p to v passes u first and then reaches v  Instead of considering the visible region  of v  that is entire q   we only need to consider the region that is  not enclosed by VRu q   i e    s1   E    However  not all the intervals  included in  VRv q     VRu q   need evaluation  Lemma 6 can further  shrink the search interval    LEMMA 6  Given a point p  a line segment q  and a node v in a  visibility graph VG  we assume the shortest path SP p   v   visits  node u right before it reaches v  Given an interval R    R l  R r       VRv q     VRu q  such that only endpoints R l and R r  but not any  point on R  are visible to u  if v is located outside the triangle  formed by u  R l  and R r  v cannot become the control point of p  over R                                                                                               PROOF  Take R    s3  s4       VRv q     VRu q  shown in Figure 6 b   as an example  Although v is outside    us3s4  we assume v is the  control point of p on at least one point x on R  Let P1 p  x  be the  shortest path from p to x via v  As v is outside the triangle    us3 s4    without loss of generality  we assume P1  p  x  intersects the line  segment q1    u  s3  at point  y   P1 p   x       p   u      dist u   v     dist v  x   Let P2 p  x  be the shortest path from p to x via u and  then via o  Here  o is the vertex of the obstacle o2 that blocks u from  R  Obviously   P2 p   x       p   u      dist u   o     dist o   x    Since in triangle     oxy  dist o   y     dist y   x     dist o   x    Consequently   P2  p  x       p  u     dist u  o    dist o  y    dist y  x       p  u     dist u  y    dist y  x      p  u     dist u  v    dist v  y      dist y  x      p  u     dist u  v    dist v  x     P1 p  x    Therefore   P1 p  x  cannot be the shortest path  i e   the shortest path from p to  x does not pass  v   and our assumption is invalid  The proof  completes                                                                                          Take the case depicted in Figure 6 b  as an example  For all the  intervals included in  VRv q     VRu q    s1  s2        s3  s4   we can  confirm that  v cannot be the control point of  p over  s3  s4  by  Lemma 6  The above two lemmas are developed to reduce the  examination cost of each traversed node of local visibility graph  VG  However  if the number of nodes included in local VG is very  big  the examination cost is still high  Actually  not all the nodes  can change the current control point list  In order to terminate the  traversal early  Lemma 7 is developed    LEMMA  7   Given current control point list of p over q  i e   CPLp q       cpi   Ri      with i      1   m   let CPLMAX   MAXi    1  m     p  cpi      dist cpi    Ri  l     p  cpi      dist cpi    Ri  r    5   A node v for sure  cannot be included in CPLp q  if   p  v     mindist v  q      CPLMAX      PROOF  If  v is included in CPLp q   there must be at least one  point s     q  such that the shortest path SP p  s  from p to s passes  through v and s is visible to v  We denote this path as P1 p  s  with   P1  p  s       p  v     dist v  s        p  v     mindist v  q      CPLMAX  On  the other hand  let    cpi   Ri         CPLp q  be a tuple in CPLp q   such  that s     Ri   and P2 p  s  be the path from p to s via current control  point  cpi    P2 p   s       p   cpi       dist cpi    s   As  dist cpi    s                                                                        5  If        cpi   Ri         CPLp q with cpi         CPLMAX         MAX dist cpi   Ri  l   dist cpi   Ri  r     P2 p  s       CPLMAX      P1 p  s     and thus  P1 p   s  cannot be the shortest path from  p to s  The  proof completes                                                                                Lemma 7 serves as the termination condition of Control Point  List Computation Algorithm  CPLC  that is shown in Algorithm 2   CPLC shares the basic idea as the approach we mentioned earlier   That is to call Dijkstra   s algorithm to gradually traverse the local  visibility graph VG and to access nodes v according to ascending  order of their obstructed distances to p  The p   s control point list  CPLp q over q is updated during the traversal  However  different  from the straightforward method  it employs Lemma 5 and  Lemma 6 to significantly reduce the node examination cost  The  Split function invoked  Line 14  is the same as the split point  computation algorithm presented in Section 3  Before  v is  considered  all the shortest paths from  p to any point along  Rint pass the control point cpi   and now we want to check whether the  path from p to any point along Rint  via v is even shorter    Algorithm 2 Control Point List Computation Algorithm  CPLC    Input     query line segment q    S  E   data point p  visibility graph VG   Output   p   s control point list CPLp q over q     1   CPLp q             S  E          2   while there exists a node in VG that has not been visited do     3       let v     VG be the one with the smallest obstructed distance to p              among those nodes not yet visited     4       if   p  v       CPLMAX then      Lemma 7     5           break     6       let u be the node that SP p  v  passes right before reaching v     7       R   VRv     VRu      Lemma 5     8       refine R based on Lemma 6     9       for each tuple    cpi   Ri     in CPLp q do      update CPLp q   10           Rint    R     Ri    11           if Rint         and cpi        then   12               replace    cpi   Ri     with    v  Rint     and    cpi   Ri     Rint       13           else if Rint         and cpi         then   14               d     p  cpi          p  v   and Split p  cpi   p  v  Rint   d    15   return CPLp q   There are four cases  as discussed in Section 3  with d     p  cpi           p  v   and a as the difference between v   s projection on q and  cpi    s projection on  q   i  Case 1   d     dist cpi    v       cpi    Ri     is  replaced with    v  Rint       Rint    R     Ri   and    cpi   Ri     Rint       ii  Case  2  a   d   dist cpi   v   interval Rint  will be decomposed into three  segments by points  x1  and  x2   with  x1  and  x2  derived based on  Equation  1   Thereafter      cpi    Ri     is replaced accordingly   iii   Case 3   a   d     a  Rint  will be decomposed into two segments by  point x1 with x1 derived based on Equation  1  too  Again     cpi   Ri     is replaced accordingly   iv  Case 4  d      a     cpi   Ri     remains  The  process proceeds until all the nodes in local VG are traversed or  the visited node satisfies   p  v       CPLMAX  As nodes in local VG are traversed based on ascending order of their obstructed  distances to  p  when currently visited node has its obstructed  distance to p larger than CPLMAX  all the remaining nodes n in VG must satisfy   p  n       CPLMAX  Note that the termination condition  relaxes Lemma 7 using zero as the lower bound of mindist n  q     EXAMPLE 1  We illustrate the CPLC algorithm with the example  shown in Figure 7  where the local VG    S   E   p   u   v  w  z   First  CPLC accesses node p     VG  and updates CPLp q       p   S   s1                s1    s3          p   s3   s4                s4    E       Second  it accesses  node v     VG and obtains its visible region VRv q    S  E   Based  on Lemma 5  it gets R    s1   s3        s4  E  and refines R to  s1   s3   according to Lemma 6  As in current CPLp q   the control point  associated with  s1  s3  is      CPLC updates CPLp q       p   S  s1          v   s1   s3         p   s3  s4               s4   E       Next  it accesses node u     VG and obtains its visible region VRu q    S  E   Similarly  it gets  R    s1    s3         s4    E  based on Lemma 5 and refines R to  s1    s3   by Lemma 6  As v is the current control point for the interval  s1   s3    the Split function is called  Since   p  v         p  u     d       a  a   R is decomposed into two segments   s1    s2   and   s2    s3    Correspondingly  CPLp q  is updated to     p   S  s1          u   s1   s2          v    s2   s3         p   s3  s4               s4   E       Nodes w and z are evaluated  similarly  and finally CPLC outputs     p   S  s1         u   s1  s2         v    s2  s3         p   s3  s4         w   s4  E      as the final CPLp q                        S E s1 s2 s3 s4 S E s1 s2 s3 s4 p o1 q    p   S  s1     o2 u v w z obstalce CPL    u   s1  s2     p q    v   s2  s3        p   s3  s4        w   s4  E     control point Figure 7  Example of CPLC algorithm   Algorithm 3 Result List Update Algorithm  RLU    Input      data point p  p   s control point list CPLp q  current result list RL   Output   the updated result list     1   for each tuple    pi   cpi   Ri         RL do     2       for each tuple    cpi      Ri            CPLp q do     3           if Ri     Ri             then     4               Rint    Ri     Ri        l  r   Rdif    Ri     Rint   Rdif       Ri         Rint      5               if  Rdif          then add    pi   cpi   Rdif     to RL    6               if  Rdif              then add    cpi      Rdif        to CPLp q     7               if   pi   l         p  l   and   pi   r         p  r   and mindist pi   Rint                            mindist p  Rint   then      Lemma 1    8                   add    pi   cpi   Rint     to NRL 6      9                   continue   10               else   11                   d     pi   cpi          p  cpi       and Split pi   cpi   p  cpi      Rint   d    12                   insert result tuples into NRL   13   return NRL   4 3 Result List Update   Once a new data point p is accessed  and its control point list  over query segment q is formed  the next step is to evaluate the  impact of p on the current result list  The basic idea is to evaluate  whether p will replace the current ONN of some points s along q   The  Result List Update Algorithm  RLU  is presented in  Algorithm 3  which utilizes the  Split  function  RLU scans the  current result list RL  For each tuple    pi   cpi   Ri         RL  it finds the  corresponding tuple    cpi      Ri            CPLp q  with Ri     Ri        l   r                                                                             6  When a new tuple     cpi    Ri     is inserted into  NRL  it might be  merged with existing tuple    cpj   Rj     if cpi     cpj  and intervals Ri    Rj  are adjacent   By solving the Equation  1   i e   dist cpi   x      dist cpi      x      pi    cpi           p   cpi       with  x       l   r    it can update the result list  accordingly  As the details of split point calculation algorithm   i e   Split  have been already presented in Section 3  we use a  running example to illustrate the RLU algorithm    q S E s1 o1 o3 o2 a b c s2 s3 s4 s5 obstalce v1 v2 v5 v6 v3 v4 Figure 8  Example of RLU algorithm   EXAMPLE 2  As shown in Figure 8  P    a  b  c   O    o1  o2   o3   and q    S  E   Suppose that point a has been processed  with  current RL       a  a   S  s3         a  v1   s3  s5         a  v2   s5  E      and  data point b is going to be evaluated with its control point list  CPLb q       b   S  s2         v5   s2  s4         v6   s4  E       Now we invoke  RLU to evaluate the impact of b on RL  First  the tuple    a  a   S   s3         RL is compared with    b   S  s2         CPLb q  Based on the  Split function   Rint     S  s3       S  s2     S  s2   is partitioned into  two segments  i e   Case 3   and NRL       a  a   S  s1         b  b   s1   s2       Next  the tuple    a  a   s2  s3         RL is compared with    v5   s2   s4          CPLb q   Again  based on the Split function  the entire Rint     s2  s3       s2  s4     s2  s3   is closer to b than to a  and thus NRL       a  a   S  s1          b  b   s1   s2          b  v5    s2   s3        Then  the tuple    a   v1    s3   s5          RL is compared with    v5    s3   s4         CPLb q   Similarly  Split function detects that Rint     s3   s5        s3   s4      s3   s4     is  closer to b than to a  and hence NRL is updated to     a  a   S  s1          b  b   s1   s2          b  v5    s2   s4        The process proceeds until all the  tuples in RL are evaluated  and the final NRL       a  a   S  s1         b   b   s1  s2         b  v5   s2  s4         b  v6   s4  E                            Algorithm 4 CONN Search Algorithm  CONN    Input      data R tree Tp  obstacle R tree To  query line segment q    S  E    Output   result list RL of the CONN query     1   RL                  S  E       RLMAX        VG    S  E   d   0     2   a min heap Hp    root Tp   0   a min heap Ho    root To   0      3   while Hp         and Hp head key   RLMAX do      Lemma 2     4       de heap the top entry  e  key  of Hp     5       if e is a data point then     6           insert e into local visibility graph VG     7           IOR  To  Ho  q  e  VG  d       see Algorithm 1     8           CPLe q   CPLC  q  e  VG       see Algorithm 2     9           remove e from VG   10           RL   RLU  e  CPLe q  RL       see Algorithm 3   11       else      e is an intermediate  i e   non leaf  node   12           for each child entry ei     e do   13               insert  ei   mindist ei   q   into Hp   14   return RL   4 4 CONN Search Algorithm   Our CONN Search Algorithm  CONN  traverses the data set P in ascending order of their minimal Euclidean distances to q  i e    mindist   For each accessed data point p     P  it invokes IOR to  retrieve all the obstacles that might affect the obstructed distances  from p to any point along q  calls CPLC to get the control point  list CPLp q  of p over q  and updates the result list via RLU  The  process proceeds until all the data points in P are evaluated or the  termination condition is satisfied  that is  mindist p   q     RLMAX  Lemma 2   Algorithm 4 presents the pseudo code of CONN    It is observed that the CONN algorithm traverses the data Rtree Tp and the obstacle R tree To at most once  Let  Tp  and  To  be  the tree size of  Tp  and  To  respectively   VG  be the number of  nodes in VG   R  be the maximal number of tuples included in RL   CPLp q        p     P   and VRn q        n     VG   and N be the number of  data points accessed during the CONN search  The time  complexity and the correctness of CONN algorithm are analyzed  as follows    THEOREM 3  The time complexity of the CONN algorithm is O   N    log  Tp     log  To      VG     log  VG                                           PROOF  For every data point p     P visited during the search  the  CONN algorithm takes O   VG     log  VG   to insert  p into VG   takes O  log  To       VG     log  VG   for IOR  takes O   VG     log   VG       R  2   for CPLC  takes O   R  2   for RLU  and incurs O   VG      log  VG   to delete  p from  VG  Consequently  the time  complexity of the CONN algorithm is O   N    log  Tp        VG     log  VG    log  To       VG     log  VG     VG     log  VG      R  2     R  2    VG     log  VG        O   N    log  Tp     log  To       VG     log  VG    as  VG      To    as demonstrated by our experimental results to be  presented in Section 5  and  R      Tp                                               THEOREM 4  The CONN algorithm retrieves exactly the ONN of  every point along a given query line segment  i e   the algorithm  has no false misses and no false hits                                                4 5 Discussion   Our previously proposed CONN algorithm assumes the data set  P and the obstacle set  O are indexed by two  different R trees   However  it can be naturally extended to perform CONN search  when  P and  O are indexed by a  single R tree  The detailed  extensions are listed as follows   i  It requires only one heap H to  store the candidate entries  including data points  obstacles  and  nodes   sorted in ascending order  of their minimum distances to  the specified query line segment q   ii  When processing the top  entry e de heaped from H  it distinguishes three cases   1  e is an  obstacle  It inserts e into VG   2  e is a data point  It calls the IOR  to retrieve all the obstacles that may affect the obstructed distance  from e to any point on q  gets e   s control point list CPLe q over q via CPLC  and finally updates the result list  RL via RLU  It is  worth noting that during the obstacle retrieval via IOR  it is  possible to access some data points which will be en heaped into  H for later processing   3   e is an intermediate node  its child  entries are en heaped to H for later evaluation    In addition  our proposed CONN query processing approaches  can be easily extended to tackle continuous obstructed k nearest  neighbor  COkNN  search  which finds the  k       1  obstructed  nearest neighbors  ONNs  to every point along a specified query  line segment  Due to the space limitation  we only list the major  differences  First  the output of a COkNN query contains a set of     ONNSi    Ri     tuples  where  ONNSi  is the set of ONNs for each  point on the interval Ri      Ri  l  Ri  r       q  Second  the pruning distance RLMAX used in Lemma 2 has to be updated to MAXi    1  t   maxodist ONNSi    Ri  l    maxodist ONNSi    Ri  r    in which  maxodist ONNS   s  represents the maximal obstructed distance  from any point in set ONNS to point s  i e   MAXp   ONNS   p  s       5  EXPERIMENTAL EVALUATION   This section evaluates the performance of the proposed  algorithms  We first describe  experimental settings  and then  present experimental results and our findings  All the algorithms  were implemented in C   and the experiments were conducted on  an Intel Core 2 Duo 2 33 GHz PC with 3 25GB RAM    5 1 Experimental Setup   Our experiments are based on both real and synthetic datasets   with the search space fixed at a  0  10000      0  10000  square  shaped range  Two real datasets are deployed  namely CA and LA 7   CA contains 2D points  representing 60 344 locations in  California  and LA includes 2D rectangles  representing 131 461  MBRs of streets in Los Angeles  All datasets are normalized in  order to fit the search range  Synthetic datasets are generated  based on uniform distribution and zipf distribution  with the  cardinality varying from 0 1   LA  to 10   LA   The coordinate of  each point in Uniform datasets is generated uniformly along each  dimension  and that of each point in  Zipf datasets is generated  according to zipf distribution with skew coefficient      0 8  We  assume a point   s coordinates on  both dimensions are mutually  independent  As COkNN  k     1  search involves a data set P and  an obstacle set O  we deploy three different combinations of the  datasets  namely CL  UL  and ZL  representing  P  O     CA  LA     Uniform   LA   and  Zipf   LA   respectively  Note that the data  points in P are allowed to lie on the boundaries of the obstacles  but not in their interiors    All data and obstacle sets are indexed by an R  tree  1   with  the page size fixed at 4KB  The performance metrics in our  performance study include I O cost  i e   number of pages  accessed   CPU time  query cost  i e   the sum of the I O time and  CPU time  where the I O time is computed by charging 10ms for  each page fault   visibility graph size  SVG   i e   number of  vertices in visibility graph   number of points evaluated  NPE   during search  and number of obstacles evaluated  NOE  during  search  Unless specifically stated  the size of LRU buffer is 0 in  the experiments  i e   the I O cost is determined by the number of  nodes accessed  We investigate the efficiency and effectiveness of  our proposed algorithms under various parameters  which are  summarized in Table 2  The numbers in bold represent default  settings  In each experiment  we evaluate the impact of one  parameter while others are fixed at their default values  and run  100 COkNN queries with their average performance reported  The  starting point and the orientation  in  0  2     of the query line  segment are randomly generated  while its length is controlled by  the parameter ql    Table 2  Parameter ranges and default values   Parameter  Range   query length ql    of data space side   1 5  3  4 5  6  7 5   k 1  3  5  7  9    P   O    0 1  0 2  0 5  1  2  5  10   buffer size bs    of the tree size    0  1  2  4  8  16  32                                                                    7 CA and LA are available in the site  http   www rtreepo rtal org    5 2 Performance Study   The first set of experiments studies the effect of query length ql    of data space side   Figure 9 shows the efficiency of the  COkNN algorithm as a function of  ql  by fixing  k   5  It is  observed that the total query time  breaking into I O and CPU  cost   NPE  and NOE grow with ql  The reason behind is that  as  the query segment becomes longer  the number of candidate data  points processed  the number of obstacles encountered  and the  number of split points generated increase  which results in more  distance computation  more control point list computation  and  more result list update  Figure 9 b  illustrates the size of visibility  graph  i e    SVG   with respect to  ql  As all the obstacles are in  rectangular shapes  there are 4       O    525 844 vertexes in a  global visibility graph when O   LA  denoted as FULL in Figure  9 b   Notice that although  SVG  ascends with the growth of ql  its  size is much smaller than the size of FULL  as also demonstrated  in the subsequent experimental results  This further demonstrates  the effectiveness of our proposed incremental obstacle retrieval   IOR  algorithm in reducing the number of obstacle traversals    1e 1 1e 0 1e 1 1e 2 1e 3 1e 4 1 5 3 4 5 6 7 5 query length    of data space side  total time  sec  1e 0 1e 1 1e 2 1e 3 1e 4 number of items evaluated I O  CPU NPE NOE 1e 0 1e 1 1e 2 1e 3 1e 4 1e 5 1e 6 1 5 3 4 5 6 7 5 query length    of data space side  visibility graph size  SVG  Full 137 693 1651 3655 6368 actual number of vertices  in visibility graph  a  CL  k   5                                         b  CL  k   5    Figure 9  Performance vs  ql    of data space side    Figure 10 depicts the performance of the COkNN algorithm  with respect to  k  with  ql fixed at 4 5   As expected  all costs  involving total query time  NPE  NOE  and  SVG  increase with k   This is because a larger value of  k implies a larger search range   for both data points and obstacles  and hence more distance  computations are incurred  Furthermore  as  k grows  the number  of answer points in the final result list increases  which results in  more frequent update operations and thus more expensive  maintenance cost of the result list    1e 1 1e 0 1e 1 1e 2 1e 3 1 3  5 7 9 k total time  sec  0 100 200 300 400 500 number of items evaluated I O  CPU NPE NOE 1e 0 1e 1 1e 2 1e 3 1e 4 1e 5 1e 6 1  3 5 7 9 k visibility graph size  SVG  Full 1545 1615 1651 1701 1740  a  CL  ql   4 5                                  b  CL  ql   4 5     Figure 10  Performance vs  k   Figure 11 plots the efficiency of the COkNN algorithm as a  function of the ratio of the cardinality of the data set P to that of  the obstacle set O  i e    P   O   with k   5 and ql   4 5   A crucial  observation is that the cost of the COkNN algorithm first drops  and then increases as  P   O  varies </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search"/>
        <doc>Finding the Most Accessible Locations   Reverse Path Nearest Neighbor Query in Road Networks ### Shuo Shang     Bo Yuan    Ke Deng     Kexin Xie     Xiaofang Zhou         School of Information Technology and Electrical Engineering  The University of Queensland    Division of Informatics  Graduate School at Shenzhen  Tsinghua University     fshangs  dengke  kexin  zxfg itee uq edu au    yuanb sz tsinghua edu cn ABSTRACT In this paper  we propose and investigate a novel spatial query called Reverse Path Nearest Neighbor  R PNN  search to    nd the most accessible locations in road networks  Given a trajectory data set and a list of location candidates speci   ed by users  if a location o is the Path Nearest Neighbor  PNN  of k trajectories  the in   uence factor of o is de   ned as k and the R PNN query returns the location with the highest in   uence factor  The R PNN query is an extension of the conventional Reverse Nearest Neighbor  RNN  search  It can be found in many important applications such as urban planning  facility allocation  tra   c monitoring  etc  To answer the R PNN query e   ciently  an e   ective trajectory data pre processing technique is conducted in the    rst place  We cluster the trajectories into several groups according to their distribution  Based on the grouped trajectory data  a two phase solution is applied  First  we specify a tight search range over the trajectory and location data sets  The e     ciency study reveals that our approach de   nes the minimum search area  Second  a series of optimization techniques are adopted to search the exact PNN for trajectories in the candidate set  By combining the PNN query results  we can retrieve the most accessible locations  The complexity analysis shows that our solution is optimal in terms of time cost  The performance of the proposed R PNN query processing is veri   ed by extensive experiments based on real and synthetic trajectory data in road networks  Categories and Subject Descriptors H 2 8  Database Applications   Spatial databases and GIS General Terms Algorithms  Performance Keywords Reverse Path Nearest Neighbor Search  Trajectories  Locations  Road Networks  E   ciency Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provid  ed that copies are not made or distributed for pro t or commercial advantage  and that copies bear this notice and the full citation on the  rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci c permis  sion and or a fee  ACM SIGSPATIAL GIS  11  November 1 4  2011  Chicago  IL  USA Copyright    c 2011 ACM ISBN 978 1  4503 1031 4 11 11    10 00 o1 o2 o3 T1 T2 T3 T4 T5 Figure 1  An example of R PNN query ### 1  INTRODUCTION The continuous proliferation of mobile devices and the rapid development of Global Positioning Systems  GPS  enable people to log their current geographic locations and share their trajectories to the web sites such as Bikely 1   GPSWaypoints 2   Share My Routes 3   Microsoft GeoLife 4   Foursquare 5   etc  The availability of the massive trajectory data creates various novel applications  An emerging one is the Reverse Path Nearest Neighbor  R PNN  query  which is designed to retrieve the most accessible locations among a set of location candidates  Given a trajectory set T  the query input is a set of location candidates O  If a location o     O is the Path Nearest Neighbor  PNN   i e   the closest data point to a speci   ed path  according to a distance metric   14  24  5  13  of k trajectories  we de   ne the in   uence factor of o as k  o if   k   The R PNN query asks for the location with the highest in   uence factor among all location candidates  i e   the location with the highest accessibility to the trajectories   A similar work is studied in  20  to    nd the most in   uential sites by considering the relationship in Euclidean space between data points  i e   points to points in Euclidean space vs  trajectories to points in road networks in our work   An example of R PNN query is demonstrated in Figure 1   1   2        5 are trajectories and o1  o2  o3 are query locations given by users  According to the de   nition of Path Nearest Neighbor  it is easy to    nd that o1 is the PNN of trajectories 1 http   www bikely com  2 http   www gps waypoints net  3 http   www sharemyroutes com  4 http   research microsoft com en us projects geolife  5 http   foursquare com  1   2 and  3  which means that the in   uence factor of o1 is 3  o1 if   3   In the meantime  o2 is the PNN of  4  o2 if   1  and o3 is the PNN of  5  o3 if   1   Therefore  o1 is the data point with the highest in   uence factor and the R PNN query will return data point o1 to users  This type of queries is useful in many important applications  such as urban planning  facility allocation  tra   c monitoring  location based services  etc  For example  suppose people want to set up a new facility  e g   post o   ce  bank  petrol station  etc   in the city and the trajectory data of potential customers are available  The R PNN query can be used to    nd the most accessible location among all given location candidates  to maximize the commercial value of the new facility  In tra   c monitoring  the R PNN query can be used to    nd the optimal location to monitor the maximum number of vehicles  In this work  the proposed R PNN query is applied in road networks  since in a large number of practical scenarios objects move in a constraint environment  e g   roads  railways  rivers  etc   rather than a free space  A trajectory is a sequence of sample points of a moving object  We assume that all sample points have already been aligned to the vertices on the road network according to some map matching methods  8  1  3  17  and between two adjacent sample points a and b  the moving objects always follow the shortest path connecting a and b  A straightforward idea to solve the R PNN problem is to search the exact PNNs for all trajectories in the data set  and then combine the results to get the location with the highest in   uence factor  However  it is not practical since every trajectory in the trajectory data set  which is supposed to be large in R PNN query  has to be processed during the query processing  The extremely high computation cost will prevent the query from being answered in real time  The R PNN query is challenging due to three reasons  First  we consider the relationship between trajectories and data points  i e   the minimum distance between a trajectory and a data point  in this query  To acquire the minimum distance between trajectory   and data point o  every sample point in   should be considered  This is more complex than the relationship considered in the Reverse Nearest Neighbor  RNN  search  i e   the minimum distance between two di   erent data points   which is the main reason why the existing techniques of RNN search  10  15  21  11  16  20  23  19  are not suitable for our work  Second  in R PNN  the query input is a list of location candidates  not a single point  e g    10  21  11  15  16  19    Conventional single point query models lack e   ective pruning techniques to address our problem e   ciently  e g   given m query locations  the query should be performed m times to    nd the most accessible locations   Third  in this work  the objects    movement is constrained in road networks  rather than a free space  e g    10  21  11  16  19  4    The optimization techniques in the free space may fail to solve the problem in spatial networks since the bounds proposed in the free space is not always valid in spatial networks  To overcome the challenges and process the R PNN query e   ciently  an e   ective trajectory data pre processing technique is conducted in the    rst place  By extending the conventional k medoids object clustering method in spatial networks  22   we can cluster the trajectory data into several groups based on their distribution  This technique has two notable bene   ts to our work  First  it can help us tighten the search range during the query processing  Second  it allows the use of a divide and conquer strategy to further enhance the performance of R PNN query  The e   ect of the proposed trajectory clustering method is demonstrated by extensive experiments on real data sets  Based on the grouped trajectory data  a novel two phase algorithm is proposed  First  we specify a tight search range over the trajectory and location data sets  since re   ning every trajectory in the data set will lead to intolerable computational cost  Here  the network expansion method  6  and branch and bound strategy are adopted  In this phase  most of the trajectories and query locations can be pruned safely  The e   ciency study reveals that our approach de   nes the minimum search area  Second  we propose a series of optimization techniques to support the searching of exact PNNs for trajectories in the candidate set  By combining the PNN query results  we can retrieve the location with the highest in   uence factor  The complexity analysis shows that our solution is optimal in terms of time cost  To sum up  the major contributions of this paper are      We de   ne a novel type of query to    nd the Reverse Path Nearest Neighbor  R PNN  in road networks  It provides new features for advanced spatial temporal information systems  and bene   ts users in many popular applications such as urban planning      We propose an e   ective trajectory clustering technique that can further enhance the R PNN query e   ciency      We devise a two phase algorithm to answer the R PNN query e   ciently  The e   ciency study reveals that our approach de   nes the minimum searching area and the complexity analysis shows that our solution is optimal in terms of time cost      We conduct extensive experiments on real and synthetic trajectory data to investigate the performance of the proposed approaches  The rest of the paper is organized as follows  Section 2 introduces the road networks and trajectories used in this paper as well as problem de   nitions  The R PNN query processing is described in Section 3  which is followed by the experimental results in Section 4  This paper is concluded in Section 6 after discussions on related work in Section 5  2  PRELIMINARIES 2 1 Road Networks In this work  road networks are modeled as connected and undirected planar graphs G V  E   where V is the set of vertices and E is the set of edges  A weight can be assigned to each edge to represent its length or application speci   c factors such as traveling time obtained from historical tra   c data  7   Given two locations a and b in road networks  the network distance between them is the length of their shortest network path  i e   a sequence of edges linking a and b where the accumulated weight is minimal   The data points aredistributed along roads and if a data point is not located at a road intersection  we treat the data point as a vertex and further divide the edge that it lies on into two edges  Thus  we assume that all data points are in vertices for the sake of clarity  2 2 Trajectory The raw trajectory samples obtained from GPS devices are typically of the form of  longitude  latitude  time     stamp   How to map the  longitude  latitude  pair onto the digital map of a given road network is an interesting research problem itself but outside the scope of this paper  We assume that all sample points have already been aligned to the vertices on the road network by some map matching algorithms  1  3  8  17  and between two adjacent sample points a and b  the moving objects always follow the shortest path connecting a and b  As the trajectory   s time stamp attribute is not related to our work  we de   ne the trajectory in the following format  De   nition  Trajectory A trajectory of a moving object   in road network G is a    nite sequence of positions       p1  p2       pn   where pi is the sample point in G  for i   1  2      n    2 3 Problem De   nition Given any two locations a and b in a road network  the shortest network path between them is denoted as SP a  b  and the length of SP a  b  is denoted as sd a  b   Given a trajectory   and a data point o in a road network  the minimum distance dM o      between data point o and trajectory   is de   ned as dM o        min vi      sd o  vi     1  where vi is the vertex belonging to     De   nition  Path Nearest Neighbor  PNN  Given a trajectory   and a set of data points O  the Path Nearest Neighbor  PNN  of   is the data point o     O with the minimum dM o       That is  dM o          dM o               o          O     o     De   nition  Reverse Path Nearest Neighbor  R PNN  Query Given a trajectory set T and a data point set O  if o     O is the Path Nearest Neighbor of k trajectories  1   2        k     T  the in   uence factor of o is k  o if   k   Reverse Path Nearest Neighbor Query    nds the data point o     O with the highest in   uence factor  That is  o if     o      if     o          O   o     3  QUERY PROCESSING Intuitively  the Reverse Path Nearest Neighbor  R PNN  query can be solved by searching the exact PNNs for all trajectories in the data set and combining the search results to    nd the location with the highest in   uence factor  i e   the most accessible location   It is not practical since every trajectory in the data set should be processed during the query processing  resulting in intolerable computation cost  To overcome this challenge and solve the R PNN query e   ciently  we conduct an e   ective trajectory data preprocessing technique  k medoids trajectory cluster  in the    rst place  Section 3 1   Then  based on the grouped trajectory data  a two phase solution is proposed  In the    rst phase  we specify a tight searching range over trajectory and location data sets  Section 3 2   The e   ciency study reveals that our approach employs the minimum search area  Section 3 3   The second phase introduces the searching process for the most accessible location  with the support of a series of optimization techniques  Section 3 4   The complexity analysis shows that our solution is optimal in terms of time cost  Section 3 5   3 1 Trajectory Data Pre Processing To enhance the performance of R PNN query processing  an e   ective trajectory data pre processing technique is conducted in the    rst place  Given a trajectory data set T  the proposed data pre processing procedure takes two steps  First  according to the k medoids clustering method  the trajectory data set is divided into k clusters  For each trajectory cluster Ci  i      1  k   the corresponding medoid mi is recorded  Second  we compute and record the minimum network distance between mi and every trajectory       Ci and other related information  This technique has two major contributions   i  it can help us tighten the searching range during the query processing   ii  it allows the use of a divide and conquer strategy to further enhance the query e   ciency  k medoids trajectory clustering method is adopted in the    rst step  It is an extension of the conventional k medoids clustering algorithm in spatial networks  22   Initially  we randomly select k vertices from the road network as the medoids  How to select a suitable value of k will be discussed in Section 3 3  Then  each trajectory is assigned to the cluster corresponding to the nearest medoid reachable from it  If trajectories in the same cluster are close to the medoid  the clustering is e   ective  Remark  In the following search  Section 3 2   we use dM mi      and sd mi  o  to estimate the minimum distance between data point o     O and     Based on our observation  small values of dM mi      will lead to small gaps between the upper and lower bounds of dM o       and enhance the pruning e   ectiveness  For this reason  an evaluation function  Equation 2  is applied to evaluate the quality of the partition  R  Ci  mi    i      1  k        k i 1         Ci dM mi       2  Here  mi is the medoid of cluster Ci for i      1  k   The lower the value of R  the better the partition  After evaluation  a medoid will be replaced by a new vertex from the road network and trajectories will be assigned to the clusters based on new medoids  If the R value of the new partition is less than the old one  the change will be committed  Otherwise  the change will be rolled back and a new medoid replacement will be attempted  This process terminates when no replacements can lead to a better result and the corresponding clusters and medoids are recorded  In the second step  for each cluster Ci  we compute and record the values of dM mi       the minimum network distance between mi and each trajectory       Ci  and other useful information to enhance the performance of the following R PNN query processing  dM mi      can be computed bymi o T o2 o1 v1 v2 v3 r v4 s t Figure 2  Identifying Candidates applying Dijkstra   s expansion  6   By expanding a browsing wavefront from mi according to the Dijkstra   s algorithm  the    rst vertex v       touched by the wavefront is just the closest vertex to mi in     That is  sd mi  v    dM mi       where v         v is also recorded as the closest vertex to mi in     Since this searching process has already been conducted in the trajectory clustering phase  the values of dM mi      can be easily obtained from the searching results of the    rst step  In addition  we compute and record the value of max d   s  v   d v    t   for vertex v and the corresponding trajectory     where   s and   t are the source and destination of trajectory   respectively and d   s  v  is the network distance between   s and v along the trajectory    not shortest path distance   The value of max d   s  v   d v    t   will be used to de   ne the lower bound of dM o       o     O  in the following search  3 2 Identifying Candidates Given a set of trajectory clusters Ci  i      1  k  and a set of data points  location candidates  O  we try to specify a tight searching range based on a series of    lter and re   nement techniques  In our approach  we estimate the minimum network distance dM o      between data point o and trajectory   by a pair of lower bound dM o      lb and upper bound dM o      ub based on the triangle inequality of shortest paths  The triangle inequality in road networks can be represented as    sd v1  v2    sd v1  v3      sd v2  v3  sd v1  v2      sd v1  v3      sd v2  v3  where sd v1  v2  indicates the shortest path distance between two vertices v1 and v2  Consider the schematic example demonstrated in Figure 2  Vertex mi is a medoid of trajectory cluster Ci and   is a trajectory in the same cluster  v1  v2  v3  v4 are vertices belonging to trajectory     and o  o1  o2 are data points  Suppose v1  v2 and v4 are the closest vertex in   to o  mi and o1 respectively  According to the triangle inequality introduced above  we have   sd o  mi    sd mi  v2      sd o  v2  sd o  v2      dM o        sd o  v1      dM mi        sd o  mi      dM o      Consequently  an upper bound of the minimum distance dM o      between data point o and trajectory   is given by dM o      ub   dM mi        sd o  mi   3  In the following paragraphs  we introduce our method of estimating dM o      lb  Obviously  P o  v1  v2  mi  is a path connecting data point o and medoid mi  According to the de   nition of the shortest path  it is easy to    nd that d o  v1  v2  mi    sd o  v1  d v1  v2  sd v2  mi      sd o  mi      dM o          sd o  mi      dM mi          d v1  v2  Then  we can use the value of max d s  v2   d v2  t    precomputed in Section 3 1  to replace d v1  v2  and get   dM o          sd o  mi      dM mi          d v1  v2  max d s  v2   d v2  t       d v1  v2      dM o          sd o  mi    dM mi        max d s  v2   d v2  t   Therefore  the lower bound of dM o      is described by the following equation  dM o      lb   sd o  mi    dM mi        max d s  v2   d v2  t    4  By comparing dM o      ub and dM o      lb  we can    nd that the gap between them are mainly a   ected by the value of dM mi       dM o      ub   dM o      lb   2dM mi      max d s  v2   d v2  t   This observation shows that lower values of dM mi      may lead to tighter lower bound gaps  and enhance the pruning e   ectiveness and justi   es conducting trajectory data clustering in the    rst place  Remark  The value of dM mi          sd o  mi  is not suitable to be used as the lower bound of dM o       According to the triangle inequality  it is easy to    nd that sd o  v1      sd v1  mi      sd o  mi   Since v2 is the closest vertex to mi in     we have sd v1  mi      sd v2  mi   Thus  we can use sd v2  mi  to replace sd v1  mi  and get sd o  v1      sd v2  mi     sd o  mi   That is  dM o          dM mi          sd o  mi   However  for any data points o1  o2     O  the value of dM mi         sd o1  mi  is always less than dM o2      ub   dM mi        sd o2  mi   Thus  dM mi          sd o  mi  cannot be used as the lower bound of dM o       Now we need to identify a data point candidate set   CS for each trajectory       T based on the upper and lower bounds introduced above  Only data points in the candidate set may turn out to be the PNN of the corresponding trajectory  For the same trajectory  any data point whose lower bound is greater than any other data point   s upper bound will be pruned  For trajectory       Ci  if the shortest path distance from medoid mi to o is known  the candidature of o can be determined  since the values of dM mi      and max d s  v2   d v2  t   are acquired in the trajectory data pre processing phase   Here  Dijkstra   s expansion  6  is adopted to select the data point candidates  From each medoid mi  i      1  k   a browsing wavefront is expanded in Dijkstra   s algorithm  Conceptually  the browsed region is restricted within a circle as shown in Figure 2 where the radius is the shortest network distance from the medoid to thebrowsing wavefront  denoted as r  Among all data points scanned by the expansion wave  we de   ne a global upper bound   ub for trajectory   as   ub   min    o   Os i   dM o      ub   5  where Os i  is the set of scanned data point  scanned by the expansion wave from mi   An example is demonstrated in Figure 2  where o  o1     Os i  and o2      Os i   The expansion of browsing wavefront stops once r     dM mi          max d s  v2   d v2  t   is greater than   ub  for every       Ci  It can be proved that all data points outside the browsed region cannot have shortest network distances to   less than   ub  and can be pruned safely  e g   o2   Lemma 1  For any data point o outside the browsed region  we have dM o          ub and o can be pruned from the data point candidate set   CS safely  Proof  As shown in Figure 2  since the Dijkstra   s algorithm always chooses the vertex with the smallest distance label for expansion  we have sd o2  mi    r  The browsing wavefront stops when r     dM mi          max d s  v2   d v2  t       ub  We can use sd o2  mi  to replace r and get dM o2      lb   sd o2  mi    dM mi        max d s  v2   d v2  t       ub  Hence  the data points outside the browsed region can be pruned from  CS safely    The data point candidates for trajectory       Ci can be described by the following equation    CS    o sd o  mi    dM mi        max d s  v2   d v2  t         ub   6  Consequently  following the approach introduced above  we can specify a data point candidate set for every trajectory       T  Next  a backward process is conducted to    nd the trajectory candidates for every data point o     O  i e   a data point o may only turn out to be the PNN of the trajectories in the candidate set o CS   If o       CS  we de   ne that       o CS  An example of the backward process is demonstrated by the following equations               1 CS    o1  o2   2 CS    o1  o2   3 CS    o1  o2   4 CS    o3   5 CS    o1  o2   6 CS    o1   7 CS    o3                         o1 CS     1   2   3   5   6  o2 CS     1   2   3   5  o3 CS     4   7  Assumption 1  The number of trajectories in T is much greater than the number of data points in O  In real scenarios  it is impractical for a user to input a large number  e g   hundreds  of query locations  data points  before successfully making a query  Therefore  it is reasonable to assume that the size of trajectory database is much greater than the number of query locations  Pigeonhole Principle  If n items are put into m pigeonholes with n   m  then at least one pigeonhole must contain more than     n m    items  Obviously  the size of o CS is greater than o if  and we can use o CS size to estimate the upper bound of o if  Suppose T num is the size of trajectory data set T and O num is the size of data point data set O  For every data point o whose candidate set size is less than     T num O num      we have   o CS size         T num O num     o if     o CS size     o if         T num O num     According to the Pigeonhole Principle and Assumption 1 introduced above  o must not be the data point with the highest in   uence factor  And we remove o from   CS        T temporally  Let us review the last example  There are 7 trajectories in T and 3 data points in O  thus     T num O num       2  Since o3 CS size   2 is no greater than     T num O num      o3 must not be the most accessible location and o3 should be removed from  i CS  i      1  7   Then  we    nd that the candidate sets for  4 and  7 are empty  That means it is not necessary to search the exact PNNs for  4 and  7  Therefore  the trajectory whose data point candidate set is empty    CS        can be pruned from the trajectory data set T  The remaining trajectories make up the trajectory candidate set T CS  and the temporally removed data points are restored  In the    rst searching phase  we specify a tight trajectory candidate set T CS from trajectory data set T  For each trajectory       T CS  we carefully maintain a data point candidate set   CS  which contains the data points that may turn out to be the PNN of     In the next phase  we will search the exact PNNs for trajectories in T CS and retrieve the data point with the highest in   uence factor  3 3 Ef   ciency Study In this section  we present our approach to selecting a suitable value of k as the number of trajectory clusters  Section 3 1   Our target is to specify the minimum search range and get the minimum amount of data point candidates  which can help us further enhance the performance of RPNN query processing  Suppose trajectories are uniformly distributed in road networks and data points are uniformly distributed as well  We now analyze the search range     nding candidates  Section 3 2  by estimating the scanned area in the    rst searching phase  According to the approaches introduced in Section 3 2  the total scanned area AS can be described as As   k  sd mi  o   2dM mi       max d   s  v   d v    t    2  7  where mi is the medoid for cluster Ci  i      1  k   o is the nearest data point to mi  due to the de   nition of   ub and the expansion nature of the Dijkstra   s algorithm  and   is the farthest trajectory in Ci to mi  due to the pruning strategy introduced in Lemma 1   Then  we use R to denote the value of dM m       r to denote the value of sd m  o  and   to denote the value of max d   s  v   d v    t    As can be expressed as As   k  r   2R     2   Suppose Area G  is the area of the network G V  E   the value of R and r can be calculated as follows   R 2   Area G  k     R       Area G   k r       Area G   k         Area G   O nums s t i sj o1 o2 o rsi rsj si   sj  are selected centers TTT n Figure 3  Lower bound and Upper bound Although the exact amount of query points O num is unavailable to us before the query is performed  we can use the average value of O num in the similar queries  or the average value of the historical queries  to estimate it  Thus  through substituting R and r into Equation 7  As can be expressed as As   ak   b     k   c  where             a      2   Area G  O num     2       Area G  O num b   6         Area G      Area G      O num   c   9    Area G  When k        b 2a   2   we can get the minimum value of As and the minimum amout of data point candidates  3 4 Searching the Most Accessible Locations Now  we have a trajectory candidate set T CS and a data point candidate set   CS for each trajectory       T CS  In this section  we present the algorithms to    nd the exact Path Nearest Neighbor to the trajectory       T CS  By combining the PNN query results  we can retrieve the data point with the highest in   uence factor  Given a trajectory       T CS and a data point set   CS  the Path Nearest Neighbor query processing takes two steps  1  Further tighten the data point candidate set   CS according to the proposed lower upper bound  2  Compute the minimum network distance between every candidate o       CS and     and then combine the results to    nd the exact PNN to     Initially  a set of vertices in   are selected as the expansion centers such that   is divided into several path segments  If the shortest path distances from o to the two ends si and sj of a path segment P si  sj   are known  the candidature of o can be determined  As the schematic example shown in Figure 3  the path segment P si  sj   is illustrated and n is a vertex on P si  sj    o  o1 and o2 are data points in   CS  An upper bound of the minimum distance from data point o to P si  sj   is given by o ub   min sd si  o   sd o  sj     In the    rst searching phase  Section 3 2   the value of dM o      is estimated by an upper bound dM o      ub  Equation 3   To further tighten the candidate set   CS  the minimum one between min sd si  o   sd o  sj    and dM o      ub is selected as the upper bound of data point o  o ub   min min sd si  o   sd o  sj     dM o      ub   8  In Figure 3  suppose n is the closest vertex to o in  si  sj    We estimate the lower bound of o according to the triangle inequality stated before    sd o  n      sd si  o      sd si  n  sd o  n      sd o  sj       sd n  sj       sd o  n      sd si  o    sd o  sj       sd si  n      sd n  sj   2 Then  we use d si  sj   to denote the path between si and sj along    not necessary a shortest path   Based on the de   nition of shortest path  it is easy to    nd that d si  sj     d si  n    d n  sj   is greater than d si  n    d n  sj    So  we can use d si  sj   to replace d si  n    d n  sj   in the last equation  sd o  n      sd si  o    sd o  sj       d si  sj   2 Next  we adopt the maximum one between sd si o  sd o sj     d si sj   2 and dM o      lb  Equation 4  as the lower bound of o  o lb   max  sd si  o    sd o  sj       d si  sj   2   dM o      lb   9  Our objective is to    nd all data points which have lower bound not more than any other point   s upper bound  From si  a browsing wavefront is expanded in road networks as Dijkstra   s algorithm  6  and so does sj   In concept  the browsed region is round as shown in Figure 3 where the radius is the network distance from the center to the browsing wavefront  denoted as r  Among all data objects scanned by the expansion wave  we de   ne a global upper bound UB as UB   min o   Os  o ub  where Os is the set of all scanned data objects  Os        i 1 Oi and   is the number of expansion centers  An example is demonstrated in Figure 3  where o  o2     Os and o1      Os  The expansion of browsing wavefront will be stopped once rsi rsj   d si sj   2 is greater than UB  Given a data point o1 outside the browsed region  since the Dijkstra   s algorithm  6  always chooses the vertex with the smallest distance label for expansion  we have sd o1  si    rsi and sd o1  sj     rsj   Thus  sd o1 si  sd o1 sj     d si sj   2   UB and o1 lb   UB  which means that the shortest network distance from o1 to P si  sj   cannot be less than UB  and can be pruned safely  Consequently  a tighter candidate set   CS is created and the    rst step of the PNN query processing terminated  In the second step  we compute the minimum distance dM o      from each data point o       CS to   by applying Dijkstra   s expansion  6   By expanding a browsing wavefront from o according to the Dijkstra   s algorithm  the    rst vertex v touched by the wavefront is just the closest vertex to o in     That is  sd o  v    dM o       The data point with the minimum value of dM o      is the PNN to trajectory     Finally  we combine the PNN query results for trajectories in T CS to retrieve the data point with the highest in   uencefactor  It is returned as the most accessible location to users 3 5 Complexity Analysis Suppose data points are uniformly distributed in spatial networks and the vertices in   are uniformly distributed as well  We now analyze the complexity of PNN query processing by estimating the cost in each step  In the    rst step  the Dijkstra   s expansion is performed from centers to identify candidates  In a given network G V  E   the complexity is O   V lg V     E   where   is the number of centers  The second step computes the minimum distance between candidates and trajectory     The cost is O   V lg V     E    where   is the number of candidates  By combining the two steps  the time complexity of PNN query processing is O       V lg V    E    Clearly  the number of centers and the number of candidates determine the overall time cost of PNN query processing  In an extreme case where every vertex in   is a center  the candidate set is minimized but the number of centers is maximized  Since the expansions are based on both candidates and centers  the overall performance may su   er  especially in case of sparse data points  In another extreme case  only the two ends of P  i e   source and destination  are selected as centers  as in  5    While the number of centers is minimized  the candidate set may be very large  The optimal selection of centers can be estimated using linear programming  Suppose  s1   s  s2    sn   1  sn   t  are vertices in   in the order from s to t  Let A be a n    n matrix where aij   1 if the i th and the j th exits are adjacent centers  i e   there are no exits in between them that are centers  and aij   0 otherwise  Our goal is to minimize the objective function          aij 1 4   dij   r  2          aij    10  subject to i   j     n j 1 a0j   1     n i 1 ai0   1     n j 1 aij     1     n i 1 aij     1  and    n j 1 aij      n k 1 aki    is the density of data points and dij is the network distance between the i th and j th centers along the trajectory  not necessarily a shortest path   We use      aij 1 4   dij  r  2    to estimate the number of candidates and     aij to estimate the number of centers  where r   min      Area G      CS size    dM o      ub  For online processing  the time cost of    nding the optimal selection of centers by solving the above objective function may not be practical  Thus  we simplify the objective function  10  by assuming that the gaps between adjacent centers are equal and the vertices in   are uniformly distributed  Our aim is changed to    nd the optimal number of centers  Then  we have           1 4      l     r  2          11  where   l is the length of     The   resulting in the minimum   can be retrieved using the derivative of the Function 11                    0                   l  2 4     r 2    12  Note that if the value of      is greater than   CS size    CS is achieved in Section 3 2   it is not necessary to conduct the Table 1  Parameter setting BRN ORN Trajectory Number 600 1600  default 1000  100 350  default 200  Location Number 40 120  default 80  20 60  default 40  PNN optimization techniques introduced in Section 3 4  since the time cost required by the network expansion o   sets the time saved by tightening the candidate set  4  EXPERIMENTS In this section  we conducted extensive experiments on real spatial data sets to demonstrate the performance of the proposed R PNN search  The two data sets used in our experiments were Beijing Road Network  BRN  6 and Oldenburg City Road Network  ORN  7   which contain 28 342 vertices and 6 105 vertices respectively  stored as adjacency lists  In BRN  we adopted the real trajectory data collected by the MOIR project  12   In ORN  the synthetic trajectory data were used  All algorithms were implemented in Java and tested on a Windows platform with Intel Core i5 2410M Processor  2 30GHz  3MB L3  and 4GB memory  In our experiments  the networks resided in memory when running the Dijkstra   s algorithm  6   as the storage memory occupied by BRN ORN was less than 1MB  which is trivial for most hand held devices in nowadays  All experiment results were averaged over 20 independent tests with di   erent query inputs  The main performance metric was CPU time and the parameter settings are listed in table 1  By default  the number of trajectories were 1600 and 200 in BRN and ORN respectively  In the meantime  the number of query locations  data points  was set to 80 for BRN  and 40 for ORN  The query locations were randomly selected from road networks  For the purpose of comparison  an algorithm based on Path Nearest Neighbor  PNN  query  5  was also implemented  referred to as R EPNN   In this algorithm  the PNN query was conducted to    nd the exact PNN for every trajectory in the data set and the most accessible location was found by combining all the PNN query results  4 1 Effect of the Trajectory Number T num  1  150  600 800 1000 1200 1400 1600 CPU Time sec   log scale  Trajectory Number RPNN R EPNN  a  BRN  1  15  100 150 200 250 300 350 CPU Time sec   log scale  Trajectory Number RPNN R EPNN  b  ORN Figure 4  E   ect of the trajectory number 6 http   www iscas ac cn  7 www cs fsu edu  lifeifei SpatialDataset htmFirst of all  we investigated the e   ect of the trajectory number on the performance of R PNN and R EPNN with the default settings  Figure 4 shows the performance of R PNN and R EPNN when the number of trajectories varies  Intuitively  a larger trajectory data set causes more trajectories to be processed during the query processing  and the CPU time is expected to be higher for both R PNN and REPNN  However  the CPU time of R EPNN increased much faster than R PNN for two reasons  Firstly  every trajectory in the data set should be addressed by an individual PNN query in R EPNN and its query time increases linearly with the number of trajectories  In the meantime  in R PNN  a tight trajectory candidate set is speci   ed  and larger values of     T num O num     can further enhance the pruning e   ect when identifying candidates  The second reason is due to the much larger number of candidates  data objects to be checked  when using bi direction network expansion method in the original PNN query  5   For instance  when T num is equal to 1600 and 350 in Figure 4 a  and Figure 4 b  respectively  R PNN outperforms R EPNN by almost three orders of magnitude  Note that this result demonstrates the importance of smart selection of trajectory candidates in the    rst search phase and the necessity of PNN query optimization in the second phase  4 2 Effect of the Query Location Number O num  1  100  40 60 80 100 120 CPU Time sec   log scale  Query Location Number RPNN R EPNN  a  BRN  0  10  20  30  40  20 30 40 50 60 CPU Time sec  Query Location Number RPNN R EPNN  b  ORN Figure 5  E   ect of the query location number In Figure 5  the experiment results demonstrate the e   ect of the query location number O num on the performance of R PNN and R EPNN  Intuitively  a larger number of query locations may lead to higher time cost in the query processing for both R PNN and R EPNN  In Figure 5 a  5 b   the CPU time of R EPNN increased linearly with the number of query locations  For R PNN  a larger O num value will result in the decrease of     T num O num      T num is    xed   and thus weakens the pruning e   ect in the    rst search phase  Consequently  in both Figure 5 a  and Figure 5 b   the CPU times of R PNN increased slowly with the query location number  Nevertheless  the CPU time required by EPNN was two orders of magnitude higher than that of R PNN  4 3 Effect of Trajectory Clustering This experiment investigated the e   ect of trajectory clustering  Section 3 1  on the performance of R PNN  This preprocessing technique is used to further enhance the R PNN query e   ciency   1  it can tighten the search range during the query processing   2  it allows the use of a divide andconquer strategy  R PNN was run with and without the  0  10  20  30  40  600 800 1000 1200 1400 1600 CPU Time sec  Trajectory Number with trajectory cluster without trajectory cluster  a  BRN  1  2  3  4  5  6  100 150 200 250 300 350 CPU Time sec  Trajectory Number with trajectory cluster without trajectory cluster  b  ORN  0  10  20  30  40  40 60 80 100 120 CPU Time sec  Q</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09nss4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_nearest_neighbor_search"/>
        <doc>Monitoring Path Nearest Neighbor in Road Networks ### Zaiben Chen       Heng Tao Shen       Xiaofang Zhou       Jeffrey Xu Yu         School of Information Technology   Electrical Engineering The University of Queensland  QLD 4072 Australia     The Chinese University of Hong Kong  Hong Kong  China  zaiben  shenht  zxf  itee uq edu au  yu se cuhk edu hk ABSTRACT This paper addresses the problem of monitoring the k nearest neighbors to a dynamically changing path in road networks  Given a destination where a user is going to  this new query returns the k NN with respect to the shortest path connecting the destination and the user   s current location  and thus provides a list of nearest candidates for reference by considering the whole coming journey  We name this query the k Path Nearest Neighbor query  k PNN   As the user is moving and may not always follow the shortest path  the query path keeps changing  The challenge of monitoring the k PNN for an arbitrarily moving user is to dynamically determine the update locations and then refresh the k PNN e   ciently  We propose a three phase Best    rst Network Expansion  BNE  algorithm for monitoring the kPNN and the corresponding shortest path  In the searching phase  the BNE    nds the shortest path to the destination  during which a candidate set that guarantees to include the k PNN is generated at the same time  Then in the veri     cation phase  a heuristic algorithm runs for examining candidates    exact distances to the query path  and it achieves signi   cant reduction in the number of visited nodes  The monitoring phase deals with computing update locations as well as refreshing the k PNN in di   erent user movements  Since determining the network distance is a costly process  an expansion tree and the candidate set are carefully maintained by the BNE algorithm  which can provide e   cient update on the shortest path and the k PNN results  Finally  we conduct extensive experiments on real road networks and show that our methods achieve satisfactory performance  Categories and Subject Descriptors H 2 8  Database Applications   Spatial databases and GIS General Terms Algorithms Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  Keywords Path Nearest Neighbor  Road Networks  Spatial Databases ### 1  INTRODUCTION Nearest Neighbor query is one of the fundamental issues in spatial database research area  It is designed to    nd the closest object p to a speci   ed query point q  given a set of objects and a distance metric  This problem is well studied in the literature  and its variants include k Nearest Neighbor search  6  17   Continuous Nearest Neighbor search  1  10  21   Aggregate Nearest Neighbor queries  14  22   etc  While all the queries mentioned above concern only the locally optimized results  in this paper  we investigate the problem of Path Nearest Neighbor  PNN  query  which retrieves the nearest neighbor with respect to the whole query path  Here     locally optimized results    means the nearest neighbors with respect to the current query location  However  sometimes a user is moving and may want to know the best choice by considering the whole path to be traveling on  thus a globally optimal choice for the nearest neighbor to a given path is required  and that is the motivation of this work  As exempli   ed in Figure 1  assume that we are traveling from s to t along a path P    s  n2  n3  t  and we hope to    nd the nearest gas station for refueling  If we use conventional Nearest Neighbor query  gas station A is returned at the beginning  However  A is not the best choice because there is another gas station B not far away which is much closer to the path we are traveling on  So PNN query suits the applications where a user wants to consume a service when traveling towards a given destination  For such applications  neither the current nearest neighbor nor the nearest neighbor at any particular point is the best for the user  instead  the user wants to know the nearest neighbor relative to the route he she will travel  B in this example       V W Q   Q   Q   Q   Figure 1  an example A similar issue called In Route Nearest Neighbor  IRNN  query is    rst proposed by Shekhar et al  in  20  to search a facility instance  e g  gas station  with the minimum detourdistance from the query route on the way to the destination  Still considering Figure 1  the detour distance of A from P is greater than that of B  so obviously IRNN P  would return B to the user  The intuition behind is that users  e g  commuters  prefer to follow the route they are familiar with  thus they would like to choose the gas station with the smallest deviation from the route  After refueling  they will return to the previous route and continue the journey  However a drawback of IRNN is that the user has to input exactly the whole query path in advance  which is identi   ed by all intersections along the path  while a user   s driving path often cannot be precisely pre decided  Imagine that a user is driving from Washington to New York  which is a long journey  It is impractical for a user to input hundreds of intersections before successfully making a query  Therefore  we propose the Path Nearest Neighbor query  requiring users to input only the destination as well as the current location rather than the whole speci   ed path  For each PNN query  we construct a shortest path connecting the destination and the current location and then search for the nearest facility instance to the shortest path  i e  the facility instance with the minimum detour distance   Since a moving user may not follow the shortest path and the driving route might change over time in the coming journey  we provide continuous monitoring of the k PNN  which always gives the user the best candidates for consideration  This raises the issue of how to dynamically query the nearest neighbors to a changing shortest path e   ciently  To provide e   cient monitoring of the k PNN  we propose in this paper a Best    rst Network Expansion  BNE  method  Speci   cally  the BNE consists of three main phases  including searching phase  veri   cation phase and monitoring phase  In the searching phase  the BNE algorithm incorporates a bi directional search method for establishing the shortest path  which conducts two independent network expansions from the starting location and the destination separately  and when the two expansions meet  the shortest path is determined  The novelty of the searching phase is that  we can also derive all encountered nodes    lower bounds and upper bounds of minimum detour distance during the bidirectional search  which are further utilized in determining a candidate set for the k PNN and examining candidates    exact detour distances  As the searching for the shortest path is inevitable for the k PNN query  if not consider precomputation for distance browsing   the BNE algorithm is designed to retrieve as much information as possible during the searching process  and improve the performance of monitoring by using this information  With a list of potential candidates that guarantee to include the k PNN results returned from the searching phase  the veri   cation phase processes these candidates in the order of their lower bounds  Here  a heuristic veri   cation function for examining candidates    exact minimum detour distances to the query path is devised  The heuristic function searches the minimum detour path from a candidate towards the query path directionally  instead of simply conducting a Dijkstra   s network expansion  By doing so  the area of searching is reduced greatly especially when the candidate is not close to the query path  In the monitoring phase  the main task is to    gure out where an update for the k PNN is needed  which could be an update of the order  or a re calculation of the k PNN  We discuss these two cases in the situation when the user follows the shortest path or deviates from the shortest path respectively  To facilitate the k PNN updates  the BNE carefully maintains an expansion tree rooted at the destination  which stores the shortest paths  from destination  to the surrounding nodes  This expansion tree is    rstly recorded during the bi directional search in the searching phase  and it enlarges or shrinks accordingly while the user   s current location is changing  Besides  the candidate set and candidates    lower bounds upper bounds acquired previously are also updated gradually in the monitoring phase  which are utilized to accelerate the update algorithms  To sum up  we make the following main contributions      We de   ne a new type of query for searching the k nearest neighbors to a changing shortest path  It provides new features for advanced spatial temporal information systems  and may bene   t users by reporting best candidates from the global view      We devise the BNE algorithm which e   ciently monitors the k PNN while the user is moving arbitrarily  An expansion tree and the candidate set are utilized with lower and upper bounds on minimum detour distance for fast k PNN update      We also propose the methods for determining the update locations which invoke potential updates on the k PNN results in di   erent user movements  as well as the algorithms for e   ciently updating the k PNN results      We conduct extensive experiments on real datasets to study the performance of the proposed approaches  The remainder of the paper is organized as follows  In Section 2 we discuss the related work  In Section 3  a formal de   nition of the problem is given  The searching phase and the veri   cation phase of the BNE algorithm are presented in Section 4  and the monitoring phase is introduced in Section 5  Finally we show our experiment results in Section 6 and draw a conclusion in Section 7  2  RELATED WORK Spatial queries in advanced traveler information system continue to proliferate in recent years  Nearest Neighbor  NN  query is considered as an important issue in such kind of applications  This query aims to retrieve the closest neighbor to a query point from a set of given objects  In  17  and  6  a depth    rst and a best    rst tree traversal approaches are proposed respectively for NN query in Euclidean space and they employ a branch and bound strategy  The Nearest Neighbor query is also extended to a road network scenario by using network distance as the distance metric  Papadias et al  present in  15  the Incremental Euclidean Restriction  IER  and Incremental Network Expansion  INE  algorithms for retrieving k NN according to network distance  IER uses the Euclidean distance as a lower bound for pruning during the search  and INE performs a network expansion similar to the Dijkstra   s algorithm  3   Jensen et al  also propose in  7  a general spatial temporal framework for NN queries in a road network which is represented by a graph  In  19   a graph embedding technique is proposed to transform a road network to a high dimensional Euclidean space and then the approximate k NN can befound  Pre computation based methods for k NN queries are also studied in  8  and  18   in which Voronoi diagrams and Shortest Path Quadtrees are utilized separately  Many variants of Nearest Neighbor search are studied as well  like Aggregate k NN monitoring  16   Trip Planning Queries  9  and Continuous Nearest Neighbor queries  CNN   1  10  13  21   CNN queries report the k NN results continuously while the user is moving along a path  The main challenge of this type of queries is to    nd the split points on the query path where an update of the k NN is required  and thus to avoid unnecessary k NN re calculations  However  a limitation of CNN queries is that the query path has to be given in advance and it can not change during the user   s movement  Therefore  in  11   Mouratidis et al  investigate the Continuous Nearest Neighbor monitoring problem in a road network  in which the query point moves freely and the data objects    positions are also changing dynamically  The basic idea of  11  is to carefully maintain a spanning tree originated from the query point and to grow or discard branches of the spanning tree according to the data objects and query point   s movements  To some extent  the motivation of our k PNN monitoring problem is similar to that of the CNN monitoring  However  we aim to provide monitoring of the k NN to a dynamically changing path rather than a moving query point  and we assume all data objects  e g  restaurants  gas stations  keep stationary  In Route Nearest Neighbor Queries  IRNN  in  20  is designed for users that drive along a    xed path routinely  As this kind of drivers would like to follow their preferred routes  IRNN queries are proposed for    nding nearest neighbor with the minimum detour distance from the    xed route  because they make the assumption that a commuter will return to the route after going to the nearest facility  e g  gas station  and will continue the journey along the previous route  Our problem is an extension of the IRNN query  by monitoring the k nearest neighbors to a continuously changing shortest path  and the user only needs to input the destination rather than exactly the whole query path  3  PROBLEM DEFINITION In this paper  a road network is modeled as a weighted undirected graph G V   E   in which V consists of all vertices  nodes  of the network  and E is the set of all edges  We assume that all facility instances  data objects  lie on the road  If a data object is not located at a road intersection  we treat the data object as a node and further divide the edge it lies on into two edges  So V is a node set comprised of all intersections and data objects and E contains all the edges between them  Each edge is associated with a nonnegative weight representing the time cost of traveling or simply the road distance between the two neighboring nodes  We de   ne the network distance Dn n1  n2  between two nodes n1 and n2 as the length of the shortest path S P n1  n2  connecting n1 and n2  A path P from node s to destination t is represented by a series of nodes P    n1  n2             nr   in which n1   s  nr   t and the length  P  is the sum of the weight of all edges on P  The minimum detour distance Dd o  P  of  a  data  ob ject o from a path P is de   ned as  Dd o  P     min ni   P  Dn o  ni   We may also denote Dd o  P  by Dd o  alternatively when in a clear context  Table 1  A list of notations Notation Description V The set of all nodes E The set of all edges weight n1  n2  The weight of edge  n1  n2  P   P  A path in a road network  and its length  n1  n2  The edge between n1 and n2  or the path from n1 to n2 if in a clear context S P n1  n2  The shortest path between n1 and n2 Dn n1  n2  The network distance between n1 and n2 Dd o  P  The minimum detour distance of data object o from path P De n1  n2  The Euclidean distance between n1  n2 LB o  P   U B o  P  The lower bound and upper bound of minimum detour distance of o from P Lf     Lr    Lv   The distance labels in forward  reverse and veri   cation searches Distp ni  s  t  The perpendicular distance from ni to line  s t  Definition 1   k Path Nearest Neighbor query  Given a starting node s  a destination node t  a road  network G V   E  and a set of data objects O  O     V    the k Path Nearest Neighbor  k PNN  query is to    nd the k data objects  O      o1  o2             ok   O       O   such that Dd oi  S P s  t       Dd oj   S P s  t       oi     O     oj     O     O   Here S P s  t  is the shortest path from s to t  We aim to monitor the k PNN relative to S P s  t  while s is moving in a road network  In our application scenarios  S P s  t  keeps changing and the k PNN needs to be reported dynamically  Table 1 shows a list of notations used in this paper  4  K   PATH NEAREST NEIGHBOR QUERY Intuitively the k PNN query can be solved by issuing at each node of the current shortest path a traditional k NN search and thereafter combining all the results together  However the cost of this method is high especially in a monitoring scenario  Therefore  in this section  we propose the Best    rst Network Expansion  BNE  algorithm for e   cient monitoring of the k PNN  The BNE is composed of three phases  the searching phase for    nding the shortest path and potential candidates at the beginning  the veri   cation phase for determining the exact k PNN results  and the monitoring phase for updating the k PNN e   ciently  In the veri   cation phase  the BNE always selects the data object which is most likely to be the closest one from the candidate set for veri     cation  and that is why we call it best    rst  As determining distance in a road network is a costly network expansion process  the BNE takes advantage of previous expansion results by maintaining an expansion tree and a candidate set of data objects that must contain the k PNN results  In our approach  we estimate the minimum detour distance of a data object by a lower bound derived from the triangular inequality of shortest path  and that is the basis of our searching and veri   cation algorithms  In a road network  the triangular inequality holds for shortest path such that  S P n1  n2      S P n2  n3         S P n1  n3    S P n1  n2        S P n2  n3         S P n1  n3   S P n1  n2  indicates the shortest path between nodes n1V W R F   F   O Q L     Figure 2  Lower bound             F        F     O    F   F    O  D E  Figure 3   a b  and n2  and  S P n1  n2   is the length of the path  Considering the illustration in Figure 2  there is a shortest path S P s  t  connecting the two nodes s and t with  S P s  t     l  while o is a data object in the road network with  S P o  s     c1 and  S P o  t     c2  ni is a node on the shortest path S P s  t   Obviously  o has an upper bound of minimum detour distance UB determined by U B o  SP s  t     min c1  c2   1  This upper bound can be further tightened during the searching phase as discussed later in this section  Now we expect to estimate the lower bound LB of the minimum detour distance for the data object o  Assume that the distance from s to ni is x  and the distance from o to ni is y  According to the triangular inequality theory stated above  we have  j c1     x     y c2      l     x      y Therefore  the distance  y  from  data  ob ject o to the shortest path S P s  t  is  no  shorter  than LB  LB   min x    0 l   max c1     x  c2      l     x     2  Consequently the lower bound LB o  SP s  t   of the minimum detour distance of o from S P s  t  is determined by    guring out the intersection point  a  b  of the two lines y   c1     x and y   c2      l     x    as shown in Figure 3  We get  a   l   c1     c2 2   b   c1   c2     l 2 So the lower bound is estimated by LB o  SP s  t     c1   c2     l 2  3  With l    xed  we can infer from Equation 3 that a smaller lower bound also implies a smaller value of  c1   c2   which means that  c1   c2  declines to l  This happens when the data object is closer to the shortest path connecting s and t  Therefore  a data object with smaller lower bound has higher opportunity in having a shorter minimum detour distance  Based on this observation  the BNE algorithm chooses data objects for veri   cation in the order of their lower bounds until the current selected data object   s minimum detour distance is smaller than the next object   s lower bound  Firstly  in the searching phase of our algorithm  the BNE    nds the shortest path between s and t  Here  we adopt a bidirectional algorithm  12  by running the forward and reverse versions of the Dijkstra   s algorithm  3  from s and t separately  The novel point is that we can also obtain the scanned nodes    lower bounds and upper bounds of the minimum detour distance during the searching for the shortest path  The forward version of the Dijkstra   s algorithm expands from s and the reverse version expands from t in the road network  while each of them maintains its own set of distance labels  Once the two searches meet  a node scanned by the forward search has also been scanned by the reverse search  or vice versa   a shortest path from s to t is detected  During the search for the shortest path S P s  t   some data objects around s and t are scanned and their distances to s or t are determined as well  We can utilize these recorded distances for the veri   caton of the k nearest neighbors in the following veri   cation phase  Another task during the bidirectional search is to get a candidate set of data objects that guarantees to include the k PNN results  To achieve that  the bidirectional expansion may need to continue even after the shortest path is found  until we    nd a data object o  satisfying that the lower bound LB o  SP s  t   is not less than at least k found data objects    upper bounds  We denote by Lf  ni  the distance label of a node ni maintained by the forward search  and by Lr ni  the distance label of a node ni maintained by the reverse search  and by l the length of the shortest path S P s  t   We formalize the process as following  assume that during the bidirectional search  so far there is a set of k   data objects  O     get scanned  expanded  by either the forward search or the reverse search or both of them  Among O     each oi     O   is assigned an upper bound U B oi  S P s  t     min Lf  oi   Lr oi   according to Equation 1  or Lf  oi  if only scanned by the forward search  or Lr oi  if only scanned by the reverse search  while those scanned by both searches also have a lower bound LB oi  S P s  t     Lf  oi  Lr oi    l 2 according to Equation 3  Theorem 1  During the bidirectional search  if there exists a data object o     O     and we can    nd at least k data objects O    o1  o2             ok  from O     such that LB o  SP s  t       max oi   O  U B oi  S P s  t    Then  the k PNN must be included in O     Proof  For any data ob ject oj that is not in O     which means it has not been scanned yet  if we continue the bidirectional search till oj gets both distance labels from the forward and the reverse searches  we have Lf  oj       Lf  oi      oi     O   Lr oj       Lr oi      oi     O   because the search process based on the Dijkstra   s algorithm always chooses the node with the smallest distance label value for expansion  o     O     then Lf  oj     Lr oj       l 2     Lf  o    Lr o      l 2     LB oj   S P s  t       LB o  SP s  t       LB oj   S P s  t       U B oi  S P s  t      oi     O Therefore  any oj must not have a minimum detour distance less than that of the k data objects in O found so far  Notice that the k data objects  o1  o2              ok  are not necessarily to be the k PNN results  We can only guaranteethat the k PNN is within the set of data objects  O      The searching phase of the BNE is shown in Algorithm 1  Algorithm 1  BNE   searching phase input   Node s  t  G V  E  output  S P s  t   Candidate Set C S 1 S  T  Qs  Qt     null  l          2    p     V   Lf  p   Lr p          Lf  s   Lr t      0  3 Qs     Qs     s  Qt     Qt     t  4 Heap Lowerbounds  Upperbounds  5 while Qs  Qt    null do    Forward search 6 u     ExtractMin Qs   7 S     S     u  8 if u     T and l       then 9 l     Lf  u    Lr u   10 record S P s  t   11 for each node v     u adjacentNodes do 12 if Lf  v    Lf  u    weight u  v  then 13 Lf  v      Lf  u    weight u  v   14 Qs     Qs     v  15   f  v      u  16 if u is a data object then 17 Upperbounds add Lf  u    18 if Lr u         then u lowerbound     Lf  u  Lr u    l 2 19   20 Lowerbounds add u lowerbound   21 k minimal values     Upperbounds minK    22 if Lowerbounds min     max the k minimal values  then 23 C S     all data objects in S     T  24 return S P s  t    C S     Reverse search 25 The same process as the forward search  with  S  Qs  Lf       f     replaced by  T  Qt  Lr      r     In Algorithm 1 the forward and reverse searches run alternately  During the initialization step  the sets of scanned nodes S and T are initialized to be null  and all nodes    distance labels except Lf  s  and Lr t  are  set  to  be      The Heaps are for recording all data objects    lower bounds and upper bounds found so far  non data object nodes    lower bounds upper bounds are also recorded in another heaps   The search process is similar to the Dijkstra   s algorithm  which always chooses the node with the minimal distance label for expansion  line 6   When a node scanned by both searches is found  the shortest path S P s  t  is recorded  line 9 10   A data object   s upper bound of minimum detour distance is stored as the min Lf  u   Lr u    line 17   and once the object gets scanned by both forward and reverse searches  it is assigned a lower bound of the minimum detour distance  line 19   This part of the algorithm stops when Theorem 1 meets  line 22 24  and a candidate set is then returned  Note that after the candidate set C S and the shortest path S P s  t  are returned  there could still be some data objects in C S that have not been scanned by both the forward and reverse searches and thus their lower bounds are unknown yet  Therefore  before going to the candidate veri   cation phase  we further continue the network expansion of the bidirectional search until all data objects in C S have their lower bounds be determined  This part of the searching phase is intuitive and we omit it in Algorithm 1 for the simplicity of presentation  During the searching phase presented above  we can also get two expansion trees Tf and Tr originated from s and t respectively  by recording parent node as   f  v    r v  at line 15   which can be re used as    pre computed    knowledge in our monitoring phase  As illustrated in Figure 4  we only show the expansion tree originated from t with thicker lines   if the user moves from s to another node s   that has already been included in Tr  then the shortest path from s   to t is    gured out to be S P s     t     s     n4  t  by using the expansion tree easily without extra search  Besides  during the network expansion after S P s  t  is found in the searching phase  we can also tighten the upper bounds of some found data objects if their ancestor nodes in the expansion tree are on S P s  t   For example  the data object o in Figure 4 has an ancestor node n3  not necessarily the parent node  on S P s  t     s  n2  n3  t   then the upper bound U B o  SP s  t   is tightened to be  Dn o  n3   and Algorithm 1 may return results faster since smaller upper bounds make Theorem 1 easier to be satis   ed  V W V   Q   Q   Q   Q   R Figure 4  Expansion tree originated from t On acquiring the candidate set C S together with lower bounds of candidates  as well as the shortest path S P s  t   the veri   cation phase executes to verify the k PNN candidates in C S in the sequence of their lower bounds as shown in Algorithm 2  Algorithm 2  BNE   veri   cation phase input   Lowerbounds  S P s  t  output  k PNN 1 count     0  Heap kpnn  kpnn max          2 while Lowerbounds    null do 3 o     Lowerbounds popMin    4 if kpnn max   o lowerbound then 5 Dd o  SP s  t       verify o  SP s  t    6 if Dd o  SP s  t     kpnn max then 7 if count   k then 8 kpnn add o   9 count      10 else 11 kpnn deleteMax    12 kpnn add o   13 else 14 return kpnn  The veri   cation phase examines the exact minimum detour distance of each candidate from C S in the order of lower bound  the node with the minimal lower bound is pop outat line 3   until a candidate   s lower bound is not less than the kpnn   s max value  line 4 12  kpnn stores the k minimal detour distances found so far   The verify   function performs a network expansion from the candidate o to get its exact minimum detour distance  As this function is invoked every time an update occurs  the expansion method can affect the e   ciency of monitoring signi   cantly  Normally  the Dijkstra   s expansion method can be used  Here  we propose a heuristic expansion approach that improves the e   ciency greatly  The basic idea is to select the next node n with the minimum  Dn n  o   n detourEstimate  for expansion  n detourEstimate is the estimate of n   s minimum detour distance  and it is determined by either LB n  SP s  t    or Distp n  s  t  which is the perpendicular distance from n to the line  s  t   Distp n  s  t  uses Euclidean distance to approximate the minimum detour distance and it can be easily    gured out by using the Cosine Theorem as follows  Let c1   De n  s   c2   De n  t  and l   De s  t   De   is Euclidean distance   then we have  Distp n  s  t     c1    sin arccos  c 2 1   l 2     c 2 2 2c1l     However  the Euclidean detour estimate may not be applicable when the weight of an edge is not measured by real geographic distance  e g  time cost   In contrast LB n  SP s  t   gives a more tightened estimate and holds for any type of edge weight  One potential drawback is that some nodes encountered during the expansion may have not been previously scanned yet and have no lower bound determined  In this case we need further expansion of Tf and Tr to get the node   s lower bound  However  in our experiments on real datasets  this situation is rare and very limited number of encountered nodes haven   t been scanned as most of them are covered by the expansion trees  Basically  the search area of the verify   function using the Dijkstra   s expansion is a circle  while the search area is normally in a triangle shape towards S P s  t  if  using  the detour estimate as a heuristic  Algorithm 3 describes the details  Algorithm 3  verify o  SP s  t   1 Sv  Qv     null detourDist          2    p     V   Lv p          Lv o      0  Qv     Qv     o  3 while Qv    null do 4 n     ExtractMin Qv   such that Lv n    n detourEstimate is minimized   5 if Lv n    n detourEstimate     detourDist then 6 return detourDist  7 if n     S P s  t  and detourDist   Lv n  then 8 detourDist     Lv n   9 Sv     Sv     n  10 for each node v     n adjacentNodes do 11 if Lv v    Lv n    weight n  v  then 12 Lv v      Lv n    weight n  v   13 Qv     Qv     v  In Algorithm 3  the node with the minimum  Dn n  o    n detourEstimate  gets explored    rst  line 4   Once a node     S P s  t  gets scanned  line 7 8   a detour path from o to S P s  t  is found and we update the current minimum detour distance detourDist if a shorter one is found  Here  Lv   is the distance label indicating how far a node is from o  Notice that the verify   function may continue the search even after it reaches the shortest path S P s  t  since it is not necessarily that a node with smaller distance label Lv n  gets explored    rst  until the current detourDist is not greater than the current  Lv n    n detourEstimate  which  is  a  lower bound of all unscanned nodes    minimum detour distances  line 5 6   The correctness of Algorithm 3 is guaranteed as stated in the following  Lemma 1  For every node n scanned by the verify   function  Lv n  is equal to the length of the shortest path S P o  n   where o is the data object for veri   cation  Proof  Denote detourEstimate by e  The verify   function   s expansion method is equal to the Dijkstra   s algorithm if we replace the distance label Lv n  by Lv n    n e  Thus we can de   ne a new weight of an edge as  weight    n1  n2    Lv n2    n2 e      Lv n1    n1 e    weight n1  n2      n1 e   n2 e weight n1  n2  is the original weight de   ned in G V   E   Straightforwardly  weight n1  n2      n1 e   n2 e     0 because of the triangular inequality  proof by replacing e with Equation 3   Suppose we replace the weight of each edge in G V   E  by the non negative weight     Then for any two nodes nx  ny  the length of any path from nx to ny changes by the same amount  ny e     nx e  Therefore  a path is the shortest path from nx to ny with respect to weight  if  and only if it is also the shortest path from nx to ny with respect to weight     The rationale of the expansion method in Algorithm 3 is similar to that of the A   algorithm  5   although a di   erent heuristic is designed  and the detour estimate is essentially a feasible potential function in  4   As Lv n  is guaranteed to be the length of the shortest path from o by Lemma 1  once the minimal detourDist is con   rmed  it must be the minimum detour distance from o to S P s  t   5  MONITORING K   PNN In this section  we present the monitoring phase of the BNE algorithm and show how to update the k PNN results when the user is moving arbitrarily  As described before  the user may deviate from the shortest path and then the current shortest path which is actually the query path may be changed from time to time  and thus an update of the k PNN results is caused by the change of the query path  Even though the user always follows the shortest path  the path is also becoming shorter while the user is going towards the destination  Therefore  we need to deal with the shortest path update and consequently the k PNN update  In this part  the candidate set CS of data objects  the expansion tree Tr and Tf rooted at t and s respectively  as well as lower bounds and upper bounds of scanned nodes that acquired previously are all further utilized and carefully maintained in the monitoring phase as they provide    pre computed    knowledge to accelerate our update algorithm  Obviously  Tr is static because the destination does not change  by which we can    gure out a node   s shortest path to the destination quickly  As the user is probably moving closer gradually towards the destination  the candidate set CS probably covers the new k PNN results  Allthese information are also updated gradually in the monitoring phase  based on which we design the update algorithms  There are basically two types of updates for the k PNN   1  update of the order  and  2  update of the members  In the    rst category  the k PNN results are still the same but the order with respect to minimum detour distance changes  while in the second category some data objects of the k PNN become invalid and new data objects are inserted into the k PNN results  Now the problem is to determine where an update of the k PNN will be needed  i e  update location   and then only refresh the k PNN results when necessary  In the following  we present our update algorithms for the cases when the user follows the shortest path  and deviates from the shortest path  5 1 Following the Shortest Path Firstly  we discuss the case that so far the user follows the shortest path found previously  Figure 5 illustrates such a 4 PNN    o2  o5  o4  o3  example  in which we assume the user follows S P s  t  and his her current position is denoted by s     The shortest path from a data object oi to S P s  t  intersects S P s  t  at ni  and we  call S P oi  ni  the minimum detour path of oi  and ni the entrance point of oi   s minimum detour path  For instance S P o2  n2  is the minimum detour path of o2  and n2 is o2   s entrance point  V V   W R   R   R   Q   Q   Q   G R     Q   Q      R   G R     G R     G R     Figure 5  Update locations It is not hard to see that before s   reaches the    rst entrance point of the current k PNN  n2 in this example   neither the order nor the members of the k PNN needs to be updated  because when s   is on S P s  t   we have S P s     t      S P s  t   which means S P s     t  is the same as the part of S P s  t  from s   to t  and hence the minimum detour path of any oi does not change  and there can not be any other data object closer to S P s     t   otherwise the closer data object must be included in the k PNN of S P s  t   Once s   overtakes the entrance point of a data object oi  the minimum detour distance of oi will increase and thus it may a   ect the order of the k PNN  For instance  when s   overtakes n2 and keeps going forwards  the minimum detour distance of o2 becomes larger and the order of o2  the 1 st PNN  and o5  the 2 nd PNN  may change when o2   s minimum detour distance rises to a certain value  If oi is just the k th PNN  it may also become invalid and the  k   1  th PNN will replace it to be the k th PNN  To detect the change of the k th PNN  we actually maintain the  k   1  PNN  results in the algorithm  and we calculate the update locations for the k PNN to indicate where a change of the order could happen  Normally  an update location for a data object oi is computed every time when s   arrives at oi   s entrance point by  d oi     S P oj   nj         S P oi  ni    4  where d oi  is the distance from oi   s entrance point ni to the update location  Let oi be the    th PNN         k   then we choose oj        1  th PNN for calculating d oi  in  Equation 4  The idea is that an upper bound of the minimum detour distance of oi from S P s     t  is  S P oi  ni      S P ni  s       and as long as this upper bound is smaller than the      1  th PNN   s minimum detour distance  S P oj   nj     the order of the k PNN keeps the same  For example in Figure 5  the 4 PNN    o2  o5  o4  o3   when s   arrives at n2  it generates an update location for o2 determined by d o2   which is equal to  S P o5  n5        S P o2  n2    While the user is traveling within the range of d o2  from n2  it is expected that no change of the order between o2 and o5 is required  However  if the       1  th PNN   s entrance point is met before s   arrives at the    th PNN   s update location  for example s   meets o5   s entrance point n5 and it generates an update location for o5 with d o5  as shown in Figure 5  in this case o5   s update location is reset to be the same as o2   s update location which is closer to s     because we need to re compute both o2 and o5   s minimum detour distances at o2   s update location to determine whether the order changes  and to    gure out their next update locations  However  if o5   s update location is closer to s     we do not need to reset o5   s update location  Similarly  if the         1  th PNN   s entrance point is met before s   arrives at the    th PNN   s update location  like that n4 is encountered before s   reaches o3   s update location as illustrated in Figure 5  since o3   s update location is closer to s     there is no need to adjust o3   s update location  Algorithm 4 shows how to determine the update location when encountering a data object   s entrance point  Algorithm 4  Encountering oi   s entrance point    oi   the    th PNN       oj   the       1  th PNN       ok   the         1  th PNN    1 oi updateLoc     pos ni    d oi   2 if ok updateLoc    null and ok updateLoc   oi updateLoc then 3 oi updateLoc     ok updateLoc  4 if oj  updateLoc    null and oi updateLoc   oj  updateLoc then 5 oj  updateLoc     oi updateLoc  Here  pos oi  is the position of oi  and d oi  is computed by Equation 4  The criteria is to reset a lower ranking PNN   s update location  denoted by updateLoc  to the higher ranking one   s update location if the higher one   s update location is closer to s    with a smaller value   On arriving at oi   s update location  the minimum detour distance of oi is re examined by running Algorithm 3 and the k PNN is refreshed accordingly  Recall Algorithm 3  note that the lower bound LB oi  S P s  t   determined previously at s can still be used as the detour estimate in the veri   cation process even the current query path has changed to be S P s     t   because LB oi  S P s  t       LB oi  S P s     t    Let Dn oi  s    c1  Dn oi  t    c2  Dn oi  s       c3  we have  LB oi  S P s  t       LB oi  S P s     t     c1   c2      S P s  t   2     c2   c3      S P s     t   2   c1     c3        S P s  t        S P s     t    2    c1     c3       S P s  s      2     0 The update algorithm is invoked when encountering an update location Loc as described in Algorithm 5  Firstly it veri   es all corresponding data objects    minimum detour distances  and then refreshes the order of the  k   1  PNN  If the previous k th PNN is not valid any longer  line 5   a recomputation of the whole  k 1  PNN is executed by calling the updateKP NN   function in Algorithm 6  Algorithm 5  Encountering an update location Loc 1 for each object oi that oi updateLoc   Loc do 2 Dd oi      verify oi  S P Loc  t    3 remove oi updateLoc  4 refresh the order of the  k   1  PNN    if the k th 5 PNN is changed then 6 updateKP NN Loc  t   for each object o   i that o   i 7  entranceP oint   Loc do calculate o   i 8  updateLoc by Algorithm 4  In some cases  oi   s minimum detour path may have a new entrance point even ahead of s   after veri   cation  such as n   5 in Figure 5  After the update of k PNN   a data ob ject  is assigned a new update location if its new entrance point is right at s    line 7 8   Algorithm 6  updateKP NN n  t  1 S  Qs     null     p     V   Lf  p           2 Lf  n      0  Qs     Qs     n  3 Heap Lowerbounds  Upperbounds  4 while Qs    null do 5 u     ExtractMin Qs   6 S     S     u  7 if u     Tr and SP n t  is not determined then 8 record S P n  t   9 for each node v     u adjacentNodes do 10 if Lf  v    Lf  u    weight u  v  then 11 Lf  v      Lf  u    weight u  v   12 Qs     Qs     v    f  v      u  13 if u is a data object then 14 Upperbounds add Lf  u    15 if u      Tr then 16 further expand Tr until Lr u          u lowerbound     Lf  u  Lr u     S P  n t   2 17   18 Lowerbounds add u lowerbound   19 k minimal values     Upperbounds minK    20 if Lowerbounds min     max the k minimal values  then 21 Tr     Tr    ni   Lr ni    Lr u    22 C S     all data objects in S     Tr  23 break  24 continue the expansion until for each ni     C S we have ni lowerbound    null  25 run Algorithm 2 for verifying k PNN  In Algorithm 6  a Dijkstra   s expansion from the current position n is conducted to update the candidate set CS  and all candidates    lower bounds and upper bounds of the minimum detour distance  This process is similar to the searching phase in Algorithm 1  Since the expansion tree Tr rooted at t and the distance label Lr u  are invariable  we just need a forward expansion from n to get Lf  u  and subsequently the lower bound of n  All Lr u   u     T r  are added to the Upperbounds in the initialization step  If a data object scanned by the forward expansion is not included in Tr  line 15   which happens when the user deviates from the shortest path too much  Tr needs a further expansion to catch up with the forward expansion  and during the expansion of Tr the shortest path S P n  t  may also be recorded if it has not been determined yet  line 16   In fact  with the user approaching the destination  a smaller search area from n is required  and the candidate set CS and the expansion tree Tr are also updated to smaller ones  line 21 22   At the same time  all scanned nodes    lower bounds and upper bounds of minimum detour distance are also updated  Note that at line 14  we choose the min Lf  u   Lr u   as u   s upper bound  Finally  the veri   cation function runs to acquire the exact  k   1  PNN results  As the k PNN is already known  we just need a veri   cation for the  k   1  th PNN  5 2 Deviating from the Shortest Path In the case that the user does not follow the shortest path  as exempli   ed in Figure 6  and leaves the current shortest path S P s  t     s  n2  n3  t  for destination t through n5  st and n4     rstly we need to update the current shortest path to the destination  There will be a split point f on the coming edge such that the shortest path from the current position s   to t is  s     s  n2  n3  t  through node s when s   is on the path  s  f   and the shortest path changes to be  s     st  n4  t  through node st after the user passes f  V W V W Q   Q   Q   Q   I  W SH   Q  W SH     Figure 6  Split point   Object types To    nd the split point f     rst of all we search along the coming edges until encountering the    rst node with out degree     3  st in this example   and it is easy to see that the shortest path from s   to t must go through S P s  t  or S P st  t  when s   is on the path  s  st   So the next step is to    nd the shortest path S P st  t   If st is already contained in the expansion tree Tr  S P st  t  can be constructed by tracing upwards from st along parent node  recorded by   r    until it reaches the root t  Otherwise  again a Dijkstra   s network expansion from st is conducted  trying to touch the expansion tree Tr  As stated before  Tr covers the surrounding area of t  therefore  as long as the user does not deviate too much  st is close to Tr and the expansion from st will meet Tr very soon  after which the shortest path from st to t is determined just like that in the bidirectional search of the searching phase  In addition  the expansion tree Tf rootedat s probably also includes st  so the branch of Tf starts from st can be re used for the expansion  This is similar to the query update in  11   and other branches of Tf are then discarded  Once the shortest path S P st  t  is determined  the split point f is    gured out by    s  f      S P s  t      S P st  t       s  st   2      S P s  t      S P st  t         S P s  t       s  st   2 where   s  f   is the length of the path  s  f   and   s  st   is the length of  s  st   the path along which the user goes from s to st   Occasionally if S P st  t  is through s  we set the split point at node st  and S P s     t  is always through s when the user moves on  s  st   In the following  we elaborate how to update the k PNN when the user is moving on  s  f  only  since after the user passes the split point f  we can monitor the k PNN as if the user follows the shortest path S P f    t  and the algorithm for that is already introduced in Subsection 5 1  During the user   s movement on  s  f   however  the computation of update locations is di   erent from the previous method in Algorithm 4  Assume the user is currently at s        s  f   we observe that the k PNN of S P s     t  must be from the k PNN results of S P s  t   or those data objects become closer enough to S P s     t  because of the movement on  s  f   Based on this observation  we develop the following lemma  Lemma 2  Let kpnn be the k PNN of S P s  t   knn be the k nearest neighbors of st and Os st be the set of all data objects located on path  s  st   When s   is on the path  s  f  between s and the split point f  the k PNN of S P s     t  must be included in  kpnn     knn     Os st    Proof  Suppose on the contrary there exists a data object o such that o belongs to the k PNN of S P s     t   and o is not in  kpnn   knn    Os st    As stated previously  S P s     t  equals to S P s  t  plus   s  s     when s   is on  s  f   If o   s entrance point is on S P s  t   straightforwardly o must be included in kpnn  Except that  the only way o connects to S P s     t  is through node st  or o is right located on the path  s  st   In the former case o can not have the minimum detour distance shorter than that of the k NN of st  while in the later case o is a data object lies on  s  st   Therefore  o must be included in  kpnn     knn     Os st    From Lemma 2  we con   rm that the k PNN must be from  kpnn     knn     Os st   when the user is moving on  s  f   and hence only data objects belong to this set may have an update location  Furthermore  for data objects belong to this set  there are only two types of data objects  type1 and type2  as exempli   ed in Figure 6 that can trigger an update on the k PNN  Data objects of type1 are all those objects on path  s  st   and data objects of type2 are the k NN of st except those belong to type1  For a data object contained in the k PNN of S P s  t  excluding those in type1 and type2  it   s minimum detour distance does not change during the user   s movement on  s f   and thus it does not have an update location  For type1 and type2 data objects  their minimum detour distances may decrease as the user moves towards the split point  and we calculate the update locations for them when a deviation occurs   1  For a data ob ject oi of type1  it may become closer to the current shortest path S P s     t  since S P s     t  extends with s   moving towards oi  If oi is already the    th PNN         k   its update location is then determined by  oi updateLoc   pos s         s     oi       Dd         1  th P N N  Here pos s     is the user   s current position which is initially equal to pos s   and   s     oi   is the distance from s   to oi along path  s  f   oi updateLoc stands for the position where the distance from s   to oi drops to Dd         1  th P N N  and oi may become the         1  th PNN  Otherwise  if oi is not included in </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_skyline_query_processing"/>
        <doc>Eliciting Skyline Trade Offs using Example Based Heuristics for E Commerce Applications ### Christoph Lofi1  Ulrich Gu  ntzer2  Wolf Tilo Balke1 1 Institute for Information Systems  University of Braunschweig Mu  hlenpfordtstra  e 23  Braunschweig  Germany  lofi  balke  ifis cs tu bs de 2 Institute of Computer Science  University of Tu  bingen Sand 13  72076 Tu  bingen  Germany            ulrich guentzer informatik uni tuebingen de Abstract  The ubiquitous access to information via the Web has changed our daily life  For instance  e commerce application allow for a wide variety of vendors to compete in a world wide market  But with a growing number of vendors  also the amount of available offers is increasing  which in turn leads to an information flood that hampers the user experience  Skyline algorithms as introduced by the information systems community  promise to winnow subop  timal offers from electronic marketplaces  For the amount of Web data and the typical interaction style  however  the result sets are still too large and hard to manage  Recently  first approaches to integrate human decision processes like compromises or trade offs have been designed  In this paper we will build on these approaches and introduce a novel heuristic into the skylining paradigm that not only allows for convenient Web style user interaction  but also focuses searches on semantic clusters of offers  Thus  the view on interesting clusters is refined  whereas less interesting clusters are reduced to only the outstanding items  Keywords  skyline queries  trade offs  preference elicitation  personalization ###  1 Introduction Today   s possibility to access data on a world wide scale has changed the way of commercial interaction  E commerce portals and electronic market  places offer platforms for vendors and buyers from different areas and coun  tries  But the flood of different products on offer also leads to severe problems 2 Christoph Lofi  Ulrich Gu  ntzer  Wolf Tilo Balke with respect to the manageability and comparability of items on offer  The traditional way of dealing with the variety is a quantitative compensation of attributes in ranking approaches such as Top K retrieval  for a comprehensive survey see  1   Here either the system or the user individually weighs and ranks the attributes  thus providing a utility function summarizing benefit of each item  Unfortunately  this compensation feature is of a purely quantitative nature and thus not very intuitive to use  For instance  assume a user wants to buy a new laptop computer  Using an internet shopping portal  he she is usu  ally faced with several thousand notebooks on offer  Now  for choosing be  tween laptops the semantics of a utility function like         cannot be grasped naturally by most users  because determining the correct weightings to reach a desired ranking is far from trivial  e g  see Fig  1  Recently  the skyline paradigm  1   2  has been successfully established in the information systems community as a qualitative technique for personal  ized filtering of suboptimal items from large datasets  In order to facilitate this task  it relies on the well known economical concept of Pareto optimality which defines qualitative domination relationships between database items  An item dominates another item  if it features better or equal attribute values with respect to all user provided attribute preferences  Hence  Pareto dominat  ed objects can safely be excluded from any query   s result set  Let us come back to our e shopping example  given the limited range of manufacturers and laptop models consider two almost identical machines  one being slightly more expensive although featuring a slower processor  In this case there is no sensible reason for preferring this on all counts inferior choice over the cheap  er and more powerful laptop and thus it should be removed from the result  Eliciting Skyline Trade Offs using Example Based Deduction Heuristics 3 Fig  1  Quantitative weighting with Sliders  from http   www myproductadvisor com  Indeed the usefulness of qualitative querying is obvious  large query results can intuitively be focused and reduced to more manageable sizes without decreasing the result quality  Also  as illustrated by the interface in Fig  2 skyline queries can be posed in a simple manner  since users only needs to provide simple preference on the individual attributes  e g  more RAM is bet  ter   Currently  so called    example critiquing interfaces    are explored for elic  iting user preferences in e commerce scenarios and first results do not only show that their usage leads to the desired items  but also to a positive user experience  3   In any case  many attributes have preferences which are commonly accepted among users and thus can safely be assumed by the sys  tem without any user interaction  e g  cheaper prices are better  longer battery life is better  lighter weight is better  etc    4 Christoph Lofi  Ulrich Gu  ntzer  Wolf Tilo Balke Fig  2  Qualitative object comparisons However  the skyline paradigm is not entirely without drawbacks  while performance issues have been mostly solved by efficient indexing  see  3  or  4   or multi processor extensions like  5  or  6   weaknesses with respect to semantic characteristics and the manageability of skyline sets remain  For example  it is often reported that the actual size of skyline sets is still far from being manually manageable by users  being swamped by up to 30  of the original database items for just 5 to 10 attributes  7   1   8   9  is no rarity  This effect is a result of the fact that often preferences are conflicting over correlated attributes  e g   small price  but high quality or low weight  but large screen   thus rendering items incomparable with respect to the Pareto semantics  In particular  the ability of compensating between attributes is missing when using skyline queries  In order to bridge the gap between quan  titative Top K and qualitative skyline approaches  we recently proposed a comprehensive theory on qualitative trade offs for extending the semantics of skylines  12   11   14   To explain the basic concept  reconsider the previous user trying to buy a laptop computer  after performing a skyline query with all    natural    prefer  ences for the notebook domain  e g  cheap price  fast processor  large display  Eliciting Skyline Trade Offs using Example Based Deduction Heuristics 5 low weight  etc    he she may still be presented with many Pareto optimal notebooks  This is because the result set spans over the full range of available notebook types  powerful desktop replacements  overall good office note  books  subnotebooks with high mobility  to low cost netbooks  Obviously  all these choices are be part of the result set as the all excel in at least one desira  ble attribute  or offer a good compromise  Actually  this problem is a common phenomenon  different product classes within a given domain almost always feature this intrinsic incomparability     if a given product class was on all counts inferior to some other  the respective class would utterly fail on the market  Unfortunately  skyline queries do not allow focusing the skyline fur  ther by removing or penalizing product categories which are undesired  Qualitative trade offs allow to compensate between different product fea  tures and bias the skyline towards each user   s individual preferences by intro  ducing new domination relationships  In particular  a trade off relating two example items may compensate between different classes  is a user consider  ing a netbook willing to pay for the superior performance and accept the high  er weight of a subnotebook  In any case  the new relationships respect the original Pareto semantics  users still do not need to exactly quantify the com  pensation  but just state a preference on  two or more  real world items  How  ever  having users provide trade offs just using object comparisons usually leads to over specified relationships that do not have a significant effect on the skyline size  In this paper  we will introduce an innovative heuristic ap  proach for exploiting trade offs relying on simple example critiques while still ensuring a smooth user experience and promoting an effective personalization of the skyline set  Moreover  we will show that exceptionally good deals in each category always remain part the skyline and thus the user   s choices are not overly restricted  The rest of the paper is structured as follows  in section 2 we showcase the elicitation heuristic using an example scenario and in section 3  the necessary 6 Christoph Lofi  Ulrich Gu  ntzer  Wolf Tilo Balke theoretical foundations and notations for trade off skylines are provided  Fi  nally  the heuristic is formalized  justified  and discussed in section 4  We conclude this paper with evaluations in chapter 0 and final discussions in chapter 6  2 Eliciting Trade Offs using Deduction Heuristics A natural approach for eliciting preference trade offs from users is to rely on object comparisons  in contrast to explicitly asking for weights utility func  tions for ranking  users are simply presented with some example objects from the database and decide which one they prefer  e g   see Fig  2   i e  what package offers the more appealing deal  While this approach is cognitively easy for users  it is far from trivial to reliably deduce usable information from a set of object comparison decisions  In the area of preference learning  such comparisons have been used to    learn    user preference for quite some while  11   Usually  the idea is to employ machine learning algorithms  e g  support vector machines  to approximate the weights of the user   s utility function  12   Unfortunately  it has been shown that quite a large number of object comparisons are necessary to estimate good utility functions  thus those ap  proaches usually either stay on a theoretical level  or are restricted to domains in which users cater for the required number of comparisons by heavy interac  tion  e g  web search result ranking   13    In this paper  we will showcase how objects comparisons can be used to elicit user trade offs and how these trade offs then can be heuristically ex  ploited to gain additional  if somewhat more general trade offs to further re  duce result set sizes  Furthermore  we will justify this heuristic by showing that the resulting semantic is analogous to a corresponding utility function  i e  that our qualitative trade offs are indeed a alternative to quantitative ap  proaches with respect to the order of result objects  albeit with improved ease Eliciting Skyline Trade Offs using Example Based Deduction Heuristics 7 of use  In the following we will motivate the individual steps leading to our trade off evaluation framework  Offering Trade Off Choices  First  it has to be decided which objects should be chosen for user comparisons  Different approaches for selecting candidates from the skyline are possible  a  candidates are selected randomly or the user freely chooses any two tu  ples him  herself  However  this imposes an unnecessary cognitive load  while on average also yielding low effects with respect to skyline reduction  b  candidates are selected considering the correlation of attribute domains  as e g   suggested in  18    This approach selects example objects based on the degree of attribute correlation  assuming that feedback on objects with opposing attribute values within anti correlated domains will result in a most informative trade off  This approach is easy to implement as it does only rely on the attribute values  c  candidates are selected employing semantic clustering  as e g   described in  15    Here  data objects are first semantically clustered by their perceived similarity  Resulting clusters usually represent different product groups  e g  netbooks or desktop replacements  Selecting the prominent representatives of clusters as candidates for comparison thus leads to strong and semantically meaningful trade offs  However  additional information  e g  content of re  view websites  blogs  or forums  is necessary such that the required semantic information can be mined  Trade Off Specification  As an example  assume in Fig  2 that a user pre  fers an Asus EEE  the EEE is a netbook  over a Dell Studio XPS  an all round notebook   Taking all the provided information this directly results in the ca  nonical        off  however  is quite weak due to its over specification  usually  a trade off trade off   is better than       This trade  8 Christoph Lofi  Ulrich Gu  ntzer  Wolf Tilo Balke will allow domination  additionally to the domination relationships already induced by the Pareto optimality criterion of the skyline computation  be  tween any object equal or better than its preferred candidate and any other object equal or worse than the second candidate  In the worst case  there are no such objects besides the EEE and Studio XPS notebook themselves  and thus only one object  the Studio XPS  would be removed from the skyline  Also  this trade off does not capture why the user preferred one notebook over the other     and this is exactly the missing semantics our heuristic aims to pro  vide  Heuristically Extending the Trade Off Information  Consider why a us  er might prefer a netbook over an all round notebook  obviously  the promi  nent advantage of a netbook is its cheap price and high mobility  light weight  small form factor   These two aspects are  when combined  strong enough to compensate all performance disadvantages a netbook might face and subse  quently lead to the specified trade off  Furthermore  this should also mean that whenever a user is willing to accept all the resulting disadvantages  he she should by all means also accept fewer disadvantages when the same ad  vantages are offered  This heuristic idea naturally leads to multiple conceptual trade offs offering views on the user   s implicit concepts driving the current trade off selection  In the case of our previous trade off for example         is better than         cheap   mobile compensates small screens  or        is better than         cheap   mobile compensates slow and inferior CPUs   Having motivated our workflow and the rationale behind it  we are now ready to formalize our concepts  Eliciting Skyline Trade Offs using Example Based Deduction Heuristics 9 3 Theoretical Foundations To be self contained  in this section we will briefly summarize the neces  sary theoretical foundations of Pareto trade offs and introduce the notions required to formalize our heuristic  For a comprehensive discussion on skyline trade offs  please refer to  12   Assume a database relation on attributes  Let be the set of all available attributes      For each attribute   a base preference is provided as a weak order over  please note that the limita  tion to weak orders is only chosen to simplify formal notations in this paper  of course all following theories can also be applied to more complex partial orders with equivalences  see e g   12    If some attribute value is preferred over another value  reads as     dominates to wrt  to      and      can be defined  For example  in the preferences in Fig  3 and hold  Now  the skyline can be computed by removing all dominated objects from the result set  An object dominates an object  written as   if is better or equal with respect to each attribute and strictly better with respect to at least one attribute  e g          with respect to   this is written as wrt  to       Similarly       is equal either dominates or is equal to wrt  to      10 Christoph Lofi  Ulrich Gu  ntzer  Wolf Tilo Balke  0     Corei7 0kg Core 2 Duo   Turion X2 Core 2 Solo   5000     Atom 8 kg Price Screen CPU Weight 1234 Fig  3  Example Preferences for Notebooks Finally  trade offs can be considered as a user preference between two par  tial tuples and utes  i e    complement set   focusing usually only on a subset of all available attrib  with   Furthermore  the is required for later considerations  Then     An example trade off using the 3 could be   with    and and of the trade off need to be Pareto incomparable  i e  they don   t dominate each other  if they did  then the trade off is either superfluous as it would not provide any</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_skyline_query_processing"/>
        <doc> Minimizing the Communication Cost for Continuous  Skyline Maintenance ### Zhenjie Zhang  Reynold Cheng  Dimitris Papadias  and  Anthony K H  Tung  May 2008 T e c h n i c a l   R e p o r t     Foreword    This technical report contains  a research paper  development or  tutorial article  which has been submitted for publication in a  journal or for consideration by  the commissioning organization   The report represents the ideas of its author  and should not be  taken as the official views of the School or the University  Any  discussion of the content of the report should be sent to the author   at the address shown on the cover        OOI Beng Chin  Dean of School Minimizing the Communication Cost for Continuous Skyline Maintenance Zhenjie Zhang Reynold Cheng Dimitris Papadias Anthony K H  Tung Department of Computer Science National University of Singapore  zhenjie  atung  comp nus edu sg Department of Computing Hong Kong Polytechnic University csckcheng comp polyu edu hk Department of Computer Science and Engineering Hong Kong University of Science and Technology dimitris cse ust hk ABSTRACT Numerous algorithms in the recent database literature deal with variants of skyline queries in di   erent problem settings  However  the existing work focuses on optimizing the processing cost  This paper aims at minimization of the communication overhead in client server architectures  where a server continuously maintains the skyline of dynamic objects  Our    rst contribution is a Filter method that avoids transmission of updates from objects that cannot in   uence the skyline  Speci   cally  each object is assigned a    lter so that it needs to issue an update only if it violates its    lter  The Filter method achieves signi     cant savings over the naive approach of transmitting all updates  Going one step further  we introduce the concept of frequent skyline query over a sliding window  FSQW   The motivation is that snapshot skylines are not very useful in streaming environments because they keep changing over time  Instead  FSQW reports the objects that appear in the skylines of at least    of the s most recent timestamps  The Filter method can be easily adapted to FSQW processing  however  with potentially high overhead for large and frequently updated datasets  To further reduce the communication cost  we propose a Sampling method  which returns approximate FSQW results without computing each snapshot skyline  Finally  we integrate the Filter and Sampling methods in a Hybrid approach that combines their individual advantages  We evaluate our techniques with extensive experiments ### 1  INTRODUCTION We consider a client server architecture  where the server receives records from various sources  A record ri has d   1  attributes  each taking values from a totally ordered domain  Therefore  it can be represented as a point pi in d dimensional space  and in the sequel we use the terms record tuple point object interchangeably  The server receives updates at discrete timestamps 1  2          t         An update alters the value of at least one record attribute  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Copyright 200X ACM X XXXXX XX X XX XX     5 00  and it corresponds to a movement of the respective point to a new position  Records can be inserted and deleted at any timestamp  Insertions and deletions can be thought of as movements from to a non existent position  e g    1   1       We use p t i to denote the status of point pi  record ri  at timestamp t  p t i    p t i  1   p t i  2           p t i  d    for each 1    k    d  A snapshot St   fp t 1  p t 2          p t ng at timestamp t contains all points alive at t  i e   all records that have been inserted before t  but have not been deleted at t  We say that p t i dominates p t j   if p t i  k  is at least as good as p t j  k  for all k  and there is an attribute l such that p t i  l  is better than p t j  l   The skyline Sky St  of a snapshot St is the subset of the records not dominated by any other point in St  Skyline computation has received considerable attention in both conventional databases and stream environments  However  the existing approaches  see Section 2  focus exclusively on the minimization of the processing cost  On the other hand  the aim of this paper is to reduce the network overhead due to update transmissions  Assume  for instance  a server that receives readings  e g   temperature  humidity  pollution level  from various sensors and continuously maintains the skyline of these readings in order to identify potentially problematic situations  e g   the most extreme combinations of values   The server has substantial resources so that optimization of its computational overhead is not critical 1   On the other hand  the sensor devices are usually battery powered and should conserve energy  Usually  uplink messages  sent to the server for updates  constitute the most important factor for energy consumption  8  and should be minimized  As another example  consider a system that monitors network tra   c such as Cisco NetFlow  In such systems  the server collects detailed tra   c logs on a per    ow granularity  which account for hundreds of GBytes of data per day  Skyline monitoring can be used to detect potential tra   c congestions  or attacks on the network  In this case minimization of updates is important for reducing the amount of network tra   c to the server  Our    rst contribution is a Filter method that avoids transmission of updates from objects that cannot in   uence the skyline  Speci   cally  the server computes  for each record  a hyper rectangle that bounds the value of each attribute  These rectangles are transmitted through downlink messages to the corresponding clients  e g   sen  1With emerging hardware architecture like the multi cores processor  the use of CPU resource to minimize communication cost will make even more sense sors   A client needs to issue an update only if the point has moved out of its    lter  i e   some attribute has exceeded the range imposed by the server   or if the server explicitly asks for the current attribute values  Filter can capture the exact skyline at each timestamp  and in most settings achieves signi   cant savings in terms of network overhead  However  in several applications  snapshot skylines may not be important because they change too fast to be meaningful  Furthermore  in the presence of communication errors and outliers in the data  it is more interesting to identify the records that consistently appear in the skyline over several timestamps  Motivated by this observation  we introduce the concept of frequent skyline query over a sliding window  FSQW   A window Ws t is as a set of s consecutive snapshots ending at t  i e  Ws t   fSt 1  s          Stg  A record constitutes a    frequent skyline point in Ws t if it appears in at least    snapshot skylines within the window  A FSQW continuously reports the frequent skyline points as the sliding window moves along the time dimension  Filter can be trivially adapted for exact processing of FSQW  However  despite its savings with respect to the naive method of transmitting all updates  it may still require a large number of update messages  To alleviate this problem  we propose a Sampling method  in which updates are transmitted at certain instances  depending on the desired trade o    between accuracy and message overhead  Finally  we integrate Filter and Sampling in a Hybrid method  which di   erentiates three modes for each record  When the server receives an update  it decides whether the corresponding point needs to switch to Filter  Sampling  or Hybrid mode  Hybrid combines the advantages of the other methods and has a balanced behavior even under extreme settings  where the performance of Filter and Sampling deteriorates  The rest of the paper is organized as follows  Section 2 reviews related work on skylines and minimization of the network overhead in other query types  Section 3 presents the necessary de   nitions and discusses some interesting skyline properties in our setting  Section 4 introduces Filter and analyzes its performance  Section 5 presents Sampling and provides guidelines for setting the sampling rate  Section 6 describes Hybrid and switching to di   erent modes  Section 7 evaluates our methods through extensive experiments  and Section 8 concludes the paper  2  RELATED WORK Skylines constitute a well researched topic due to their importance in multi criteria decision making and related applications  16  27   The skyline operator was    rst introduced to the database community in  3   Since then a large number of algorithms have been proposed for conventional databases  These methods can be classi   ed in two general categories depending on whether they use indexes  e g   Index  28   Nearest Neighbor  15   Branch and Bound Skyline  22   or not  e g     Divide and Conquer  Block Nested Loop 3   Sort First Skyline 7  10   Furthermore  skylines have been studied in the context of mobile devices  13   distributed systems  2   and unstructured  32   as well as structured networks  30   In addition  several papers focus on skyline computation when the dataset has some speci   c properties   4  extends Branch and Bound Skyline for the case where some attributes take values from partially ordered domains   4  focuses on skyline processing for domains with low cardinality   5  deals with high dimensional skylines  Finally  a number of interesting variants of the basic de   nition have been proposed  Skyline cubes  34  33  compute the skylines in a subset or all subspaces  Probabilistic skylines  23  asp1 q5 q6 safe rectangle  safe circle p2 q3 q1 q4 dist p1 q1  q2 dist p2 q3  range query Figure 1  Example of safe regions sume that each record has several instances  in which case the dominance relationship is probabilistic  Spatial skylines  25  return the set of data points that can be the nearest neighbors of any point in a given query set  A reverse skyline  9  outputs the records whose dynamic skyline contains a query point  The above methods deal with snapshot query processing and do not include mechanisms for maintaining the skyline in the presence of updates  On the other hand   31  proposes a space decomposition for re computing the skyline when a record is deleted   17  applies z ordering to achieve e   cient insertion as well as deletion   18  and  29  study skyline maintenance over sliding windows  utilizing some interesting properties to expunge records  before their expiration  that cannot become part of the skyline  Morse et al   19  assume streams  where the records are explicitly deleted or modi   ed independently of their arrival order  In all cases  the underlying assumption is that the server receives all updates  and the goal is to minimize its processing cost  Thus  these techniques are orthogonal to ours in the sense that they can be integrated within a system that optimizes both the processing and the transmission cost during skyline maintenance  Although minimization of the communication overhead in client server architectures is new to the skyline literature  it has been applied before to monitoring of spatial queries  Q index  24  assumes a central server that receives the positions of objects  while maintaining the results of continuous range queries  In order to reduce the number of location updates  the server transmits to each object a rectangular or circular safe region  such that the object does not need to issue an update as long as it remains within its region  Figure 1 illustrates the safe regions of two points p1 and p2  given six running range queries q1 to q6  While p2 is in its safe rectangle or circle  it belongs to the result of q4  As soon at it exits the safe region  it may stop being in the result of q4 and or start being in the range of q3  Similarly  p1 cannot in   uence any query while it remains within its safe region  Analogous concepts have also been applied to continuous nearest neighbors in  12  20   In data stream management systems  stream    lters have been used to o   oad some processing from the server  21  14  6  26   In particular  each stream source is installed with a simple    lter  so that a data item is sent to the central server only if its value satis   es the conditions de     ned in the    lters  For instance  Babcock and Olston  1  consider a scenario where a central server continuously reports the largest k values obtained from distributed data streams  Their method maintains arithmetic constraints at the stream sources to ensure that the most recently reported answers remain valid  Up to date information is obtained only when some constraint is violated  thus re ducing the communication overhead  These concepts are similar in principle to the proposed    lter method  with however an important di   erence  For spatial ranges  a safe region is based on the object   s location with respect to each query range  independently of the other objects in the dataset  For nearest neighbors  safe region computation takes into account just a few objects around the queries  typically  only the NNs   Similarly  the    lters used in DSMS can be easily computed using the conditions imposed by the query  On the other hand  for the skyline there are no queries  instead  the    lter of a record depends on the attribute values  or the    lters  of numerous other tuples  Therefore  as we show in the subsequent sections     lter computation in our context is more complex and expensive  3  PROBLEM DEFINITION AND PRELIMINARIES Our setting is a client server architecture  where the server receives record updates from various clients  We assume a time slotted system  where each client noti   es the server about updates at discrete timestamps  i e   there is a minimum interval dt between two consecutive updates of the same record  such that the round trip time of a message between the server and any client is negligible compared to dt  An uplink message refers to a transmission from a client to the server  and a downlink message to the opposite direction  If cu  Nu  resp  cd  Nd  is the cost and cardinality of uplink  resp  downlink  messages  the total transmission overhead of the system can be measured as cuNu   cdNd  Our goal is to minimize this cost  The status of record ri at time t corresponds to a point in the d dimensional unit space p t i    p t i  1   p t i  2           p t i  d    where 0    p t i  k     1 for each 1    k    d  Records can be updated or deleted at any timestamp after their insertion  i e   there is no particular order depending on their arrival  as in the sliding window model   A snapshot St at time t contains all records alive at time t  A window Ws t contains s consecutive snapshots ending at St  i e   Ws t   fSt 1  s          Stg  To simplify notation  we omit the timestamp  when it is clear from the context or not important for the discussion  Without loss of generality  in order to determine dominance relationships  we assume that smaller attribute values are preferable over larger ones  Definition 1  Point Dominance A point pi dominates another pj at time t  if p t i  k     p t j  k  for all k  and p t i  l    p t j  l  for at least one attribute l  A    lter F t i is a hyper rectangle that covers point p t i at time t  F t i is de   ned by d pairs of boundaries  F t i  l 1   F t i  u 1               F t i  l d   F t i  u d    where each pair  F t i  l k   F t i  u k   is the lower and the upper bound of the    lter on dimension k  F t i  l and F t i  u denote the lower left and upper right corner of the    lter  respectively  Intuitively  a    lter constrains a point whose exact location is unknown  A client needs to issue an update to the central server in two di   erent situations     lter failure and probe request  A failure occurs when a record ri moves out of its    lter Fi  otherwise  we say that Fi is valid  A probe request happens when the central server asks for the exact value of a record  Figure 2 illustrates a set of seven points and the corresponding    lters  If  for instance p1 causes a    lter failure  the corresponding client has to issue an update  Upon receiving the update  the server may probe for the current status of other records  e g   p2  in order to determine if there is a change in the dominance relationships  Note that in the example of Figure 1 the violation of a spatial p 1 x y F1 p 2 F2 p 3 F3 p 4 F4 p 5 F5 p 6 F6 p 7 F7 Figure 2  Example of skyline    lters    lter does not involve any probe because a range    lter is computed solely on the position of the object with respect to the queries  Next  we generalize the de   nition of dominance to capture the case where a record ri is represented either by a point pi  or a    lter Fi  Definition 2  Certain Dominance A record ri certainly dominates another rj at timestamp t  if at least one of the following conditions holds   1  p t i dominates p t j    2  F t i  u dominates p t j    3  p t i dominates F t j  l   4  F t i  u dominates F t j  l  where point dominance is based on De   nition 1  Definition 3  Possible Dominance A record ri possibly dominates another rj at timestamp t  if at least one of the following conditions holds   1  F t i  l dominates p t j    2  p t i dominates F t j  u   3  F t i  l dominates F t j  u  where point dominance is based on De   nition 1  Clearly  if ri certainly dominates rj   it also possibly dominates rj   but the opposite is not true  The concept of possible dominance is only applicable when at least one the two records is represented by a    lter and certain dominance cannot be established  When there is no ambiguity  we use the term dominance to also refer to certain dominance  In Figure 2  F2 u dominates F6 l  consequently r2 dominates r6  even if the exact attribute values of both records are unknown  provided that their    lters are valid   On the other hand  assuming that r5 and r6 are represented by F5 and F6  they both possibly dominate each other  note that F5 l dominates F6 u and F6 l dominates F5 u   Definition 4  Snapshot Skyline Sky St  A skyline Sky St  over snapshot St is the set of all alive records that are not dominated at timestamp t  Definition 5  Frequent Skyline Point A record ri is a    frequent skyline point in the window Ws t   if ri appears in at least    skylines within Ws t   Definition 6  Frequent Skyline Query over Sliding Window F SQW     Ws t   Given a threshold     F SQW     Ws t   returns the set of all    frequent skyline points over Ws t   Note that the snapshot skyline constitutes a special case of FSQW  where s and    equal 1  Figure 3 includes three consecutive snapshots over 7 records  At St  the skyline is Sky St    fp1  p2  p3  p4g  At St 1  p7 replaces p1 inSky St 1   At St 2  Sky St 2    fp2  p4  p7g  Assuming      0 5 and Ws t 2  F SQW 0 5  W3 t 2    fp2  p3  p4  p7g  If the threshold    is raised to 0 7  the result contains only two records  fp2  p4g  Given the skyline at each snapshot in Ws t   the server can calculate F SQW     Ws t   by simply counting the frequency of each record  A more interesting question is whether the server can obtain the exact F SQW results without computing the skyline at each timestamp  Theorem 3 1  Every algorithm that returns exact FSQW results must compute the skyline at each timestamp  Proof  Let A be an algorithm for FSQW processing that does not compute the skyline for some timestamp t  We can always construct a data set that leads A to wrong results as follows  If A misses a skyline point pi in Sky St   we generate a data set with pi in exactly       1 skylines at the following s    1 timestamps  At time t   s    1  A will omit pi from F SQW     Ws t s  1   although it should be reported  If A wrongly includes pi in Sky St   we also construct a data set with pi in exactly       1 skylines at the following s    1 timestamps  At time t   s    1  A will report pi in F SQW     Ws t s  1   although it should be excluded  The implication of the theorem is that the skyline computation at each timestamp is unavoidable for any exact FSQW algorithm  Hence  existing methods  18  19  29  for snapshot skyline maintenance could be adopted for FSQW processing  However  these techniques assume that each client transmits all updates to the server  although most do not in   uence the skyline  which is usually small compared to the data cardinality  In the next section  we utilize    lters to reduce the communication cost  The proposed method is applicable to both snapshot skylines and  consequently FSQW  processing  4  FILTER METHOD Section 4 1 introduces the general algorithmic framework of Filter  Section 4 2 analyzes the goals of    ltering and proposes a model for the update cost  Section 4 3 utilizes this model to provide a concrete algorithm for    lter generation  4 1 Framework Filter follows the framework summarized in Algorithm 1  For generality  we present the version for FSQW processing since it subsumes snapshot skyline computation  The server    rst receives the initial status of all objects  generates the skyline  computes the    lters  and transmits them to the clients  At every subsequent timestamp t  it re computes the current skyline and the result of each F SQW     Ws t   installed in the system  Note that depending on the application  there may be multiple FSQW with di   erent threshold    and window size s parameters  Finally  the server updates the    lters of the objects  if necessary   and sends them to the a   ected clients  Following the DSMS literature  we assume that processing takes place entirely in main memory  In the rest of the section  we cover the details of ComputeSkyline and FilterConstruction in Algorithm 1  Algorithm 2 illustrates skyline computation on the current snapshot St  If a record ri is certainly dominated by another rj according to De   nition 2  it is discarded immediately  If rj possibly dominates ri by De   nition 3  rj is inserted into a candidate dominator list L  After this round  if L is not empty  we need to continue in order to determine whether ri is in skyline  If the server has not received an update for ri at the current timestamp  because Fi is still valid   it sends a probe request to obtain Algorithm 1 Filter 1  Get the initial status of the records fr 0 1           r 0 ng 2  Sky S0   ComputeSkyline S0  3  FilterConstruction S0  4  Send    lters to the clients 5  for each timestamp t do 6  Receive updated record information 7  Sky St   ComputeSkyline St  8  for each ri in St and each F SQW     Ws t   do 9  if ri is a    frequent skyline point in Ws t then 10  Output ri as part of FSQW result 11  FilterConstruction St  12  Send updates to objects with new    lter Algorithm 2 ComputeSkyline  Snapshot St  1  Construct skyline bu   er S 2  Construct a candidate dominator list H 3  for each record ri do 4  Clear H 5  for each record rj  j  6 i  do 6  if rj certainly dominates ri then 7  Discard ri and go to  3  8  if rj possibly dominates ri then 9  Append rj to H 10  if H is not empty then 11  if Fi is still valid then 12  p t i  Probe ri  13  if any object in H certainly dominates p t i then 14  Discard ri and go to  3  15  for each rj 2 H with valid Filter Fj do 16  p t j  Probe rj   17  if p t j dominates p t i then 18  Discard ri and go to  3  19  Append ri into skyline 20  Return S its current status p t i   If after the probe  any object in L dominates p t i   ri is discarded  Otherwise  the server probes the up to date versions for all records in the candidate list  If no point dominates p t i   p t i is inserted into the skyline  Note that Algorithm 2 minimizes the number of probes  by    rst resolving dominance relationships that do not require any probes  Then  it obtains the current status of ri with a single probe  Only if ri remains a skyline candidate after the above tests  the server probes records in L  Given a set of properly generated    lters  it is easy to verify the correctness of the algorithm since each record is compared against every other record  unless it is dominated  All ambiguities regarding dominance relationships are resolved by probes  Therefore  the skyline contains all non dominated records and no false hits  FilterConstruction generates a    lter Fi for each record ri without a valid    lter  Before proceeding to its description  we study some    lter requirements  Recall that    lter failures trigger updates  which in turn determine the skyline  To detect all skyline updates  the    lters should be constructed in way that guarantees that as long as there is no    lter failure  there cannot be any changes in the skyline  The following lemmas provide su   cient conditions for the above requirements  Lemma 4 1  A record ri is in the skyline  if no    lter Fj  j  6 i  possibly dominates Fi  This lemma implies that for any current skyline record ri and any other tuple rj   there is at least one dimension k such that Fi u k    Fj  l k  by De   nition 2  According to this property  no record can dominate ri before at least a    lter violation occurs p 1 x y p 2 p 3 p 4 p 5 p 6 p 7  a  At time t p 1 x y p 2 p 3 p 4 p 5 p 6 p 7  b  At time t   1 p 1 x y p 2 p 3 p 4 p 5 p 6 p 7  c  At time t   2 Figure 3  Examples of snapshot skylines and FSQW Lemma 4 2  A record ri is not in the skyline  if there is at least one    lter Fj  j  6 i  that certainly dominates Fj   The second lemma implies that for any point ri that is not in the skyline  there is a skyline point rj such that Fj dominates Fi by De   nition 3  Therefore  ri cannot become part of the skyline  unless there is a failure of Fi or Fj   If Fj certainly dominates Fi  we say that  Fj   Fi  is a    lter dominance pair  or Fj is the dominator of Fi  A    lter set fF1  F2          Fng is said to be robust  if each skyline record satis   es Lemma 4 1 and each non skyline record satis   es Lemma 4 2  The    lter set of Figure 2 is robust because  i  none of F1  F2  F3  F4 is possibly dominated and  ii  all non skyline    lters are certainly dominated by a skyline    lter  F2 is the dominator of F5  F6  and F3 is the dominator for F7   Algorithm 3 describes the construction and maintenance of a robust    lter set  The lower and upper bounds of each new    lter are initialized to 0 and 1  respectively  Then  the server gradually shrinks the    lters  following di   erent methods for skyline and non skyline records  Speci   cally  if ri is in the skyline  steps 3 to 10   two values V and Q  V   Q  are selected between ri and every other record rj on a chosen dimension k  V and Q are used to update the upper bound of F t i and lower bound of F t j   respectively  These steps enforce Lemma 4 1  If ri is not in the skyline  steps 11 to 18   the server selects a dominator rj for ri and performs similar split operations on every dimension k  enforcing Lemma 4 2  For simplicity  the pseudo code does not distinguish whether rj has a valid    lter not  In the former case  the algorithm uses F t j  k  instead of p t j  k  without a   ecting correctness  Note that    lter construction is identical for both the initial and subsequent timestamps  The di   erence is that in the former case none of the records has a valid    lter  Algorithm 3 satis   es the two lemmas  but it does not specify concrete techniques for the selection of the best dimension  V   Q values  or the dominator of non skyline records  For instance  Figure 4 illustrates an alternative    lter set for the records of Figure 2  where the size of F2  F4 has increased  while that of F1  F3 has decreased  The choice of the    lter set may have a signi   cant e   ect on the performance of the system  The next section analyzes the communication overhead  in order to provide guidelines for setting these parameters  4 2 Cost Model Before proceeding to the details of the proposed model  we qualitatively analyze the update cost of a    lter based method through the following theorem  Algorithm 3 FilterConstruction  Snapshot St  1  for each record pi 2 St without valid    lter do 2  Initialize F t i with F t i  l k    0 and F t i  u k    1 for all k 3  for each record pi 2 Sky St  without valid    lter do 4  for each record rj 2 St  j  6 i  do 5  Choose a dimension k that p t i  k    p t j  k  6  Choose two values V  Q that p t i  k     V   Q    p t j  k  7  if F t i  u k    V then 8  F t i  u k    V 9  if rj has no valid    lter AND F t j  l k    Q then 10  F t j  l k    Q 11  for each record pi 62 Sky St  without valid    lter do 12  Choose a dominator rj for pi 13  for each dimension k do 14  Choose two values V  Q that p t j  k     V   Q    p t i  k  15  if F t i  l k    V then 16  F t i  l k    V 17  if rj has no valid    lter AND F t j  u k    Q then 18  F t j  u k    Q 19  Return the    lter set p 1 x y F1 p 2 F2 p 3 F3 p 4 F4 p 5 F5 p 6 F6 p 7 F7 Figure 4  Alternative    lter set Theorem 4 1  Any method A following the framework of Algorithm 1 incurs update cost cu X  Y    cd X  2Y    where X is the number of    lter failures  Y the number of probe requests and cu  resp  cd  is the cost of an uplink  resp  downlink  message  Proof  For each    lter failure  the server receives one uplink message from a client and responds with a downlink message for a    lter update  For each probe  the server sends a request  receives a response  and transmits back a new    lter  i e   two downlink and one uplink messages   Therefore  if there are X    lter failures and Y probe requests  the update cost is at least cu X  Y    cd X  2Y    Since there is no additional communication overhead in the system  this is also the total cost  According to the previous theorem  in order minimizep  2  p  1  l  1  l  2  l  3  l  4  x  y  F  1  F  3  d i s t a n c e s   b e t w e e n  p          a n d   b o u n d   o f  2  F  1  Figure 5  Examples of probe requests the update cost  we have to reduce the number of    lter failures and probe requests  However  these tasks are contradictory and di   cult to optimize  For instance  the enlargement of F2 in Figure 4 compared to Figure 2  reduces the number of F2 violations  At the same time it causes the reduction of other    lters  e g   F1  F3   so that they may fail sooner and lead to probes on r2  A good    lter for ri  should balance the probabilities of    lter failure and probe request  If Pf  Fi  is the probability of    lter failure and Pr Fi  is the probability of probe request on Fi  the expected update cost of Fi is C Fi    cu Pf  Fi    Pr Fi     cd Pf  Fi    2Pr Fi    Fi is optimal if it can minimize the expected update cost compared to any other    lter for pi  The optimal    lter set fF1          Fng should minimize X i C Fi    X i   cu   cd Pf  Fi     cu   2cd Pr Fi   The next step concerns the derivation of Pf  Fi  and Pr Fi   The probability of    lter failure Pf  Fi  can be estimated based on the shortest time that a violation can occur in Fi  Given a record ri with    lter Fi and maximum rate of change Ci 2   the shortest time that ri can reach the lower bound on dimension k is  ri k   Fi l k   Ci  Similarly  the shortest time to reach the upper bound on dimension k is  Fi u k     ri k   Ci  Therefore  the probability to have a failure of Fi on any dimension is Pf  Fi    1 2d X k Ci Fi u k     ri k    Ci ri k     Fi l k  As for the probability of a probe request  we note that there are two types of probes  The    rst happens when the previous dominator rj ceases to dominate a non skyline record ri  The server needs the current status of ri in order to determine if it becomes part of the skyline  Similar to the case of    lter failure  we can estimate the probability using the distances from the dominator rj to the lower bounds of Fi on all dimensions  Figure 5 shows the distances between the current location of dominator p2 and the lower bounds of F1  Based on these distances  the average probability for the    rst type of probes on a    lter Fi can be estimated as P 1 r  Fi    1 d X k Cj Fi l k     rj  k  The second type of probe request occurs to Fi when ri possibly dominates another record rj   in which case the 2 Ci can be visualized as the maximum distance that pi can move between two consecutive timestamps  1  P  r  P  P  f  2  P  r    1    1  t    1    p  2  t  p    1    1  l  1      3  l  Figure 6  Probability functions on boundary value server needs to determine whether rj is in skyline  The status of rj is recorded as a probe source  The server stores the most recent M probe sources in an array L   fl1  l2          lMg to approximate their distribution  Given a    lter Fi  the probability of second type probe requests on Fi is estimated by the ratio of recorded probe covered by UDR Fi  over their total number  UDR Fi  is the area dominated by Fi l  but not dominated by Fi u  Any point in UDR Fi  is possibly  but not certainly  dominated by Fi  P 2 r  Fi    jfli 2 L j li 2 UDR Fi gj M In Figure 5  two out of the four probe sources  l1  l2  l3 and l4 are in UDR F1   while only l1 is in UDR F3   Therefore  according to the previous de   nition P 2 r  F1    1 2 and P 2 r  F3    1 4  Intuitively  P 2 r  Fi  use the past L probes to estimate the likelihood that a further probe will come from some point within UDR Fi  which will cause Fi to become invalid  The    rst type of probes only happens to non skyline records  while the second one can occur to all records  Therefore  the overall probability of a probe request can be summarized as  Pr Fi    P 2 r  Fi    if p t i 2 Sky St  P 1 r  Fi    P 2 r  Fi   if p t i 62 Sky St  If we wish to obtain the optimal lower  or upper  bound x on dimension D1 for    lter F1 with the rest of the bounds constant  the probabilities Pf  F1   P 1 r  F1  and P 2 r  F1  can all be expressed as a function with a single variable x  Given the two objects in Figure 5  Figure 6 presents the probability functions of Pf   P 1 r and P 2 r on x   F1 l k   Since p2 is the dominator for p1  the valid range of x   F1 l 1  is in the interval  p t 2 1   p t 1 1    otherwise there will be a violation of Lemma 4 2  Pf  F1  is a monotonically increasing function on x  since the increase of the lower bound will decrease the distance from p t 1 to the boundary  P 1 r  F1  is a monotonically decreasing function because a smaller lower bound allows the dominator to move away more easily  P 2 r  F1  is a step wise constant function on different intervals  depending on the content of L  Since the update cost C F1  is a weighted sum of the three di   erent probabilities  it must be represented by a complicated function on x  where the optimal x is the global minimum  However  if we split the interval  p t 2 1   p t 1 1   into three subintervals   p t 2 1   l1 1     l1 1   l3 1   and  l3 1   p t 1 1    then on each sub interval  the second order derivative on the cost function is always positive  This implies that the local minimum in each interval can be found e   ciently  Since the number of intervals is decided by the cardinality of L Algorithm 4 ChooseBoundary  pair fri  Fig  pair frj   Fjg   dimension k  probe history L   fl1  l2          lMg  1  Construct Li   flm k  j lm 2 L    lm 2 DR Fi g 2  Split the interval  p t i  k   Fj  l k   into at most jLij   1 subintervals  fI1          IjLij 1g  with split points at each lm k  2 Li 3  Set C     C Fi   V      Fi u k  4  for each interval Im do 5  let V be the largest dimension k value within Im 6  Construct F 0 i by using V instead of Fi u k  7  if C F 0 i     C   then 8  C     C F 0 i   and V      V 9  if Fj has no valid    lter then 10  Split the interval  V      p t j  k   into sub intervals  fJ1          JjLij 1g with split points at each lm k  2 Li 11  Set C     C F 0 j   and Q     Fj  l k  12  for each interval Jm do 13  Let Q be the smallest dimension k value in Jm 14  Construct F 0 j by using Q instead of Fj  l k  15  if C F 0 j     C   then 16  C     C F 0 j   and Q     Q 17  Return  C F 0 j     C F 0 i    V      Q     18  else 19  Return  C Fj     C F 0 i    V      Fj  l k   we can derive algorithms for    nding the optimal x with acceptable cost  4 3 Choice of Filters Given the cost model of the last subsection  a    lter construction algorithm should generate a    lter set minimizing the total estimated update cost P i C Fi   Doing so optimally  however  leads to a NP complete problem similar to MAX SAT  Instead  we propose a greedy heuristic  Recall that Algorithm 3 de   nes the boundaries for each pair of    lters fFi  Fjg by selecting V and Q between r t i  k  and r t j  k   to replace the old upper bound of Fi and lower bound of Fj on dimension k  By Theorem 4 1  the central server only updates    lters for those records that have invalid    lters  Thus  the proposed greedy algorithm only considers two cases   1  one of ri and rj has a valid    lter  and  2  neither ri nor rj has valid    lter  Algorithm 4 summarizes    lter boundary construction for the case of skyline points  modi   cations for non skyline records are discussed shortly   The input consists of ri  rj   their    lters Fi  Fj   a speci   ed dimension k  and L  the set of M most recent probes l1     lM   The output contains three values  the reduction on the estimated update cost if the server chooses dimension k to split  and the optimal values of V and Q for this dimension  Given Algorithm 4  we can replace steps 5 and 6 of Algorithm 3 as follows  To choose a dimension k  the server    rst executes Algorithm 4 d times on each dimension  From the d returned tuples  it chooses the one with the maximum reduction on update cost and modi   es the upper bound and lower bounds accordingly  The optimal V and Q are calculated separately  Since ri has no valid    lter  the algorithm    rst derives V   a new upper bound for Fi on dimension k  steps 1 to 8   To do so  we must estimate the value of V that minimizes   cu   cd Pf  Fi     cu   2cd P 2 r  Fi  based on our cost model  While Pf  Fi  can easily be computed  P 2 r  Fi  has to be estimated using the position of the M most recent probe sources  Since Q must fall within the interval  p t i  k   Fj  l k    we break this interval into at most jLj subintervals I1     Im by projecting l1      lm onto dimension k  and select the best V in each interval to minimize the expected update cost  This involves a simple linear scan of the intervals since P 2 r  Fi  changes only across intervals  and P f  Fi  is minimized at the largest dimensional k value for each interval  The best value V    among all the intervals is then selected  If Fj has a valid    lter  the algorithm stops here and returns the new boundaries  V      Fj  l k    Otherwise  a new value Q    is calculated in a similar manner  step 9 to 19   and a new pair of boundaries  V      Q      is returned  Algorithm 4 is modi   ed for generating the    lter of a nonskyline point as follows  First  new lower bounds must be computed on all dimensions  instead of one  Second  by Lemma 4 2  given a non skyline record ri  there is a dominator rj dominating ri on all dimensions  Consequently  for ri  we should execute the Algorithm 4 nd times  where d is the dimensionality and n is the number of dominators  The best dominator has the largest update cost reduction sum on all dimensions  5  SAMPLING METHOD By Theorem 3 1  any exact algorithm for FSQW must compute the skyline at each snapshot  potentially leading to high update cost  Section 5 1 introduces Sampling  which outputs approximate results of FSQW  Section 5 2 discusses the setting of the sampling rate  5 1 Framework In Sampling  at any time t  all clients collectively report their current status to the central server with probability R  In order to achieve this  the server sends the same random seed S to clients  which enables them to generate the same random sequence  Based on this sequence  a client knows the time of the next update  The only downlink message is transmitted at the    rst timestamp when the client joins the system  This message contains the random seed as well as the sampling probability R  shared by all clients  Assuming the sequence fx1  x2          xt       g  0    xi    1  with uniform distribution between 0 and 1 for every client  the client will issue an update to the server at timestamp t if xt    R  Algorithm 5 summarizes F SQW processing at the server  The number of sampled snapshots in the current window is stored in a snapshot counter T  Each record ri in the system has a skyline counter Ci that keeps the skyline frequency of ri on the sampled snapshots  The server also maintains bu   ers with the skyline set on each sampled snapshot  At time t  if the current snapshot St is scheduled to be sampled  the algorithm computes the skyline and increments the counter Ci of each skyline record ri  as well as T  If the snapshot St  s was previously sampled  the server decreases T by 1  and decrements the skyline counters for each object in Sky St  s   Points with skyline counter exceeding   T are reported as results of F SQW     Ws t    5 2 Parameter Setting The sampling probability R determines the trade o    between accuracy and communication overhead  The following analysis provides guidelines for the choice of R  Lemma 5 1  If we have 2 ln 1        2   sampled snapshot in the current window Ws t   any point reported by Algorithm 5 at timestamp t has skyline frequency larger than          s with probability 1        This lemma is an easy extension of the Cherno    bound  11  and implies that the F SQW result is robust enough if we can guarantee that the number of sampled snapshots in every sliding window is more than 2 ln 1        2    Algorithm 5 Sampling 1  Calculate the sampling probability R 2  Send R and S to all objects 3  for each 1    i    n do 4  Ci   0 5  Send S and R to ri 6  for each timestamp t do 7  if sampling snapshot St is scheduled then 8  T   T   1 9  Receive updates from all objects 10  Compute Sky St  11  for each pi 2 Sky St  do 12  Ci   Ci   1 13  if snapshot St  s was previously sampled then 14  T   T    1 15  for each ri 2 Sky St  s  do 16  Ci   Ci    1 17  Output all objects with Ci      T Theorem 5 1  When R    q ln 1     2s   2 ln 1        2  s   any point reported by Algorithm 5 at any time has skyline frequency larger than           s with probability 1    2   Proof  In the    rst part of the proof  we want to show that the probability of having more than 2 ln 1        2   sampled snapshots is larger than 1       in any sliding window  This is proven by using Chebychev   s inequality of binomial distribution  If X is the number of sampled snapshots  we have Pr X    x     exp    2 sR    x  2 s By replacing x with 2 ln 1        2   and R with q ln 1     2s   2 ln 1        2  s   the probability is less than     Thus  there is 1       probability to get enough samples  The second part of the proof is done by considering the correctness of the frequent skyline points  By applying Lemma 5 1  the probability of outputting true frequent skyline point is at least 1        Therefore  the probability of both above events happening at the same time is  1            1           1    2    This completes the proof of the theorem  Given the input requirements on the error rate    and the con   dence     the minimum acceptable sampling rate can be calculated using Theorem 5 1  6  HYBRID METHOD Filter exploits the fact that often updates are gradual and infrequent  Subsequently  it achieves signi   cant savings for records that exhibit these properties  However  highly dynamic objects  involving abrupt and or very frequent updates  incur a large number of    lter failures  which may cancel its advantage  Sampling  on the other hand  is oblivious to the update characteristics  implying that it may incur unnecessary uplink messages for records that are rather stable  Motivated by these observations  Section 6 1 proposes Hybrid  which integrates    ltering and sampling in a common framework  In Hybrid  each record is in one of the following modes     lter mode FM   sampling mode SM  or mixed mode MM   Section 6 2 discusses the transition between modes  6 1 Framework For records in FM or MM     lters are constructed and maintained following the framework of Filter  i e   at each Mode Condition FM Filter update cost is smaller than sampling cost SM Sampling cost is smaller than    lter update cost MM The point is in    lter mode and in current skyline Table 1  Record modes and their conditions Algorithm 6 Hybrid 1  Calculate the sampling probability R 2  Send R and S to all objects 3  for each 1    i    n do 4  Ci   0 5  Set each object in FM 6  Send S and R to ri 7  for each timestamp t do 8  Receive updates from the objects 9  if sampling snapshot St is scheduled then 10  T   T   1 11  Sky St   ComputeSkyline St  12  for each pi 2 Sky St  do 13  Ci   Ci   1 14  Construct set P F with all objects in FM or MM 15  Sky P F   ComputeSkyline P F  16  FilterConstruction Sky P F   17  if snapshot St  s was previously sampled then 18  T   T    1 19  for each ri 2 Sky St  s  do 20  Ci   Ci    1 21  Switch the mode of the objects if necessary 22  Output all objects with Ci      T timestamp the server receives failure messages and transmits probe requests  For records in SM or MM  snapshots are sampled according to the Sampling framework  The motivation behind MM is that skyline records are more important  than non skyline points  for the accuracy of FSQW results  Thus  MM ensures that they are regularly sampled  even if they are in FM mode 3   Non skyline records are either in FM or SM  depending on their estimated update cost to be discussed shortly  Table 1 summarizes the conditions governing the setting of the modes  Algorithm 6 presents the general framework of Hybrid  Compared to Algorithm 5  there are two di   erences  The    rst is an additional step to update the    lters for records in FM and MM  which requires the computation of the local skyline on all records maintained by    lters  steps 15 16   The second di   erence is the mode switching test conducted on each record  step 21   Figure 7 shows an example where r6 is in FM  r2  r5 and r7 are in SM  while r1  r3 and r4 are in MM  Therefore  the    lters are constructed based only on r1  r3  r4 and r6  Compared to Figure 2  the    lters are now larger  On the other hand  since r2 is in sampling mode  there is no    lter update  even if r2 moves inside F1 or F3  Lemma 6 1  Algorithm 6 outputs the correct skyline set at each sampling snapshot  Proof  Any record in FM cannot be in the skyline because there must be some skyline point in MM dominating it and bounding its    lter  On the other hand  all tuples not in FM must issue updates at each sampling timestamp  Thus  the skyline of records in MM and SM is the correct skyline for the entire snapshot  The above theorem implies that the results of FSQW reported by Hybrid must be the same as that of Sampling  leading to the following corollary of Theorem 5 1  3 Note that a skyline record in SM does not have to be in FM p 1 x y F1 p 2 p 3 F3 p 4 F4 p 5 p 6 F6 p 7 MM MM MM SM SM SM FM Figure 7  Example of Hybrid Corollary 1  If the sampling rate R    q ln 1     2s   2 ln 1        2  s   any frequent skyline point reported by Hybrid is in the skyline for at least           s snapshots with con   dence 1    2    6 2 Mode Switching The choice of the proper mode of each record is crucial for Hybrid  MM is only used for skyline tuples in FM  Therefore  we focus on the transition between FM and SM  The server keeps the number of messages related to each record ri in FM  When this number exceeds the sampling rate R by a pre de   ned factor  in our implementation we use 2R   ri is switched to SM  in order to reduce the transmissions  The corresponding client will receive the random seed sent by the server to start the synchronized sampling process  If ri is in SM  the server needs to estimate the transmission cost if ri were in FM  This is accomplished by simulating a virtual    lter over ri  which is updated at every sampling timestamp according to the current locations of the objects in FM  If the    lter does not need any update after a number of consecutive sampling timestamps  in our implementation we use 2   the server switches ri to SM  7  EXPERIMENTS This section evaluates experimentally the proposed methods and compares them against Naive  a baseline algorithm that transmits all updates to the server without incurring downlink messages  We only include FSQW queries because  as discussed in Section 3  snapshot skylines constitute a special case of FSQW where s and    equal 1  All experiments are executed on a PIII 1 8GHz CPU  with 1GB main memory  The programs are compiled by GCC 3 4 3 in Linux system  Section 7 1 describes the experimental setup  Sections 7 2 and 7 3 present the results on synthetic and real data  respectively  7 1 Experiment Setup The real data used in the experiments were collected by Lawrence Berkeley Laboratory  They contain a 30 day trace of the TCP connections between their local network and the internet 4   The remote IP addresses in all connections are divided into groups  according to the    rst 24 bits of their IPs  For example     172 18 179 20    and    172 18 179 38   are in the same group  while   172 18 180 22    is not  The connections are classi   ed into four categories based on their protocol type  NNTP  TCP DATA  SMTP and OTHERS  By taking the snapshot every 100 seconds  an address group Gi dominates another group Gj at time 4 http   ita ee lbl gov html contrib LBL CONN 7 html Parameter Range Dimensionality 2 3 4 5 6 Distribution C I A Data Size  K  2 5 5 10 20 Threshold    0 6 0 7 0 8 0 9 Error rate    0 1 0 2 0 3 0 4 Con   dence    0 05 0 1 0 15 0 2 Uncertainty u 0 01 0 02 0 04 0 08 0 16 Table 2  Experimental parameters t  if Gi has no fewer connections than Gj on all four types in the last 1000 seconds and more on at least one category  The skyline contains the non dominated groups  Given the original data set with 782281 connections recorded in 2591987 seconds  we transform it into a new 4 dimensional data set with 25920 snapshots and 7776 address groups  The synthetic data sets are created using the standard static data generator  3  with three common distributions  independent I   correlated C  and anti correlated A   Every attribute on every dimension is a real number between 0 and 1  We introduce uncertainty on the generated dataset by applying an uncertainty parameter u  Speci     cally  an object is allowed to move within the space de   ned by a rectangle with edge extent 2u and center at the original position  At each timestamp t  the locations of the objects follow uniform distribution in their corresponding rectangles  Table 2 illustrates the range and default values  in bold  of the parameters involved in our experiments  Recall that        are used to tune the sampling rate  and are applicable only to Sampling and Hybrid  For all experiments  we set the window size of FSQW queries to 1000  i e   a query returns the frequent skyline points in the last 1000 timestamps   We evaluate the performance of the algorithms on six measures  The    rst three are the number of uplink  downlink  and total messages  Assuming that the uplink  cu  and downlink  cd  costs are equal  the total number of messages re   ects the overall transmission cost  The fourth measure is the CPU time  The above measures assess the e   ciency of the algorithms  The last two measures  recall and precision  assess the quality of the query results  averaged over all timestamps  Speci   cally  if At is the result of algorithm A and F St is the correct result at time t  recall is de   ned as the ratio jAt   F Stj jF Stj and precision as jAt   F Stj jAtj  7 2 Results on Synthetic Data Sets The experiments are divided into two parts  First  we test the e   ciency of the algorithms on update and CPU cost  Then  we focus on the quality of the output by Sampling and Hybrid  7 2 1 Ef   ciency Test Figure 8 presents the e   ciency measures as a function of dimensionality for the independent distribution  We use acronym FBM for Filter  SBM for Sampling and HM for Hybrid  In 2D space  FBM outperforms the other methods in terms of transmission cost due to its advantage on the number of uplink messages  However  its overhead increases dramatically with the dimensionality  and when the dimensionality reaches 6  it is outperformed even by Naive  SBM has the best overall behavior since it is independent of the dimensionality  HM requires fewer messages than SBM in the 2D dataset  but in general its performance lies between that of SBM and FBM  Although Naive does not incur any downlink messages  it is the worst method in all but one settings  0  20  40  60  80  100  120  140 2 3 4 5 6 number of messages  1e 5  dimensionality Naive SBM FBM HM  a  Uplink Messages  0  10  20  30  40  50  60 2 3 4 5 6 number of messages  1e 5  dimensionality Naive SBM FBM HM  b  Downlink Messages  0  20  40  60  80  100  120  140 2 3 4 5 6 number of messages  1e 5  dimensionality Naive SBM FBM HM  c  All Messages  0  500  1000  1500  2000  2500 2 3 4 5 6 CPU time  sec  dimensionality Naive SBM FBM HM  d  CPU time Figure 8  E   ciency vs  dimensionality  independent  Figure 9 tests the algorithms on di   erent types of distributions  FBM has a clear advantage when the records are positively correlated and the skyline cardinality is small  However  if the distribution is anti correlated  most of the object are in skyline  Consequently  the    lter updates are so frequent that each record incurs one uplink message due to    lter failure and one downlink message for the new    lter per timestamp  The update cost of HM is stable even on anti correlated distributions  on which most of the objects are in sampling mode  instead of    lter or mixed mode  SBM has again good overall performance in terms of both network and CPU overhead   0  20  40  60  80  100  120  140 C I A number of messages  1e 5  distribution 100 100 100 3 13 85 17 17 17 27 30 33 Naive SBM FBM HM  a  Uplink Messages  0  20  40  60  80  100 C I A number of messages  1e 5  distribution 0 0 0 3 14 92 26 30 34 Naive SBM FBM HM  b  Downlink Messages  0  50  100  150  200 C I A number of messages  1e 5  distribution 100 100 100 7 27 178 17 17 17 53 60 68 Naive SBM FBM HM  c  All Messages  0  500  1000  1500  2000  2500  3000  3500 C I A cpu time  sec  distribution 87 229 987 299 1075 3121 65 88 220 516 1089 848 Naive SBM FBM HM  d  CPU time Figure 9  E   ciency vs  distribution  4D  In Figure 10  we vary the uncertainty parameter u on the synthetic data sets  For small values of u  objects move within a small range of the underlying space  FBM and HM take advantage of the locality property  since the records are usually bounded by a stable    lter  The update cost of HM is sometimes lower than that of FBM when the uncertainty is below 0 2  as it switches the more uncertain objects to sampling mode  However  the CPU costs of FBM and HM are still much worse than that of SBM   1  10  100  1000 0 01 0 02 0 04 0 08 0 16 number of messages  1e 5  uncertainty Naive SBM FBM HM  a  Uplink Messages  0  10  20  30  40  50 0 01 0 02 4 0 08 0 16 number of messages  1e 5  uncertainty Naive SBM FBM HM  b  Downlink Messages  1  10  100  1000 0 01 0 02 0 04 0 08 0 16 number of messages  1e 5  uncertainty Naive SBM FBM HM  c  All Messages  10  100  1000  10000  100000 0 01 0 02 0 04 0 08 0 16 cpu time  sec  uncertainty Naive SBM FBM HM  d  CPU time Figure 10  E   ciency vs  uncertainty u  independent 4D  Figure 11 evaluates the e   ciency measures as a function of the number of records in the system  Our methods scale better than Naive and their transmission overhead is linear to the data cardinality  The CPU cost has quadratic complexity because of the dominance checks  in all algorithms  and the    lter computations  in FM  HM    0  50  100  150  200  250  300  350  400 2 5 5 10 20 number of messages  1e 5  number of objects  1000  Naive SBM FBM HM  a  Uplink Messages  0  20  40  60  80  100  120 2 5 5 10 20 number of messages  1e 5  number of objects  1000  Naive SBM FBM HM  b  Downlink Messages  0  50  100  150  200  250  300  350  400 2 5 5 10 20 number of messages  1e 5  number of objects  1000  Naive SBM FBM HM  c  All Messages  0  2000  4000  6000  8000  10000  12000 2 5 5 10 20 cpu time  sec  number of objects  1000  Naive SBM FBM HM  d  CPU time Figure 11  E   ciency vs  data cardinality  independent 4D  7 2 2 Result Quality Test Since Naive and FBM output exact FSQW results  in this part of the experiments we focus on the recall and precision of SBM and HM  By Theorem 5 1 and Corollary 1  the sampling rate of SBM and</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09skp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_skyline_query_processing"/>
        <doc>Query oriented Relaxation for Cardinality Assurance ### Manasi Vartak Department of Computer Science  Worcester Polytechnic Institute  Massachusetts  USA mvartak wpi edu ABSTRACT Although a large number of queries used in applications ranging from web search to business intelligence have associated cardinality constraints  current database engines have minimal support for ensuring query cardinality  This leads to two main types of problems  the empty result set problems and the too few too many problems  Inability to meet query cardinality constraints requires the user to undertake a frustrating trial and error process that can be extremely cumbersome and time consuming  In fact this process wastes system resources without any guarantee of success  A possible strategy for solving the cardinality assurance problem is query relaxation  However  obtaining exact cardinality assurance with query relaxation has been proven to be NP Hard  In this work we present QRelX  a novel algorithm for cardinality assurance using query relaxation  QRelX not only relaxes queries ef   ciently but also ensures that result queries have minimal relaxation  Our algorithm uses QXForm  a query space transformation framework  to enable the relaxation of join and select queries  Following this transformation  QRelX uses layer based navigation and incremental cardinality estimation to    nd result queries that meet the expect cardinality but are also close to the original query  Our preliminary experimental results indicate that QRelX is computationally more ef   cient than traditional cardinality assurance algorithms and it indeed successfully    nds minimal relaxation queries ### 1  INTRODUCTION A large number of database queries used every day have implicit or explicit cardinality constraints  For instance  consumer oriented applications like web search  shopping or travel often have implicit cardinality constraints  For a satisfactory user experience a travel application should not return only one result and similarly a shopping application should not return    ve thousand results  Explicit cardinality constraints on the other hand are found in domains such as information retrieval  business intelligence and others  In these domains  satisfying the query cardinality can be as essential as answering the query itself  For instance  in order to run a medical study  a research agency may need a    xed number of volunteers satisfying certain health related criteria  Likewise  a supermarket may need to send out surveys to a    xed number of customers having particular shopping trends  However  in spite of the importance of cardinality constraints  state of the art database systems have minimal support for cardinality assurance  The lack of query support for cardinality assurance leads to two common types of problems  empty result set problems  15  and few many problems  18   The former type arises from queries that return no results  also called failing queries  20   while the latter  a superset of the former  arises from queries whose result size is too large or too small for the user   s purpose  In both cases the user is offered neither an explanation for the inconsistency in cardinality nor any suggestions for remedying it  The burden of formulating precise queries attaining the required cardinality falls on the user  However  this can be a dif   cult task because the user seldom has knowledge about the underlying database  Instead he or she has to resort to a frustrating trial and error process  Moreover  although there is no guarantee that this process will produce appropriate results  the user needs to try potentially numerous queries  This repeated query execution leads not only to a large increase in the query   s response time but also a decrease in server throughput   a  Query Q1   b  Query Q2  Q2     SELECT  WHERE   FROM Flight  Hotel  Flight price    7 Hotel price   1500  AND   beachDist   5  Expected Cardinality   20 Q1     WHERE SELECT   FROM People  25   age   75   AND  BMI   30  AND  familyHist   0  AND  income   55000  Expected Cardinality   2000 Age BMI familyHist Income 25 age 70 28 0 65000 25 age 80 27 1 70000 22 age 76 30 0 75000 21 age 77 26 1 65000 24 age 78 29 0 60000 Total Price beachDist 1800 5 5 1500 6 1900 5 2 1600 7 1 1570 6 5  c  Q1 Alternative Queries  d  Q2 Alternative Queries Figure 1   a  Q1 Medical study query selecting volunteers for an obesity study   b  Q2 Travel reservation query with constraints on price and distance to the beach Consider for example Query Q1 shown in Figure1 a  A medical research organization has received a grant to run an obesity study involving 2000 participants  To determine the connection between obesity and low income  the research agency wants to study peo ple with age between 25 and 50 years  BMI  body mass index  greater than 30  having no family history of obesity and having income less than  55000  However  on running this query against the database only 1300 people are found to meet the required criteria  Since the agency has resources to study 2000 people  700 additional people must be selected for the study  With no additional information about the shortage of results or alternative queries to    x it  the user has to resort to trial and error  While formulating the modi   ed query  the user can try various queries  For example  the age range can be expanded to 25   70 years  the upper bound on the income can be changed to  65000  etc  Figure 1 c shows some other alternate queries that a user may try  Each modi   cation seeks to enlarge or relax the original query to include more results  Even for a simple query like Q1 with four select predicates we see that there are a very large number  this number is in fact exponential with respect to the query predicates  12   of alternate queries a user can enter  Continually executing these queries increases query response time and wastes server resources  Query Q2 in Figure 1 b gives another example for a query with cardinality constraints  In this query  the user is planning a week long trip and wants to    nd a Flight Hotel deal such that the hotel is close to the beach but the entire trip is not very expensive  Although the user   s de   nitions of    close to the beach    and    expensive    are    exible  the travel database requires the user to set strict parameters  Q2 shows one such set of parameters  For this example  the cardinality constraint is implicit  so we assume that the user expects about 20 results for this query  However  suppose the user only gets 10 results  The user now has to undertake the cumbersome trial and error to formulate an appropriate query  For both queries Q1 and Q2 discussed above  a better   i e  less frustrating  faster and meaningful   user experience can be obtained by directly providing the user with alternate queries  The alternate queries should not only attain the required cardinality but also be as close as possible to the initial query and hence to the original query semantics  Closeness to the original query is an important factor because a user will always prefer alternate queries that have little change from the original one  By automating the query relaxation process  we can improve the user experience  increase user satisfaction  decrease query response time and also save server resources  Towards this goal  this paper presents QRelX an incremental query relaxation framework that assures query cardinality while minimizing relaxation  Before proceeding to the details of the algorithm  we note that while the above scenarios involving Q1 and Q2 discuss the case where a query returns too few results  similar arguments can be made for the case where a query returns too many results  If  for instance  the medical study query Q1 returns too many results  then the user has to formulate a smaller query by shrinking Q1   s predicates  Automated Query Relaxation  Query relaxation is the process of selectively loosening a set of query predicates to alter query cardinality  8   The aim is to carefully control relaxation to provide the user with alternate queries that not only meet the required cardinality but also minimize relaxation  However  employing the technique of query relaxation poses several challenges   1  the number of combinations of query predicates that can be relaxed is exponential in the number of predicates  12    2  the degree of relaxation required is not known beforehand  and  3  the process is computationally expensive since repeated query execution is required to explore the space of relaxed queries  In  3  Bruno et al  discuss the complexity of solving the cardinality assurance problem exactly and prove that the problem is NP hard  An additional challenge in producing alternate queries lies in minimizing the relaxation or amount of change  Various approaches for tackling the problems of cardinality assurance and query relaxation can be found in the literature   8  12  20  21  propose methods based on skylines  arti   cial intelligence and deduction to relax empty result set queries  However  these techniques mainly focus on solving the empty result set problem and not on meeting query cardinality  Mishra et al  propose an interactive framework for query re   nement in  18   This method uses information about the underlying database to help users to narrow down the choices for modi   ed queries  However  the authors do not address the problem of relaxing join predicates  Cardinality assurance has also been studied in the content of generating test queries for databases in  3 19   The limitation of these methods is not only that they don   t relax join predicates but also that they do not focus on minimizing relaxation since query semantics are not relevant for database testing queries  Top k algorithms have traditionally been used to solve cardinality assurance problems by providing the user with the required number of tuples  6  7  9  10  14   The STOP AFTER operator also functions similarly in the case where there are too many results  4   However  these methods require the formulation of a specialized ranking function and don  t generate a query characterizing how the desired results were obtained  Further  top k can also lead to a biased distribution of data in the query results  Tuple oriented vs  Query oriented approaches  We classify the above methods like top k  6  and skyline based relaxation  12  as tuple oriented methods for query relaxation since their goal is to return the given number of tuples without being concerned about the query that produces them  Methods for generating test queries  like  3   on the other hand are query oriented because they are concerned with the actual queries that produce the required number of results  However  the utility of the query oriented approach is not limited to testing  In many queries like Q1 and Q2 shown above  providing the user with only the required number of tuples does not suf   ce  The user also needs to know why the tuples were selected  i e   the query that generated them  Let us consider query Q1 to contrast the tuple oriented and queryoriented approaches  First  tuple oriented methods require much effort to ensure that the algorithm returns tuples that make sense for the given query  For instance  a poorly designed ranking function for top k or the use of skyline algorithms on Q1 may give results containing people with BMI less than 30  and hence not considered obese  or with incomes over  65000  throwing off the income factor   The query oriented method on the other hand can assure that all the tuples produced by the given relaxed query will satisfy a    xed set of criteria  Query oriented methods can also capture user preferences by providing the user with a set of alternate queries and allowing him or her to pick the one that best matches the preferences  Additionally  knowing the query that produced a given set of results is important in scenarios where a user may want to go back and generate a slightly different data set by modifying the query  The second difference between the two approaches relates to queries where we need to ensure that the result tuples are picked uniformly using a consistent set of criteria  Tuple oriented approaches can cause uncontrolled departure from the original query and lead to the creation of a biased result set  For example  suppose that 700    closest    neighbors are generated for Q1 and we    ndthat the volunteers now satisfy the criteria  age between 25 and 70  BMI greater than 30  having no family history of obesity  and income less than  65000  However  these 700 additional people do not accurately represent the group of people with age 50 to 70  BMI over 30  with no family history and with income between  55000 and  65000  A query oriented method however returns all the tuples satisfying the given criteria and hence accurately represents the underlying population  The last drawback of using tuple oriented methods is that the non uniform selection of result tuples can lead to unreliable results  In the context of Q1  without a representative sample of people satisfying the given criteria  the conclusions of the study will have low con   dence  Lastly  if such a study were to be repeated with a different data set  it is likely that the results may not hold because a biased sample was used in the initial study  Thus  the advantage of the query oriented approach over the tupleoriented one is that it can provide an explanation for the selection of results  preserve the original data distribution and facilitate the process of future query re   nement  Our Contributions  The above discussion shows that while tupleoriented methods are appropriate for certain applications  a queryoriented approach with its    ner grained control over relaxation and thus ability to return more meaningful results is a critical technique needed by many classes of applications  In this work we propose to tackle the query oriented cardinality assurance problem  Given an initial user query  QRelX relaxes the query so that it not only meets the expected cardinality but also has the least change from the initial query  QRelX is based on the key ideas of query space transformation  layer based navigation and incremental cardinality estimation  Our query space transformation framework enables the relaxation of select as well as join queries  Layer based navigation is deployed to minimize relaxation  and incremental cardinality estimation reduces computational expenses associated with relaxation  The above principles enable our algorithm to attain the following four goals   1  Given a query Q I  initial query  and an expected cardinality C0  QRelX relaxes the query Q I to Q F     nal query  such that Q F satis   es the given cardinality   2  Q F minimizes the relaxation with respect to Q I    3  QRelX can relax select as well as join predicates  and  4  The process of relaxation from Q I to Q F is computationally ef   cient  By attaining all the four goals for both the empty result set and few many problems  we present a comprehensive solution to the query oriented cardinality assurance problem  To summarize  our main contributions are    We formally de   ne the query oriented cardinality assurance problem that minimizes query relaxation  We also introduce the classi   cation of the relaxation algorithms into tupleoriented and query oriented algorithms    We present QXForm  a framework for transforming the initial query space into a relaxation space to relax select and join queries  We design special relaxation mapping functions for this purpose    We present QRelX   a novel algorithm for query relaxation that uses layer based navigation and incremental cardinality estimation for ef   cient query relaxation  To the best of our knowledge QRelX is the    rst algorithm proposed for queryoriented join relaxation that ensures query cardinality    We propose a novel incremental cardinality estimation algoPredicate pFunction pInterval  25   age   75  age  25  75   BMI   30  BMI  30  1   familyHist   0  familyHist  0  0   income   55000  income  0  55000   Flight price     Flight price     0  1500   7   Hotel price   1700   7   Hotel price   beachDist   5  beachDist  0  5  Table 1  Division of query predicates into pInterval and pFunction rithm that delays tuple level computations until absolutely necessary and that removes the need to repeatedly re evaluate tuples for multiple queries  This reduction in tuple level computations increases the ef   ciency of our relaxation technique    We present results from experimental studies comparing our method to state of the art methods for cardinality assurance  The remainder of this paper is organized as follows  The formal problem de   nition is presented in Section 2  The proposed method is described in Section 3  In Section 4  evaluation of the method is presented  Section 5 presents the related work  and    nally  Section 6 presents our conclusions  2  PROBLEM DEFINITION In this section we introduce the notations used in this work and formally de   ne the problem of query oriented relaxation for cardinality assurance  2 1 Query Formulation Consider a query Q over relations R1  R2      Rk comprised of query predicates P1  P2       Pd  each of which is either a select or join condition  In this work  we limit ourselves to conjunctive queries  Hence query Q can be denoted as  Q    P1   P2           Pd   1  Each predicate Pi in the query is assumed to be made up of two parts  the predicate function   pFunction and the interval of expected values for the predicate   pInterval  pFunction is a mathematical function consisting of one or more attributes from the relations R1  R2      Rn  In this work we consider pFunctions that are monotonic  To illustrate  pFunction   age for the age predicate of Q1 while it is F light price   7   Hotel price for the pricepredicate of Q2  For each pFunction  pInterval gives the range of values of pFunction that are acceptable for Q  pInterval takes the form  pIntervallower  pIntervalupper  where pIntervallower is the minimum acceptable value of pFunction and pIntervalupper is the maximum acceptable value  For example  the pInterval for the age predicate of Q1 is  25  75  while it is  0  1500  for the price predicate of Q2  Table 2 1 shows the breakdown of the predicates of query Q1 and Q2  The above de   nition of predicates is general and holds for generalized select predicates of the form  l1   k1   R1 x1   k2   R2 x2         kn   Rn xn   l2  where xi is an attribute of relation Ri  For these predicates  pFunction    k1   R1 x1   k2   R2 x2         kn   Rn xn  and pInterval    l1  l2   A slightly different division of predicates is used for joins including equi joins like  R1 x1   R2 x2  and non equijoins like  2   R1 x1   3   R2 x2   For join predicates  pFunction takes the form  k pF unction1   pF unction2 k  where pF unction1and pF unction2 are monotonic functions of attributes belonging to relations R1  R2     Rn  The de   nition of pInterval is unchanged and it consists of the minimum and maximum acceptable values of pFunction  2 2 Measuring Query Relaxation Suppose Q is the original query with predicates P1  P2       Pd such that Q    P1  P2       Pd  and Q 0 is a potential relaxed query  Since Q has been relaxed to Q 0   the pInterval of a subset of predicates has been expanded to include more values  Therefore  for a particular predicate Pi  we de   ne the relaxation of Q 0 with respect to Q as the sum of the difference between the lower bound of Pi in Q and Q 0 and the difference between the upper bound of Pi in Q and Q 0   RelX Q 0 Pi     pIntervallower  QPi    pIntervallower  Q 0 Pi    pIntervalupper  Q 0 Pi    pIntervalupper  QPi    2  RelX Q 0      i 1   dRel Q 0 Pi    3  where RelX Q 0 Pi   is the relaxation of Q 0 with respect to predicate Pi  while RelX Q 0   is the total relaxation of Q 0   Following the same principle  we can symmetrically de   ne the relaxation of a contracted query where the pIntervals of predicates have been shrunk to reduce the number of tuples returned  2 3 Query Oriented Relaxation Using the above de   nitions  we now formally de   ne the problem of query oriented cardinality assurance  Consider an initial user query Q I with predicates P1  P2       Pd such that Q I    P1  P2       Pd   Let the expected cardinality of Q I be C0 while its actual cardinality be CI such that CI   C0  In order to increase the cardinality of Q I   some subset of the query predicates P1  P2       Pd must be expanded to include more results i e   the pIntervals of this subset of predicates must be made bigger to increase the range of accepted values  The query oriented relaxation problem has two main goals   1  to modify Q I to Q F   the relaxed query  such that Q F is within a threshold of the expected cardinality C0  and  2  Q F minimizes relaxation with respect to Q I   The problem of satisfying cardinality constraints exactly is NPhard  3   However  for a meaningful user experience  a queryoriented relaxation algorithm is required to be ef   cient  Therefore  instead of satisfying the query cardinality exactly  we aim to    nd a query Q F that has cardinality within a tolerance threshold   of C0  where   is a tunable parameter   Similarly     nding the answer query with the absolute minimum relaxation would require an exhaustive search  Since this is not feasible nor appropriate for a good user experience  query oriented relaxation focuses not on    nding the query with the absolute minimum relaxation but one that has relaxation within a tolerance threshold of this minimum  We introduce the concept of IdealRelX   the Minimum Relaxation in the ideal Case to formalize this notion  DEFINITION 1  IdealRelX Given a query Q I with expected cardinality C0  and a cardinality threshold    IdealRelX denotes the minimum relaxation that any query having cardinality C0       can have  IdealRelX is thus the minimum relaxation that can possibly be obtained while satisfying the query cardinality  Based on the above de   nition  we can formally state the problem of query oriented relaxation  In general  our goal is not to    nd the query with relaxation IdealRelX but instead to    nd a query within a tolerance threshold of it  DEFINITION 2  Query oriented Relaxation Given a query Q I with expected cardinality C0  a cardinality threshold   and a relaxation threshold    compute a query Q F such that Cardinality Q F     C0         4  RelX Q F     IdealRelX         5  3  OUR APPROACH In this section we describe the QRelX algorithm and its underlying principles  3 1 Overview The goal of QRelX is to perform query oriented relaxation for cardinality assurance  The key ideas underlying QRelX are query space transformation  layer based navigation and incremental cardinality estimation  Query space transformation is the process in which the input tables associated with a query are processed and combined to produce an output space or relaxation space  This transformation allows QRelX to relax both select and join predicates  Layer based navigation is a traversal method that allows the algorithm to navigate the relaxation space in such a manner that relaxed queries with lower relaxation are always evaluated before those with higher relaxation  Layer based navigation ensures that our algorithm can successfully minimize relaxation  Incremental cardinality estimation is a novel method of cardinality estimation introduced in this work  Incremental estimation allows the algorithm to delay tuple level computations until they are absolutely necessary  Further this technique ensures that once a tuple has been found to satisfy a given query it is never reevaluated for any other query  The combined result of these three principles is that QRelX can ef   ciently relax queries that meet the cardinality constraint but also minimize relaxation  Figure 2 shows the overall architecture of our system  The system consists of two main components  QXForm  the query space transormation module and IncRelX  the query relaxation module  The functions of these modules are descibed as follows    QXForm  This module of QRelX is responsible processing the input tables and creating the output or relaxation space  In particular      It processes all input tables and partitions them into multi dimensional input partitions  These partitions make up the input space     Once the input space has been created  all combinations of input partitions are considered and relaxation mapping functions are used to create corresponding output regions      An output space or relaxation space is generated using the output regions and a grid structure is imposed on this space to facilitate the search for relaxed queries       IncRelX  This module is responsible for searching the relaxation space created by QXForm for relaxed queries that meet the expected cardinality  It is comprised of the following parts      Layer based navigation scheme  Since QRelX aims to produce queries that minimize relaxation  layer based navigation is used to evaluate potential relaxed queries in order of increasing relaxation      Incremental Cardinality Estimation  This algorithm presents a novel way to perform incremental cardinality calculations using previous cardinality information and thus reduce the expenses associated with cardinality estimation  Each of these modules are discussed in detail in the following subsections  Relaxed  Queries Create Output  Space  Mapping  via  Functions Relaxation Space  Traversal Module SELECT     WHERE    FROM    JOIN Query Q   Cardinality C  Input  Processing by  Partitioning Data Sources Incremental Cardinality Estimator Cardinality     C   Undershooting  Overshooting  Repartition Query Space  Transformation Query Relaxation Figure 2  System architecture for QRelX 3 2 Query Space Transformation The purpose of the QXForm  the query space transformation module is create an output space or relaxaton space for    nding potential relaxed queries  QXForm takes an input the initial user query Q   P1 P2       Pd and the set of tables R1  R2       Rn associated with it  The transformation process consists of three steps  Input Space  The    rst step in query space transformation is to use the input tables to create an input space  The input space is an representation of all the input tuples at a higher level of abstraction  namely as input partitions  The purpose of creating this space is to reduce computational expenses by using a higher level of abstraction and to delay tuple level computations until a latter stage  To create this input space  each table associated with query Q is partitioned and all its tuples are placed into multi dimensional partitions based on the attributes present in the query predicates  For example     gure 3  a  and  b  depict the input partitions made for query Q1  Two tables  namely the Hotel and Flight table  are involved in this query  Since two attributes from the Hotel table   price and distFromBeach   are included in Q1  the tuples in Hotel are partitioned based on their price and beachDist values  Similarly  the Flight table is partitioned based on price since this is the only Flight table attribute present in the query  The number of divisions associated with each attribute for an input table is tunable paramter k  Further  if each attribute has k divisions  then the maximum number of input partitions for a given input table is k p where p is the number of attributes from a particular table present in the query  0                    200                      400                  600     9 6 3 0 Table   Hotel Hotel  price Beach  distance 1200 800 400 0 Table   Flight Flight  price  a   b  Figure 3   a  Partitioning of Hotel table  b  Partitioning of Flight table Result tuples and Relaxation Mapping Functions  Before we proceed with the description of QXForm  we de   ne the notion of result tuples  Given a query Q involving relations R1  R2      Rn  a result tuple is an ordered set of tuples  t1  t2      tn  such that tuple ti belongs to relation Ri for i   1      n  That is  a result tuple is any valid combination of one tuple from each relation R1 to Rn  If Q consists only of one relation R  then result tuples correspond directly to tuples in R  We also note that the result tuple concept is a generalization of the joined tuple concept since all joins are valid with the right amount of relaxation  Relaxation Mapping Functions  A concept central to the QRelX algorithm is a notion of the    amount of relaxation     Relaxation with respect to relaxed queries has already been de   ned in Section 2  However  this applies to result tuples too and this measure of relaxation is essential while constructing the output or relaxation space  In general  relaxation or RelX   corresponding to a variable is the difference between its acceptable value and its actual value  i e   RelX variable   k acceptablevalueactualvalue k  6  In this work  we introduce the concept of relaxation mapping functions  or simply mapping functions  to measure the relaxation of result tuples with respect to individual query predicates  Figures 4 a to d show a few examples of relaxation mapping functions  The x axes in these    gures correspond to the actual value of a variable x  while the y axes measure the relaxation of variable x with respect to the given range of acceptable values  For example  Figure 4 a shows the relaxation mapping function when the acceptable range of values for variable x is  0  75  and x can take values from  0  1   The mapping function RelX   is de   ned such that while the value of x is within the range  0  75   there is no relaxation and RelX X    0  However  for  x    75   RelX X     x   75  measures how far the actual value of x is from the acceptable range  For instance  if x   85 then the distance from the acceptable value is  85   75    10  Figures 4 b  c  d show mapping functions fo different ranges of acceptable values  Relaxation of a result tuple  When applied to a result tuple t  a relaxation mapping function measures how far t is from a given   75 Relaxation Predicate  value 0 0 Relaxation 75 Predicate  value 0 0 Relaxation 75 Predicate  value 65 0 0 Predicate    x     75  Rel x                                  0   x     75 x   75       x     75 Predicate    x     75  Rel x                                      0   x     75  x   75    x      75                                       Predicate    x     75  Rel x                    0   65      x      75 75      x   x     75 x      65   x     65                                     Relaxation 75 Predicate  value 0 0 Predicate    x     75  Rel x                                  0   x     75 75   x       x     75  a   c   b   d  Figure 4  Examples of mapping functions query predicate  Consider once again Figure 4 a discussed above  the mapping function in this    gure can easily be used to calculate the the relaxation of a result tuple  Suppose our query Q involves the relation R with contains attribute x  Further  suppose that predicate P belonging to query Q is de   ned such that P    R x   75   For a tuple t belonging to relation R  relaxation of t with respect to P is calculated as follows  Let t x  denote the value of attribute x for t  For  0   t x    75   Rel t x     0 since the tuple t satis   es the predicate exactly  However  for  t x     75   relaxation measures the difference between t x  and 75 i e   t x    75   So if t x  was 85  then based on the relaxation function  Rel t x     10  This is exactly the mapping function we discussed above  If we de   ne mapping functions for each predicate of Q then we can calculate the total relaxation of t with respect to Q  If Q is a query with predicates P1  P2       Pd such that Q    P1 P2       Pd  then the total relaxation of result tuple t can be measured as follows  RelX t     i 1   dRelX tPi    7  where RelX tPi   is the relaxation of t with respect to predicate Pi and RelX t  is the total relaxation of t  On similar lines  we can de   ne RelXVector    the relaxation vector of a result tuple t that lists in order the individual relaxations of a result tuple with respect to each query predicate  RelXV ector t     RelX tP1    RelX tP2          RelX tPd     8  Relaxation Space  The relaxation mapping functions described above provide a powerful means to measure the relaxation of resulttuples  We use these mapping functions to create a relaxation space or output space that measures the relaxation of all possible resulttuples  By representing result tuples according to their relaxation vectors  we can traverse the relaxation space to easily    nd potential relaxed queries  The relaxation space is a d dimensional space with the original query Q lying at the origin  Further  each dimension of this space corresponds to the relaxation of result tuples with respect to each of the d predicates of Q  The relaxation space represents two kinds of information  First  every point in the relaxed output space is a relaxed query that is a potential answer  Second  the output space also represents the relaxation of all result tuples that can be generated from the input tables  This information about result tuples is stored in terms of output regions  An output region is an abstract data structure corresponding a unique combination of input partitions  It is a region in the output space such that all result tuples from the given combination of input partitions lie within that region  i e   an output region delimits the relaxation of result tuples generated from the associated input partitions  The process of creating the output space consists of two steps     rst  to create output regions  and second  to create a grid in the output space that assigns output regions to grid cells  The    rst step provides us information about the location of result tuples while the second facilitates the process of cardinality estimation  Creating Output Regions  Since a tuple from one table can com        Travel  Scenario  query  Output  Space  0   0     1700   1  Price  relaxation Table   Flight Beach  Distance  relaxation  0     400  Flight  price Table   Hotel  200   0     400   3  Hotel  price Beach  distance 9 6 3 0 0        200          400          600     1200 800 400 0 0       1000      2000          3000      4000 0        200          400          600     3 2 1 0 Figure 5  Creating an output space bine with any other tuple from another table in Q  with the right amount of relaxation   we have to consider all combinations of the input partitions while creating output regions  For each combination of input partitions  we use one mapping functions per query predicate of Q to calculate the maximum and minimum relaxation that any result tuple from these partitions could have  To actually compute these limits on relaxation  QXForm assumes that resulttuples called pseudo result tuples are present at the upper right and lower left points of each input patition  Once we know these resulttuples  mapping functions can be used to calculate the corresponding relaxations  Figure 5 shows an example of this process for the running query example Q2  The    gure shows the partitioning of the Flight and Hotel tables along with the output space  Once input partitions have been created  one partition is picked from each table to calculate the minimum and maximum bounds for relaxation of result tuples  In the current example  the pseudo result tuples are  200  0  and  400  3  for the Flight table partition while they are  0    400  for the Hotel table partition  Consider the    rst predicate of Q2   Flight price   7 Hotel price   1500   For the given pair of input partitions  the value of this predicate ranges from 1400 to 3200  But the expected value is less than 1500  Hence  the minimum and maximum relaxation with re spect to this predicate is the interval distance between  0  1500  and  1400  3200  which is  0  1700   A similar calculation is done for the relaxation of the distance from the beach  For this predicate  we get  0  1  as the range of relaxations  Thus we obtain an output region with the upper right corner at  1700  1  and the lower left corner at  0  0   The    gure shows the other    ve output regions created from the remaining combinations of input partitions  Note that while the output regions may overlap  they never share resulttuples  Creating the output grid  The second step in creating the output space is to create a grid that stores the distribution of output regions over grid cells  This data is required while estimating cardinality and for optimization  For instance  consider a query located at point  1000  1  in the output space  Based on the grid shown in the    gure  we know that only the output region with lower bound  0  0  and upper bound  1700  1  needs to be examined for estimating query cardinality  The grid structure thus succeeds in reducing the computational expenses  To create the output grid  each dimension of the output space is partitioned according to a stepsize that re   ects the preference of the predicate corresponding to that dimension  For ease of computation  we will assume that all all the predicates have equal preference and hence  all dimensions have a unit stepsize  The following terminology is used to describe the output grid  a grid point is any point that lies at a point of intersection in the grid  a gird cell corresponding to a grid point is the unit orthotope that has the given grid point as its upper right corner  neighbors of a grid point are the grid points at a unit distance from the given grid point in only one dimension  For the purpose of our algorithm  we only consider the neighbors of a grid point that have higher relaxation than the given grid point  3 3 Relaxation Algorithm In this section we present IncRelX our core query oriented relaxation algorithm for cardinality assurance  The goals of IncRelX are fourfold   1  IncRelX is able to relax select as well as join predicates   2  Given an initial user query Q I and an expected cardinality C0  IncRelX relaxes the query Q I to Q F     nal query  such that Q F satis   es the given cardinality   3  Q F minimizes the relaxation with respect to Q I   and  4  The process of relaxation from Q I to Q F is computationally ef   cient  The QXForm framework described in Section 3 2 ensures that IncRelX satis   es the    rst goal of relaxing both select and join conditions  IncRelX satis   es the second goal of cardinality assurance by evaluating various queries in the relaxation space created by QXForm and selecting those that are closest to the expected cardinality  The third goal of minimizing relaxation is met by using a layer based navigation scheme which ensures that queries with lower relaxation are always evaluated before queries with higher relaxation  The last goal is met by using an incremental cardinality estimation technique which reduces computational expenses by  a  evaluating only those tuples that are likely to satisfy any given query and  b  ensuring that once a tuple has been found to satisfy a query  it is never re evaluated for any other query  We now describe the IncRelX algorithm beginning with our navigation scheme  In the scenario where the original query does not meet its associated cardinality constraint  intuitively  the user would prefer a relaxed query that attains the required cardinality but is as close to the original query as possible  This implies that the search for potential relaxed queries in the relaxation space must be done in a way that Layer Queries 0  0  0  1  0  1    1  0  2  0  2    1  1    2  0  3  0  3    1  2    2  1    3  0  k  0  k    1  k 1    2  k 2        k 2  2    k 1  1    k  0  Table 2  Composition of layers in the output space prefers queries with lower relaxation  Our solution to this problem is to adopt an iterative layer based navigation scheme to minimize relaxation  Layer based traversal  as the name suggests  is an approach where potential relaxed queries are grouped into layers of equal relaxation  and are evaluated based on these layers  Moreover  relaxation layers are traversed in the order of increasing relaxation so that all queries lying in a layer with relaxation p are evaluated before queries lying in layer  p 1   The intuition of the layer based approach is presented in Figure 6  Figure 6  a  of the    gure depicts a two dimensional relaxation space similar to the one created by running query Q1  In a two dimensional space  the total relaxation of a query is the sum of the relaxations on the two axes  and therefore  the relaxation layers are lines having slope  1  Figure 6 a shows relaxation Layers 0  1  2 and 3 having total relaxation equal to 0  1  2  and 3 units respectively  Layer based traversal begins evaluation of queries from Layer 0 and then proceeds along Layers 1  2  and 3  The numbering of the queries re   ects the order in which queries are examined in each layer     rst  query  0  0  belonging to layer 0 is evaluated  second  queries  1  0  and  0  1  belonging to layer 1 are evaluated  third  queries  2  0    1  1  and  0  2  belonging to layer 2 are evaluated  next  queries  3  0    2  1    1  2  and  0  3  belonging to layer 3 are evaluated and so on  Table 3 3 lists in tabular form the queries that are in the above layers of the relaxed space  From the values in the table  we know that a query  r1  r2  belonging to Layer k in the 2 dimensional relaxation satis   es the constraint that r1    0  r2    0 and  r1   r2    k  In general  given a d dimensional relaxation space  the queries belonging to Layer k can formulated mathematically as follows  Layer k    f r1  r2       rd  j  ri    0  for i   1       d AND  r1   r2           rd   k g  9  This also turns out to be the formulation of the weak number theoretic compositions of the integer k and hence gives us an upper bound on the number of queries in each layer  Using combinatorial arguments  we can prove that such a layer k will contain  d k1 d1   queries at the most  Due to the similarities between the layer based navigation scheme and breadth    rst traversal  our system implements the layer based navigation scheme by using a modi   ed breadth    rst traversal strategy  As in traditional BFS  a queue is used to store the algorithmic data  Let us now look at the walk through example of the layerbased navigation scheme as shown in Figure 6  b   Step 1 in above    gure shows the initial state of the traversal queue containing  0  0   In Step 2   0  0  is popped from the head of the queue and its cardinality evaluated  Following this  its neighbors are computed to be  0  1  and  1  0  and added to the queue  In Step 3   0  1  is popped and its neighbors  0  2  and  1  1  are added to the queue  Finally    when we come to Step 4  where  1  0  is popped and its neighbors   1  1  and  2  0  are computed  However  only  2  0  is added to the queue since  1  1  is already present in it  This omission of  2  0  avoids re examination of already evaluated queries  The overall effect of the traversal algorithm is that the queries are evaluated in the layer based order depicted in Part  a  of same    gure   0  0    0  1    1  0    0  2    1  1    2  0    0  3    1  2    2  1   and  3  0   Figure 6   a  Order in which grid cells are traversed by the Traverse algorithm   b  Walk through example of Traverse Algorithm 3 3 lists the pseudocode for the traversal algorithm  We    rst pop the element at the head of the queue  this is the query to be evaluated next  Line  1   Lines 2 8  For loop generates all the neighbors of the most recently popped query by incrementing the query co ordinates in a dimension wise manner  Lines 5 7  Checks if the given neighbor is already present in the queue  The neighbor is added only if the same value is not currently present in the queue  Algorithm 1 Traverse  Queue q  91  int   counter   q pop   92  for i   0 to d  1 do 93  int   counterCpy   Copy counter  94  counterCopy i    95  if  q Contains counterCopy  then 96  q push counterCopy  97  return counter Lemma 1  Traverse evaluates query Q with relaxation p before evaluating query Q 0 with relaxation p 0 if and only if p   p 0   Proof  The objective of the Traverse algorithm is to traverse the output space in such a way that no query with total relaxation r is evaluated before all queries with relaxation r 1 have been evaluated  To prove this  we can model the output space as a graph G V  E  such that the set of vertices of G is the set of all grid points and E is the set of edges created by connecting each grid point to its neighbors  The Traverse algorithm then performs a breadth    rst traversal of G  and correctness of breadth    rst traversal proves the correctness of Traverse  In summary  the Traverse algorithm ensures that if a relaxed query Q 0 has relaxation smaller than another relaxed query Q 00   the cardinality of Q 0 is evaluated before Q 00   Consequently  this order of query evaluation guarantees that the    rst query Q   found to satisfy the cardinality constraint has the minimum relaxation possible  Moreover  once we    nd a relaxed query that satis   es the required cardinality  we do not need to evaluate any more queries  and thus we can reduce computational expenses  3 3 1 Incremental cardinality estimation One of the main challenges of query relaxation is the computational expense associated with repeated query execution  Whether the re     nement is user driven or automatic  relaxation algorithms require that the cardinality of a large number of queries be estimated  However  traditional relaxation techniques have no memory about prior cardinality estimations and do not reuse previously computed cardinalities to increase computational ef   ciency  Instead of the repetitive cardinality estimation model  in this work  we propose a novel incremental cardinality estimation model that takes advantage of space partitioning and query containment for performing rapid cardinality calculations  We start with some basic observations of the relaxation space  Cardinality in the Relaxation Space  Suppose the original query Q given by Q    P1 P2       Pd  and its expected cardinality is C  As discussed previously  the relaxation space represents not only all possible relaxed queries but also the relaxations of all combined tuples  As a result  the cardinality of a query Q 0 corresponding to the point X    x1  x2      xd  in the relaxation space is equal to the number of combined tuples included in the orthotope extending from the origin to X  In terms of the relaxations  this orthotope includes all tuples T such that   0    Rel TPi      xi fori   1       d  10  Figure 3 3 1 a shows a visual representation of this orthotope for a two dimensional output space created by Q1  Query Containment  The above de   nition of query cardinality provides us a key insight that can lead to dramatic improvements in cardinality estimation ef   ciency  Consider the two queries Q 0    r 0 1  r 0 2  and Q 00    r 00 1   r 00 2   shown in Figure 3 3 1 b  Q 00 is said to be contained in Q 0 since the rectangle corresponding to Q 00 is completely contained inside the rectangle corresponding to Q 0   Containment implies that all the combined tuples belonging to Q 00 also belong to Q 0   Therefore  if we evaluate the cardinality of Q 00 before evaluating Q 0   then we can avoid the expensive cardinality computation by reusing the already computed cardinality of Q 00   In general  given two queries Q 0 and Q 00 in a d dimensional space such that Q 0    r 0 1  r 0 2      r 0 d  and Q 00    r 00 1   r 00 2       r 00 d    we say that Q 00 is contained in Q 0 if r 0 i    r 00 i for  i   1       d   Moreover  the containment also implies that  i 1   dr 0 i    i 1   dr 00 i   In other words  the total relaxation of Q 00 is lesser than that of Q 0 and hence our layer based navigation scheme will necessarily evaluate Q 0 before evaluating Q 00   This above set of observations allows us to take advantage of space partitioning and query containment in the relaxation space to formulate the following incremental cardinality estimation model  Incremental Estimation Model  The principle guiding our Incremental Estimation Model is that once a combined tuple has been found to satisfy a query Q  that combined tuple is never re evaluated for any other relaxed query Q 0 that contains Q  However  this combined tuple is incorporated in the cardinality of Q 0 automatically  The estimation model of IncRelX examines only a small subset of combined tuples for each query while mainly reusing cardinality values of queries from the previous layer  This approach ensures that IncRelX does not waste resources in rerunning previous computations  We    rst describe the framework that allows cardinality to be calculated incrementally   0 0  C B A  x y 1   x y   x 1 y  1A B C D  x 1 y z   x y z   x y 1 z   x y z 1   0 0 0   c   d   a   b  Q   x y   0 0  Q   x y   0 0  Q      x    y     Figure 7   a  Representation of cardinality in output space   b  Motivation for incremental cardinality estimation   c  Decomposition of a 2 D orthotope  and  d  Decomposition of a 3 D orthotope Decomposition of the Orthotope  Each query in the relaxation space has an associated query orthotope and the    rst step towards incremental estimation is determining how this orthotope can be divided to reuse cardinality values from the previous relaxation layer  Formally  given a query query Q corresponding to point X    x1  x2       xn   we seek to partition the query Q   s orthotope into smaller sub orthotopes such that  1  the sub orthotopes are disjoint  and  2  the upper right corners of sub orthotopes correspond to queries from the previous relaxation layer  Figures 3 3 1 c and d show one such partitioning of two and three dimensional orthotopes  In each of the above    gures we see that not only are the sub orthotopes disjoint  but their upper right corners also correspond to queries in the previous relaxation layer i e  the total relaxation of these queries is one unit less than the relaxation of Q  As the    gures demonstrate  a two dimensional orthotope has to be partitioned into three sub orthotopes to satisfy the above criteria while a three dimensional orthotope requires four sub orthotopes  To aid the understanding of this concept  the sub orthotopes have been named to re   ect their geometry  Suborthotope A is called a    cell     B a    pillar     C a    wall     and D a    block     Moreover  to preserve uniqueness  the sub orthotopes are associated with the points at their their upper right corner  Thus  in    gure 3 3 1 c  the orthotope or wall corresponding to query  x  y  is composed of the cell of  x  y   the pillar of  x 1  y  and the wall of  x  y 1   Similarly  the orthotope or block of  x  y  z  in part d is composed of the cell of  x  y  z   the pillar of  x 1  y  z   the wall of  x  y 1  z  and the block of  x  y  z 1   In general  a ddimensional orthotope has to be divided into  d 1  sub orthotopes to satisfy the above criteria  The previous discussion implies that to estimate cardinality incrementally  we need to calculate the cardinality values of each of the d 1 sub orthotopes  To illustrate  in a 2 dimensional space  we need to calculate the values of the cell  A   pillar  B  and wall  C  for each query  as shown in Figure 8   while in the 3 dimensional space  we have to calculate the values of the cell  A   pillar  B   wall  C  and block  D   Figures 3 3 1   In ddimensional space  the following equations mathematically de   ne the  d 1  sub orthotopes associated with each query  For instance  the cell denoted by O1 is the sub orthotope having unit length in each dimension and having  x1  x2      xd  as its upper right corner  The pillar  denoted by O2  has length x1 in the    rst dimension while it has unit length in the other  d 1  dimensions  The wall  i e  O3  has length x1 and x2 in the    rst two dimensions while it has unit length in the other  d 2  dimensions  The orthotopes are de   ned as follows in a d dimensional relaxation space  O1 cell    f y1  y2       yn  j  xi  1   yi    xi  for i   1       dg  11  O2 pillar    f y1  y2       yn  j  0    y1    x1  AND  xi  1   yi    xi  for i   2       dg  12  O3 wall    f y1  y2       yn  j  0    y1    x1  AND  0    y2    x2  AND  xi  1   yi    xi fori   3       dg  13  Oi   f y1  y2       yn  j  0    yi    xi  for i   1       i AND  0    y2    x2  1  AND  xi  1   yi    xi  for i   i   1       dg  14  Od   1   f y1  y2       yn  j  0    yi    xi fori   1       dg  15  Using the above equations  we can formally write the compositions of orthotopes in two and three dimensions Fig 3 3 1 c and d  as follows  O3 x  y    O1 x  y    O2 x  1  y    O3 x  y  1   16  O4 x  y  z    O1 x  y  z    O2 x  1  y  z    O3 x  y  1  z    O4 x  y  z  1   17  Generalizing the above formulas for a d dimensional space we get  Od x1  x2  x3       xd    O1 x1  x2  x3       xd    O2 x1  1  x2  x3       xd    O3 x1  x2  1  x3       xd            Od 1 x1  x2  x3       xd  1   18  However  for every point  we not only need to calculate the total cardinality of the orthotope  but we also have to calculate the cardinalities of the other d sub orthotopes de   ned above  Referring back to Figures 8 and 3 3 1  we can make the following observations    For the two dimensional relaxation space in Figure 8  we have      Pillar  x  y    Cell  x  y    Pillar  x 1  y      Wall  x  y    Pillar  x  y    Wall  x  y 1    For the three dimensional relaxation space in Figure 3 3 1  we have      Pillar  x  y  z    Cell  x  y  z    Pillar  x 1  y  z                  Wall  x  y  z    Pillar  x  y  z    Wall  x  y 1  z      Block  x  y  z    Wall  x  y  z    Block  x  y  z 1  Thus  we begin to see a pattern that can be converted into a recurrence  Oi x1  x2       xd    Oi1 x1  x2       xd  Oi x1  x2       xi1 1 for i   2       d   1  The only sub orthotope that doesn   t have a recurrence is the cell since this is the part of the cardinality that is unique to every query  To calculate the cardinality of the cell  combined tuples belong to various output regions have to be examined to    nd those that lie in the given cell  Thus  in summary   0 0  C B  x y   1   x y   0 0  C B A  x y   1   x y   x   1 y   0 0  C  x y   a   b   c  Figure 8  Orthotope recurrences in a two dimensional space  1A B C D  x 1 y z   x y z   x y 1 z   x y z 1   0 0 0  1B C D  x y z   x y 1 z   x y z 1   0 0 0  C D  x y z   x y z 1   0 0 0  D  x y z   0 0 0   a   b   c   d  Figure 9  Orthotope recurrences in a three dimensional space  we can incrementally calculate the cardinality of queries using the equations below  O1   f y1  y2       yn  j  xi  1   yi    xi  for i   1       dg  19  Oi x1  x2       xd    Oi1 x1  x2       xd    Oi x1  x2       xi1  1       xd  for i   2       d   1  20  These recurrences ensure that once the cardinality of the cell has been determined  it takes a constant number of steps to calculate the other cardinalities  Algorithm 3 3 1 presents the pseudo code that performs the incremental cardinality estimation  Note that Algorithm 2 CardinalityEstimation int   upper  Hash h String  int     91  int d 1  card 92  card 0    ComputeCellCardinality upper  93  for i   1 to d do 94  upper i     95  int   cardi   h get asString upper   96  card i    card i 1    cardi i  97  upper i    98  h insert asString upper   card  99  return card d  the sub orthotope cardinalities are stored in an array as  O1  O2  O3      Od 1   The CardinalityEstimation algorithm takes as input the query being evaluated  and a pointer to the recording keeping data structure  which in this case is a hash  The hash stores previously evaluated queries along with the values of the associated  d 1  orthotopes  Brie   y  the algorithm functions as follows  Line 2  Computes the cardinality of the associated cell by examining combined tuples  The ComputeCellCardinality algorithm below is responsible for this evaluaion  Lines 3 7  Computes cardinalities of the remaining  d 1  orthotopes using the previously de   ned recurrences  Line 8  Updates the hash with the given cardinality values The other part involved in cardinality estimation of IncRelX is the algorithm that computes the cardinality of the cell associated with each query  The goal of this algorithm is to    nd the combinedtuples tha lie in the query cell and remove them from the pool of combined tuples so that they are not evaluated again  Algorithm 3 3 1 presents this algorithm  The algorithm functions as follows  Lines 2 4  Computes the lower bound of the cell Line 5  Gets the list of output regions that are present in the cell Line 6  Gets the combined tuples from each of the above regions that have not satis   ed a previous query  Output regions are also materialized in this step  Lines 7 10  Checks the combined tuples  and removes those that satisfy the current query  Also updates cardinality  Algorithm 3 ComputeCellCardinality int   upper  91  int card   0 92  int d  lower   0  0      0 93  for i   0 to d  1 do 94  lower i    upper i    1 95  List OutputRegions  list   GetOutputRegions upper  lower  96  List CombinedTuples  tuples   GetCombinedTuples list  97  for t in tuples do 98  if t Satis   es lower  upper  then 99  tuples remove t  910  card   911  return card 3 3 2 Overall Algorithm The cardinality estimation of IncRelX proceeds through the interleaving of the traversal algorithm and the cardinality estimation algorithm  The algorithm begins at the origin and sequentially traverses queries in higher layers  For each query  estimates the cell cardinality of the query and the cardinality of the other d suborthotopes  Once the all the cardinalities for a query have been        evaluated  its total cardinality is compared to its expected cardinality  If the cardinality is within   of the expected cardinality  the algorithm evaluates other queries in the current layer and then returns the answers  If a query undershoots the expected cardinality  the algorithm proceeds to the next higher layer  However  if the query overshoots the expected cardinality  the query cell is repartitioned  Algorithm 3 3 2 shows the pseudo code for the overall algorithm  Algorithm 4 IncRelX int expectedCardinality  int delta  91  Queue q  Hash h String  int     List int    answers 92  int d  counter   0 0      0 93  q push counter  94  counter   Traverse q  h  95  int minimumRelaxationLayer   0 96  int currentRelaxationLayer   MAX INTEGER VALUE 97  while  minimumRelaxationLayer    currentRelaxationLayer  do 98  int card   CardinalityEstimation counter  99  currentRelaxationLayer   GetRelaxation counter  910  if  kcard   expectedCardinalityk      then 911  answers add counter  912  minimumRelaxationLayer   currentRelaxationLayer 913  else if  card   expectedCardinality  then 914  answers add Repartition counter   4  EXPERIMENTS In this section we describe the preliminary evaluation of the QRelX algorithm  4 1 System Implementation For the evaluation of the QRelX algorithm  we implemented in Java the system described in Section 3 1  As shown in Figure 2 of Section 3 1  our system implementation consists of two major components  QXForm  the query space transformation component and IncRelX  the query relaxation component  The query space transformation component QXForm takes as input the user query along with its associated input tables and processes them to create the relaxation space  In our implementation  the QXForm component is composed of three modules  The    rst module is responsible for processing the input tables and partitioning them into multi dimensional input partitions  The second module is responsible for the creation of output regions  It computes all possible combinations of input partitions and constructs an output region for each individual combination  Output region construction is done by using the special interval distance functions that implement relaxation mapping functions  The last module of QXForm creates a grid in the relaxation space  It then assings to each grid cell the set of output regions that overlap with the given cell  This information about the output space is stored in a global hash table H  H is indexed by keys which are strings representing the bounds of the grid cells  The value associated with each key of H is a composite data structure storing  1  a list of references to output regions containing the given grid cell   2  the d 1 cardinality values required for cardinality estimation  and  3  meta data about the cell such as whether the cell has already been evaluated  The second component of our system implements IncRelX  our core relaxation algorithm that meets query cardinality while minimizing the amount of relaxation  This component is also composed of three modules  the traversal module  the cardinality estimation module and the overall module  The traversal module implements the layer based navigation scheme described in Section 3 3 for minimizing relaxation  Using the global H table to run the traversal algorithm  this module iteratively computes the next grid cell to be evaluated  The second relaxation module is responsible for cardinality estimation and implements the Algorithms 3 3 1 and 3 3 1 in Section 3 3  Once again  information from the global hash table H is used for rapid cardinality calculations  After the grid cell cardinality has been calculated  the cardinality values stored in H are used to calculate the other d cardinalities in a constant number of steps  Additionally  this module uses a supplementary hashbased data structure to store information about materialized output regions  This data structure is required because we do not want to repeatedly materialize the output regions for each grid cell  Moreover  this data structure enables us to only keep track of the resulttuples that haven   t already satis   ed a previous query  The last module of IncRelX implements the overall algorithm described in 3 3 2 by utilizing the functionality of the traversal module to navigate the relaxation space and cardinality estimation module to ef   ciently estimate cardinality of queries in the relaxation space  4 2 Experimental Setup In order to evaluate our solution  we ran preliminary experiments testing the ef   ciency of our algorithm for various parameters and compared the performance of our system with the TQGen algorithm proposed by Mishra et al  in  19   The TQGen algorithm was chosen because this approach has similarities with QRelX and it focuses on cardinality assurance  TQGen however  does not focus on minimizing relaxation with respect to the original query  As a result  our experiment only compare the performance of the two methods  For our experiments  we implemented TQGen in Java for the single cardinality case  All experiments were run on the TPC H benchmark data  speci     cally using data from the PartSupp and LineItem tables  The PartSupp table stored the part key  supplier key  available quantity  supply cost and comments  The LineItem table included sixteen attributes  of which our queries included the attributes order key  part key  supplier key  quantity and extended price  The size of these input tables varied from 1k to 6k tuples for our experiments  4 3 Experimental Results Effect of Stepsize on Performance  Our    rst set of experiments analyzed the effect of input and output  or relaxation  stepsize on the performance of QRelX  Figure 4 3 shows the results of this set of experiments where a    xed query was relaxed to obtain a particular cardinality  The input and output stepsize for this query was varied in turn while keeping the other stepsize constant at 100 units  The x axis of Figure 4 3 shows the size of the stepsize that is being varied  The y axis on the other hand shows the time required to relax the given query for that particular stepsize  The two bars for each stepsize respectively show the execution time when one step size is 100 and the other has the value corresponding to the x axis  For QRelX  the input stepsize determines the number of input partitions that will be created during query space transformation  It affects the time required in partitioning the input tables but does not affect the quality of the results produced by QRelX  Relaxation stepsize or output stepsize on the other hand denotes the size of grid cells created in the relaxation space  As a result  output stepsize affects not only the time required for generating the output space but also the quality of answers produced  A lower stepsize increases computational expenses involved in creating the relaxation spacebut it also allows the algorithm to obtain answers very close to the ideal minimum relaxation  Figure 4 3 shows this inverse relationship between stepsize and execution time  A higher stepsize consistently leads to lower execution time  0 5000 10000 15000 20000 25000 5 50 100 500 1000 Input Space Stepsize Output Space  Stepsize Units Time  ms  Time vs  Step size Figure 10  Graph showing the effect of input and output stepsize on QRelX performance  Performance comparison with TQGen  The second set of experiments we ran compared the ef   ciency of our QRelX technique to that of TQGen  While TQGen provides cardinality assurance  it does not focus on minimizing relaxation  As a result  the QRelX algorithm always outper</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security"/>
        <doc>Data Generation using Declarative Constraints ### Arvind Arasu Microsoft Research Redmond  WA arvinda microsoft com Raghav Kaushik Microsoft Research Redmond  WA skaushi microsoft com Jian Li University of Maryland College Park  MD lijian cs umd edu ABSTRACT We study the problem of generating synthetic databases hav  ing declaratively speci ed characteristics  This problem is motivated by database system and application testing  data masking  and benchmarking  While the data generation problem has been studied before  prior approaches are either non declarative or have fundamental limitations relating to data characteristics that they can capture and e ciently support  We argue that a natural  expressive  and declara  tive mechanism for specifying data characteristics is through cardinality constraints  a cardinality constraint speci es that the output of a query over the generated database have a certain cardinality  While the data generation problem is in  tractable in general  we present e cient algorithms that can handle a large and useful class of constraints  We include a thorough empirical evaluation illustrating that our algo  rithms handle complex constraints  scale well as the num  ber of constraints increase  and outperform applicable prior techniques  Categories and Subject Descriptors D 2 5  Software Engineering   Testing and Debugging  Testing tools  H 2 4  Database Management   Systems  Query processing General Terms Algorithms  Performance  Reliability  Experimentation Keywords Data Generation  Testing  Masking  Benchmarking  Con  straints ### 1  INTRODUCTION We consider the problem of generating a synthetic database instance having certain data characteristics  Many applications require synthetically generated data  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  1  DBMS testing  When we design a new DBMS compo  nent such as a new join operator or a new memory manager  we require synthetic database instances with speci c char  acteristics to test correctness and performance of the new component  7  22   For example  to test the code module of a hybrid hash join that handles spills to disk  we might need a database instance with a high skew on the outer join attribute  As another example  to study the interaction of the memory manager and multiple hash join operators  we might need a database instance that has particular interme  diate result cardinalities for a given query plan  9   2  Data masking and database application testing  Organi  zations sometimes outsource the testing of their database applications to other organizations  However an outsourc  ing organization might not be able to share its internal databases  over which the applications run  with the test  ing organization due to privacy considerations  requiring us to generate a synthetic database that behaves like the orig  inal database for the purposes of testing   We emphasize that our goal here is not to study the general data masking problem with its privacy considerations  we are merely sug  gesting that data generation might be a useful component of a general data masking solution   3  Benchmarking  In order to decide between multiple com  peting data management solutions  a customer might be in  terested in benchmarking the solutions  22   The standard benchmarks such as TPC H might not capture many of the application scenarios and data characteristics of interest to the customer  motivating the need for synthetic data genera  tion  A related scenario is upscaling  where we are interested in generating a synthetic database that is an upscaled ver  sion of an existing database  Upscaling is useful for future capacity planning purposes  Data characteristics and cardinality constraints  The applications of data generation above require a wide variety of data characteristics in the generated synthetic databases  A natural class of characteristics are schema properties such as key and referential integrity constraints  functional de  pendencies  and domain constraints  e g   age is an integer between 0 and 120   A synthetic database for DB appli  cation testing often needs to satisfy such constraints since the application being tested might require these constraints for correct functioning  If DB application testing involves a visual component with a tester entering values in  elds of a form  the synthetic database might need to satisfy natu ralness properties  e g   the values in an address  eld should  look like  real addresses  In benchmarking and DBMS testing  we typically need to capture characteristics that can in uence the performance of a query over the generated database  These include  for example  ensuring that values in a column be distributed in a particular way  ensuring that values in a column have a certain skew  or ensuring that two or more columns are cor  related  We note correlations can involve joining multiple tables  For example  in a customer product order database  we might need to capture correlations between the age of customers and the category of products they purchase  In data masking  we might require synthetic data to result in the same application performance as the original data  with  out revealing sensitive information from the original data  In addition to the richness of data characteristics  appli  cations might require several properties and constraints be together satis ed in a generated database  This requirement motivates the need for a declarative approach to data gen  eration as opposed procedural approaches considered in  7  15   As a concrete example  consider generating a customer  product order database where we need to capture correla  tions between several pairs of columns such as customer age and product category  customer age and income  and prod  uct category and supplier location  It is fairly nontrivial for a programmer to design a procedure that outputs a database with all of the above properties  even with the right proce  dural primitives  A natural and expressive language for specifying data char  acteristics is a set of cardinality constraints  A cardinality constraint speci es that the output of a given query over the generated database should have a particular cardinality  As a simple example  we can  approximately  specify the distri  bution of values in a column by providing a histogram  and a histogram can be represented as a collection of cardinal  ity constraints  one for each bucket  In Section 2  we show that many of the data characteristics discussed earlier can be represented using cardinality constraints  The idea of using cardinality constraints for data gener  ation is not new and has been proposed in QAGen  6  and its extension MyBenchmark  22   However  in this work car  dinality constraints are mostly used for capturing workload characteristics and the ability of cardinality constraints to express more general data characteristics is not discussed  Motivated by the above discussion  the goal of this pa  per is to design e cient algorithms for generating synthetic databases that satisfy a given set of cardinality constraints  The set of constraints provided as input can be large  say  thousands   for example  even specifying a simple histogram can require 10s or 100s of constraints  The queries in the constraints can be complex  possibly involving joins over multiple tables  Prior Work  While QAGen  6  and MyBenchmark  22  do not discuss the expressiveness aspects of cardinality con  straints  their techniques are quite general and can be used for our purposes  However  they have some basic limita  tions  QAGen and MyBenchmark assume that cardinality constraints are available in a particular form called anno  tated query plans  AQP   An annotated query plan is a query plan with a subset of plan nodes annotated with cardinali  ties  We can show that we can encode cardinality constraints as AQPs and vice versa  For data generation  QAGen uses a novel approach called symbolic query processing  Brie y  it starts with a symbolic database  a symbolic database is like a regular database  but its attribute values are symbols  vari  ables   not  constants   It then translates the input AQPs to constraints over the symbols in the database  and invokes a black box constraint satisfaction program  CSP  to identify values for symbols that satisfy all the constraints  One limitation of QAGen is that it can handle a sin  gle AQP  and therefore cannot be directly used to generate databases that satisfy multiple arbitrary constraints  This limitation is identi ed and addressed in MyBenchmark  22   Brie y  to handle n AQPs  MyBenchmark uses QAGen to generate n symbolic databases with constraints and per  forms  matching  between these databases to heuristically identify m   n databases that together satisfy all the AQPs  MyBenchmark is not guaranteed to produce a single database instance and this functionality can be unsuitable for some applications requiring synthetic data  For example  we cannot use multiple database instances for DB applica  tion testing  since no single instance re ects all the charac  teristics of the original database  One advantage of using a general purpose CSP is that it enables QAGen to handle complex queries  e g   queries with HAVING clauses  However  this generality comes with a performance cost  The number of times QAGen and My  Benchmark invoke a CSP grows with the size of the gener  ated database and this has serious performance implications as the experiments in  6  22  indicate  1 The algorithms that we propose do not have these limitations  they always gen  erate a single database instance and their dependence on the generated database size is limited to the cost of materializing the database  Interestingly  recent work on cardinality estimation using maximum entropy principle  27  28  can be adapted to de  rive algorithms for data generation  and we discuss this pos  sibility in detail in Section 4  However  brie y  cardinality estimation using maximum entropy is known to be a very hard problem and adaptations of current solutions do not e ciently handle complex constraints  Summary of Contributions  We formally introduce car  dinality constraints in Section 2 and show that a set of car  dinality constraints forms an expressive language for speci  fying data characteristics  In Section 3  we state the formal data generation problem and show that the general prob  lem is NEXP complete and therefore hard  We present our algorithms in Section 4  While the general data genera  tion problem is hard  our algorithms are able to handle a large and useful class of constraints  Our algorithms are probabilistically approximate  meaning that they satisfy all constraints in expectation  We note that this is su cient for most applications of data generation  Our algorithms are also sensitive to the complexity of the input cardinal  ity constraints in a precisely quanti able way and use ideas from probabilistic graphical models  26   We include detailed experimental evaluation of our algorithms in Section 6 and conclude  1 One aspect of QAGen that we do not consider in this paper is parameters in AQPs  In the full version of the paper  we show that a data generation problem instance having cardinality constraints with parameters can be transformed to an instance not involving parameters  for a large class of constraints 2  CARDINALITY CONSTRAINTS In this section  we formally introduce cardinality con  straints and discuss their expressiveness  We need a few notations  rst  We denote a relation R with attributes A1          An as R A1          An   We use Attr  R    fA1          Ang to denote the set of attributes of relation R  A database D is a collection of relations R1          Rl  Given the schema of D  a cardinality constraint is of the form j A P  Ri1 1       1 Rip  j   k where A is a set of attributes  P is a selection predicate  and k is a non negative integer  A database instance satis es a cardinality constraint if evaluating the relational expression over the instance produces k tuples in the output  Through  out this paper  we assume that relations are bags 2 and rela  tional operators use bag semantics  The projection operator in cardinality constraints is duplicate eliminating  the input and output cardinalities of a duplicate preserving projection operator are identical  and therefore duplicate preserving projections are not interesting in constraints   The projec  tion operator is optional   We now show that a set of a cardinality constraints can be used to declaratively encode various data characteristics of interest  Schema Properties  3 We can specify that a set of attributes Ak   Attr  R  is a key of R using two constraints j Ak  R j   N and jRj   N  We can specify that R A is a foreign key referencing S B using the constraints j A R 1A B S j   N and jRj   N  We can similarly represent more general inclusion dependencies between attribute values of one table and attribute values of another  Such inclusion dependencies can also be used with reference  knowledge  tables such as a table of all US addresses to ensure that the generated databases satisfy various naturalness properties  Value distributions  As mentioned earlier  we can approxi  mately capture the value distribution of a column using a histogram  We can specify a single dimension histogram by including one constraint for each histogram bucket  The constraint corresponding to the bucket with boundaries  l  h  having k tuples is j l A h R j   k  We can capture correla  tions between attributes using multi dimension histograms such as STHoles  8   which can again be encoded using one constraint for each histogram bucket  Correlations span  ning multiple tables can be speci ed using joins and multi  dimension histograms  For example  we can specify cor  relations between customer age and product category in a customer product orders database using multi dimension histograms over the view  Customer 1 Orders 1 Product   We can approximately constrain the performance of a query plan over generated data by specifying intermediate cardi  nalities as shown in Figure 1  Each intermediate cardinality maps to a cardinality constraint  In the data masking sce  nario from Section 1  these intermediate cardinalities can be obtained by evaluating the query plan on the original data to ensure that the performance of the plan on original and synthetic data are similar   We have not fully explored the privacy related issues in this setting  We note that cardinal  ity constraints integrate nicely with di erential privacy  13   2 A bag is a multi set where an element can appear multiple times  3 Although  we can encode schema properties using general cardi  nality constraints  for e ciency purposes  our algorithms handle them in a special way  di erent from other constraints  LINEITEM ORDERS CUSTOMER       150000 600000 70000 150000 20000 17550 100000 Type      M    Age   40 Figure 1  Query Plan intermediate cardinalities instead of using actual intermediate cardinalities  we can fudge cardinalities using di erential privacy algorithms and use the fudged constraints for data generation  We are cur  rently exploring these directions   The full version of the paper includes examples of using cardinality constraints to capture more complex attribute correlations such as those of  14   join distributions between relations  and skew of values in a column  We view cardinality constraints as a useful programmatic abstraction for data generation and we envision automated techniques  not manual approaches  generating the constraints  The techniques for generating constraints are application speci c and outside the scope of this work  We note that the set of cardinality constraints is complete  in a sense that any database instance can be fully speci ed using such constraints  for each tuple  we use a constraint to specify its existence   Also  the cardinality constraints that we consider in this paper are a special class of logic programs and cardinality constraints studied in  30   the focus of this work is to extend logic programs with cardinality constraints and establish formal semantics  3  PROBLEM FORMULATION We now formally state the data generation problem  Data Generation Problem  DGP   Given a database schema and a collection of cardinality constraints C1          Cm  generate a database instance conforming to the schema that satis es all the constraints  In the decision version of the problem  the output is Yes if there exists a database instance that satis es all the con  straints and No  otherwise  We can show even the decision version of the problem is extremely hard  Theorem 1  The decision version of the data generation problem is NEXP complete  Due to space constraints  we defer all proofs to the full ver  sion of the paper  We note that the problem of checking whether a logical program with cardinality constraints has a model is NEXP complete  30   but Theorem 1 does not fol  low from the results in  30  since logical programs of  30  are more general than the cardinality constraints we consider  While the general DGP problem is hard  there exist prob  abilistically approximate e cient algorithms for a large anduseful class of constraints that we present next  These al  gorithms satisfy the input constraints only in expectation  as we observed in Section 1  such approximation is usually acceptable in practice  4  ALGORITHMS We begin  in section 4 1  by presenting an algorithm for the simple case of a single table with a single attribute that involves solving a linear program  LP   A straightforward generalization of the LP approach for multiple attributes produces an exponentially large LP  and we use ideas from from probabilistic graphical models to reduce the size of the LP  In section 4 3  we present algorithms for generating mul  tiple tables possibly involving join constraints  Sections 4 1  4 3 assume that the input constraints do not involve projec  tions  We discuss constraints with projections in section 4 4  We  rst present some notation and simplifying assump  tions  We denote the domain of attribute A using Dom A   We assume the domains of all attributes are positive in  tegers  this assumption is without loss of generality since values from other domains can be 1 1 mapped to positive integers  To simplify presentation  we further assume that the domain of all attributes is  D    f1          Dg for a known positive integer D  Removing the assumption that D be known in advance and handling di erent domains for di er  ent attributes is straightforward  For presentation simplicity  we assume selection predicates are conjunctions of range predicates of the form A 2  l  h   equality predicate is a special case where l   h  Our al  gorithms can be extended to work with selection predicates with disjunction and non equalities such as          and    In the rest of this section  we use C1          Cm to denote the input constraints  We specify the exact form of the constraints in each subsection  4 1 Single Table  Single Attribute Let R A  denote the table being generated  Without loss of generality  each constraint Cj  1   j   m  is in the canonical form j lj A hj  R j   kj   We generate a simple integer linear program  ILP  to solve this instance of DGP  For each i 2  D  we create a variable xi that represents the number of copies of i in R  We create the following system of equations to capture the m constraints  hXj1 i lj xi   kj for j   1          m We further require that each xi is a nonnegative integer  We can show that any solution to the above ILP corresponds to a solution of the DGP instance  In general  solving an ILP is NP hard  19   However  the above ILP has a special structure  we can show that the ma  trix corresponding to the system of equations has a property called unimodularity  19   This property implies that a so  lution of the corresponding linear programming  LP  relax  ation is integral 4    We obtain the LP relaxation by dropping the integer requirement   An LP can be solved in polynomial time  but this does not imply a polynomial time solution to DGP since the number of variables in the LP is proportional 4 This solution is integral only in the presence of a linear opti  mization criterion  so we need to add a dummy criterion to get integral solutions  to domain size D  which may be much larger than the sizes of the input and the database instance being output  We next present a simple intervalization trick to reduce the size of the LP  Intervalization  Let v1   1  v2          vl   D   1 denote in increasing order the distinct constants occurring in predi  cates of constraints Cj including constants 1 and D   1  We de ne  l  1  basic intervals  vi  vi 1   1   i   l   We intro  duce a variable x vi vi 1  for each basic interval  vi  vi 1  to represent the number of tuples in R A  that belong to the interval  Consider a constraint Cj   j lj A hj  R j   kj   By construction  there exist vp   lj and vq   rj   and we use the following equation to capture Cj   Xq1 i p x vi vi 1    kj As before  a solution to the above LP can be used to con  struct a solution for the DGP instance  We can easily see that the number of variables is at most twice of the number of constraints  implying a polynomial time solution 5 to the DGP problem  Example 1  Consider a DGP instance with three con  straints j 20 A 60 R j   30  j 40 A 101 R j   40  and jRj   50 and assume a domain size D   100  There are 4 basic in  tervals   1  20    20  40    40  60    60  101   The correspond  ing linear program consists of the three equations  x 1 20    x 20 40    x 40 60    x 60 101    50 x 20 40    x 40 60    30 x 40 60    x 60 101    40 One solution to the LP is x 1 20    2  x 20 40    8  x 40 60    22  and x 60 101    18  To generate R A   we pick 2 values  e g   at random  from  1  20   8 values from  20  40   and so on  This intervalization trick is applicable and implicitly part of all of our algorithms  although for presentation simplicity we may not explicitly mention this fact  4 2 Single Table  Multiple Attributes Let R A1          An  denote the table being generated  Each constraint Cj is of the form j Pj  R j   kj   For conciseness  we sometimes denote constraint Cj as the pair hPj   kj i in this subsection  The following theorem implies that the DGP problem becomes signi cantly harder when we move from single to multiple attributes  Theorem 2  The decision version of the single table data generation problem without projections is NP complete for even two attributes  n   2   We  rst consider a generalization of the algorithm in Sec  tion 4 1  For every tuple t 2 Dom A1            Dom An   we create a variable xt representing the number of copies of t in R  For each constraint Cj   hPj   kj i  we generate a linear equation  X t Pj  t  true xt   kj 5 Given an LP solution  the time for generating the actual table is linear in the size of the output  which may be independent of the input size  However  since any algorithm takes that much time  we do not count it in the running time    With LP relaxation  a solution to the above set of equations might not always be integral  otherwise  Theorem 2 would imply P   NP  However  slightly violating some cardinal  ity constraints is acceptable for most applications of data generation  We can derive a probabilistically approximate solution by starting with an LP relaxation solution and per  forming randomized rounding  Round xt to bxtc with prob  ability xt  bxtc and to dxte with probability dxte  xt  We can prove that relation R generated in this manner satis   es all constraints in expectation  E j Pj  R j    kj for all constraints Cj   We refer to this algorithm as LPAlg  Even with intervalization  the number of variables cre  ated by LPAlg can be exponential in the number of at  tributes  We next present more sophisticated algorithms that use ideas from graphical models  26   If the input con  straints are low dimensional  involve a small number of at  tributes  and sparse this is often the case in practice  these algorithms signi cantly outperform LPAlg  4 2 1 Algorithms based on Graphical Models We begin with a toy example illustrating a more e cient strategy for data generation than LPAlg  Example 2  Consider a DGP instance with domain size jDj   2 and 2n   1 constraints jRj   1000  j Ai 1 R j   500 and j Ai 2 R j   500  1   i   n   LPAlg solves an LP involving 2 n variables for this instance  There exists a simpler strategy  We generate 1000 random tuples  where each tuple is generated by picking each of its attribute values from f1  2g uniformly at random  It is easy to see that this generated instance satis es all constraints in expectation  Informally  the strategy in Example 2  decouples  attributes from one another and generates values for each attribute  independently   The algorithms we present next are essen  tially generalizations of this idea that identify and exploit various  independence  properties between attributes for ef   cient data generation  Figure 2 presents a general class of algorithms  which in  cludes the algorithm of Example 2  We assume for simplic  ity that the size of R  jRj   N  is provided as input  With each attribute Ai  we associate a random variable Xi that assumes values in Dom Ai   Let X   fX1          Xng  We denote a joint probability distribution over X1          Xn us  ing p X1          Xn   or p X    for short   In the  rst step  we identify a distribution p X   that belongs to a special class called generative distributions  Definition 1   Generative Distribution  A generative distribution is a probability distribution p X   that satis es the property that for each constraint Cj   hPj   kj i  the prob  ability that predicate Pj is true for a tuple sampled from p X   is kj N  In the next step  we independently sample N times from this distribution to generate an instance R  By construc  tion  this instance satis es all constraints in expectation  i e   E j Pj  R j    kj   Moreover  using Hoe ding s inequal  ity  we can show that the probability mass is concentrated around the mean  Pr jj Pj  R j  kj j   t    2 exp  2t 2 N    Example 3  The algorithm in Example 2 uses the uni  form generative distribution p x1          xn    1 2 n for all x1          xn  Input A data generation problem involving R A1          An  and constraints C1          Cm and jRj   N 1  Identify a generative probability distribution p X  2  Sample N times from p X  to generate R Figure 2  A general probabilistic algorithm The following proposition asserts that the algorithm of Figure 2 is always feasible  Proposition 3  If an instance of single table data gen  eration problem  without projections  has a solution  there exists a generative probability distribution for the instance  We next discuss the problem of identifying a generative probability distribution p X    In general  there could be several generative probability distributions for an instance of single table DGP  Theorem 4 states there always exists a generative distribution that factorizes into a product of sim  pler functions  we later use this factorization to identify such a distribution  We  rst introduce some notation used in the statement of Theorem 4  For each constraint Cj   hPj   kj i  we use Attrs Cj   to denote the set of attributes appear  ing in predicate Pj and X  Cj   to denote the set of random variables corresponding to these attributes  For example  if Cj is j A1 5 A3 4 R j   10  then Attrs Cj     fA1  A3g and X  Cj     fX1  X3g  For any X 0   X   we use f X 0   to denote a function f over random variables in X 0   Func  tion f X 0   maps an assignment of values to random vari  ables in X 0 to its range  which is usually nonnegative re  als R  0    We assume that each attribute Ai appears in at least one constraint  otherwise  we add the trivial constraint j 1 Ai D R j   N  Theorem 4  If an instance of single table data genera  tion problem  without projections  has a solution  then there exists a generative probability distribution p X   that factor  izes as  p X     Y Xi 9Cj s t Xi X Cj   fi Xi  for some functions fi  Example 4  Consider a DGP instance where Attrs C1    fA1  A2g and for all other constraints Cj  j   1  6   jAttrs Cj  j   1  Theorem 4 asserts that there exists a generative prob  ability distribution p X1          Xn  for this instance that can be expressed as f1 X1  X2 f3 X3       fn Xn   where fi are some functions  Note that a DGP instance can have several generative distributions and all such distributions need not factorize as above  but there exists at least one that does  The factorization of a probability distribution implies var  ious independence properties of the distribution  It is conve  nient to use an undirected graph to infer independence prop  erties implied by a factorization  26   The Markov network of a DGP instance is an undirected graph G    X   E  with vertices corresponding to the random variables X1          Xn  Graph G contains an edge  Xi  Xj   whenever fXi  Xjg   X  Cj   for some constraint Cj   The following lemma char  acterizes the independence properties of distributions p X   that factorize according to Theorem 4  It follows from a simple application of the well known Hammersley Cli ord theorem  16  26      X1 X2 X3 X4 X1 X2 X4 X3 X1 X2 X4 X3  a   b   c  X1 X2 X3 X4 X7 X6 X8 X9 X5  d  Figure 3  Example Markov Networks Lemma 1  Let XA  XB  XC   X be nonoverlapping sets such that in the Markov network G every path from a vertex in XA to a vertex in XB goes through a vertex in XC  Then for any probability distribution that factorizes according to Theorem 4   XA   XBjXC    The notation  XA   XBjXC  denotes that XA and XB are conditionally independent given XC   As a special case of Lemma 1  if XA and XB belong to di erent connected com  ponents then XA   XB  unconditionally  for any distribu  tion p X   that factorizes according to Theorem 4  Example 5  Continuing Example 4  the Markov network for this instance has n vertices and a single edge  X1  X2   Lemma 1 implies that there exists a distribution p X   for which  Xi   Xj   for all pairs fXi  Xjg 6  fX1  X2g  Us  ing chain rule and the above independences  we can show that p X     p X1  X2  p X3       p Xn   where  for exam  ple  p X1  X2  denotes the marginal distribution of  X1  X2   The algorithm we present next uses this observation to di  vide the problem of identifying p X   into  smaller  problems of identifying the marginals p X1  X2   p X3           p Xn   Example 6  Consider a DGP instance whose Markov net  work is the path graph shown in Figure 3 a   There exists a generative p X   for which  X1   X3jX2   since the only path from X1 to X3 passes through X2  For the graph of Figure 3 b    X1   X3jX2  X4   but  X1 6  X3jX2   We now present two algorithms for identifying a gener  ative distribution  While the algorithms are easy to state  the rationale for some steps of the algorithms and their for  mal correctness depend on nontrivial properties of graphical models and are presented in the full version of the paper  The two algorithms are incomparable and depending on the input one can outperform the other  Algorithm based on Chordal graphs The  rst algorithm is designed based on the insight that the factors fi in Theorem 4 allow a natural probabilistic interpretation if the Markov network is a chordal graph 6   A graph is chordal if each cycle of length 4 or more has a chord  a chord is an edge joining two nonadjacent nodes of a cycle  The graph in Figure 3 b  is not chordal  but adding the edge  X2  X4  results in the chordal graph shown in Figure 3 c   The formal algorithm is presented in Figure 4  We begin by constructing  Step 1  the Markov network of the input 6 In the language of graphical models  the distribution p X  is a decomposable distribution if the Markov network is chordal  26   Input A data generation problem involving R A1          An  and constraints C1          Cm and jRj   N Output  A generative probability distribution p X  1  Construct the Markov network G    X  E  2  Add edges to G to get a chordal graph Gc    X  Ec  3  Identify maximal cliques Xc1          Xcl in Gc 4  Solve for marginal distributions p Xc1           p Xcl  5  Use chordal graph property to construct p X  from the marginals p Xc1           p Xcl  Figure 4  Indentifying a generative distribution us  ing Chordal graphs DGP instance  In general  the Markov network of a DGP in  stance need not be chordal  In Step 2  we use the algorithm of Tarjan and Yannakakis  31  to convert the Markov net  work G    X   E  to a chordal graph Gc    X   Ec   E   Ec  by adding additional edges if necessary  In Step 3  we iden  tify the maximal cliques Xc1          Xcl of Gc  In Step 4  we  solve  for the marginal distributions p Xc1           p Xcl   this step is discussed in detail later  Chordal graphs have a spe  cial property  26  that allows us to construct  in Step 5  the desired distribution p X   using the marginals p Xc1           p Xcl   The following example illustrates this property  Example 7  Consider a DGP instance whose Markov net  work is the path graph of Figure 3 a  and let p X1  X2  X3  X4  be a distribution that factorizes according to Theorem 4  The graph has no cycles and is therefore chordal with maximal cliques fX1  X2g  fX2  X3g  and fX3  X4g  We show that p X1  X2  X3  X4  can be computed using the marginals over these cliques  p X1  X2  X3  X4    p X1  X2 p X3jX1  X2 p X4jX1  X2  X3    p X1  X2 p X3jX2 p X4jX3    p X1  X2  p X2  X3  p X2  p X3  X4  p X3  The second step follows from the  rst using the independence properties described in Example 6  p X2  and p X3  can be obtained from p X2  X3  by summing out X3 and X2  respectively  Finally  note that such a distribution is easy to sample from  we  rst pick x1  x2 from p X1  X2   then pick x3 from p X3jX2   x2  and then pick x4 from p X4jX3   x3   To identify the marginal distributions p Xc1           p Xcl   Step 4  Figure 4   we construct and solve a system of linear equations  The variables in these equations are probability values p x   x 2 Dom Xci  of the distributions p Xci   In the following  we use pXci to denote the marginal over vari  ables Xci  The  rst set of equations below ensures that pXci are valid probability distributions  The second set ensures that the marginal distributions satisfy all constraints within their  scope   X x2Dom Xci  pXci  x    1 1   i   l X x2Dom Xci  Pj  x  true pXci  x    kj N X  Cj     Xci Consider any two cliques Xci and Xcj such that Xci Xcj  6    For any x 2 Dom Xci   Xcj    let ExtXci  x  denote the set of assignments to Xci that is consistent with the assignment x Input A data generation problem involving R A1          An  and constraints C1          Cm and jRj   N Output  A generative probability distribution p X  1  Construct the Markov network G    X  E  2  Identify maximal cliques Xc1          Xcl 3  Solve for marginal distributions p M Xc1            p M Xcl   4  Construct p X  from the marginals p M Xc1            p M Xcl   Figure 5  Indentifying a generative distribution us  ing Markov Blankets We include the following equation for each x 2 Dom Xci   Xcj   in the linear program  X y2ExtXci  x  pXci  y    X z2ExtXcj  x  pXcj  z  The marginal distribution p Xci   Xcj   can be computed either by starting with p Xci  and summing out the variables in Xci  Xcj or by starting with p Xcj   and summing out variables in Xcj  Xci  The above set of equations ensure that we get the same marginal distribution in either case  Example 8  Consider a DGP instance R A1  A2  A3  A4  with 3 constraints  j A1 0 A2 0 R j   5  j A2 0 A3 0 R j   5  and j A3 0 A4 0 R j   5  Assume N   10 and a binary domain f0  1g for each attribute  The Markov network for this instance is the path graph of Figure 3 a   The maximal cliques are the three edges  We solve the following system of equations to identify the marginals over the edges  The notation p12 00  is a shorthand for p X1   0  X2   0   p12 00    p12 01    p12 10    p12 11    1  1  p23 00    p23 01    p23 10    p23 11    1  2  p34 00    p34 01    p34 10    p34 11    1  3  p12 00    1 2  4  p23 00    1 2  5  p34 00    1 2  6  p12 00    p12 10    p23 00    p23 01   7  p12 01    p12 11    p23 10    p23 11   8  p23 00    p23 10    p34 00    p34 01   9  p23 00    p23 10    p34 00    p34 01   10  Equations 1 3 ensure that the marginals are probability dis  tributions  equations 4 6 ensure that the marginals are con  sistent with the constraints  and equations 7 10 ensure the marginals produce the same  submarginals  p X2  and p X3   We refer to the algorithm of Figure 2 that uses chordal graph method to identify p X   as CLPAlg  For the DGP instance in Example 8  CLPAlg solves an LP with 12 vari  ables and we can show that LPAlg uses with 16 variables  While this di erence is small  for a similar  path graph  in  stance with 10 attributes and domain size D   10  we can show that CLPAlg uses 9 10 2   900 variables while LPAlg uses 10 10 variables  Algorithm based on Markov Blankets The second algorithm is structurally similar to the algo  rithm based on chordal graphs  It solves for a set of low  dimensional marginal distributions using the input constraints and combines these distributions to get a genera  tive probabilistic distribution  We  rst introduce de nitions required to present the algorithm  Let G    X   E  denote the Markov network corresponding to the DGP instance  Definition 2  The Markov blanket of a set of variables XA   X   denoted M XA   is de ned as  M XA    fXij Xi  Xj   2 E    Xi 62 XA     Xj 2 XA g  The Markov blanket of XA is the set of neighbors of vertices in XA not contained in XA  For example  in Figure 3 a   M fX2g    fX1  X3g  We use M XA  as shorthand for M XA    XA  The formal algorithm using Markov blankets is presented in Figure 5  In Step 2  we identify maximal cliques Xc1          Xcl in G  In Step 3  we solve for the marginal distribu  tions p M Xc1            p M Xcl    We do this by setting up a system of linear equations quite similar to the one described earlier  In the  nal step  we construct a generative probabil  ity distribution p X   by combining the marginals identi ed in the previous step  Constructing p X   using marginals p M Xci   is quite in  volved and uses a canonical representation of probability distributions presented in  1   We present these details in the full version of the paper  The full version also includes some subtleties relating to ensuring the p X   is a positive distribution that is required for the above construction to work and describes how to sample from p X   using Gibbs sampling  Complexity  We measure the complexity of the algorithm by the number of variables the algorithm creates  This is because theoretically a linear program can be solved in time polynomial in the number of variables  In practice  the ac  tual running time depends on the e ciency of the chosen linear programming solver  For the chordal graph approach  suppose   is the size of the largest maximal clique of Gc identi ed in Step 3  Then  the number of the variables in the linear equations is upper bounded by O lD      where l is the number of maximal cliques identi ed in step 3 and D  the domain size  For the Markov blanket approach  let     maxi jM Xci j  We can show that the number of the variables is upper bounded by O lD      Depending on the actual Markov network  one approach can be more e cient than another  The following example illustrates this claim  Example 9  Consider the 3   3 grid Markov network G shown in Figure 3 d   The maximal cliques of G are its edges  so the marginals solved by the algorithm correspond to M fX1  X2g    fX1  X2  X3  X4  X5g  M fX2  X3g    fX1  X2  X3  X5  X6g  and so on  More generally  consider an n n grid Markov network G  Since each Markov blanket is of a constant size and there are 2 n  1 n edges  Markov blankets approach uses at most 2n n1   jDj O 1  variables in its LP  We can show that the size of the largest maximal clique of any chordal super graph of G is at least n   1  and therefore the chordal graph approach uses jDj n 1 variables in its LP  which is less e cient  In contrast  for the Markov networks in Figure 3 a   c   we can show that the chordal graph method is more e cient  Maximum Entropy based approaches  Another approach to identifying a generative probability distribution p X    in Figure 2  is to pick a distribution with maximum entropy  MaxEnt  that satis es the constraints C1          Cm  Identifying such MaxEnt distributions is the subject of re  cent work on cardinality estimation using query feedback  27     R1  K1  A  FK2  FK3  R2  K2  B  R3  K3  C  FK4  R4  K4  D  V1 A  B  C  V2 B  V3 C  D  V4 D  Figure 6  Example snow ake schema 28   MaxEnt based approaches have two drawbacks   1  Current techniques for identifying MaxEnt distributions can  not handle complex constraints involving joins and  dupli  cate eliminating  projections   2  Identifying a MaxEnt dis  tribution involves solving a set of non linear equations and is therefore expensive  In other words  settling for non  MaxEnt distributions allows us to stay within linear pro  gramming and also handle a larger class of constraints  4 3 Multiple Tables In this section  we present our algorithms for data gener  ation problem involving multiple tables  For the rest of this section  assume a DGP instance involving relations R1          Rn and constraints C1          Cm  Each constraint Cj is of the form j Pj  Ri1 1       1 Ris  j   kj   We assume that the tables R1          Rn form a snow ake schema  11  and all joins are foreign key joins  A snow ake schema has a central fact table and several dimension ta  bles which form a hierarchy  We can represent a snow ake schema as a rooted tree T with nodes corresponding to the tables R1          Rn  and directed edges corresponding to for  eign key relationships  The root of the tree is the fact table  Each relation has a single key attribute  zero or more foreign keys that reference keys of other tables  and any number of non key attributes that we call value attributes  We make the fairly natural assumption that selection predicates in constraints involve only value attributes  We note that keys and foreign keys also represent constraints that need to be satis ed by an output instance  We can extend our algo  rithms to work with non snow ake schemas  e g   directed acyclic graphs instead of trees  however  we do not have al  gorithms that can handle non foreign key joins and designing such algorithms is future work  Example 10  Figure 6 shows four relations R1  R2  R3  R4 that form a snow ake schema with R1 being the fact table  The keys of the relations are shown underlined and the for  eign keys are named by pre xing  F  to the key that they ref  erence  e g   FK2 is the foreign key referencing R2 K2  The value attributes are A  B  C  D  Two example constraints are j C 5 D 2 R3 1 R4 j   20 and j D 2 R1 1 R3 1 R4 j   30  We use regular tree terminology to specify relationships be  tween tables  e g   R1 is the parent of R2 in Figure 6  For each relation Ri  we de ne a view Vi formed by join  ing all its descendant tables and projecting out non value attributes  This projection is duplicate preserving unlike the projections in our constraints  For example  in Figure 6  V3    C D R3 1 R4   where we use   to denote duplicate preserving projection  With this de nition  we can rewrite each constraint Cj as simple selection constraint over ex  Input  A data generation problem involving R1          Rn and constraints C1          Cm  Output Instances of R1        Rn satisfying constraints 1  Generate instances of each view Vi satisfying constraints associated with Vi 2  Root to leaf  Update each view Vi  Vi   Vi     Attr Vi   Vpi   Vi  where Vpi is the parent of Vi 3  Generate instances R1          Rn from V1          Vn Figure 7  Algorithm for multiple tables actly one of the views Vi  For instance  the  rst constraint in Example 10 can be rewritten as j C 5 D 2 V3 j   20  The above observation forms the basis of our algorithm  which is presented in Figure 7  In Step 1  we generate an instance of each view Vi that satis es all cardinality con  straints associated with it  Since the constraints are all  sin  gle table  selection constraints  we can use algorithms from Section 4 2 to generate these instances  However  these inde  pendently generated view instances may not correspond to valid relation instances  Consider relation instances R1          Rn that satisfy all key foreign key constraints and let Rpi de  note the parent of some relation Ri  We can show that views Vi and Vpi should satisfy the property  Attr Vi  Vpi    Vi  note    is duplicate eliminating   For example  in Figure 6  every distinct value of B in V1 A  B  C  occurs in some tu  ple V2 B   However  the view instances generated in Step 1 may not satisfy this property  Therefore  in Step 2  we add additional tuples to each Vi to ensure that this containment property is satis ed in the resulting view instances  These updates might cause some cardinality constraints to be vi  olated  however we show shortly how the degree of these violations can be bounded  In the  nal step  we construct relation instances R1          Rn consistent with V1          Vn  We now discuss how to minimize the error introduced in Step 2   Error is the absolute di erence between required and actual cardinalities of expressions in constraints   Re  call that all of our algorithms use the intervalization trick  see Section 4 1   With intervalization  the value of an at  tribute is constrained to come from some interval  vi  vj   during data generation  but we are free to select any value from the interval  We can minimize the error introduced in Step 2 by picking values from an interval in a consistent manner across all views  The following example illustrates this idea  Example 11  Consider two relations R1 K1  A  F K2  and R2 K2  B  and four constraints  j B2 1 5  R2 j   2 j B2 5 10  R2 j   3 j B2 1 2  R1 1 R2 j   2 j B2 2 10  R1 1 R2 j   2 Assume domain size D   10  To identify view instances V1 A  B  and V2 B  that satisfy all cardinality constraints  LPAlg solves the following two LPs  using intervalization   x 1 10  1 2    2 x 1 10  2 5    x 1 10  5 10    2 y 1 2    y 2 5    2 y 5 10    3 Here x       denote LP variables corresponding to V1 and y       denote LP variables corresponding to V2  One solution of these LPs is x 1 10  1 2    2  x 1 10  2 5    1  x 1 10  5 10    1  y 1 2    0  y 2 5    2  y 5 10    3  If we pick the minimum possible value when selecting attribute values from an inter  val  we get the instances of V1 and V2  without boxed 1  shown below  A B 1 1 1 1 1 2 1 5 B 2 2 5 5 5 1 K1 A F K2 1 1 6 2 1 6 3 1 1 4 1 4 K2 B 1 2 2 2 3 5 4 5 5 5 6 1 V1 A  B  V2 B  R1 K1  A  F K2  R2 K2  B  In Step 2 of our algorithm  we add the  boxed  tuple 1 to V2 B  to ensure  B V1    V2  This update results in an ad  ditive error of 1 in one of the constraints  The instances of R1 and R2 that are consistent with V1 and V2 are also shown above  We can verify that randomly and independently pick  ing values from intervals results in a solution with a larger error 7   In the full version  we present rounding techniques that to  gether with the intervalization trick above ensure that the additive error of any constraint is bounded by O m   the number of cardinality constraints  The full version also presents extensions to the algorithm in Figure 7 that can handle degree constraints  e g   each tuple in R2 is refer  enced by at most 10 tuples in R1  Such constraints allow us to have  ner control over the joins across relations  4 4 Projections This section brie y discusses how to handle cardinality constraints with projections  Recall from Section 2 that only duplicate eliminating projections are useful as constraints  The general data generation problem involving projections is very hard as the NEXP completeness result suggests  In the full version  we show that the data generation problem over a single table with constraints that have just projections and no selections is nontrivial and has connections to known hard problems in combinatorial geometry  Here we present an algorithm for the case of a single table and a single attribute  The ideas behind this algo  rithm can be combined with techniques from earlier sec  tions to derive a more general solution for the case where at  tributes involved in di erent projection constraints are non  overlapping  meaning  if j A1        j   k1 and j A2        j   k2 are two constraints then either A1   A2 or A1   A2      Let R A  denote the table being generated  Each con  straint Cj has one of two forms  j A  A2 lj  hj   R  j   kj or j A2 lj  hj   R j   kj   We identify basic intervals  vi  vi 1   1   i   l  exactly as we did in Section 4 1  We now in  troduce two variables x vi vi 1  and y vi vi 1  for each ba  sic interval  x vi vi 1  denotes the number of tuples of R A  belonging to the interval and y vi vi 1   the number of dis  tinct tuples belonging to the interval  We generate one lin  ear equation corresponding to each constraint using x  vari  ables for constraints not involving a projection and y  vari  ables for constraints involving projections  For each inter  val  vi  vi 1   we add two additional equations  y vi vi 1    7 There exist techniques for consistently picking attribute values from intervals that ensure greater randomness in the output in  stances compared to the strategy of picking the minimum  x vi vi 1  and y vi vi 1     vi 1vi   The  rst equation cap  tures the constraint that in any interval the number of dis  tinct values is not greater than the number of values  count  ing duplicates  and the second captures the natural bound on the number of distinct values in the interval  As in Section 4 2  we solve the LP and perform random  ized rounding  One tricky case happens when q   y vi vi 1    x vi vi 1     q   1  where q is some nonnegative integer  If we independently round x vi vi 1  and y vi vi 1   we might end up with y   q   1 and x   q  which is inconsistent  To resolve this problem  we use a slightly di erent rounding procedure  We pick a random value r uniformly between q and q   1  We round x vi vi 1   resp   y vi vi 1   to q   1 if x vi vi 1    r  resp   y vi vi 1    r  and to q  otherwise  It is not hard to see that the solution is consistent and satis es all constraints in expectation  5  EXPERIMENTS 5 1 Setup To generate a large number of meaningful cardinality con  straints  we consider the following hypothetical scenario  We have an instance of TPC H benchmark database and our goal is to generate a synthetic database instance such that a workload of 8 queries Q1 Q10  not including Q4 and Q9  has similar performance characteristics over the synthetic and original database  We do not consider queries Q4 and Q9 since Q4 has a nonequality predicate between two attributes and Query Q9 has a LIKE predicate as the main predicate of the query  and our algorithms currently do not handle ei  ther of these types of predicates  Each TPC H query has an associated parameterization  and our workload consists of various parameterizations of these 8 queries  For example  in query Q3  we can substitute the parameter  SEGMENT  with  ve di erent values and the parameter  DATE  with 30 di erent values  The value distributions in TPC H are fairly simple  and we can  cheat  and generate an instance similar to the orig  inal TPC H database by generating attribute values almost independently  But such generation would not be possible if our database had the TPC H schema  but complex value correlations  We note in this context that once we  x the constraints  the performance of our algorithms is not sen  sitive to such correlations these correlations only change the cardinalities of the constraints  not the structure of the constraints and the performance of our algorithms depends on the structure of the constraints  not cardinality values  If we assume that the performance of queries is a func  tion of various intermediate join cardinalities  we get the following methodology for generating constraints  For each query  we consider the query plan produced by the query optimizer and the various nodes in the query plan  Each node corresponds to a relational expression  and we iden  tify all nodes whose relational expressions involve only joins and selections  we evaluate all such relational expressions for various parameterizations  and use the resulting expressions and cardinalities as input constraints  This produces a set of 1100 constraints ranging from single table constraints to join constraints involving all the TPC H tables  In the following  we report on various performance char  acteristics of our algorithms when run over various subsets of these 1100 constraints  Although we do not explicitly present this result  the synthetic database generated using 0 50 100 150 200 250 300 350 400 450 500 100 300 500 700 900 1100 Total time  sec  Number of Constraints Figure 8  Total runtime all of these 1100 constraints as input does indeed have a per  formance characteristic similar to the original database for the workload of 8 queries we start with  Our overall experiment methodology is very similar to the methodology used by  7  22   Also  since QAGen  6   reports performance numbers over TPC H as well  we can  roughly  compare the performance of our algorithm with that of QA  Gen using the numbers reported in  6   All of our experiments were run on a dual core 2 4 GHz machine with 6GB of main memory  5 2 Results We now report various performance results of our algo  rithms  In all of the results that we report here  we use the CLPAlg algorithm for identifying probability distribution within a single view  The algorithm that uses markov blan  kets has a worse performance than CLPAlg for this class of inputs  For solving the the linear equations  we used a commercial state of the art LP solver  Unless mentioned otherwise  we use TPC H 1G for all of our experiments  Figure 8 shows the overall running time of our algorithms as we vary the number of input constraints  To vary the number of constraints  we  rst ordered all the constraints by the queries from which they were obtained  For example  the constraints from Q1 occurred before constraints from Q2 in this ordering  and we picked various pre xes of this ordering to obtained subsets of di erent sizes  Recall that our overall algorithm has three stages  In the  rst stage  the algorithm sets up a linear program by an  alyzing the input constraints  in the second step  the lin  ear program is solved using the LP solver  The solution of the linear program represents a probability distribution for each view  and in the  nal step our algorithm samples from this probability distribution to produce the output database  Most of the overall time of our algorithm goes into the third and  nal stage  The  rst stage takes negligible amount of time and we report the time taken for the second stage  LP solving  shortly  Overall we note that our algorithm is able to generate a database instance for an input involving all the 1100 constraints in less than 10 minutes  Figure 9 shows the relative error of all the 1100 constraints  when we use all the 1100 constraints as input   Each con  straint is shown as a point in Figure 9  To better highlight the errors  we ordered all the constraints  on the horizon  tal axis  based on their cardinalities  constraints with larger cardinalities are shown to the right and constraints with 0 2 4 6 8 10 12 0 200 400 600 800 1000 1200 Relative Error  percentage  Constraints  increasing order of cardinalities  Figure 9  Relative errors in constraints Table Extra tuples Table size LINEITEM 0 6000000 ORDERS 24 1500000 CUSTOMER 18 150000 PARTSUPP 20 800000 PART 202 200000 SUPPLIER 0 10000 Figure 10  Error due to additional referential in  tegrity tuples smaller cardinalities are shown to the left  The relative error is shown on the vertical axis  As expected  and predicted by standard sampling theory   the constraints with larger car  dinalities have smaller relative error and constraints with smaller cardinalities have a larger relative error  But over  all  most of the constraints are satis ed with less than 5  error  There are two sources of errors in our algorithm  The  rst is introduced by sampling  and the second due to addition of extra tuples in Step 2 of Figure 6 to ensure referential integrity  Figure 10 shows the extra tuples added for each of the 6 tables  with foreign keys  in TPC H schema and the overall table size  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security"/>
        <doc>A Framework for Testing Query Transformation Rules ### Hicham G  Elmongui   Purdue University  elmongui cs purdue edu  Vivek Narasayya  Microsoft Research  viveknar microsoft com  Ravi Ramamurthy      Microsoft Research   ravirama microsoft com  ABSTRACT  In order to enable extensibility  modern query optimizers  typically leverage a transformation rule based framework  Testing  individual rule correctness as well as correctness of rule  interactions is crucial in verifying the functionality of a query  optimizer  While there has been a lot of work on how to architect  optimizers for extensibility using  a rule based framework  there  has been relatively little work on how to test such optimizers  In  this paper we present a framework for testing query  transformation rules which enables   a  efficient generation of  queries that exercise a particular transformation rule or a set of  rules and  b  efficient execution of corresponding test suites for  correctness testing   Categories and Subject Descriptors H 2 4  Database Systems   Query Processing General Terms Algorithms  Measurement  Performance   Keywords Database Testing  Query Optimization  Transformation rules ###  1  INTRODUCTION  Query optimizers in today   s DBMSs are responsible for obtaining  a good execution plan for a given query  Since a query optimizer  plays a crucial role in determining the performance of a query  it  is very important to rigorously test the optimizer to ensure that it  functions correctly  There has  been extensive work on how to  architect query optimizers in order to make them extensible  e g     12  13  16   using a rule based framework  However  there has  been relatively little work on how to effectively test such query  optimizers  It is well recognized that testing is an integral part of  any development cycle and typically more than 50  of the entire  development cycle is spent in testing  2     Testing the query optimizer has several dimensions which include  accuracy of cardinality estimation and costing modules  the search  space of the optimizer etc  In this paper we focus on query  optimizers that use a rule based architecture  Examples include  industrial query optimizers such as IBM Starbust  16   Microsoft  SQL Server  13   Tandem   s NonStopSQL  7  as well as academic  prototypes such as the Volcano optimizer  12   Such optimizers  use transformation rules as the basic primitive in order to generate  different alternative plans for a query  The set of transformation  rules  e g   join commutativity  and associativity  pushing GroupBy below join etc   used by an optimizer largely determines the  search space of plans considered by the optimizer and thus is a  key factor in determining the quality of the final plan  While  problems related to testing the components of the optimizer such  as the cardinality estimation and costing modules remain pertinent  for a rule based optimizer  in this paper we focus on issues related  to testing the transformation rules   One way to broadly categorize the issues that arise in the context  of rule testing is as follows  1   Coverage  Ensure that a  transformation rule has been exercised  during query optimization  in several different queries  2  Correctness  Ensure that when a  transformation rule is exercised for a query  it does not alter the  results returned when the query is executed  3   Performance   Analyze how the transformation rule impacts the performance of a  query workload  In this paper  we focus on the first two aspects   namely coverage and correctness   From the perspective of rule coverage  it is desirable to have tests  cases in the form of SQL queries such that when the queries are  optimized  they exercise all rules  In addition to ensuring that each  rule is exercised  it can also be important to test that pairs of rules   in general  a set of rules  are exercised together in a query     to    help capture rule interactions  Although the rule coverage problem  is important  there is little previous work in this area  The state ofthe art approach is to use stochastic methods to generate SQL  queries  e g   1  17   until we find a query that exercises the  desired rule or rule pair  Such  a trial and error approach has the  problem that it can take many trials to even find a single query  that exercises the given rule or rule pair  and rule coverage testing  requires finding several such queries  This is compounded by the  fact that such randomly generated queries tend to be rather  complex  and thus optimizing the query in each trial can take a  large amount of time  Another alternative is to build APIs that  support manual generation of SQL queries  9   however this  approach can be too time consuming and simply does not scale to  crucial scenarios such as pair wise  or larger  rule interactions       We note that in general the above problem of generating a query  such that a given rule  or set of rules  is exercised  is very  challenging  This is because it is hard to precisely capture the  sufficient conditions for a rule to be exercised by the optimizer   For example  modern optimizers use pruning steps in the  optimizer   s search algorithm that discards a rule based on  constraints or heuristics  As one illustrative example  consider the  rule that pushes down a Group By Aggregate over a join  3   This  rule is exercised only if certain functional dependencies are                                                                      Work done while visiting Microsoft Research  The author is also  affiliated with Alexandria University  Egypt   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit  or commercial advantage and that  copies bear this notice and the full citation on the first page  To copy  otherwise  or republish  to post on servers or to redistribute to lists   requires prior specific permission and or a fee   SIGMOD   09  June 29 July 2  2009  Providence  Rhode Island  USA   Copyright 2009 ACM 978 1 60558 551 2 09 06    5 00   257guaranteed to hold in the join result  e g   the Group By must  include the joining columns    An important contribution of this paper is a technique for query  generation that can address some of the above limitations  Our  technique leverages the intuition that for a particular rule to be  exercised during query optimization  the input logical query tree  must contain a    pattern    corresponding to that rule   12  16    In  other words  existence of such a pattern in the logical query tree is  a  necessary  but not a sufficient  condition for the rule to the  exercised  For instance  for the rule that pulls up a Group By  operator above a join operator  we know that a necessary  condition for the rule to be exercised is that the input logical tree  should contain a join operator with a Group By operator in one of  its subtrees  We leverage the above intuition by extending the  DBMS with a new API that exports such patterns  This in turn   enables the query generation method to directly leverage these  rule patterns while generating the SQL query  Although the above  technique does not guarantee that the generated query will  exercise the rule  our experiments indicate that our technique can  dramatically improve the number of trials  and hence the time   required to create test cases for rule coverage    Another important scenario is  correctness testing  of  rules   One  approach for testing the correctness of the rules uses the following  methodology  For each randomly generated query  check which  rules were exercised during query optimization  For each such  rule   a  execute the original query and obtain the results and  b   execute the plan obtained for the original query with the  corresponding rule turned off  i e  disabled   and then check if the  results of the query are identical or not  Naturally  if the results are  not identical  it indicates a correctness bug  In order to have  sufficient confidence in the correctness of a rule  we may need to  perform this validation over several  say  k  such randomly  generated queries for each rule  Since this methodology requires  executing queries for correctness validation  the time taken to run  these test suites can be significant  Thus the key challenge in such  correctness validation is  efficiency  i e  improving the time taken  to execute the test suites    A second contribution of this paper is that we show how to  significantly reduce the time taken to execute a test suite for  correctness testing of transformation rules  We exploit the  following key observations   a  When a query is optimized  often  multiple rules are exercised   b  The cost of a query when a rule is  turned off can sometimes be much higher than the cost of the  query when the rule is turned on  We introduce the novel problem  of  test suite compression  Given an initial set of randomly  generated queries  we identify the best way to map queries to rules  such that the time taken to run the entire test suite is minimized   while maintaining certain invariants   In this paper  we study one  version of this problem  where the constraint that the number of  distinct queries  k  validated for each rule remains the same  we  discuss another version of the problem in Section 7   We show  that the test suite compression problem is NP Hard  and present  algorithms for it including a constant factor approximation of the  optimal solution  Our experimental evaluation confirms our  intuition that the above optimization can indeed help significantly  reduce the time required for correctness testing of rules   In Section 2  we present a brief overview of rule based query  optimizers and introduce our framework for transformation rule  testing  In Section 3  we discuss our approach to the query  generation problem  In Section  4  we introduce the test suite  compression problem  and present algorithms for it in Section 5   We present experimental results in Section 6  discuss extensions  in Section 7  related work in Section 8  and conclude in Section 9   2  PRELIMINARIES  2 1 Transformation based Query Optimizers  In this paper we consider query optimizers that use a  transformation rule based architecture as described in  12   13    Such a framework has been used to build both industrial query  optimizers  e g  Tandem   s NonStop SQL  7  and Microsoft SQL  Server  as well as optimizers used in academic prototypes  e g  the  Volcano optimizer  12  and the Columbia optimizer  4    In this  section we give a brief overview of a ruled based framework for  query optimization  see  13  for more details    Transformation rule based optimizers use a top down approach to  query optimization  The optimizer is initialized with a logical tree  of relational operators corresponding to the input query  The goal  of the optimizer is to transform the input logical tree to an  efficient physical operator tree that can be used to implement the  query  For this purpose  transformation rules are used to generate  different alternative plans for executing a query  The set of rules  that are available to the optimizer essentially determines the  search space of plans considered by the optimizer and thus is a  key factor in determining the quality of the final plan   There are two main kinds of transformation rules   Exploration  rules or logical transformation rules  when exercised  transform  logical operator trees into equivalent logical operator trees  Some  examples of exploration rules include join commutativity and  pushing group by below join   Implementation rules or physical  transformation rules  when exercised  transform logical operator  trees into hybrid logical physical trees  Example implementation  rules include rules that transform a logical join into a physical  hash join    We note that other extensible optimizers such as Starburst  16   also leverage the idea of transformation rules during the query  rewrite phase to generate alternative logical representations of the  input query  In principle  the techniques described in this paper  can be extended to such optimizers even though they are not  based on the Cascades framework    2 2 Definitions  We use the following definitions and notations in the paper   Set of transformation rules  We denote the set of transformation  rules for the optimizer by R    r1       rn     RuleSet for a query  When a query q is optimized  we denote the  subset of transformation rules that are exercised as RuleSet q     Execution plan and cost  For a given query q  we use Plan q    and Cost q  to refer to the execution plan chosen by the optimizer  and its cost respectively  Let R     R be a set of rules  We denote  the execution plan and cost of a query q when the set of rules R is  disabled  i e  turned off  by Plan q    R  and Cost q    R     Logical query tree  A logical query tree is a tree of logical  relational operators where each operator has been instantiated  with its arguments    Figure 1 shows an example of a logical query tree  where the two  leaf operators Get T1   Get T2  refer to accessing relations T1 and  T2 respectively  Similarly  the join and projection operators also  contain the respective arguments    2582 3 A Framework for Testing Rules  Since transformation rules are a critical component of the query  optimizer  testing individual transformation rules and their  interaction is an important part of testing the overall query  optimizer  In this paper we assume that we are given as input a  test database  i e   the database is fixed  The techniques we present  are therefore general in the sense that they can be invoked against  any database     There are at least two key aspects to rule testing   One important  aspect is from the perspective of rule coverage i e  we would like  to have test cases in the form of SQL queries where a given rule   or a given set of rules  is exercised  This is important for code  coverage which can ensure that the code corresponding to the  rules have been covered  Observe that this does not require  execution of the query  It relies on optimizing the query and  requires the ability to track which rules are exercised during query  optimization    A second aspect is  correctness testing of the rule  While testing  cannot in general  prove that a transformation rule has been  correctly implemented in the DBMS  it is possible to find test  cases where the rule has  not been correctly implemented  One  methodology for finding such correctness bugs for a rule is to  check that the results produced by a query when the rule is  exercised are identical to the results of the same query when the  rule is not exercised  This requires   a  The ability to turn on off a  given rule during query optimization   b  Executing the two plans   when they are different   By  repeating this methodology for  several different randomly generated queries  e g  generated via a  stochastic method   we can increase confidence in the correctness  of the rule  Unlike the case of code coverage  the queries used for  validating correctness need to be executed  Thus  efficiency of  executing the above queries for  correctness testing is a key  challenge   Figure 2  Overiew of architecture  In this paper  we describe an initial framework for testing  transformation rules that can address the above scenarios  The key  components of our framework are shown in Figure 2  We now  provide an overview of each of the components   Query Optimizer Extensions  We assume a query optimizer with  support for the following functionality  First  is the ability to track  which rules are exercised during query optimization  Using this  extension allows us to determine RuleSet Q  for any query Q    Second  we support the ability to optimize  and execute  a query  when a given set of transformation rules is turned off  In other  words  this extension enables obtaining  Plan Q    R  for any Q  and set of rules R  We note that many existing optimizers may  already have support for one or both of these extensions     Query Generation  The query generation component takes as  input a set of rules R  and generates a SQL query such that  all rules in R are exercised when that query is optimized  Such a  module is useful for both code coverage as well as correctness  validation  We identify two key modules for query generation   The first module  which we refer to as Generate Logical Query  Tree generates a logical query tree that can potentially exercise a  given rule or rule pair  This is shown as the shaded box in Figure  2  and is the focus of Section  3  The second module  which we  refer to as Generate SQL  takes as input a logical query tree  see  Figure 1 for an example  and generates a SQL statement  corresponding to the query tree  We use a module whose  functionality is similar to one presented in  9   and therefore we  do not focus on it in this paper   Generate Logical Query Tree  Observe that the problem of  generating a logical query tree such that a given rule or rule pair is  exercised is non trivial  Manually generating a logical tree that is  guaranteed to exercise a rule or rule pair is both difficult and timeconsuming  and does not scale with the number of rules  For  example  if there are 25 transformation rules  generating test cases  for all  25 C2  rule pairs manually is not feasible  The alternative  trial and error approach of using randomly generated queries  e g   as in  1  17   is also not adequate since   a  It is inefficient  i e   it  can require many trials before a randomly generated query  exercises a given rule  or rule  pair    b  Randomly generated  queries can be hard to interpret  For debugging and  understandability purposes  it is desirable to generate a query with  a small number of logical operators such that the rules in R are  exercised  Thus a key challenge is to efficiently generate a logical  query tree with a small number of operators that exercises a given  rule  The efficiency of this  module can be measured by the  number of trials  and or time   required to find a query that  exercises the rule  Logical query tree generation for exercising  rules is the subject of Section 3    Finally  note that logical query tree generation module can also be  extended for generating more complex queries that exercise a  given rule  To enable such scenarios  the above module exposes  the ability to add an additional the number of  random  operators  to an existing logical query tree as a constraint  e g   generate a  logical query tree with 10 operators that exercises a given rule    For instance  such queries are useful for correctness testing    Correctness Testing   Correctness testing can be performed for  singleton rules  rule pairs or in general over any subsets of rules   In this paper  we focus on singleton rule and rule pairs since these  are the most fundamental cases that need to be covered  We  present the discussion below for singleton rules  but the arguments  carry over to rule pairs as well  To validate correctness of each  rule  we need to generate  k distinct queries  each of which  Figure 1   Example of a logical query tree  259exercises  at least  that rule  We refer to these queries as the test  suite for a singleton rule  r i    denoted by TSi   If there n singleton  rules  we require k distinct queries for each of the n rules  Thus   the overall test suite for all rules is  TS TS i n U i  1     The  Test Suite Generation module generates a test suite as  described above for a given set of rules  k is a parameter to this  module   Queries in the test suite can be generated by invoking  the Query Generation module described previously    For a given test suite  the Test Suite Execution module executes  the test suite as follows  For each query  q in TSi   we execute  Plan q  and Plan  q     ri     the plan obtained when we disable  rule r i   and check if the results of executing the two plans are  identical  Thus  the total cost of executing a test suite is 1                          n i TSi q i TotalCost Cost q Cost q r 1              We refer to the above technique of generation and execution of a  test suite  where k distinct queries are generated and executed for  each rule independently  as the BASELINE method  Since the  queries have to be executed   the time taken for the BASELINE  method can be significant  Thus  a key question is whether the  efficiency of correctness testing can be improved significantly  while still ensuring that each rule is validated for  k distinct  queries  The  Test Suite Compression module  in Figure 2   addresses this problem  In particular  the test suite compression  step identifies a subset TS        TS  while satisfying the constraint  that TS    contains for each rule  k distinct queries where that rule  is exercised  The objective is to minimize the cost of executing the  test suite thereby substantially improving upon the BASELINE  method  Test suite compression is the focus of Sections 4 and 5   We begin by first discussing the query generation problem in  Section 3  in particular the logical query tree generation problem    3  LOGICAL QUERY TREE GENERATION  As mentioned earlier  the problem of testing that a rule has been  exercised can be viewed as a query generation problem  Given a  transformation rule  we need to generate a SQL query which  exercises the rule when optimized  In this section  we first  highlight some of the challenges  and then present our approach to  the query generation problem   The key challenge in generating a query that exercises a particular  rule is that it is difficult to precisely capture the  sufficient conditions for the rule to be exercised  In general  the exact  preconditions necessary for a rule to be exercised can be  arbitrarily complex  For example  the search algorithm used by  the optimizer could discard a rule based on constraints heuristics   There could also be cases of rule dependencies  where the  exercising of one rule occurs only when one or more other rules  are first exercised  For example  consider the input logical query  tree  R Join  S LOJ T   where LOJ stands for left outer join   Consider the following two rules   1  Associativity of Join and  Outer join   2  Join commutativity  We know that in general                                                                   1  Note that if Plan q  and Plan q     r i    are identical  it is not  necessary to execute the query since the results are guaranteed  to be the same   outer joins and joins do not commute  However  if the join  predicate is between R and S  then the first rule can be exercised   which results in a logical tree  R Join S  LOJ T  Observe that the  second rule can now be applied on  R Join S      The state of the art approach for query generation  e g    1  17    is to keep generating queries using a stochastic process until one  finds a query that exercises the required transformation rule  Note  that we can track which optimizer rules actually were exercised  during query optimization by using the RuleSet interface  Section  2 2   We note that none of the previous work has however focused  on generating queries that exercise a certain transformation rule in  the query optimizer  As discussed earlier  the above trial and error  approach can require many trials before it finds a query that  exercises the given rule  For instance  consider a transformation  rule that pulls up a Group By operator over a left outer join   Obviously  a randomly generated query is not likely to succeed  unless it happens by chance to include a Group By  and a left  outer join in the same query  Thus  the random generation  approach can require a large number of trials before it finds an  appropriate query   In this section  we study how we can significantly improve upon  the state of the art for this problem  Our key observation is that  we can leverage rule patterns that serve as a necessary  although  not sufficient  condition for a transformation rule to be exercised  in the query optimizer for the purpose of query generation  In  most cases  this significantly reduces the number of trials needed  to find a query that exercises the given rule s   We present the  discussion below for the case of a singleton rule  We discuss  extensions to support rule pairs in Section 3 2    3 1 Exploiting Rule Patterns  Rules in a transformation based optimizer can be in general be  represented by the triple  Rule Name  Rule Pattern  Substitution    13   During query optimization  the rule engine checks if the  input logical tree matches the Rule Pattern  If so  it invokes the  Substitution function that generates a new logical tree that should  be included as part of its search  Thus a necessary condition for a  rule to be exercised is that the logical tree considered during the  search contains the pattern of the corresponding rule           Figure 3  Example Rule Patterns  Figure 3 illustrate examples of rule patterns for two  transformation rules   the join commutativity rule  and a rule for  pulling a Group By Aggregate above a join operator   As the figure indicates  the rule patterns include operators that  must be present  such as the Join and the GBAgg operator in the  second example  as well as placeholders for  generic operators   represented by circles in the patterns   These generic operators  can match any logical operator  Thus  for the first rule pattern  for  the join commutativity rule  to be exercised  the input logical  260query tree that should have a join operator  irrespective of what its  children are     Recollect that the Generate Logical Query Tree module  Section  2 3  takes as input a rule and outputs a logical query tree  Below   we described how this is achieved  We have extended the  database server with an API through which it returns the rule  pattern tree for a rule in a XML format  To generate a query that  exercises a particular transformation rule  the query generation  module first builds a logical query tree starting with the rule  pattern and   a  instantiates actual operators in place of the generic  operators  For example  for the join commutativity rule  we can  instantiate each of the generic operators with Get operators  These  are leaf operators that correspond to accessing base relations   b   Once the operators are instantiated  we select the  arguments for  each operator  For example  the Get operators can be instantiated  with relations T1 and T2 respectively as their arguments   Similarly  the join operator can be instantiated with a join  predicate such as T1 a   T2 b as its argument  Thus  at the end of  this step we have generated a valid logical query tree  e g  a tree  such the one shown in Figure 1   Finally  the Generate SQL  module  Section 2 3  is invoked with the above logical query tree  to generate a valid SQL statement  Note that rule patterns can also  provide sufficient conditions for implementation rules to be  exercised  For example  for the hash join implementation rule to  be exercised  the input pattern would need to include a join  operator node  Thus  the idea of leveraging rule patterns from the  optimizer can enable us to automatically generate queries that  exercise a particular rule   As mentioned above  in general  a logical query tree that contains  a rule pattern is not sufficient to guarantee that the particular rule  is exercised during optimization  For example  for the rule that  pulls up the Group By Aggregate over a join  some additional  conditions are required to hold  for e g   the join predicate does  not reference the aggregate results   However  observe that if such  constraints are well abstracted in the database engine  they can  potentially be added as additional preconditions on the input  pattern and leveraged by the query generation module  Certain  rules may also require that certain constraints on the schema or  the data instance hold in order to guarantee that it is exercised  we  discuss such cases in Section 7   Despite the fact that leveraging a rule pattern does not guarantee  that a rule is exercised  for the set of transformation rules used in  our experiments  Section 6   we  observed that by exploiting the  basic rule patterns in query  generation  we can significantly  reduce the number of trials required compared to the random  query generation method    Finally  it is interesting to note that if despite the use of the rule  pattern we are not able to find a query that exercises that rule  it  could be an indication that the rule is dependent on other rules  being exercised  We plan to study such handling such rule  dependencies as part of future work   3 2 Extensions for Rule Pairs  So far  we have focused on the query generation problem for  singleton rules  In addition to testing single rules  it is also  important to test pairs  in general  a set  of rules to cover rule  interactions  In Section 3 1  we outlined how to leverage the rule  patterns that are used during optimization for query generation  In  this section  we look at the corresponding problem for rule pairs  i e  given a pair of transformation rules  r1    r2   we need to  generate a SQL query which can exercise  both  the rules when  optimized  The rule patterns for individual rules can also be  leveraged for generating necessary conditions  as in Section 3 1   to exercise a pair of rules by using the idea of rule pattern composition    Consider the two rule patterns shown in Figure 3 for the join  commutativity rule and the rule for pulling up the Group By  Aggregate over a join  In order to generate a query that can  exercise  both the rules  we can combine the rule patterns in the  following ways   1  Create a new pattern with a root operator as  join or UNION and both the initial patterns as the corresponding  children   2  Substitute any generic operators in a pattern   represented as circles in the patterns in Figure 3  with the other  pattern to create a composite pattern   We have extended the query generation module to handle query  generation for a pair of rules as follows  We compose the two rule  patterns as described and generate a query corresponding to each  of the composite patterns and pick the query with the  least  number of operators that exercises both the rules  Note that rule  composition captures an important interaction between rules  rule  r1 is exercised on an expression which is an  input  to the  expression on which rule  r2 is exercised  Of course  there are  potentially other interesting patterns of rule interactions  We  discuss other variants of the query generation problem in Section  7   4  TEST SUITE COMPRESSION  PROBLEM  One approach for testing rule correctness is to leverage stochastic  testing  e g  as in  1  11  17    The idea is to generate a complex  random query that exercises a given rule  We then  1  Execute the  original query  2  Execute the plan obtained for the query with the  rule turned off  3  Check if the results of  1  and  2  are the same  or not  In order to have sufficient confidence in the correctness of  a rule  we may need to repeat the above validation step for several  such randomly generated queries   Thus  for each transformation  rule we need to validate its correctness for  k distinct queries   where k is an input parameter that we refer to as the test suite  size   Since the queries generated are potentially complex and  need to be executed  the time taken to run these test suites can be  significant  In this section  we formally present the problem of test  suite compression  first described in Section 2 3   which can  significantly improve the efficiency of correctness testing  We  first show that this problem is NP Hard  In Section 5  we present  two algorithms for solving the test suite compression problem    4 1 Problem Statement  Let R    r1       rn   denote the set of transformation rules  Let the  test suite size be k  and let TS denote the overall test suite for all  rules  Section 2 3   i e  TS      i  TSi   The relationship between the  rules and the queries in the test suites can be represented by a  bipartite graph  see Figure 4   An edge between a rule ri  and a  query qj  denotes the fact that rule ri  is exercised when query qj  is  optimized  Note that a query belonging to TSi   the test suite  generated for rule i i   can potentially exercise other transformation  rules as well  By exploiting this information  we can improve the  efficiency of test suite execution  illustrated by the following  example   Example 1  Consider the case when R    r1  r2    Let the rule test  suite size  k  be 1  and the corresponding test suite for the rules  are TS1    q1  and TS2    q2   Thus  TS    q1  q2   Assume that  r1  is the only rule triggered when q1  is optimized  whereas both  261rules are triggered when q2 is optimized  Suppose the costs  associated with the queries are as follows  Cost q1    Cost q2     100  Cost q1     r1     180  Cost q2     r2     120  Cost q2     r1      120    The BASELINE method for test suite execution  Section 2 3   would be as follows       Execute Plan q1  and Plan q1     r1         Execute Plan q2  and Plan q2     r2     The cost of the BASELINE method for this example is    100 180     100 120    500   One alternative is to use query q2 for validating both rules  The  cost of this strategy would include       Execute Plan q2  and Plan q2     r1         Execute Plan q2      r2    Note that we do not need to execute Plan q2  when validating r2 since we have executed it when validating r1  and thus its results  are already available  Thus  the cost of this strategy is  100 120      120    340  which is less expensive than the BASELINE  method   From the above example  we note that there are two important  observations that can be leveraged in test suite compression  First   when a query  q exercises multiple rules  Plan q   with all rules  enabled  needs to be executed  only once  Second  since the  randomly generated queries can have widely varying costs  we  can leverage this fact to reduce the cost by choosing queries with  lower cost  Finally  the cost of the query with the rules disabled could be significantly higher than when the rules are enabled  e g   if the rule is responsible for pushing selection below a join   disabling that rule can dramatically increase the cost of the query    Therefore  ideally this also needs to be factored in during test suite  compression  We now formally define the test suite compression  problem  We describe it for the singleton rule case  although the  formulation extends in the obvious way for the case of rule pairs       Test Suite Compression Problem  Consider the bipartite graph  G    V E   where V    R     TS  and E  the set of edges in the  graph  is defined as follows  add an edge between a rule ri     R   and query q    TS if optimizing q exercises rule ri    Each node r i  in  R is assigned a cost 0  and each node q     TS is assigned a cost  equal to Cost q   For each edge  ri    q  assign the cost equal to  Cost q     r i    i e  the cost of executing the query q with the rule ri disabled  see Figure 4   The test compression problem is to find a  subgraph G       V    E     of the above bipartite graph such that   1  V       R      TS     and TS         TS  In other words  the  subgraph contains all rules from G and a subset of the  queries from G    2  Each node r     R in G    has degree equal to the test suite  size k    3  The sum of the edge and node costs in G    is minimized   Intuitively  we intend to find the mapping of queries to rules with  the minimum cost  condition 3  such that every rule is accounted  for  condition 1  while ensuring that each rule is mapped to  exactly k queries which is the size of the test suite  condition 2    Note that any subgraph of the  bipartite graph that satisfies  property 1  and 2  is a valid test suite  The node cost for the query  nodes is used to model the fact that for queries shared between  multiple rules  the original Plan q  needs to be executed only  once  The execution of the test suite would proceed as follows   For each q     TS     we execute Plan q  once  For each edge  q  r    we execute the Plan q     r   and compare the results with those  obtained from Plan q   Thus  the sum of the edge and node costs  is equal to the cost of executing the test suite  Since the out degree  of each node r      R in the subgraph is known to be  k  we are  guaranteed to execute k distinct queries for each rule in the set R   4 2 Hardness  Claim  The Test Suite Compression Problem is NP Hard   Proof  See Appendix A for the proof  We show hardness by  reducing an arbitrary instance of the Set Cover problem  10  to a  simplified version of the Test Suite Compression  TSC  problem    5  ALGORITHMS FOR TEST SUITE  COMPRESSION PROBLEM  In Section 4  we introduced the Test Suite Compression  TSC   problem  and why it can be important in significantly improving  the efficiency of correctness testing of rules  We also showed that  TSC is computationally hard  In  this section  we present two  algorithms for this problem  In Section 6  we study the  effectiveness of these two algorithms via an empirical evaluation   and compare them with the BASELINE method  described in  Section 2 3     5 1 Applying the Set Cover Heuristic  In section 4 2 we showed that the test suite compression problem  is NP Hard  The reduction  see Appendix A for details   demonstrated that the Set Cover problem is isomorphic to a  simplified version of the test suite compression problem  that uses  a test suite size k  1   Since good approximation algorithms exist  for the set cover problem  19   a natural question is whether such  an algorithm can be leveraged  for the test suite compression  problem   Observe that the simplified version of the test suite compression  problem  that was shown to be isomorphic to the set cover  problem  uses a test suite size k   1  To incorporate this parameter  we therefore adapt an algorithm for the corresponding general  version of the set cover problem  which is called the Constrained  Set Multicover problem   19   The constrained set multicover  problem takes as input a set U  a number me  for each e     U  and a  set of subsets S of elements in U  Each s     S has a cost C s   The  goal is to find the subset with the minimal cost such that        Each element e of U is covered me  times      Each s     S can be picked at most once  Figure 4   Bipartite Graph Representation  262The test suite compression problem can be mapped to it as  follows  The set of rules R maps to input set U  For each query q   we map RuleSet q  to the corresponding subset s     S  the node  cost Cost q  to the corresponding C s   For all rules r    R  we set  mr  to k  Figure 5 shows how we can adapt the greedy algorithm  for the Set Multicover problem  19  to compute the set of queries  with minimal cost such that each rule is mapped to exactly  k distinct queries in the test suite  The algorithm collects queries to  be picked in the set TS     It tracks the set of rules that have already  been covered in R     In Step 2 we check if the set of rules that have  been covered in R    is complete  Note that this step includes the  check that each rule in R has been covered k times  In Step 3  we  compute the    benefit    of each query that has not been picked  We  define a rule to be remaining if it exercised by less than k of the  queries already picked  The benefit of a query is defined as the  number of remaining rules that are covered normalized by the cost  of the query  For example consider a query q with RuleSet q       r1  r2  r3   Let the test suite size k be set to 2  Assume that the  rule r1 has already been covered by 2 queries in the set TS     The  remaining rules for query q are thus  r2   r3    The greedy algorithm  picks at any point the query with the highest    benefit     Step 4      Consider Example 1  see Section 4 1  where query  q1 exercises  the rule  r1  and q2 exercises the rules  r1  r2   Since Cost q1     Cost q2    q2 has the higher benefit and as a result the greedy  algorithm in Figure 5 would find  the optimal solution for the  example  In general  however this algorithm tries to minimize the  total cost of the query nodes in the subgraph and does not model  the edge costs  see Section 4  which accounted for the costs of  executing queries with the corresponding rules disabled  Since  in  general  the edge costs could be potentially significant  we now  present another algorithm that takes into account the edge costs    5 2 A Constant Factor Approximation  Algorithm  In this section  we present a heuristic for the test suite  compression problem that takes into account the edge costs  We  also show that our algorithm is a factor 2 approximation of the  optimal solution to the test suite compression problem  Intuitively   the algorithm selects for each rule  the k queries with the lowest  cost with that rule disabled  i e  edge cost   Unlike the  SetMultiCover algorithm  Section 5 1   this algorithm assumes  independence between the rules  thereby ignoring the benefits  obtained from potentially sharing queries between the test suites  of different rules  see Example 1      The algorithm  we refer to it as  TopKIndependent  is shown in  Figure 6  For each rule r in the set R  we first obtain the set of all  queries in TS that exercise rule r   Step 4   The loop  lines 5 11   picks the k queries with the lowest edge cost  i e  the cost of the  query when the rule r is disabled  This step is repeated for all the  rules in the set R    Referring once again to Example 1  Section 4  where q1 exercises   r1  and  q2 exercises the rules  r1   r2   Since  k is 1  the  TopKIndependent algorithm would choose the query that has the  minimum edge cost for each rule  For both the rules  q2 has the  smaller edge cost when compared to q1  Thus  for Example 1 the  algorithm in Figure 6 would also find the optimal solution   Although the  TopKIndependent algorithm ignores query node  costs  we can show that it provides a solution that is guaranteed to  be within a factor 2 of the optimal solution  Note that the  SetMultiCover algorithm  on the other hand does not provide a  constant factor approximation    Claim   TopKIndependent algorithm is a factor 2 approximation  algorithm for the test suite compression problem   Proof  Consider any rule r     R    For rule r  the TopKIndependent algorithm chooses the  k  queries with the least expensive edge  costs  i e  cost of query with rule r disabled   Let us denote this set  of queries by TS r   Note that the maximum cost of any solution  obtained by TopKIndependent over all rules in R cannot exceed            r                                                r r R q TS MaxCost Cost q Cost q The above  upper bound  occurs when for each rule  there is no  query chosen for that rule which is shared with any other rule in  R     Now we present a lower bound for any solution to the test suite  compression problem  This lower bound occurs when   a  The k cheapest queries  in terms of edge costs  are chosen for each rule  r  and  b  For each rule  all the queries picked for the rule r are  shared with some other rule    Input   Bipartite Graph G     R     TS   E   Test Suite Size  k     Output  A bipartite graph G        R      TS      E      a subgraph of G  with outdegree of each node in R   k 1  TS          R          E          2  While  R       R  Do 3   For each q     TS     TS      compute     Benefit q     number of remaining rules covered   Cost q   4   Pick q     Q     Q     with the largest Benefit value  5   TS      TS         q   R      R         RuleSet q    6    Add edges corresponding to q and the remaining rules it  covers  to E      7  End While  8  Return G        R     TS     E     Figure 5  Greedy Algorithm based on the Set MultiCover  problem   Input   Bipartite Graph G     R     TS   E  Test Suite Size k   Output  A bipartite graph G        R      TS      E      a subgraph of  G with outdegree of each node in R   k 1      TS          R          E          2       For each rule r in R Do 3         count   0  4         Let W   Subset of queries in TS that includes rule r                     in its RuleSet  5         While  count   k  Do  6            Pick q     W with minimal value Cost q     r   value  7            W  W    q   9      TS      TS         q   count   count 1  10       Add edge corresponding to  r q  to E    11    End While  12    R      R         RuleSet q   13  End For  14  Return  G        R     TS     E      Figure 6  TopKIndependent Algorithm  263     r                   R                          r r q TS MinCost Cost q Observe that the above cost does not correspond to any valid  solution since it ignores Cost q  entirely  However  it is a valid  lower bound on the cost of any solution    Notice that for any rule r and query  q   Cost q      Cost q    r   since for a well behaved optimizer disabling a rule can only  increase the cost of the resulting plan  This is because when a rule  is disabled  one of the following two possibilities can occur   1  It  may not impact the plans considered by the optimizer  in which  case the resulting plan  and hence the cost  is the same  or  2  It  can reduce the number of plans considered by the optimizer  in  which case the cost of the plan chosen can only be higher    Now consider the ratio f    MaxCost   MinCost   Since Cost q      Cost q    r     f      2  Note that f 2 occurs when Cost q     Cost q    r    Since the optimal solution has a cost higher than  MinCost  and actual cost of the solution picked by  TopKIndependent  is no higher than MaxCost  we know that the  solution picked by  TopKIndependent has a cost that is within a  factor of 2 of the optimal solution    5 3 Extensions for Rule Pairs  In this section  we discuss how the algorithms we described in  Sections 5 1 and 5 2 for the test suite compression problem can be  extended for testing pair wise rule interactions   The test suite compression problem for testing rule pairs is very  similar to the original formulation  with the key difference being  that the input is a set of rule pairs rather than a set of individual  rules  We denote the set of all rule pairs by P  i e  P     r1  r2     r1   r3        rn 1   rn     Thus  for each element p     P  we need to  find the mapping of  k  distinct queries such that the cost of  executing the test suite is minimized  An example of the bipartite  graph corresponding to the test suite compression problem for rule  pairs is shown in Figure 7   Note that we add an edge between a  rule node and a query node  q  only if  both the corresponding  rules are exercised when query q was optimized  In the example  bipartite graph shown in Figure 7  all the queries exercise both  rule pairs  For an edge between a rule node  ri   rj   and a query q   the edge cost is the cost of executing q with both rules disabled  i e  Cost q     ri  rj     Thus the cost of executing q1 when both r1 and r2 are disabled is 150    The algorithms presented in Section 5 extend in a straightforward  manner for the case of rule pairs  However  the main difference is  that the cost of  creating the bipartite graph which is an input to  both algorithms itself can become significant  Consider a query q that exercises five rules  In the original formulation of the  problem  we need to add five edges to the corresponding rule nodes  For the case of pairs  we need to add  5 C2 edges  corresponding to all the pairs of the rules  Recall that the edge  costs model the cost of turning off a particular pair of rules  In  general for a query that exercises n rules  we need  n C2 invocations  of the query optimizer to compute the edge costs corresponding to  that query node  For complex queries  the number of rules  exercised could potentially be high and thus the cost of building  the initial bipartite graph could be significantly higher when  compared to the case of singleton rules  Thus  scalability of the  algorithms with number of rules becomes a significant issue   5 3 1 Exploiting Monotonicity to Reduce Optimizer  Invocations  We observe that the increase in the cost of constructing the  bipartite graph only affects the  TopKIndependent algorithm   Section 5 2   This is the because the  SetMultiCover algorithm  described in Section 5 1 does not model the edge costs and uses  only the query node costs as a basis for picking nodes    For the TopKIndependent algorithm  we present a technique that  can help reduce the number of edges for which the cost needs to  be determined by the algorithm  in Steps 5 10 of Figure 6    Our  idea is to exploit the observation that for any query Cost q       Cost q    R   where R is any subset of rules in RuleSet q     Suppose for a rule pair  p      P the queries that exercise the rule  pair is TS p   Our goal is to find the k edges  with lowest cost   from p to queries in TS p   We sort queries in TS p  in increasing  order of the original node cost  i e  by Cost q   We also maintain a  priority queue of size k of edges for which we have computed the  actual cost  by invoking the optimizer   Initially  the priority  queue is empty  Each time we consider the next edge  say e  from  TS p  we check if the node cost corresponding to that edge has a  higher value than the edge with the kth highest cost in the priority  queue  If so  we can terminate since we know that the cost of the  edge e  and all remaining edges for p  must have a higher cost  If  not  we compute the actual cost and add the edge to the priority  queue  potentially evicting an existing edge with the highest cost  to maintain the size of k in the priority queue   Our experimental  results  Section 6  indicate that this optimization can significantly  reduce the costs of creating the bipartite graph for testing rule  pairs   The following example illustrates the above optimization   Consider the bipartite graph shown in Figure 7 and let the test  suite size k be 1  For the node  r1   r2    we need to find the edge  with minimum cost  We illustrate how we can potentially achieve  this without having to compute all the edge costs  Consider the set  of all queries with edges to  r1   r2    In our case this is  q1   q2   q3     We order the queries in the increasing order of node costs  We  first compute the edge cost for query q1  i e   Cost  q1     r1  r2    by invoking the query optimizer  As shown in the figure  suppose  this edge cost is 150 units  Note that the original query cost of  q2  200  and q3  300  are higher than this value  This implies that  the corresponding edge costs can only be higher  since disabling a  pair of rules can only increase the cost of the resulting plan    Thus  we can stop enumerating the edges at this point and return  the current edge  corresponding to q1  as the minimal cost edge    In this paper  we have primarily focused on testing single  transformation rules and the interactions of rule pairs because  these are the most common scenarios for testing transformation  rules  We discuss interesting extensions to study as part of future  work in Section 7   Figure 7  Bipartite Graph for Rule Pairs 2645 4 Summary  In this section  we presented two algorithms for solving the test  suite compression problem  We first showed how to leverage the  greedy heuristic for the SetMultiCover problem  which ignores the  edge costs   Next we presented the  TopKIndependent algorithm  that ignored the benefits of using the same query for different  rules  i e  the query node costs   However  this algorithm  guarantees a constant factor  approximation to the optimal  solution  We have implemented  both these algorithms  and we  describe results of our experimental comparison in Section 6   6  EXPERIMENTS  We have prototyped the framework described in this paper  see  Figure 2  on Microsoft SQL Server  In this section  we present the  results of our experiments for evaluating the effectiveness of the  techniques presented in this paper  In particular  the goals of our  experiments are       Compare the efficiency of query generation using rule  patterns  Section 3  with the randomized query generation  approach  We do this for singleton rules as well as rule pairs       Compare the effectiveness of the  SetMultiCover algorithm   Section 5 1  and  TopKIndependent algorithm  Section 5 2   for the Test Suite Compression problem for both singleton  rules as well as rule pairs       Study the importance of exploiting monotonicity  Section  5 3 1   6 1 Databases  As described in Section 2  we are given as input a test database   For our experiments  we use tables from the TPC H  21  database   We focus on logical transformation rules  see Section 2 1  in this  evaluation  these rules are by and large exercised regardless of the  data size or distribution  We use a set of around 30 logical  transformation rules of the optimizer  that cover the most  commonly used operators including selections  joins  outer joins   semi joins  group by etc    We have also evaluated our tests on  other databases with different schemas and sizes  and the results  are similar to those presented below  so we do not report those  results here  We defer a more thorough evaluation  that includes  implementation rules  to future work   6 2 Results  6 2 1 Leveraging Rule Patterns for Query  Generation   In our first experiment  we study the impact of rule patterns for  query generation  see Section 3   We report our results both for  singleton rules as well as rule pairs    Figure 8 shows the number of trials required to generate a query  for each singleton rule  Observe  that our technique of pattern  based generation of logical query trees  Section 3  is able to  generate a query that exercises  the given rule in a very small  number of iterations  typically 1  or 2  and never more than 4    This is a significant improvement relative to random query  generation  where generating for certain rules it takes close to 40  attempts before a query which exercises that rule can be  generated  For the entire set of 30 rules  the total number of trials  for RANDOM is 234 whereas for PATTERN it is 38   The difference in efficiency between RANDOM and PATTERN  is even more significant for rule pairs  as can be seen in Figure 9   Note that the y axis uses a logarithmic scale  We show the results  when the number of rules  n    15  30  and therefore number of  rule pairs    15 C2 and  30 C2 respectively  For n 15  RANDOM  requires 1187 trials  whereas PATTERN requires only 383 trials   For n 30  PATTERN shows a 13x improvement  RANDOM  requires over 13 000 trials whereas PATTERN requires less than  1 000 trials   This is because in general  the chance that a query  generated using a random generation procedure exercises a set of  rules drops rapidly as the cardinality of that set increases  For  example  we observe rule pairs for which it takes close to 100  trials to generate a valid query  On the other hand  with  PATTERN  most queries were again generated within 1 or 2  trials  and the maximum number of trials we observed for a rule  pair was 5    Figure 10 shows the time required to generate the queries for the  same data points as in Figure 9  once again the y axis uses a log  scale   This figure shows that the efficiency of PATTERN over  RANDOM in number of trials also extends directly to a reduction  in the time required to generate the test cases    Together  these results clearly show the importance of rule pattern  based query generation for singleton rules as well as rule pairs   Figure 8  Random vs  Pattern based generation for  singleton rules  Figure 9  Random vs  Pattern based generation for rule pairs  2656 2 2 Test Suite Compression  Recall that the test suite compression problem  presented in  Section 4  is important for the efficiency of correctness testing of  rules  In this experiment we compare the three approaches for the  test suite compression problem  BASELINE  Section 2 2    SetMultiCover  Section 5 1   which we refer to as SMC in the  graphs  and TopKIndependent  Section 5 2   which we refer to as  TOPK in the graphs  we use a test suite size k of 10   We show  results for singleton rule as well as rule pairs  Note that for all  graphs in this section  we use a log scale for the y axis   which  represents the total cost  we use the optimizer estimated cost  of  the solution  as the number of rules is varied    For singleton rules  see Figure 11   we observe that both SMC and  TOPK obtain solutions that are significantly better than  BASELINE  anywhere between one and three orders of  magnitude   This shows that  unlike BASELINE  both SMC and  TOPK are able to take advantage of using a single query for  validating multiple rules    For the case of rule pairs  see Figure 12  however  the results are  somewhat different  While TOPK continues to produce the lowest  cost solutions  SMC   s solution vary between good to significantl</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security"/>
        <doc>Static Checking of Dynamically Varying Security Policies in Database Backed Applications ### Adam Chlipala Impredicative LLC Abstract We present a system for sound static checking of security policies for database backed Web applications  Our tool checks a combination of access control and information    ow policies  where the policies vary based on database contents  For instance  one or more database tables may represent an access control matrix  controlling who may read or write which cells of these and other tables  Using symbolic evaluation and automated theorem proving  our tool checks these policies statically  requiring no program annotations  beyond the policies themselves  and adding no run time overhead  Speci   cations come in the form of SQL queries as policies  for instance  an application   s con   dentiality policy is a    xed set of queries  whose results provide an upper bound on what information may be released to the user  To provide userdependent policies  we allow queries to depend on what secrets the user knows  We have used our prototype implementation to check several programs representative of the data centric Web applications that are common today ###  1 Introduction Much of today   s most important software exists as Web applications  and many of these applications are thin interface layers for relational databases  Realworld requirements impel developers to implement many application speci   c schemes for access control     who can do what      and information    ow     who can learn what       To reason about correctness of these implementations  the programmer must consider all possible    ows of control through a program  This task is hard enough if a security policy can be expressed statically  as  for instance  a list of which of a    xed set of principals is allowed to perform each of a    xed set of actions  However  the needs of real applications tend to force use of evolving security policies  and usually the most convenient place to store a policy is in the same database where the rest of application data resides  For instance  a database often encodes some kind of access control matrix  where entries reference rows of other tables  The peculiar structure of an organization may require access control based on customized schema design and checking code  An effective security validation tool must be able to    understand    these policies  Many program analysis and instrumentation schemes have been applied to provide some automatic assurance of security properties  In this space  the traditional dichotomy is between dynamic and static tools  based on whether checking happens at run time or compile time  The two extremes have their characteristic advantages    Dynamic analysis can often be implemented without requiring any program annotations included solely to make analysis easier    Real developers have an easier time writing speci   cations compatible with dynamic analysis  since these speci   cations can often be arbitrary code for inspecting program states    Static analysis can provide strong guarantees that hold for all possible program executions  even those exercising weird corner cases that may not have been considered    Static analysis adds no run time overhead  In this paper  we present a tool UrFlow for static analysis of database backed Web applications  We have tried to reap some of all of the advantages just described  Our tool requires no program annotations and provides fully sound static assurance about all possible executions of a program  and it requires no changes to the run time behavior of programs  We take advantage of the fact that it is already common for Web applications to be implemented at quite a high level  relying on an SQL engine to implement the key data structures  Our tool models 1the semantics of SQL faithfully  at a level that makes formal  automated analysis quite practical  We use popular ideas from symbolic execution and automated theoremproving to build detailed models of program behavior automatically  which saves developers the trouble of explaining these models with code annotations  It is natural for developers to write speci   cations that look much like the program code they are already writing  Traditional assertions  e g   with the C assert macro  fall under this heading  In an application that depends on an SQL engine to manage its main data structures  it seems similarly natural to express security policies using SQL  Our tool is based on that model  allowing developers to write detailed statically checkable speci     cations without learning a new language  Queries can express con   dentiality properties by selecting which information the user may learn  and queries can express database update properties by selecting allowable state transitions  We need only one extension to the standard SQL syntax and semantics  to allow policies to vary by user  we introduce explicit consideration of which secrets  e g   passwords  the user knows  UrFlow is integrated with the compiler for Ur Web  3   a domain speci   c language for Web application development  Ur Web presents a very high level view of the domain  with explicit language support for the key elements of Web applications  For instance  the SQL interface uses an expressive type system to ensure that any code that type checks accesses the SQL database correctly  In the present project  we have used the    rst class SQL support to avoid the need for program analysis to recover a highlevel view of how an application uses the database  We begin by introducing our policy model and demonstrating its versatility  After that  we present our program analysis  including its symbolic evaluation and automated theorem proving aspects  Next  we discuss the scope and limitations of our analysis  describe some case study applications that we have checked with UrFlow  and compare with related work  2 SQL Queries as Policies Consider a simple application that maintains a database of users and per user secret strings  We can declare our schema to Ur Web with table declarations  Following standard practice in relational databases  each table includes a unique integer ID  which provides a convenient handle to pass to row speci   c operations  Besides an ID  a user record contains a username and password  and a secret record contains the owning user ID and the data value  table user     Id   int  Nam   string  Pass   string   table secret     Id   int  User   int  Data   string   We also declare an HTTP cookie  which acts like a typed global variable which exists separately on each Web browser  This cookie tracks the authentication information for the currently logged in user  While a more realistic program would probably rely on unique session IDs  here we adopt the less secure strategy of storing a user ID and password pair in each cookie  to simplify the example  cookie login     Id   int  Pass   string   We can write a function that checks this cookie and returns its user ID if the password is correct  The code is written in a functional style  where we collapse    expressions    and    statements    into a single syntactic class  Thus  instead of determining the function return value with explicit return statements  we just say that the function result is the value of the single expression that is the function body  Ur Web code makes a lot of use of tagged unions  a safe analogue to C unions that is popular in functional programming languages  A tagged union value is either a simple tag  which is like an enum value in C  or a pairing of a tag and another value  which is like a C union  but with a convention to ensure that it is always possible to inspect a value and determine which union alternative is being used  For tag T  a simple tag expression is written like T  while the pairing of that tag with expression e is written T e   For instance  instead of allowing every object type to be inhabited by a special value null  we instead represent null with an explicit tag None  and we represent non null object o as Some o   A patternmatching construct case is used to deconstruct tagged union values  Here is the code for a function to check the correctness of the information in the login cookie  It is written in a compiler intermediate language in which some higher order functional programming idioms have been replaced with more standard imperative code  fun userId     case getCookie login  of None    None   Some li     let b   query  SELECT COUNT      0 AS B FROM user WHERE user Id    li Id  AND user Pass    li Pass    r acc    r B  False in if b then Some li Id  else error  Wrong user ID or password    2Our userId function begins by retrieving the current value of the login cookie  This will either be None  if no value of the cookie is set  or Some li   if the ID password record li has been set as the cookie value  If the cookie is not set  there is no user ID to return  Otherwise  we must consult the database to see if the password is correct  We have literal SQL syntax embedded in the code  with splicing of variable values using curly braces  The query checks if there are any rows in the user table matching the cookie contents  In this intermediate language  every database read is expressed as a loop over the results of a query  The body of the loop is written as an expression with two explicitly named new local variables  r  the latest row to process  and acc  an accumulator that is modi   ed as we process rows  The body expression after the    determines the new accumulator value after every iteration  We give False as the initial accumulator value  In our example here  the loop body ignores the accumulator  and we simply project the one    eld of any result row to save as the accumulator  The error function aborts program execution with an error message  which we do here when the user provides invalid credentials  We can write the main entry point of our application to display all of the logged in user   s secrets  fun main     case userId   of None    write  You   re not logged in      Some u     query  SELECT secret Id  secret Data FROM secret WHERE secret User    u    r acc    write   li   i     write toString r Secret Id    write    i       write escape r Secret Data    write    li        In this query loop  the accumulator is still ignored  and in fact we execute the function body solely for its side effects  which involve writing HTML to be sent to the client  We would like to verify that this application satis   es a reasonable con   dentiality policy  Intuitively  every cell of the database belongs to a particular user  We want to ensure that no user is able to read cells belonging to a different user  This simple policy expresses our intent for the cells of the user table  policy sendClient  SELECT   FROM user WHERE known user Pass   The informal meaning of this policy is that the user may learn any value that could be returned from this query  Every policy statement is followed by a keyword naming a kind of policy  In this case  that keyword is sendClient  which is used for con   dentiality policies  Speci   cally  the user may learn anything about any row of user whose password he knows  The new predicate known models which information the client is already aware of  We assume the client knows the text of the program and the text of the HTTP request it sent  In our example  when we disclose any secret information  we know that the user   s own password is known because it came from the login cookie  which was part of the incoming HTTP request  A more complicated policy allows the release of information about secrets  policy sendClient  SELECT   FROM secret  user WHERE secret User   user Id AND known user Pass   We use a join between the secret and user tables  requiring that the client demonstrate knowledge of the password for the user who owns the secret  Our tool veri   es that the application satis   es these security policies  That is  every cell of the database whose value might be disclosed could have been selected by one of these queries  based on an interpretation of known drawn from the HTTP request that prompted an execution  There are several opportunities for mistakes in implementing the policy  Consider what would happen if we had implemented userId to always return 17  When we run the compiler  we get an error message  The compiler tells us which secret may be leaked  and  in addition to the location of the offending write  we are given a    rstorder logic characterization of the state of the program at the time when the leak might occur  User learns  r Secret Data Hypotheses  secret x1   r    Secret    Id   x1 Id  Data   x1 Data    x1 User   17 The hypotheses are generated directly from the SQL query in main  The    rst hypothesis tells us that row x1 is in the secret table  Our row variable r is equated with a record built by projecting the requested    elds from x1  and the last hypothesis represents the WHERE clause  In the correct implementation  UrFlow explores every static path through the program  maintaining a logical state at each point  When the analysis reaches the point that triggered the error above  we have this more informative state  3c   cookie login  known c   c   Some c2   user x1   x1 Id   c2 Id  x1 Pass   c2 Pass  secret x2   x2 User   c2 Id  r    Secret    Id   x2 Id  Data   x2 Data   The variable c stands for the cookie value  which is asserted to be known to the user  The SQL query from userId is re   ected with assertions about a variable x1  which is the row of user that must have matched the query for execution to reach this point  The con   dentiality policy used a join between secret and user to describe when information on secrets may be released  The program code  on the other hand  contains no joins  UrFlow understands join semantics to the point where it is able to deduce that the above logical state implies that a join  performed as in the policy  would authorize the release of everything included in the record r  2 1 What is Being Checked  We can give a simple characterization of exactly what con   dentiality property the analyzer enforces  as a function of the policy the user speci   es  First  we need to de   ne exactly what we mean by the known predicate  Informally  a known piece of data is something that the user is already aware of  so that no con   dentiality requirement is violated by echoing back that value or another value derived from it in a predictable way  More formally  known is the most restrictive predicate satisfying the following rules  1  Any constant appearing in the program text is known  2  The initial value of every cookie is known  These cookies may have arbitrary structured types  as in the record type given to the login cookie in the last example  3  The value of every explicit parameter to the application is known  For page requests generated by submission of HTML forms  this includes all form    eld values  4  A record is known iff all of its    elds are known  5  For any union tag T  e g   Some in our example   a value v is known iff T v  is known  We say that a value v is allowed in a speci   c database state D if there exists a sendClient policy that  when executed in state D  would return v as one of its outputs  We say that a value v is built from a set S if v is in S or can be constructed out of the elements of S by combining a subset of them with record and tagged union operations  Now we can give a concise description of exactly what UrFlow checks  For any execution of a program that the analysis approved  1  Whenever a write command sends some value v to the client  v is built from the set of values that are known or allowed  2  Whenever the program branches based on the value v of some test expression  such that the branch chosen in   uences what might be sent to the client later  v is built from the set of values that are known or allowed  This prevents some implicit    ows  where the very fact that a program reaches a particular line of code may reveal secret information  Since implicit    ows are a notorious source of false alarms in information    ow analysis  programmers might want to turn off this piece of checking  which would be easy to do via a compiler    ag  The same kind of characterization does not work well for ruling out implicit    ows induced by SQL WHERE clauses  so we leave additional checking of that kind for future work  This means that a checked program may leak information about the existence of rows  based on tests against arbitrary SQL expressions  but the contents of those rows will not be leaked directly  2 2 Authorizing Database Writes UrFlow also checks every database modi   cation  For example  consider this page generation function  which would be given as the action to run upon submission of an HTML form for adding a new secret  fun addSecret fields    case userId   of None    write  You   re not logged in      Some u    let id   nextId   in dml  INSERT INTO secret  Id  User  Data  VALUES   id    u    fields Data     main   If we do not assert an explicit database update policy  then UrFlow rejects this program  Here is one policy that would allow the insertion  policy mayInsert  SELECT   FROM secret AS New  user WHERE New User   user Id AND known user Pass  AND known New Data   We reuse the same SQL query notation for modi   cation policies  though the choice of SELECT clause is ignored  so we will always write SELECT    One of the 4tables in the FROM clause must be given the name New  this is the table for which we are authorizing insertion  UrFlow only allows a row insertion if the new row could be returned by one of the mayInsert queries  in a certain sense  In checking against a particular policy query  we interpret the New relation as the universal relation  containing all possible tuples  The policy may join it with other  real database tables and perform    ltering with WHERE  leading to a result set of rows that may be in   nite  The insertion is permitted if the New part of one of these rows matches the values being inserted  Our insertion policy lets any user add secrets if he associates them with his own user  We can also authorize deletions and updates  based on similar criteria  policy mayDelete  SELECT   FROM secret AS Old  user WHERE Old User   user Id AND known user Pass   policy mayUpdate  SELECT   FROM secret AS Old  secret AS New  user WHERE Old User   user Id AND New User   Old User AND New Id   Old Id AND known user Pass  AND known New Data   A mayDelete policy must tag a FROM table as Old  to stand for the table being deleted from  A mayUpdate policy needs both Old and New tables  standing for the part of a table being updated and the new data being written into it  Both new policies retain the logic for checking that the client knows the password for the user whose secret is affected  and the update policy also requires that the secret ID is not changed  The insertion and update policies require that the new data value is known  which provides a simple guard against inadvertent leaking of privileged information into a part of the database that is considered to be less privileged  3 Flexibility of Query Based Policies We have found that this approach to writing speci   cations leads to natural descriptions of many natural policies  For instance  we have implemented a simple Web message forum system  Our implementation contains a table representing an access control list  Each entry gives a user permissions in a speci   c forum  at a particular numeric level of access  table acl     Forum   forumId  User   userId  Level   int   One policy allows release of information about any message in a forum that the current user has been granted any kind of access to  policy sendClient  SELECT   FROM message  acl  user WHERE acl Forum   message Forum AND acl User   user Id AND known user Pass   Posting a new message requires access at level 2 or higher  policy mayInsert  SELECT   FROM message AS New  user  acl WHERE New User   user Id AND New Forum   acl Forum AND user Id   acl User AND known user Pass  AND acl Level    2 AND known New Subject  AND known New Body   Regular users may not delete messages from forums  This right is only granted to admins  who have access level 3 or higher  The following policy formalizes the deletion rule  policy mayDelete  SELECT   FROM message AS Old  user  acl WHERE Old Forum   acl Forum AND user Id   acl User AND known user Pass  AND acl Level    3  Our implementation allows forums to be marked as public  in which case any visitor may read their contents  There is also another ACL table which grants users admin access to all forums  Additional policies allow information    ows and updates based on these rules  The UrFlow policy language supports access control techniques besides user accounts with passwords  For example  we have implemented a simple Web based poll system without user accounts  Anyone may create a new poll  at that time  the creator learns a secret code that grants admin rights to the poll  That code allows him to add poll questions  After adding all of the questions  the poll creator may mark the poll as live  After that time  no further changes to the poll are allowed  and the poll is added to a list on the application   s front page  Anyone may vote in a live poll  but no one may vote on a poll that is not yet live  After submitting his votes  a user receives a code that allows him to view the results of the poll  Results should never be released without    rst checking that the user has provided a code that matches the poll admin code or a code associated with a vote that has been cast  The policy below controls the conditions under which a new question may be added to a poll  In particular  the question must be linked to a valid poll  the user must know the admin code for the poll  and the poll must not be live yet  5policy mayInsert  SELECT   FROM question AS New  poll WHERE New Poll   poll Id AND known poll Code  AND NOT poll Live AND known New Text   Anyone with a poll   s admin code may update the poll only to mark it as live  This policy expresses that requirement with equality assertions between old and new values of every column besides Live  policy mayUpdate  SELECT   FROM poll AS New  poll AS Old WHERE New Id   Old Id AND New Nam   Old Nam AND New Code   Old Code AND New Live AND known Old Code   We allow release of information about answers to a poll  whenever the user proves he already voted in that poll by providing a code associated with an appropriate answer set  policy sendClient  SELECT   FROM answer  answers AS Other  answers AS Self WHERE answer Answers   Other Id AND Other Poll   Self Poll AND known Self Code   We believe that this speci   cation approach is very general  while being much more accessible to the average developer than most speci   cation languages are  To investigate the potential for static analysis based on these speci   cations  we implemented the UrFlow prototype  which handles a restricted subset of all SQL queries  In particular  in both policies and programs  we only process queries containing just SELECT  FROM  and WHERE clauses  where the FROM clauses must be simple comma separated lists of tables  We also have not implemented any analysis optimizations like procedure summaries  19   and the analysis only succeeds at understanding loops and recursion following a few simple patterns  Perhaps surprisingly  this is enough to enable sound checking of a variety of paradigmatic Web applications  We will now describe the analysis and then argue for its effectiveness with statistics about a set of representative applications that it has validated  4 An Outline of the Analysis Sound program checking requires considering all possible paths of execution  Since most any non trivial Web application can effectively follow in   nitely many paths  we must apply some abstraction  In implementing UrFlow  we adopted the strategy associated with tools like ESC  10   the Extended Static Checker family  While concrete program evaluation involves program states consisting of variable values  memory states  and so on  the kind of symbolic evaluation that we apply involves program states consisting of formulas of    rstorder logic  Such a formula can be thought of as describing concrete states  so that each abstract state may stand for in   nitely many concrete states  Every basic program operation can be modeled as a predicate transformer  Some operations may not always be safe  In the classical setting  this may be an array dereference  where the index might be out of bounds  In our case  possibly unsafe operations include write commands and database updates  No matter which setting we are in  the safety of operations is checked by associating each operation with a logical condition that implies its safety  This gives us the outline of a sound checking procedure  Start with the abstract state    true     Explore all program paths  extending the abstract state as we go  Each time we reach an operation with safety condition C while in state S  ask an automated theorem prover whether S   C  The ESC projects used the Simplify prover  8  for this purpose  Today  the functionality provided by Simplify is most commonly known by the name SMT  for satis   ability modulo theories  and there is a rich base of tools and users in the domain of static program checking  Our outline omits a critical element of the problem  Even after abstracting program states with formulas  there are probably still in   nitely many feasible program paths  The ESC approach requires additional program annotations that can be used to    nitize the path space  In the design of UrFlow  we have instead taken advantage of the control    ow simplicity of the average Web application  Many interesting applications can be implemented with just one kind of loop  iteration over writing some output for every row returned by an SQL query  Such loops effect no state changes that must be taken into account in the remainder of the program  so in a sense they have trivially inferable    loop invariants     Since loop iteration does not accumulate side effects  it is sound to traverse each loop body just once  which ensures that each program can be broken into a    nite set of    nite analysis paths  UrFlow thus works by literal exploration of all control    ow paths through a program  The next section goes into more detail on the exploration strategy  pointing out the theorem prover operations that will be required  The following section presents our implementation of those prover primitives  in an engine that extends the standard SMT approach with a few new features  65 Symbolic Evaluation The abstract states of UrFlow are de   ned in terms of a simple language of logical expressions and predicates  We write c for constants  drawn from integer     oating point  and string literals   T for union tags  x for logical variables  X for program variables  F for record    eld names  and R for SQL table names  The following grammar describes the syntax of program states  For a token sequence t  we write t for a comma separated list of zero or more ts  Expression e     c j x j T e  j fF   eg j e F Predicate p     known e  j R e  j e   e j       State S      p  X 7  e  A state is a pair of a variable assignment and a set of predicates  For a particular program point  a variable assignment maps every in scope program variable into a logical expression  The predicates are expressed only in terms of logical variables  not the program variables  Since we inline all function calls  every execution path to analyze begins at the entry point of some function that has been registered to be called in response to a particular URL pattern  The arguments to this function stand for explicit parameters and form    eld values  extracted from an HTTP request  Where the function arguments are named Xi   we create an initial state  known xi   Xi 7  xi   for fresh  distinct variables xi   At many other points in path exploration  we will generate fresh logical variables  which we always assume to be distinct from any previously chosen variables  For each function  we explore all paths through it  Most program expression forms are easy to process  as they admit direct translation into logical expressions  The more interesting cases come from branching and database interaction  Our single branching construct is case expressions  which test a value against a number of patterns  which may bind new variables if they match  We model if expressions as a special case of case expressions  where the patterns to match against are true and false  As an example  consider an expression like the following  case e of None    e1   Some X     e2 If e is just the tag None  then we continue with evaluating e1  Otherwise  e is Some v for some v  and we evaluate e2 with X set to v  To capture this with symbolic evaluation  we consider both e1 and e2 as starts of separate execution paths  For the e1 case  we extend the state with the predicate v   None  where v is the result of evaluating e  For the e2 case  we choose a fresh variable x  add the variable mapping X 7  x  and add the predicate v   Some x   With case  it is easy to write code with exponentially many control    ow paths  but where all but a few are logically impossible  For instance  we can sequence several case expressions that analyze the same program variable with the same patterns  Variables are immutable  so each case must choose the same pattern  reducing the number of feasible paths to the number of patterns  We want our automated theorem prover to detect the infeasibility of the other paths as early as possible  Concretely  this will happen on a path where two cases lead to assertions like v   None and v   Some x   on a path that assumes matching of a None pattern the    rst time and a Some pattern the second time  The prover knows that values built with different union tags are disjoint  so it can signal a contradiction here  Whenever a contradiction is detected at some point on a path  we can skip exploring the rest of that path  A number of primitive operations send output to the client  The simplest of these is write  which appends a piece of HTML to the page being generated  UrFlow enforces that the value being sent can be constructed from known and allowable pieces of data  Recall that allowable values are those that could be produced by executing sendClient policies in the current database state  Consider this line of our earlier example program  write escape r Secret Data    The record r has come out of a database query  To verify that this write conforms to the policy  we must check that r Secret Data is known  allowable  or built from such values out of record and union operations  At this point in symbolic execution  the variable mapping will map the program variable r to some logical variable r  and our predicate set will be  c   cookie login  known c   c   Some c 0    user x1   x1 Id   c 0  Id  x1 Pass   c 0  Pass  secret x2   x2 User   c 0  Id  r   fSecret   fId   x2 Id  Data   x2 Datagg The state tells us that we know of two rows that must exist in the database  x1 from table user and x2 from table secret  Each of our declared con   dentiality policies is phrased as a SELECT query whose FROM clause mentions one or more tables  To check if a value may be written  we need to consider ways of matching the policy queries with the logical state  The same table may be mentioned multiple times in one policy or one state  so  in general  there may be many ways to match a policy   s FROM clause with the table predicates of a state  In UrFlow  we apply the heuristic of considering at most one matching per policy  The analysis enumerates every matching of policies with row variables  subject to that constraint  Our running example included these two policies  7policy sendClient  SELECT   FROM user WHERE known user Pass   policy sendClient  SELECT   FROM secret  user WHERE secret User   user Id AND known user Pass   They can be expressed in logical form  where each is a set of predicates that  if all are true  implies the allowability of a set of values  Predicates  user r1   known r1 Pass  Values  r1 Id  r1 Nam  r1 Pass Predicates  user r1  secret r2   known r1 Pass   r2 User   r1 Id Values  r1 Id  r1 Nam  r1 Pass  r2 Id  r2 User  r2 Data Matching a policy against a state is a two step process  First  we consider a mapping of the policy   s ri row variables to variables appearing in the state  For any table predicate R ri  appearing in the policy  we try setting ri to x  for any R x  appearing in the state  Once we have found a plausible mapping for every policy row variable  we apply that mapping to the remaining predicates in the policy  If the theorem prover veri   es that the state implies every one of these predicates  then we have found a viable policy instantiation  and we can continue matching the remaining policies  We repeat the process to try every combination of instantiating every policy at most once  For every set of policy instantiations  we compute the set of expressions that those policies say are fair game to write  Our running example has exactly one feasible instantiation per policy  every policy variable in user uni   es with x1  and every policy variable in secret uni   es with x2  The remaining predicates are all implied by the state  Most interestingly  we must verify that the state implies known x1 Pass   which follows by reasoning from this subset of the state predicates  known c   c   Some c 0    x1 Pass   c 0  Pass The reasoning goes like this  Because the union value c is known  its contents c 0 are known  too  Because the record c 0 is known  its    eld Pass is known  That    eld is asserted equal to the value x1 Pass that we want to prove known  so we are done  The theorem prover provides a complete decision procedure for reasoning chains of this kind  Having veri   ed correct instantiation of each policy  we arrive at this set of allowable expressions  x1 Id  x1 Nam  x1 Pass  x2 Id  x2 User  x2 Data We are trying to prove that the expression r Secret Data is allowable  which requires proving that it is equal to one of the above expressions  It turns out that our state implies that the written value equals x2 Data  because the state contains this predicate  r   fSecret   fId   x2 Id  Data   x2 Datagg That completes the check for this write operation  The procedure scales to handling much more complicated cases  and we also apply the same procedure to any expression used in a branching construct  such that the result of the test in   uences what is written to the client  Especially in this latter case  we need to be able to reason about values that are neither known nor allowable  but that are built from such values via record and union operations  Our theorem prover handles the automation of that kind of reasoning  too  The heart of symbolic evaluation is the treatment of database queries  Recall the form of queries  as illustrated by the main output loop of our example application  query  SELECT secret Id  secret Data FROM secret WHERE secret User    u    r acc            We execute an SQL query  which may contain injected program values  and loop over the result rows  An accumulator is initialized to some speci   ed value  which here is the dummy value     since we execute this loop body only for side effects  Every iteration runs the loop body with r bound to the latest result row and acc bound to the current accumulator  After an iteration  the accumulator is replaced with the value of the     body expression  Traditional veri   cation tools require manual annotation of loops with invariants  to help tame the undecidability of the program analysis problem  To avoid that cost  we designed UrFlow around some observations about the loops that appear in practice in Web applications  Most are run solely for their side effects of writing content to the client  so that there is no need to track state changes from iteration to iteration  Ur Web variables are all immutable  so it is not even possible for them to change across iterations  Side effects are restricted to database tables and cookies  which tend not to be used in the same way that variables are used in traditional imperative languages  All this implies that a simple loop traversal strategy can be very effective  traverse each loop body only once  Concretely  when we reach a query in a symbolic execution path  we consider two possible sub paths  First  8the query may return no results  in which case we proceed taking the initial accumulator as the    nal value  More interestingly  the loop may execute one or more times  We perform a quick linear pass over the body     to see which cookies it might set and which tables it might modify with SQL UPDATE or DELETE commands  All references to those cookies and tables are deleted from the symbolic state  Since all other aspects of concrete state are immutable  this new logical state is guaranteed to be an accurate description of the concrete state at the beginning of any iteration of the loop  Thus  by running the loop body with its local variables set to fresh logical variables  we consider all possible behaviors of the loop  We can continue execution afterward as if we had just executed the loop body once as normal  non loop code  The symbolic state at loop exit can just as well stand for the last iteration of the loop as for any other iteration  At the beginning of a loop iteration  we must enrich the logical state with predicates capturing the behavior of the query  This is best illustrated by example  Consider again the main loop of our example application  We execute its loop body with variable r set to r and acc set to some arbitrary value  since the accumulator is not referenced in the body   Assume that program variable u is mapped to logical variable u  We add these predicates to the logical state  secret x2   x2 User   u  r   fSecret   fId   x2 Id  Data   x2 Datagg Queries with joins just add more table predicates  as we have seen in the modeling of queries as policies  Larger WHERE conditions add additional non table predicates  A SELECT clause determines which    elds to project from the tables  in building the record expression to equate with r  This basic algorithm works for most of the queries that we support  In general  UrFlow does not yet support SQL grouping or aggregation  We include one special case for queries selecting just the aggregate function COUNT     Here  we consider that the loop body always iterates exactly once  Either the query result is 0  and we do not enrich the state with any new table information  or the result is greater than 0  and we assert that there exists some set of rows matching the conditions of the query  To check database updates  we use a hybrid of the query and write checking  Any modi   cation must match with an update policy  using the same matching procedure as for writes  but without the need to check allowability of a value  After an UPDATE or DELETE  we delete any state predicates mentioning the affected tables  UrFlow also has basic support for simple recursive functions  Calls to recursive functions are effectively in  c  C c 0 r x2 x1 Id Secret Pass User Id Data Data Pass Some Id Id Figure 1  E graph for the state from the write example lined like regular function calls  with further self calls skipped  To make this omission sound  we analyze each recursive function to    nd all effects it might have on the database and cookies  and every self call is treated as a nondeterministic modi   cation of those parts of the state  followed by generation of an unknown return value  Further analysis allows us to abstract the initial state so that it can stand for any set of arguments that might be used at any recursion depth  such that we only preserve state information that can be shown not to vary across calls  As a result  just like for query loops  a single pass over the function body suf   ces to consider all possible behaviors  We want to emphasize some useful consequences of the way that our analysis handles SQL  First  unlike in some related work  14   despite the fact that our policies are themselves SQL queries  the analysis does not require that program code use exactly those queries  Semantic modeling of queries makes it possible for one policy query to justify in   nitely many possible program queries  Second  the soundness of our analysis depends on knowledge of the database schema  but not knowledge of database contents  Schema changes can invalidate analysis results by  for example  rede   ning data integrity constraints that the theorem prover might have relied on  However  arbitrary changes to the database rows  by arbitrary programs with no relation to UrFlow  cannot invalidate past analysis results  6 The Theorem Prover The last section highlighted the key theorem prover operations that symbolic evaluation depends on  We can summarize them like this    Assert a predicate p  If p contradicts the predicates already asserted  raise an exception indicating so    Check if a predicate is implied by those already asserted    Determine if a logical expression can be constructed from members of a set of allowable expressions  9The    rst two points are supported by the classic model of    rst order logic theorem proving that is embodied in tools like Simplify  8   The third point is new and not directly supported by usual prover interfaces  but the usual implementation techniques can support it very directly  Provers like Simplify are based on the Nelson Oppen architecture  We do not use many of the elements of that architecture  since our prototype implementation omits features like reasoning about arithmetic  Instead  we just adopt the key data structure  the E graph  An E graph is a directed graph representation of the possible worlds that are consistent with a set of predicates  Nodes stand for objects  and  for function symbol f  an edge labeled with f goes from node u to node v if  in any compatible world  the object associated with v equals the result of applying f to the object associated with u  A node is labeled with logical variables and constants to indicate that any compatible world must assign this node to an object equal to those variables and constants  In UrFlow  we only use two kinds of function symbols  union tags and record    eld names  For tag T  there is a T labeled edge from u to v if v must be u tagged with T  i e      v   T u       For    eld name F  there is an F labeled edge from u to v if u is a record whose F component equals v  For each node that came from a literal record expression  we mark that node as complete  in the sense that the    eld edges coming out of it provide a complete description of the available    elds  An example of an incomplete record node is one representing a row selected in an SQL query  the state will only mention those columns relevant to the query  and it would be unsound to treat this row as if it had no further columns  Figure 1 shows an E graph representing the logical state given earlier for checking the code write escape r Secret Data    Nodes are boxes when the state implies that they are known  other nodes may not be known  Complete record nodes are diamonds  We abbreviate cookie login as C  The basic prover algorithm understands two kinds of predicates  e1   e2 and known e   When either kind is asserted  its expressions are    rst evaluated into nodes of the E graph  adding new nodes as necessary  A variable or constant is evaluated to the node labeled with it  A union tag application T e  is evaluated by following the T edge from the node that e evaluates to  and a    eld projection e F is evaluated analogously  A record expression fF1   e1          Fn   eng is evaluated by checking for existing complete nodes whose Fi edges point to the nodes to which the eis evaluate  When a fact e1   e2 is asserted  the nodes u1 and u2 standing for e1 and e2 are merged  taking the unions of their sets of labels and incoming and outgoing edges  Alternatively  this fact might trigger a contradiction  That happens when u1 and u2 are labeled with different constants or have incoming tag edges labeled with different tags  When a fact known e  is asserted  and e evaluates to u  we    change u to a box     and we propagate this knownness information across edges  That propagation follows record    eld edges in the forward direction only and tag edges in either direction  The same propagation is implied when merging a known node with a not known node for an equality assertion  The heart of the procedure is in this handling of assertion  E graphs have nice properties which make implication checking very ef   cient  To check if e1   e2  we only check if e1 and e2 evaluate to the same node  To check if known e   we only check if e evaluates to a boxed node  One useful addition  implemented outside of the theorem prover core  takes advantage of key information for SQL tables  where  for instance  an ID column is asserted not to be duplicated across rows of a table  and the SQL engine maintains this invariant with dynamic checks  Whenever a new predicate asserts that some row r is in table R  we check  for every pre existing predicate R r 0    if r and r 0 agree on the values of R   s key columns  These checks can be implemented by querying the prover core with the appropriate equality predicates  Whenever a matching r and r 0 pair is found  we can skip adding the new predicate R r  to the state  instead asserting r   r 0   This enrichment of the prover is useful in analyzing applications that  for example  query a user password table multiple times  where correctness relies on the fact that the query always returns the same result  The last ingredient is checking if the value of expression e can be constructed out of the values of expressions e1          en  using only record and union operations  To implement the check  we evaluate each ei in turn  marking its node as allowable  Next  we evaluate e to a node u  If u is marked as allowable  we are done  Otherwise  if u has an incoming union tag edge from a node v  we repeat the procedure for v  If u is a complete record node  we repeat the procedure for each target of a    eld edge out of u  returning success only if the check is successful for each of these new nodes  In any other case  we return failure  7 Discussion We can get a sense for the breadth of UrFlow by considering how it helps with the most common Web application security    aws  The OWASP Top 10 Web Application Security Risks project 1 is a popular reference for security conscious Web developers  Based on analysis 1 http   www owasp org index php Category  OWASP Top Ten Project 10of databases of real vulnerabilities  the OWASP team has identi   ed which classes of security    aw pose the greatest risks  The Ur Web compiler rules out injection  ranked  1  and cross site scripting   2  vulnerabilities and partially mitigates cross site request forgery   5  and unvalidated redirects and forwards   10  using techniques unrelated to UrFlow  Risk  6  security miscon   guration  is a whole system property that cannot really be addressed by any single tool  and UrFlow   s lack of integrated reasoning about cryptography prevents it from helping to avoid insecure cryptographic storage   7   UrFlow can contribute to the mitigation of the remaining risk categories  Risk  3  broken authentication and session management  is helped by the ability to use UrFlow policies to specify exactly which secure tokens may be sent to which clients  It is still possible to make mistakes in the policies  but these policies should be signi   cantly easier to audit than programs  with the many possible control    ow paths of the latter  The next two risk categories  insecure direct object references   4  and failure to restrict URL access   8   are very similar  as both involve the omission of access control checks for particular system objects  UrFlow can enforce that appropriate checks are always performed whenever database objects are used in particular ways  Insuf   cient transport layer protection   9  could be avoided by adding a variant of sendClient policies which speci   es values that may only be sent to clients over SSL connections  Comparing against the pros and cons of security types  16   we    nd some interesting trade offs  UrFlow uses high level knowledge of programs to provide more sound reasoning without program annotations  Securitytyped languages generally rely on declassi   cation techniques where trust is granted to particular spans of code  This creates a contrast between the security typed approach  requiring trusted code but granting soundness with respect to implicit    ows  and the UrFlow approach  which requires no trusted Ur Web functions but ignores some implicit    ows  Security type annotations tend to be required throughout a program  while UrFlow avoids the need to mark up program code  However  SQL queries as policies involve some gotchas that would be less applicable to security types  For instance  it is easy to forget all or part of a policy WHERE clause  which has the unfortunate consequence of allowing behaviors by default  The problem of implicit    ow checking is a serious one in all kinds of information    ow analysis  Where UrFlow checks implicit    ows  the checking is not particularly clever  and implicit    ows caused by WHERE clauses are ignored  Future work may be able to plug part of this hole statically  and we suspect there will also be a large role for dynamic monitoring systems  for detecting brute force password cracking attempts and other attacks that involve many HTTP requests  Many different logical languages have been used for speci   cation writing in static veri   cation tools  We found SQL to be a convenient choice  because it is expressive enough to allow direct expression of interesting policies  and declarative enough to enable effective automated reasoning  We do not mean to claim that SQL has great expressivity or succinctness advantages over more traditional speci   cation languages  Rather  most Web programmers are accustomed to SQL  which should help in overcoming some of the social obstacles faced in the past by attempts to get programmers to write logical speci   cations  Our implementation today only handles a subset of the common SQL features  We omit support for outer joins  These should be easy to model via disjunctive formulas  covering all the possible cases of whether a row matching the join condition exists in a table  though a naive realization of this idea would probably have poor performance consequences for the theorem prover  Grouping and aggregation are harder to encode in the quanti   erfree    rst order logic that we are employing  We suspect that most real programs can be checked with conservative encodings of aggregation  where we model aggregate function values as unknowns  Alternatively  we can restrict reasoning about aggregate functions to simple syntactic pattern matching against policies  That approach also seems most practical for handling of the SQL EXCEPT operator  which implements a kind of negative reasoning about which rows do not exist  This is needed to write down policies like  for a conference management system     reviewer A may see the reviews for paper B only if A does not have a con   ict with B     More advanced policies might also need to include non trivial program code  For instance  a custom hashing or encryption scheme might be used  Here we encounter a common situation for static veri   cation  where it is always possible to expand the reach of your theoremprover to handle new program features  No single implementation will ever be able to handle all realistic programs  but we suspect that very good coverage will be possible  after the incorporation of signi   cant practical experience with the tool  8 Evaluation The UrFlow prototype is implemented in about 2200 lines of Standard ML code  We have used the analysis to check a number of Ur Web applications  There is a live demo of the applications  with links to syntaxhighlighted source code  at  http   www impredicative com ur scdv  11Application Program  LoC  Policies  LoC  Check  sec  Secret 138 24 0 02 Poll 196 50 0 035 User DB 84 8   Calendar 255 46 0 28 Forum 412 134 17 68 Gradebook 342 61 1 49 Figure 2  Lines of code breakdown in case studies  with time required to check the code with UrFlow Our case studies include Secret  a minimal application for storing secrets that may later be retrieved via password authentication  which was used as the model for this paper   s    rst set of running examples  the Forum and Poll applications from which Section 3   s examples were drawn  a Gradebook application  for managing a database of student grades in courses  and a reimplementation of the Calendar application from the paper  5  that introduced the SIF system for combined static and dynamic checking of information    ow in Web applications  Calendar  Forum  and Gradebook share a common user authentication component  The Calendar application lets users save details of their schedules on the Web  with controlled sharing of information  By default  no one may learn anything about an event  The creator of an event may learn everything about it  and the creator may add invitees who inherit the same read privileges  The creator may also authorize users to know only the time of an event  so that those users see that time slot only as    busy    on the creator   s calendar  Only event creators may modify any state related to their events  The Gradebook application is based on a database of courses and assignments of users to be instructors  teaching assistants  TAs   or students in courses  Each student membership record contains an optional grade  Only system administrators may create courses and modify instructor lists  Instructors may set grades and control TA assignments  A TA may view all of the state associated with a course  but may not modify it  A student may view his own grades  and a student in a course may only affect that course   s part of the database by dropping the course  Figure 2 gives the number of lines in code in each of these components  An application   s code is separated into the program itself and the policies  The    gures here make    policy overhead    appear bigger than it would probably be in production applications  since our case studies include minimal code dedicated to providing fancy user interfaces  Still  these numbers compare favorably to those for systems like SIF  where Calendar requires 1779 lines of code  While we have a similar ratio of program to annotation  our annotations are of a different kind  443 lines of the SIF version include annotations  in the form of security types  20  and explicit downgradings  The latter involve annotations that effectively say    the owner of a piece of information trusts this span of code  so let that span release derived information that would not otherwise be allowed     The SIF Calendar case study includes 17 such downgrades  The UrFlow approach is very different  As no annotations are required in programs  there is no need to accept any part of a program as trusted  All checking is with respect to the declarative speci   cation provided by the policy queries  Our analysis detects    aws similar to those that occur frequently in real deployed systems  For instance  we examined reports for July 2010 in the National Vulnerability Database 2   Among the relevant issues  we found CVE 2009 4927  involving privilege escalation via a surprising setting of a speci   c cookie  and CVE 2010 2685 and CVE 2009 4929  which allow administrative actions to be taken without proper credentials  via hand crafted HTTP requests  UrFlow makes it easy to catch these problems  since it is not necessary to enumerate all possible attack vectors  thanks to policies that talk directly about underlying resources  For instance  we introduced a bug in the Gradebook application to mimic the cookie bug above  where we allow anyone to set any student   s grade if a particular cookie is set to 1  The compiler complains that the database update policy may be violated  referencing the exact span of source code where the offending UPDATE statement occurs  The same output appears if we simulate a forgotten access control check  in the style of the second two issues above  by commenting out an important if test  UrFlow also requires no change to the runtime behavior of a program  and this baseline performance level is greater than for most popular Web languages and frameworks  thanks to the general purpose and domainspeci   c optimizations performed by the Ur Web compiler  We present the performance of the UrFlow analysis itself in Figure 2  for runs on a Linux machine with dual 1 GHz AMD64 processors with 2 GB of RAM  Of our case studies  only Forum takes much longer than a second to check  This is because Forum has a complicated main function  with many security checks  Many different actions call the ma</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ts4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_testing_and_security"/>
        <doc>Dynamic Updates for Web and Cloud Applications ### Pamela Bhattacharya Iulian Neamtiu Department of Computer Science and Engineering University of California  Riverside Riverside  CA 92521  USA fpamelab neamtiug cs ucr edu Abstract The center of mass for newly released applications is shifting from traditional  desktop or server programs  toward web and cloud computing applications  This shift is favorable to end users  but puts additional burden on application developers and service providers  In particular  the newly emerging development methodologies  based on dynamic languages and multi tier setups  complicate tasks such as veri   cation and require end to end  rather than program local guarantees  Moreover  service providers need to provide continuous service while accommodating the fast evolution pace characteristic of web and cloud applications  A promising approach for providing uninterrupted service while keeping applications up to date is to permit dynamic software updates  i e   applying dynamic patches to running programs  In this paper we focus on safe dynamic updates for web and cloud applications  we point out dif   culties associated with dynamic updates for these applications  present some of our preliminary results  and lay out directions for future work  Categories and Subject Descriptors C 4  Performance of Systems   Reliability  availability  and serviceability  D 2 7  Software Engineering   Distribution  Maintenance  and Enhancement   Corrections  Enhancement  F 3 2  Semantics of Programming Languages   Program analysis General Terms Reliability  Veri   cation Keywords Web applications  cloud computing  dynamic software updating  online updates  on the    y upgrades  endto end properties ### 1  Introduction We are currently witnessing a shift in how applications are developed and deployed  Desktop applications are transiPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  APLWACA    10 June 6  Toronto  Canada  Copyright  c 2010 ACM 978 1 60558 913 8 10 06       10 00 tioning to web applications  and server applications are transitioning to    solution stacks    and cloud computing  For example  of   ce suites are moving from local applications to web applications such as Google Docs and Microsoft Of     ce Web Apps  More generally  traditional client server applications   where the client is    thin    and provides little functionality   are giving way to    thick     feature rich clients  On the server side  developing applications has recently been facilitated by open source frameworks such as Django  Ruby on Rails  or Google App Engine  Rather than writing C C   Java SQL code for each tier  front  application  database   developers can use one of these frameworks which allow easy software construction and deployment  The Cloud Computing paradigm has gained popularity for hosting applications in recent years  as it provides businesses with pay as you go computation and storage services  Data infrastructures and applications associated with cloud computing and web applications require regular maintenance and updates  to provide the newest features or incorporate the latest security    xes   while also having to provide 24 7 service  Traditional maintenance and update practices are ill suited for these environments  because they are based on stop restart  stopping the system  followed by restart to install updates  or rolling upgrades  where one part of the infrastructure is updated at a time  rendering it unavailable to clients   This temporary unavailability can lead to failures  21   customer dissatisfaction  or even loss of revenue  as service providers use a pay per use model for cloud services  unavailability results in loss of business  For example  in 2009  Google offered to compensate users of Google Apps Premier Edition customers for periods of unavailability  25  due to two major Gmail outages  12  28   these outages took place during rolling upgrades  Further anecdotal evidence from Facebook and Oracle  13  stresses the need for continuous availability  Therefore  on the    y updates are becoming a required feature for maintaining client satisfaction in light of more frequent patches  23  and regular maintenance  In prior work  we have shown that dynamic updates are effective for Internet servers  such as FTP servers  SSH servers  media streaming and web caching servers  18   20    in more recent work  we have explored on the    y changesto database schemas  However  web applications and cloud computing present unprecedented challenges in software updating in general  and dynamic software updating in particular  Our paper points out how the models underlying these novel applications complicate updates  the dynamic nature of the applications makes veri   cation tasks dif   cult  and their distributed architecture raises consistency issues  We discuss these challenges  point out how state of the art solutions fail to completely address them  and present our preliminary results on two fronts   1  providing end to end dynamic update mechanisms  and  2  identifying and eliminating update safety issues due to cross tier and cloud wide inconsistencies  2  Challenges And State Of The Art 2 1 Dynamic Languages The advent of Web 2 0 and the concept of Web as a    participation platform    gave the users more interactivity than just retrieving information  by allowing them to run software applications entirely through a browser  This paradigm led to the popularity of many dynamic scripting languages  such as JavaScript  Python  and PHP  and frameworks like Django or Ruby on Rails  that permit either direct generation or facilitate the construction  of multi tier software  The evidence for this shift is not merely anecdotal  language popularity statistics 1 show that the number of lines of code written for new software  in dynamic languages  is comparable to that of C  C    and Java  Similarly  the Tiobe index  2 a measure of language popularity  shows that  as of March 2010  dynamic languages  e g   Python  JavaScript  Ruby  are gaining popularity  whereas C  C    and Java are losing ground  While these dynamic languages and frameworks based on them enable rapid construction of complex web applications  they introduce two main hurdles to safe dynamic updates  1  Since these new languages are dynamic  the lack of static checking and the lack of mature analysis and veri   cation tools makes them more prone to error  Moreover  as applications increase in size  they become increasingly dif   cult to maintain  2  Multi tier applications require end to end veri   cation and property enforcement  e g   security   as opposed to local guarantees associated with monolithic applications  Our current and future efforts tackle these issues by leveraging emerging veri   cation tools for dynamic languages and formal ways of ensuring end to end properties  9  15   Preliminary results  by us and other researchers suggest that  1  designing or incorporating static checking into dynamic scripting languages are effective veri   cation mechanisms  5  14  22  24  27   and  2  formally modeling the semantics of 1 http   www langpop com  2 http   www tiobe com index php content paperinfo tpci  index html each tier in multi tier applications and checking cross tier consistency helps guarantee end to end properties for the whole application  8  10  11  17   2 2 Cloud Computing Cloud Computing is evolving as a powerful paradigm for hosting Internet scale applications in large computing infrastructures  In particular  IT services are migrating from enterprise scale computing infrastructures  i e   in house networked cluster of servers  to cloud computing infrastructures  i e   pay for service large data centers with thousands to tens of thousands of machines   The pay per use model  zero up front investment  and perception of unlimited resources in   nite scalability  are major features attracting a large group of users  2  3   On the    ip side  applications based on cloud computing must provide continuous availability   paying clients can not tolerate downtime associated with system reboot to perform updates   hence fast  on the    y  updates are a necessity  Therefore  traditional maintenance and updates practices based on stop restart or rolling upgrades become ill suited in this environment  In particular  rolling upgrades  despite their prevalence  are problematic because the upgrade is not an atomic operation and it risks introducing inconsistencies in the application stack  as illustrated in Section 2 4  2 3 Update Mechanisms Prior work by us and others  18   20  has shown that onthe    y software update mechanisms are practical for standalone applications  Extending these mechanisms to web and cloud applications is not trivial  due to several factors  In particular  dynamic typing and dynamic linking  in contrast to statically typed and linked C C   Java applications  increase    exibility  but decrease the testing and veri   cation capacity  Similarly  dynamically generated code  in contrast to the manually written code characteristic of C C   Java applications   open the way for cross tier inconsistencies and update failures  2 4 Update Safety Prior to Web 2 0  the thin client model was prevalent  most of the computation took place on the server  and the client was relegated to HTML rendering  Therefore  in that model  verifying the application logic  reasoning about consistency issues  and enforcing safety properties for server programs alone was suf   cient  In Web 2 0  the application model is based on thick clients supported by feature rich browsers  computation is of   oaded to the client  which communicates with the server more sparsely  In cloud computing with multi tenant architectures  a single application instance partitions its data and con   guration to provide different  custom interfaces to different client organizations  Thus  in these novel models  delineating data associated with one connection or one client is dif   cult    the multi tier or multi tenantFigure 1  Inconsistencies due to long running clients and server side updates  setup transforms the need for local property checking into the need for end to end property checking  For example  many web applications include plugins or even embedded sub applications that personalize larger  container applications according to user needs   in Gmail  an email message which includes keywords for appointments automatically gives the user the option to add the message to the calendar  Though convenient  plugins and embedded applications raise security issues which can undermine the security of the entire container application  Therefore  guaranteeing application security requires the consideration of all components and across all tiers  i e   end to end consistency  Programming models and frameworks such as Links  10   Swift  8   and Google Web Toolkit 3 address this problem by allowing developers to write a single application  and letting the compiler generate separate code for each tier while preserving consistency  However  even when using this model  updates are problematic  e g   due to long lived clients or rolling upgrades  as illustrated by the following examples  Unsafe updates due to long lived clients  Long lived clients whose sessions last longer than the interval between server side upgrades can lead to inconsistent updates  as illustrated in Figure 1  In this scenario  the client side application starts its communication with the server at version 1  At some point  the server side application undergoes an update from version 1 to version 2  Any post update communication is potentially unsafe  as the client side part of the application is now out of sync  i e   still at version 1   Adding versioning to client server messages addresses this problem  but anecdotal evidence suggests that  in practice  versioning is too complicated and unpopular with service providers  13   As a consequence  service providers prefer the simplicity of non versioning approaches  at the expense of safety   or permit addition only updates which alleviate safety concerns  However  even in the presence of versioning  updating thick client applications is problematic  due to 3 http   code google com webtoolkit  Figure 2  Multiple application versions as a result of rolling upgrades in the cloud  client stored persistent data  one such example is Gears  4 a framework used in popular web applications such as Google Docs and Gmail  Gears provides caching  storage and parallel execution functionality by extending the client   s browser  Schema updates  e g   as a result of a new application version  can lead to the database schema assumed by the web application and the database schema assumed by local storage to differ  which is potentially problematic  Unsafe updates due to server side rolling upgrades  Update safety is further threatened in cloud computing applications  Rolling upgrades allow service providers to sustain service yet perform upgrades by partitioning the serverside applications into    update domains     We illustrate rolling upgrades in a cloud computing environment in Figure 2  business logic applications run on top of virtualized hardware  and communicate with clients  via a load balancer  and cloud storage  Prior to applying an update from version 1 to version 2  cloud service providers partition the applications into two update domains  UD1 and UD2  Systems in update domain UD1 will continue serving clients at version 1  while systems in update domain UD2 wait for client requests to complete  shut down and restart at version 2  The result  Figure 2  is that the UD1 partition runs application version 1  while the remaining applications run version 2  This situation is problematic when application instances communicate with other instances  e g   when an instance from UD1 requests a    eld deleted in version 2 from an application instance in UD2  or with the storage system  e g   when an application instance assumed data schema differs from the schema assumed by the storage services   Note that Microsoft Azure  6  7  employs rolling upgrades and allows different application versions to run in parallel  rolling upgrades at Google have led to Gmail outages  12  28   3  Preliminary Results and Future Work We now proceed to presenting our current work and preliminary results on update mechanisms  i e   extending our local dynamic update mechanisms to all links in an end to  4 http   gears google com end chain  and update safety  i e   ensuring a uniform semantics across tiers and across cloud side applications  Together  these two results lead to safe  end to end dynamic updates  3 1 Update Mechanisms Preliminary results  In prior work  we have demonstrated a practical approach for performing on the    y updates to popular server programs written in C  memcached  FTP and SSH servers  routers  media streaming servers  online gaming servers  18   20    other researchers have shown similar results for dynamic updates to Java applications  26   All these updates could be applied while sustaining service to clients  These solutions have proved appropriate for server applications without persistent state  but inadequate for applications that use databases   as applications evolve  so do the schemas at which they store data  so we must support onthe    y schema changes as well  Therefore  in more recent work we have started to explore the feasibility of dynamic updates to persistent data  via on the    y schema evolution  Initial results on SQLite based systems 5 using actual schema changes from Mozilla show that database applications can enjoy safe schema updates with little performance cost  Future work  Our existing techniques place us part way there in offering dynamic update mechanisms to multi tier applications  i e   we can perform updates to the database tier and the application tier  as long as the application tier is written in C or Java   We are  however</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ud1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ud1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_understanding_data_and_queries"/>
        <doc> Query by Output ### Quoc Trung Tran  Chee Yong Chan  and Srinivasan Parthasarathy  April 2009 T e c h n i c a l   R e p o r t     Foreword    This technical report contains  a research paper  development or  tutorial article  which has been submitted for publication in a  journal or for consideration by  the commissioning organization   The report represents the ideas of its author  and should not be  taken as the official views of the School or the University  Any  discussion of the content of the report should be sent to the author   at the address shown on the cover        OOI Beng Chin  Dean of School Query by Output Quoc Trung Tran 1 Chee Yong Chan 1 Srinivasan Parthasarathy 2  1 Department of Computer Science National University of Singapore ftqtrung chancyg comp nus edu sg 2 The Ohio State University srini cse ohio state edu Technical report TRA4 09 Abstract  It has recently been asserted that the usability of a database is as important as its capability  Understanding the database schema  the hidden relationships among attributes in the data all play an impor  tant role in this context  Subscribing to this viewpoint  in this paper  we present a novel data driven approach  called Query By Output  QBO   which can enhance the usability of database systems  The central goal of QBO is as follows  given the output of some query Q on a database D  denoted by Q D   we wish to construct an alternative query Q 0 such that Q D  and Q 0  D  are instance equivalent  To generate instance equivalent queries from Q D   we devise a novel data classi  cation based technique that can handle the at least one semantics that is inherent in the query derivation  In addition to the basic framework  we design several opti  mization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility  Our framework is evaluated comprehensively on three real data sets and the results show that the instance equivalent queries we obtain are interest  ing and that the approach is scalable and robust to queries of di  erent selectivities ###  1 Introduction A perennial challenge faced by many enterprises is the management of their increasingly large and complex databases which can contain hundreds and even thousands of tables  10  24   The problem is exacerbated by the fact that the metadata and documentation for these databases are often incomplete or missing  11   Several di  erent approaches have been proposed to address this important practical issue  One example here is database structure mining where the goal is to discover structural relationships among the database tables  1  11  24   Another example is intensional query answering where the goal is to augment query answers with additional information to help users understand the results as well as relevant database content  16     This work was done when the author was on sabbatical at National University of Singapore in 2008  1In this paper  we present a novel data driven approach called Query by Output  QBO   QBO aims to derive interesting query based characterizations of an input database table which can be the result of some query or materialized view  In contrast to conventional querying which takes an input query Q and computes its output  denoted by Q D   w r t  an input database D  the basic idea of QBO is to take as input the output Q D  of some query Q and compute a set of queries Q0 1              Q0 n such that each Q0 i  D  is  approximately  equivalent to Q D   We say that two queries Q and Q0 are instance equivalent w r t  a database D  denoted by Q    Q0    if Q D  and Q0  D  are equivalent  Before we discuss the speci  c contributions of this paper  we brie  y highlight some of the use case scenarios of our proposal  QBO Applications  The most obvious application of QBO is in conventional database querying where Q is known  Consider the scenario when a user sub  mits a query Q to be evaluated on a database D  Instead of simply return  ing the query result Q D  to the user  the database system can also apply QBO to compute additional useful information about Q and D in the form of instance equivalent queries  Besides providing alternative characterizations  po  tential simpli  cations  of Q D   IEQs can also help users to better understand the database schema  Speci  cally  since many enterprise data schema are very complex and large  potentially involving hundreds and even thousands of rela  tions  24   the part of the database schema that is referenced by the user s query may be quite di  erent from that referenced by an IEQ  The discovery of this alternative  path  in the schema to generate an instance equivalent query can aid the user s understanding of the database schema or potentially help re  ne the user s initial query  Another obvious application of QBO is that it can help the user better under  stand the actual data housed within the database  Unusual or surprising IEQs can be useful for uncovering hidden relationships among the data  In several instances simpler or easier to understand relationships may be uncovered which can again aid in the understanding of the data contained within the complex database and help the user re  ne future queries posed to the database  QBO may also have interesting applications in database security where at  tackers who have some prior domain knowledge of the data may attempt to derive sensitive information  For example  if an attacker is aware of the existing correlation structure in the data  they can easily use this information to for  mulate two or more separate queries which on paper look very di  erent  e g  using di  erent selection criteria  but in reality may be targeting the same set of tuples in the database  Such sets or groups of queries can potentially be used to reverse engineer the privacy preserving protocol in use  Subsequently  sensitive information can be gleaned  As a speci  c example  consider a protocol such as    diversity  25  which relies on detecting how similar the current query is with a previous set of queries  history  answered by the database  to determine if the current query can be answered without violating the privacy constraints  The notion of similarity used by such methods relies primarily on the selection attributes and thus such protocols will fail to recognize IEQs that use di  erent 2selection attributes  Privacy in such protocols will then be breached  Automati  cally recognizing such IEQs via the methods proposed in this paper and subse  quently leveraging this information to enhance such protocols may provide more stringent protection against such kinds of attacks  Another important class of QBO applications is in scenarios where the input consists of Q D  but not Q itself  Such scenarios are common in data analysis and exploration applications where the information provided is often incom  plete  the data set Q D   which is produced by some query view Q  is available but not the query view  11   The ability to reverse engineer Q from Q D  then becomes important  Annotating data with relevant metadata is essential in cu  rated databases  6   Such reverse engineering is also useful for generating concise query based summaries of groups of tuples of interest to the user  e g   dominant tuples selected by skyline queries  4    Contributions  In this paper  we introduce the novel problem of QBO and pro  pose a solution  TALOS  that models the QBO problem as a data classi  cation task with a unique property that we term at least one semantics  To handle data classi  cation with this new semantics  we develop a new dynamic class labeling technique and also propose e  ective optimization techniques to e  ciently com  pute IEQs  Our experimental evaluation of TALOS demonstrates its e  ciency and e  ectiveness in generating interesting IEQs  2 Overview of Our Approach The QBO problem takes as inputs a database D  an optional query Q  and the query s output Q D   w r t  D  and computes one or more IEQs Q0   where Q and Q0 are IEQs if Q D     Q0  D   We refer to Q as the input query  Q D  as the input result  and Q0 as the output query  First  let us state the following theoretical results that we have established for variants of the QBO problem  Theorem 1  Given an input query Q  we de  ne QBOS to be the problem to   nd the output query Q0 where Q0 is a conjunctive query that involves only projection and selection  with predicates in the form  Ai op c   Ai is an attribute  c is constant and op 2 f           6        g  such that  1   Q0  D     Q D  and  2  the number of operators  AND  OR and NOT  used in the selection condition is minimized  Then QBOS is unlikely to be in P  Proof Sketch  We prove Theorem 1 by reducing the Minimization Circuit Size Problem to QBOS  Details are given in Appendix A     Theorem 2  Given an input query Q  we de  ne QBOU to be the problem to   nd an output query Q0 of the form Q0   Q1 UNION Q2          UNION Qk where each Qi is in the SPJ form and the select clause refers to only attributes in the schema such that Q0  D    Q D  and k is minimized  Then QBOU is NP hard  3Proof Sketch  We prove Theorem 2 by reducing the Set Cover Problem to QBOU   Details are given in A     Theorem 3  Given an input query Q  we de  ne QBOG to be the problem to   nd an output query Q0 such that Q0  D     Q D  and Q0 can contain arbitrary arithmetic expressions in the select clause  Then QBOG is PSPACE hard  Proof Sketch  We prove Theorem 3 by reducing the Integer Circuit Evaluation Problem to QBOG  Details are given in A     Given the above results  in this paper  we consider relational queries Q where the select clause refer to only attributes  and not to constants or arith  metic aggregation string expressions  to ensure that Q0 can be derived e  ciently from Q D   We also require that Q D   6   for the problem to be interesting  For simplicity  our approach considers only select project join  SPJ  queries for Q0 where all the join predicates in Q0 are foreign key joins  Thus  our ap  proach requires only very basic database integrity constraint information  i e   primary and foreign key constraints   Based on the knowledge of the primary and foreign key constraints in the database  the database schema can be mod  eled as a schema graph  denoted by SG  where each node in SG represents a relation  and each edge between two nodes represents a foreign key join between the relations corresponding to the nodes  For ease of presentation and without loss of generality  we express each Q0 as a relational algebra expression  To keep our de  nitions and notations simple and without loss of generality  we shall assume that there are no multiple instances of a relation in Q and Q0   Running Example  In this paper  we use a database housing baseball statis  tics 1 for our running example as well as in our experiments  Part of the schema is illustrated in Figure 1  where the key attribute names are shown in bold  The Master relation describes information about each player  identi  ed by pID   the attributes name  country  weight  bats  and throws refer to his name  birth coun  try  weight  in pounds   batting hand  left  right  or both   and throwing hand  left or right   respectively  The Batting relation provides the number of home runs  HR  of a player when he was playing for a team in a speci  c year and season  stint   The Team relation speci  es the rank obtained by a team for a speci  ed year  Notations  Given a query Q  we use rel Q  to denote the collection of relations involved in Q  i e   relations in SQL s from clause   proj Q  to denote the set of projected attributes in Q  i e   attributes in SQL s select clause   and sel Q  to denote the set of selection predicates in Q  i e   conditions in SQL s where  clause   2 1 Instance Equivalent Queries  IEQs  Our basic de  nition of instance equivalent queries  IEQs  requires that the IEQs Q and Q0 produce the same output  w r t  some database D   i e   Q D     Q0  D   1 http   baseball1 com statistics  4pID name country weight bats throws P1 A USA 85 L R P2 B USA 72 R R P3 C USA 80 R L P4 D Germany 72 L R P5 E Japan 72 R R pID year stint team HR P1 2001 2 PIT 40 P1 2003 2 ML1 50 P2 2001 1 PIT 73 P2 2002 1 PIT 40 P3 2004 2 CHA 35 P4 2001 3 PIT 30 P5 2004 3 CHA 60 team year rank PIT 2001 7 PIT 2002 4 CHA 2004 3  a  Master  b  Batting  c  Team Fig  1  Running Example  Baseball Data Set D The advantage of this simple de  nition is that it does not require the knowledge of Q to derive Q0   which is particularly useful for QBO applications where Q is either missing or not provided  However  there is a potential  accuracy  tradeo   that arises from the simplicity of this weak form of equivalence  an IEQ may be  semantically  quite di  erent from the input query that produced Q D  as the following example illustrates  Example 1  Consider the following three queries on the baseball database D in Figure 1  Q1     country   bats  R  throws  R  Master    Q2     country   bats  R  weight  72 Master    and Q3     country   bats  R  Master    Observe that although all three queries produce the same output after pro  jection  fUSA Japang   only Q1 and Q2 select the same set of tuples fP2  P5g from R  Speci  cally  if we modify the queries by replacing the projection at  tribute  country  with the key attribute  pID   we have Q1 D    fP2 P5g  Q2 D    fP2 P5g and Q3 D    fP2 P3 P5g  Thus  while all three queries are IEQs  we see that the equivalence between Q1 and Q2 is actually  stronger   compared to that between Q1 and Q3  in that both queries actually select the same set of relation tuples     However  if Q is provided as part of the input  then we can de  ne a stronger form of instance equivalence as suggested by the above example  Intuitively  the stricter form of instance equivalence not only ensures that the instance  equivalent queries produce the same output  w r t  some database D   but it also requires that their outputs be projected from the same set of  core  tuples  We now formally characterize weak and strong IEQs based on the concepts of core relations and core queries  Core relations  Given a query Q  we say that S    rel Q  is a set of core relations of Q if S is a minimal set of relations such that for every attribute Ri  A 2 proj Q    1  Ri 2 S or  2  Q contains a chain of equality join predicates  Ri  A              Rj  B  such that Rj 2 S  Intuitively  a set of core relations of Q is a minimal set of relations in Q that  cover  all the projected attributes in Q  As an example  if Q     R1 X  p R1    R2    R3  where p    R1 X   R3 Y      R2 Z   R3 Z   then Q has two sets of core relations  fR1g and fR3g  5Core queries  Given a query Q where S    rel Q   we use QS to denote the query that is derived from Q by replacing proj Q  with the key attribute s  of each relation in S  If S is a set of core relations of Q  we refer to QS as a core query of Q  Strong   weak IEQs  Consider two IEQs Q and Q0  w r t  a database D   i e   Q D     Q0  D   We say that Q and Q0 are strong IEQs if Q has a set of core relations S such that  1  Q0 S is a core query of Q0   and  2  QS D  and Q0 S D  are equivalent  IEQs that are not strong are classi  ed as weak IEQs  The strong IEQ de  nition essentially requires that both Q and Q0 share a set of core relations such that Q D  and Q0  D  are projected from the same set of selected tuples from these core relations  Thus  in Example 1  Q1 and Q2 are strong IEQs whereas Q1 and Q3 are weak IEQs  Note that in our de  nition of strong IEQ  we only impose moderate restric  tions on Q and Q0  relative to the weak IEQ de  nition  so that the space of strong IEQs is not overly constrained and that the strong IEQs generated are hopefully both interesting as well as meaningful  As in the case with weak IEQs  two strong IEQs can involve di  erent sets of relations  As an example  suppose query Q selects pairs of records from two core relations  Supplier and Part  that are related via joining with a  non core  Supply relation  Then it is possible for a strong IEQ Q0 to relate the same pair of core relations via a di  erent relationship  e g   by joining with a di  erent non core Manufacture relation   We believe that each of the various notions of query equivalence has useful applications in di  erent contexts depending on the available type of informa  tion about the input query and database  At one extreme  if both Q and the database integrity constraints are available  we can compute semantically equiv  alent queries  At the other extreme  if only Q D  and the database D are avail  able  we can only compute weak IEQs  Finally  if both Q and the database D are available  we can compute both weak and strong IEQs  Precise   approximate IEQs  It is also useful to permit some perturbation so as to include IEQs that are  close enough  to the original  Perturbations could be in the form of extra records or missing records or a combination thereof  Such generalizations are necessary in situations where there are no precise IEQs and useful for cases where the computational cost for   nding precise IEQs is considered unacceptably high  Moreover  a precise IEQ Q0 might not always provide insightful characterizations of Q D  as Q0 could be too  detailed  with many join relations and or selection predicates  The imprecision of a weak IEQ Q0 of Q  w r t  D  can be quanti  ed by jQ D     Q0  D j   jQ0  D     Q D j  the imprecision of a strong IEQ can be quanti  ed similarly  Thus  Q0 is considered an approximate  strong weak  IEQ of Q if its imprecision is positive  otherwise  Q0 is a precise  strong weak  IEQ  As the search space for IEQs can be very large  particularly with large com  plex database schema where each relation has foreign key joins with other rela  tions  users should be able to restrict the search space by specifying hints  pref  erences in the form of control parameters  Some examples include   1  restricting 6Q0 to be conjunctive queries   2  setting an upper bound on the number of selec  tion predicates in Q0    3  setting an upper bound on the number of relations in Q0    4  specifying a speci  c set of relations to be included  excluded  in  from  Q0   and  5  specifying a speci  c set of attributes to be included  excluded  in  from  the selection predicates in Q0   In addition to these query speci  c controls  some method speci  c controls can also be applied on the IEQs search space  we discuss some of these in Section 4  We note although all the above user hints can be easily incorporated into our proposed algorithms  we do not delve on these control knobs any further in the paper but instead focus on the core problem of computing IEQs  2 2 TALOS  Conceptual Approach In this section  we give a conceptual overview of our approach  named TALOS  for Tree based classi  er with At Least One Semantics   for the QBO problem  Given an input result Q D   to generate a SPJ Q0 that is an IEQ of Q  we need to basically determine the three components of Q0   rel Q0    sel Q0    and proj Q0    Clearly  if rel Q0   contains a set of core relations of Q  then proj Q0   can be trivially derived from these core relations 2   Thus  the possibilities for Q0 depends mainly on the options for both rel Q0   and sel Q0    Between these two components  enumerating di  erent rel Q0   is the easier task as rel Q0   can be obtained by choosing a subgraph G of the schema graph SG such that G contains a set of core relations of Q  rel Q0   is then given by all the relations represented in G  Note that it is not necessary for rel Q     rel Q0   as Q may contain some relations that are not core relations  The reason for exploring di  erent possibilities for rel Q0   is to   nd interesting alternative characterizations of Q D  that involve di  erent join paths or selection conditions from those in Q  TALOS enumerates di  erent schema subgraphs by starting out with minimal subgraphs that contain a set of core relations of Q and then incrementally expanding the minimal subgraphs to generate larger  more complex subgraphs  We now come to most critical and challenging part of our solution which is how to generate  good  sel Q0   s such that each sel Q0   is not only succinct  without too many conditions  and insightful but also minimizes the imprecision between Q D  and Q0  D  if Q0 is an approximate IEQ  We propose to formulate this problem as a data classi  cation task as follows  Consider the relation J that is computed by joining all the relations in rel Q0   based on the foreign key joins represented in G  Without loss of generality  let us suppose that we are looking for weak IEQs Q0   Let L denote the ordered listing of the attributes in proj Q0   such that that the schema of   L J  and Q D  are equivalent 3   J can be partitioned into two disjoint subsets  J   J0 J1  such that 2 Note that even though the de  nition of a weak IEQ Q 0 of Q does not require the queries to share a set of core relations  we   nd this restriction to be a reasonable and e  ective way to obtain  good  IEQs  3 If the search is for strong IEQs  then the discussion remains the same except that L is the ordered listing of the key attributes of a set of core relations S of Q  and we replace Q D  by QS D   7  L J1     Q D  and   L J0    Q D       For the purpose of deriving sel Q0    one simple approach to classify the tuples in J is to label the tuples in J0  which do not contribute to the query s result Q D   as negative tuples  and label the tuples in J1 as positive tuples  Given the labeled tuples in J  the problem of   nding a sel Q0   can now be viewed as a data classi  cation task to separate the positive and negative tuples in J  sel Q0   is given by the selection conditions that specify the positive tuples  A natural solution is to examine if o   the shelf data classi  er can give us what we need  To determine what kind of classi  er to use  we must consider what we need to generate our desired IEQ Q0   Clearly  the classi  er should be e  cient to construct and the output should be easy to interpret and express using SQL  i e   the output should be expressible in axis parallel cuts of the data space  These criteria rule out a number of classi  er systems such as neural networks  k nearest neighbor classi  cation  Bayesian classi  ers  and support vector machines  17   Rule based classi  ers or decision trees  a form of rule based classi  er  are a natural solution in this context  TALOS uses decision tree classi  er for generating sel Q0    pID name country weight bats throws year team stint HR P1 A USA 85 L R 2001 PIT 2 40 P1 A USA 85 L R 2003 ML1 2 50 P2 B USA 72 R R 2001 PIT 1 73 P2 B USA 72 R R 2002 PIT 1 40 P3 C USA 80 R L 2004 CHA 2 35 P4 D Germany 72 L R 2001 PIT 3 30 P5 E Japan 72 R R 2004 CHA 3 60  a  J   Master   pID Batting N1 N2 N3 Weight    72 Weight   72 N4 N5 HR   30 HR 30 D B  E A C DT1 N1 N2 N3 HR   50 HR   50 A  B  C  D B  E DT2  b  Decision trees DT1 and DT2 Fig  2  Example of deriving IEQs for Q4     name    bats  R  throws  R  Master  on D We now brie  y describe how a simple binary decision tree is constructed to classify a set of data records D  For expository simplicity  assume that all the attributes in D have numerical domains  A decision tree DT is constructed in a top down manner  Each leaf node N in the tree is associated with a subset of the data records  denoted by DN  such that D is partitioned among all the 8leaf nodes  Initially  DT has only a single leaf node  i e   its root node  which is associated with all the records in D  Leaf nodes are classi  ed into pure and non  pure nodes depending on a given goodness criterion  Common goodness criteria include entropy  classi  cation error and the Gini index  17   At each iteration of the algorithm  the algorithm examines each non pure leaf node N and computes the best split for N that creates two child nodes  N1 and N2  for N  Each split is computed as a function of an attribute A and a split value v associated with the attribute  Whenever a node N is split  w r t  attribute A and split value v   the records in DN are partitioned between DN1 and DN2 such that a tuple t 2 DN is distributed into DN1 if t A    v  and DN2   otherwise  A popular goodness criterion for splitting  the Gini index  is computed as follows  For a data set S with k distinct classes  its Gini index is Gini S    1    Pk j 1  f 2 j   where fj denote the fraction of records in S belonging to class j  Thus  if S is split into two subsets S1  S2  then the Gini index of the split is given by Gini S1  S2    jS1j Gini S1    jS2j Gini S2  jS1j   jS2j   where jSi j denote the number of records in Si   The general objective is to pick the splitting attribute whose best splitting value reduces the Gini index the most  the goal is to reduce Gini to 0 resulting in all pure leaf nodes   Example 2  To illustrate how decision tree classi  er can be applied to derive IEQs  consider the following query on the baseball database D  Q4     name    bats  R  throws  R  Master   Note that Q4 D    fB  Eg  Suppose that the schema subgraph G considered contains both Master and Batting  i e   rel Q0 4     fMaster  Battingg  The output of J   Master   pID Batting is shown in Fig  ure 2 a   Using ti to denote the i th tuple in J  J is partitioned into J0   ft1  t2  t5  t6g and J1   ft3  t4  t7g  Figure 2 b  shows two example decision trees  DT1 and DT2  constructed from J  Each decision tree partitions the tuples in J into di  erent subsets  represented by the leaf nodes  by applying di  erent se  quences of attribute selection conditions  By labeling all tuples in J1 as positive  the IEQ derived from DT1 is given by Q0 4     name   stint  1  stint 1 HR 50   Master    Batting    More details are described in Section 4 1     2 3 TALOS  Challenges There are two key challenges in adapting decision tree classi  er for the QBO problem  At Least One Semantics  The   rst challenge concerns the issue of how to assign class labels in a   exible manner without over constraining the classi  cation problem and limiting its e  ectiveness  Contrary to the impression given by the above simple class labeling scheme  the task of assigning class labels to J is actually a rather intricate problem due to the fact that multiple tuples in J1 can be projected to the same tuple in   L J1   Recall that in the simple class labeling scheme described  a tuple t is labeled positive i   t 2 J1  However  note that it 9is possible to label only a subset of tuples J 0 1    J1 as positive  with tuples in J    J 0 1 labeled as negative  and yet achieve   L J 0 1       L J1   without a  ecting the imprecision of Q0    In other words  the simple scheme of labeling all tuples in J1 as positive is just one  extreme  option out of many other possibilities  We now discuss more precisely the various possibilities of labeling positive tuples in J to derive di  erent sel Q0    Let   L J1    ft1             tkg  Then J1 can be partitioned into k subsets  J1   P1              Pk  where each Pi   ft 2 J1 j the projection of t on L is tig  Thus  each Pi represents the subset of tuples in J1 that project to the same tuple in   L J1   De  ne J 0 1 to be a subset of tuples of J1 such that it consists of at least one tuple from each subset Pi   Clearly    L J 0 1       L J1   and there is a total of Qk i 1  2 jPij    1  possibilities for J 0 1   For a given J 0 1   we can derive sel Q0   using a data classi  er based on labeling the tuples in J 0 1 as positive and the remaining tuples in J1    J 0 1 as negative  Based on the above discussion on labeling tuples  each tuple in J can be classi  ed as either a bound tuple or free tuple depending on whether there is any freedom to label the tuple  A tuple t 2 J is a bound tuple if either  1  t 2 J0 in which case t must be labeled negative  or  2  t is the only tuple in some subset Pi   in which case t must certainly be included in J 0 1 and be labeled positive  otherwise  t is a free tuple  i e   t is in some subset Pi that contains more than one tuple   In contrast to conventional classi  cation problem where each record in the input data comes with a well de  ned class label  the classi  cation problem for  mulated for QBO has the unique characteristic where there is some   exibility in the class label assignment  We refer to this property as at least one semantics  To the best of our knowledge  we are not aware of any work that has addressed this variant of the classi  cation problem  An obvious approach to solve the at least one semantics variant is to map the problem into the traditional variant by   rst applying some arbitrary class label assignment that is consistent with the at least one semantics  In our ex  perimental study  we compare against two such static labeling schemes  namely  NI  which labels all free tuples as positive  and RD  which labels a random non  empty subset of free tuples in each Pi as positive 4   However  such static labeling schemes do not exploit the   exible class labeling opportunities to optimize the classi  cation task  To avoid the limitations of the static scheme  TALOS employs a novel dynamic class labeling scheme to compute optimal node splits for deci  sion tree construction without having to enumerate an exponential number of combinations of class labeling schemes for the free tuples  Example 3  Continuing with Example 2  J1 is partitioned into two subsets  P1   ft3  t4g and P2   ft7g  where P1 and P2 contribute to the outputs  B  and  E   respectively  The tuples in J0 and P2 are bound tuples  while the tuples in P1 are free tuples  To derive an IEQ  at least one of the free tuples in P1 must be labeled positive  If t3 is labeled positive and t4 is labeled negative  DT2 in 4 We also experimented with a scheme that randomly labels only one free tuple for each subset as positive  but the results are worse than NI and RD  10Number of free tuples Exactly One to be labeled positive Labeling of free tuples Constraint Propagation Case f1 f2 positive negative S1 S2 C1 Pm i 1 ni 1 Pm i 1 ni 2 S1   S2       C2 Pm i 1 ni 1 T2 S1 SP12 sets in S2   SP2 sets C3 T1 Pm i 1 ni 2 S2 SP12 sets in S1 SP1 sets   C4 T1 m    T1   SP12 sets in S1 SP1 sets All subsets C5 m    T2 T2   SP12 sets in S2 All subsets SP2 sets Table 1  Optimizing Node Splits Figure 2 b  is a simpler decision tree constructed by partitioning J based on a selection predicate on attribute HR  The IEQ derived from DT2 is Q 00 4     name   HR 50  Master    Batting      Performance Issues  The second challenge concerns the performance issue of how to e  ciently generate candidates for rel Q0   and optimize the computation of the single input table J required for the classi  cation task  To improve per  formance  TALOS exploits join indices to avoid a costly explicit computation of J and constructs mapping tables to optimize decision tree construction  3 Handling At Least One Semantics In this section  we address the   rst challenge of TALOS and present a novel approach for classifying data with the at least one semantics  3 1 Computing Optimal Node Splits The main challenge for classi  cation with the at least one semantics is how to optimize the node splits given the presence of free tuples which o  er   exibility in the class label assignment  We present a novel approach that computes the optimal node split without having to explicitly enumerate all possible class label assignments to the free tuples  The idea is based on exploiting the   exibility o  ered by the at least one semantics  Let us consider an initial set of tuples S that has been split into two subsets  S1 and S2  based on a value v of a numeric attribute A  the same principle applies to categorical attributes as well   i e   a tuple t 2 S belongs to S1 i   t A    v  The key question is how to compute the optimal Gini index of this split without having to enumerate all possible class label assignments for the free tuples in S such that the at least one semantics is satis  ed  Without loss of generality  suppose that the set of free tuples in S is partitioned  as described in Section 2 3  into m subsets  P1             Pm  where each jPi j   1  Let ni j denote the number of tuples in Pi   Sj   and fj denote the number of free tuples in Sj to be labeled positive to minimize Gini S1  S2   where i 2 11 1  m   j 2 f1  2g  We classify Pi   i 2  1  m   as a SP1 set  resp  SP2 set  if Pi is completely contained in S1  resp  S2   otherwise  Pi is a SP12 set  i e   ni 1   0 and ni 2   0   To satisfy the at least one semantics  we need to ensure that at least one free tuple in each Pi   i 2  1  m   is labeled positive  Let Tj   j 2 f1  2g  denote the minimum number of free tuples in Sj that must be labeled positive to ensure this  Observe that for a speci  c Pi   i 2  1  m   if Pi a SP1 set  resp  SP2 set   then we must have T1    1  resp  T2    1   Thus  Tj is equal to the number of SPj  sets  More precisely  Tj   Pm i 1 maxf0  1    ni 3  jg  j 2 f1  2g  Thus  f1 and f2 must satisfy the following two conditions   A1  Tj    fj    Pm i 1 ni j   j 2 f1  2g  and  A2  f1   f2    m  Condition  A1  speci  es the possible number of free tuples to be labeled positive for each Sj   while condition  A2  speci  es the minimum combined number of tuples in S to be labeled positive in order that the at least one semantics is satis  ed for each Pi   Based on conditions  A1  and  A2   it can be shown that the optimal value of Gini S1  S2  can be determined by considering only   ve combinations of f1 and f2 values as indicated by the second and third columns in Table 1  The proof of this result is given in Appendix B  These   ve cases correspond to di  erent combinations of whether the number of positive or negative tuples is being max  imized in each of S1 and S2  case C1 maximizes the number of positive tuples in both S1 and S2  case C2 maximizes the number of positive tuples in S1 and maximizes the number of negative tuples in S2  case C3 maximizes the number of negative tuples in S1 and maximizes the number of positive tuples in S2  and cases C4 and C5 maximize the number of negative tuples in both S1 and S2  The optimal value of Gini S1  S2  is given by the minimum of the Gini index values derived from the above   ve cases  3 2 Updating Labels   Propagating Constraints Once the optimal Gini S1  S2  index is determined for a given node split  we need to update the split of S by converting the free tuples in S1 and S2 to bound tuples with either positive negative class labels  The details of this updating depends on which of the   ve cases the optimal Gini value was derived from  and is summarized by the last four columns in Table 1  For case C1  which is the simplest case  all the free tuples in S1 and S2 will be converted to positive tuples  However  for the remaining cases  which involve maximizing the number of negative tuples in S1 or S2  some of the free tuples may not be converted to bound tuples  Instead  the maximization of negative tuples in S1 or S2 is achieved by propagating another type of constraints  referred to as  exactly one  constraints  to some subsets of tuples in S1 or S2  Similar to the principle of at least one constraints  the idea here is to make use of constraints to optimize the Gini index values for subsequent node splits without having to 12explicitly enumerate all possible class label assignments  Thus  in Table 1  the fourth and   fth columns specify which free tuples are to be converted to bound tuples with positive and negative labels  respectively  where an     entry means that no free tuples are to be converted to bound tuples  The sixth and seventh columns specify what subsets of tuples in S1 and S2  respectively  are required to satisfy the exactly one constraint  where an     entry column means that no constraints are propagated to S1 or S2  We now de  ne the exactly one constraint and explain why it is necessary  An exactly one constraint on a set of free tuples S 0 requires that exactly one free tuple in S 0 must become labeled as positive with the remaining free tuples in S 0 labeled as negative  Consider case C2  which is to maximize the number of positive  resp  negative  tuples in S1  resp  S2   The maximization of the number of positive tuples in S1 is easy to achieve since by converting all the free tuples in S1 to positive  the at least one constraints on the SP1 sets and SP12  sets are also satis  ed  Consequently  for each SP12 set Pi   all the free tuples in Pi   S2 can be converted to negative tuples  to maximize the number of negative tuples in S2  without violating the at least one constraint on Pi   However  for a SP2 set Pi   to maximize the number of negative tuples in Pi while satisfying the at least one semantics translates to an exactly one constraint on Pi   Thus  for case C2  an exactly one constraint is propagated to each SP2 set in S2  and no constraints are propagated to S1  A similar reasoning applies to cases C3 to C5  Thus  while the at least one constraint is applied to each subset of free tuples Pi in the initial node split  the exactly one constraint is applied to each Pi for subsequent node splits  This second variant of the node split problem can be optimized by techniques similar to what we have explained so far for the   rst variant  In particular  the   rst condition  A1  for f1 and f2 remains unchanged  but the second condition  A2  becomes f1   f2   m  Consequently  the optimization of the Gini index value becomes simpler and only needs to consider cases C4 and C5  Example 4  To illustrate how class labels are updated and how constraints are propagated during a node split  consider the following query on the baseball database D  Q5     stint    country  USA  Master  pID Batting   Suppose that the weak IEQ Q0 5 being considered has rel Q0 5     fMaster  Battingg  Let J   Master  pID Batting  shown in Figure 2 a    Since Q5 D    f1  2g  we have J0   ft6  t7g  P1   ft1  t2  t5g  corresponding to stint   2  and P2   ft3  t4g  corresponding to stint   1   The tuples in J0 are labeled negative  while the tuples in P1 and P2 are all free tuples  Suppose that the splitting attribute considered is  weight   and the optimal splitting value for  weight  is 72  The Gini S1  S2  values computed  w r t   weight   72   for the   ve cases  C1 to C5  are 0 29  0 48  0 21  0 4 and 0 4  respectively  Thus  the optimal value of Gini S1  S2  is 0 21  due to case C3   We then split tuples with weight    72  i e   ft3  t4  t6  t7g  into S1 and tuples with weight   72  i e   ft1  t2  t5g  into S2  Thus  P1 is a SP2 set while P2 is a SP1  set  Since the optimal Gini index computed is due to case C3  i e   maximizing negative tuples in S1 and maximizing positive tuples in S2   all the free tuples 13in S2  i e   t1  t2 and t5  are labeled positive  and an exactly one constraint is propagated to the set of tuples P2   S1  i e   ft3  t4g      In summary  TALOS is able to e  ciently compute the optimal Gini index value for each attribute split value considered without enumerating an exponential number of class label assignments for the free tuples  4 Optimizing Performance In this section  we   rst explain how TALOS adapts a well known decision tree classi  er for performing data classi  cation in the presence of free tuples where their class labels are not   xed  We then explain the performance challenges of deriving Q0 when rel Q0   involves multiple relations and present optimization techniques to address these issues  For ease of presentation and without loss of generality  the discussion here assumes weak IEQs  val row A 1 B 2 C 3 D 4 E 5 rM rB rT 1 1 1 2 3 1 2 4 2 3 5 3 4 6 1 5 7 3 rM SrJ 1 f1g 2 f2  3g 3 f4g 4 f5g 5 f6g nid cid sid 1 0 0 1  1 1 1  1 1 1 0 0 1 0 0 1 1 2  a  ALname  b  Jhub  c  MMaster  d  CL Fig  3  Example data structures for Q4 D  4 1 Classifying Data in TALOS We   rst give an overview of SLIQ  14   a well known decision tree classi  er  that we have chosen to adapt for TALOS  We then describe the extensions required by TALOS to handle data class  cation in the presence of free tuples  Finally  we present a non optimized  naive variant of TALOS  It is important to emphasize that our approach is orthogonal to the choice of the decision tree technique  Overview of SLIQ  To optimize the decision tree construction on a set of data records D  SLIQ uses two key data structures  First  a sorted attribute list  denoted by ALi   is pre computed for each attribute Ai in D  Each ALi can be thought of as a two column table  val  row   of the same cardinality as D  that is sorted in non descending order of val  Each record r    v  i  in ALi corresponds to the i th tuple t in D  and v   t Ai   The sorted attribute lists are used to speed up the computation of optimal node splits  To determine the optimal node split w r t  Ai requires a single sequential scan of ALi   Second  a main memory array called class list  denoted by CL  is maintained for D  This is a two column table  nid  cid  with one record per tuple in D  The 14i th entry in CL  denoted by CL i   corresponds to the i th tuple t in D  where CL i  nid is the identi  er of leaf node N  t 2 DN  and CL i  cid refers to the class label of t  CL is used to keep track of the tuples location  i e   in which leaf nodes  as leaf nodes are split  Class List Extension  In order to support data classi  cation with free tuples  where their class labels are assigned dynamically  we need to extend SLIQ with the following modi  cation  The class list table CL nid  cid  sid  is extended with an additional column  sid   which represents a subset identi  er  to indicate which subset  i e   Pi  a tuple belongs to  This additional information is needed to determine the optimal Gini index values as discussed in the previous section  Consider a tuple t which is the i th tuple in D  The cid and sid values in CL are maintained as follows  if t belongs to J0  then CL i  cid   0 and CL i  sid   0  if t is a bound tuple in Pj   then CL i  cid   1 and CL i  sid   j  otherwise  if t is a free tuple in Pj   then CL i  cid     1 and CL i  sid   j  Example 5  Figure 3 shows some data structures created for computing IEQs for Q4 D   Figure 3 a  shows the attribute list created for attribute Master name  and Figure 3 d  shows the initial class list created for Jhub  where all the records are in a single leaf node  with nid value of 1      Naive TALOS  Before presenting the optimizations for TALOS in the next section  let us   rst describe a non optimized  naive variant of TALOS  denoted by TALOS    Suppose that we are considering an IEQ Q0 where rel Q0     fR1             Rng  n   1  that is derived from some schema subgraph G  First  TALOS  joins all the relations in rel Q0    based on the foreign key joins represented in G  to obtain a single relation J  Next  TALOS  computes attribute lists for the attributes in J and a class list for J  TALOS  is now ready to construct a decision tree DT to derive the IEQ Q0 with these structures  DT is initialized with a single leaf node consisting of the records in J  which is then re  ned iteratively by splitting the leaf nodes in DT  TALOS  terminates the splitting of a leaf node when  1  its tuples are either all labeled positive or all labeled negative  or  2  its tuples have the same attribute values w r t  all the splitting attributes  Finally  TALOS  classi  es each leaf node in DT as positive or negative as follows  a leaf node is classi  ed as positive if and only if  1  all its tuples are labeled positive  or  2  the ratio of the number of its positive tuples to the number of its negative tuples is no smaller than a threshold value given by      In our experiments  we set      1  sel Q0   is then derived from the collection of positive leaf nodes in DT as follows  Each set of tuples in a positive leaf node is speci  ed by a selection predicate that is a conjunction of the predicates along the path from the root node to that leaf node  and the set of tuples in a collection of positive leaf nodes is speci  ed by a selection predicate that is a disjunction of the selection predicate for each selected leaf node  In the event that all the leaf nodes in DT are classi  ed as negative  the computation of Q0 is not successful  i e   there is no IEQ for rel Q0    and we refer to Q0 as a pruned IEQ  154 2 Optimizations The naive TALOS described in the previous section su  ers from two drawbacks  First  the overhead of computing J can be high especially if there are many large relations in rel Q0    Second  since the cardinality of J can be much larger than the cardinality of each of the relations in rel Q0    building decision trees directly using J entails the computation and scanning of correspondingly large attribute lists which further increases the computation cost  In the rest of this section  we present the optimization techniques used by TALOS to address the above performance issues  Join Indices   Hub Table  To avoid the overhead of computing J from rel Q0    TALOS exploits pre computed join indices  22  which is a well known tech  nique for optimizing joins  For each pair of relations  R and R0   in the database schema that are related by a foreign key join  its join index  denoted by IR R0   is a set of pairs of row identi  ers referring to a record in each of R and R0 that are related by the foreign key join  Based on the foreign key join relationships represented in the schema sub  graph G  TALOS computes the join of all the appropriate join indices for rel Q0   to derive a relation  called the hub table  denoted by Jhub  Computing Jhub is much more e  cient than computing J since there are fewer number of join op  erations  i e   number of relevant join indices  and each join attribute is a single integer valued column  Example 6  Consider again query Q4 introduced in Example 2  Suppose that we are computing IEQ Q0 4 with rel Q0 4     fMaster  Batting  Teamg  Figure 3 b  shows the hub table  Jhub  produced by joining two join indices  one for Master   pID Batting and the other for Batting   team year Team  Here  rM  rB  and rT refer to the row identi  ers for Master  Batting  and T eam relations  respec  tively     Mapping Tables  Instead of computing and operating on large attribute lists  each with cardinality equal to jJj  as in the naive approach  TALOS operates over the smaller pre computed attribute lists ALi for the base relations in rel Q0   together with small mapping tables to link the pre computed attribute lists to the hub table  In this way  TALOS only needs to pre compute once the attribute lists for all the base relations  thereby avoiding the overhead of computing many large attribute lists for di  erent rel Q0   considered  Each mapping table  denoted by Mi   is created for each Ri 2 rel Q0   that links each record r in Ri to the set of records in Jhub that are related to r  Speci  cally  for each record r in Ri   there is one record in Mi of the form  j  S   where j is the row identi  er of r  and S is a set of row identi  ers representing the set of records in Jhub that are created from r  Example 7  Figure 3 c  shows the mapping table MMaster that links the Master relation in Figure 1 and Jhub in Figure 3 b   The record  2  f2  3g  in MMaster indicates that the second tuple in Master relation  with pID of P2   contributed two tuples  located in the second and third rows  in Jhub     16Computing Class List  We now explain how TALOS can e  ciently compute the class list CL for J  without having explicitly computed J  by using the attribute lists  hub table  and mapping tables  The key task in computing CL is to partition the records in J into subsets  J0  P1  P2  etc    as described in the previous section  For simplicity and without loss of generality  assume that the schema of Q D  has n attributes A1             An  where each Ai is an attribute of relation Ri   TALOS   rst initializes CL with one entry for each record in Jhub with the following default values  nid   1  cid   0  and sid   0  For each record rk that is accessed by a sequential scan of Q D   TALOS examines the value vi of each attribute Ai of rk  For each vi   TALOS   rst retrieves the set of row identi  ers RIvi of records in Ri that have a value of vi for attribute Ri  Ai by performing a binary search on the attribute list for Ri  Ai   With this set of row identi  ers RIvi   TALOS probes the mapping table Mi to retrieve the set of row identi  ers JIvi of the records in Jhub that are related to the records referenced by RIvi   The intersection of the JIvi  s for all the attribute values of rk  denoted by Pk  represents the set of records in J that can generate rk  TALOS updates the entries in CL corresponding to the row identi  ers in Pk as follows   1  the sid value of each entry is set to k  i e   all the entries belong to the same subset corresponding to record rk   and  2  the cid value of each entry is set to 1  i e   tuple is labeled positive  if jPkj   1  otherwise  it is set to   1  i e   it is a free tuple   Example 8  We illustrate how TALOS creates CL for query Q4  which is shown in Figure 3 d   Initially  each row in CL is initialized with sid   0 and cid   0  TALOS then access each record of Q4 D  sequentially  For the   rst record  with name    B    TALOS searches ALname and obtains RIB   f2g  It then probes MMaster with the row identifer in RIB and obtains JIB   f2  3g  Since Q4 D  contains only one attribute  we have P1   f2  3g  The second and the third rows in CL are then updated with sid   1 and cid     1  Similarly  for the second record in Q4 D   with name    E    TALOS searches ALname and obtains RIE   f5g  and derives JIE   f6g and P2   f6g  The sixth row in CL is then updated with sid   2 and cid   1     5 Ranking IEQs In this section  we describe the ranking criteria we adopt to prioritize results presented to the user  Speci  cally  we consider a metric based on the Mini  mum Description Length  MDL  principle  19   and two metrics based on the F measure  23   Minimum Description Length  MDL   The MDL principle argues that all else being equal  the best model is the one that minimizes the sum of the cost of describing the data given the model and the cost of describing the model itself  If M is a model that encodes the data D  then the total cost of the encoding  cost M  D   is de  ned as  cost M  D    cost DjM   cost M   where cost M  is the cost to encode the model  i e   the decision tree in our case  and cost DjM  is the cost to encode the data given the model  We can rely on succinct tree  based representations to compute cost M   The data encoding cost  cost DjM   17is calculated as the sum of classi  cation errors  The details of the encoding computations are given elsewhere  14   F measure  We now present two useful metrics based on the popular F measure  The   rst variant follows the standard de  nition of F measure  the F measure for two IEQs Q and Q0 is de  ned as Fm   2  jpaj 2  jpaj jpbj jpcj   where pa   Q D  Q0  D   pb   Q0  D   Q D   and pc   Q D   Q0  D   We denote this variant as F measure in our experimental study  The details to calculate the   rst variant of F measure are as follows  Recall that after the tree construction step  all tuples have the explicit class labels  The main idea to determine pa  pb and pc is to   rstly   nd out the query output Q0  D  of Q0 and then sort merge join with Q D  to determine pa  pb and pc  To create the query output Q0  D  TALOS needs to scan the attribute lists of attributes in the query  Let us consider the scenarios when Q0  D  contains m attribute A1             Am from m relations R1             Rm respectively 5   First  TALOS creates a m attributes table QP A1             Am   The cardinality of QP is equal to the number of rows in CL that have class label positive  TALOS scans each attribute list ALAi   for each record  val  row   it probes row to   nd a set of position Pi   Then  for each value p 2 Pi   TALOS update QP correspondingly by QP p  Ai   val  After scanning all attribute lists ALAi   we obtain table QP which is the query output Q0  D   The next step is easily done by joining Q0  D  with Q D  to calculate pa  pb and pc respectively  Observe that the   rst variant is useful only for approximate IEQs  and is not able to distinguish among precise IEQs as this metric gives identical values for precise IEQs since pb and pc are empty  To rank precise IEQs  we introduce a second variant  denoted by F est m   which relies on estimating pa  pb  and pc using existing data probabilistic models  as opposed to using the actual values from the data set   F est m captures how the equivalence of queries is a  ected by database updates  and the IEQ with high F est m is preferable to another IEQ with low F est m   For simplicity  we use a simple independent model to estimate F est m   other techniques such as the Bayesian model by Getoor and others  8  can be applied too  The second variant has the bene  t that estimates which are computed from a global distribution model may more accurately re  ect the true relevance of the IEQs than one computed directly from the data  This of course pre supposes that future updates follow the existing data distribution  The detail to compute F est m is as follows  For output query Q0   we maintain two set of rules  S 0 p to contain all rules that are labeled positive and S 0 n to contain all rules that are labeled negative  To create S 0 p and S 0 n   TALOS visits each leaf node N in decision tree T 0 of Q0   For every leaf node N that is labeled positive  the selection condition of attributes along this path from root to this node is converted into a rule rp  then rp is put into S 0 p   In contrast  if N is labeled negative  the rule derived from the root to this node is inserted into S 0 n   TALOS uses the same process to derive the two sets of rule Sp and Sr for input query 5 Relation Ri is not necessarily di  erent from relation Rj  i  6 j   18Q where Sp  resp  Sr  has the same meaning with S 0 p  resp  S 0 r    Our main task is to estimate when a new tuple  or set of tuples  is inserted into the database  whether tuple s  satisfy both Q and Q0   Recall that pa represents the set of tuples that satisfy both Q and Q0   Then  jpaj could be estimated by counting the number of tuples that satisfy one rule in Sp and another rule in S 0 p   In other words  jpaj is estimated by jpaj   X 8ri2Sp  8rj2S0p sel ri   rj     1  where sel ri   rj   denotes the selectivity of a  virtual  query Q   ri   rj   Similarly  pb represents set of tuples that satisfy Q but do not satisfy Q0 whereas pc represents set of tuples that satisfy Q0 but do not satisfy Q  Then  pb and pc are estimated in the similar manner with pa as follows  jpbj   X 8ri2Sp  8rj2S0n sel ri   rj    2  jpcj   X 8ri2Sn  8rj2S0p sel ri   rj    3  We can easily see that the main component in  1   2   3  is the selectivity of virtual queries which are created from the corresponding rules in Sp  S 0 p   Sn and S 0 n   Our task now turns into estimate the selectivity of these queries  This is a well know problem that has been solved by many techniques  In our approach  we have adopted the model with the independent assumption about predicate in the query  More speci  cally  let assume Q   C1              Ck then sel Q    sel C1                 sel Ck  where sel Ci  is the selectivity of each term Ci   6 Experimental Study In this section  we evaluate the performance of the proposed approaches for computing IEQs and study the relevance of the results returned  The algorithms being compared include our proposed TALOS approach  which is based on a dy  namic assignment of class labels for free tuples  and two static class labeling techniques  NI labels all the free tuples as positive  and RD labels a random number of at least one free tuple in each subset as positive  We also examined the e  ectiveness of our proposed optimizations by comparing against a non  optimized naive variant of TALOS  denoted by TALOS   described in Section 4 1  The database system used for the experiments is MySQL Server 5 0 51  and all algorithms are coded using C   and compiled and optimized with gcc  Our experiments are conducted on dual core 2 33GHz machine with 3 25GB RAM  running Linux  The experimental result timings reported are averaged over 5 runs with caching e  ects removed  196 1 Data sets   Queries We use three real data sets  one small size  Adult   one medium size  Baseball  and one large data set  TPC H   All the test queries are given in Table 2  where sf refers to the selectivity factor of a query  Adult  The Adult data set  from the UCI Machine Learning Repository 6   is a single relation data set that has been used in many classi  cation works  We use this data set to illustrate the utility of the IEQs for the simple case when both the input query Q as well as the output IEQ Q0 involve only one relation  The four test queries for this data set are A1  A2  A3 and A4 7   The   rst three queries have di  erent selectivities  low  A1   medium  A2  and very high  A3   Query A4 is used to illustrate how TALOS handles skyline queries  4   In addition  we also run three sets of workload queries with varying selectiv  ities  low  medium  high  shown in Table 3  Each workload set Wi consists of   ve queries denoted by Wi1 to Wi5  The average selectivity factor of the queries in W1  W2  and W3 are  respectively  0 85  0 43  and 0 05  Query sf A1   nc    occ  Armed Force  adult  0 91 A2   edu    ms  Married AF  race  Asian  adult  0 17 A3   edu occ    ms  Never married  64  age  68   race  White  gain 500 sex  F  adult  0 06 A4   id    SKY  LINE gain MAX age MIN  adult  0 06 B1   name   team  ARI  year 2006 HR 10  Master    Batting   0 0004 B2   name   sum HR  600   Master    Batting  0 001 B3   name    SKY  LINE HR MAX SO MIN   Master    Batting   0 002 B4   name year rank    team  CIN  1982 year 1988  Manager    T eam   0 0004 T1   mfgr   brand  Brand 32  part   0 2 T2   name   price 500 000 priority  Urgent   customer    order   0 0004 Table 2  Test queries for experiments Baseball  The baseball data set is a more complex  multi relation database that contains Batting  Pitching  and Fielding statistics for Major League Baseball from 1871 through 2006 created by Sean Lahman  There are 16  639 records of baseball players  88  686 records in Batting  37  598 records in P itching  128  426 records F ielding and other auxiliary relations  AwardsPlayer  Allstar  Team  Managers  etc    The queries used for this data set  B1  B2  B3  B4  are common queries that mainly relate to baseball players  performance  TPC H  To evaluate the scalability of our approach  we use the TPC H data set  with a scaling factor of 1  and two test queries  T1 and T2  6 http   archive ics uci edu ml datasets Adult 7 We use gain  ms  edu  loss  nc  hpw  and rs  respectively  as abbreviations for capital gain  marital status  education  capital loss  native country  hours per week  and relationship  20Query sf W11  ms    19  age  22 edu  Bachelors  adult  0 79 W12  nc    occ  Armed Force  adult  0 91 W13  occ ms    nc  Phillipines  30  age  40 adult  0 83 W14  edu age    wc  Private  race  Asian  adult  0 82 W15  occ edu    gain 9999 adult  0 89 W21  edu    23  age  24 nc  Germany  adult  0 53 W22  age wc edu    hpw  19 race  White  adult  0 64 W23  edu age ms    wc  Private  race  Asian  adult  0 61 W24  edu age    ms  Separated  wc  State gov    race  White  adult  0 2 W25  edu    ms  Married AF  race  Asian  adult  0 17 W31  age    ms  Divorced  wc  State  age 70 adult  0 002 W32  occ edu    ms  NM  64  age  68 race  White    gain 2000 sex  F  adult  0 06 W33  age wc edu    hpw  19 race  White  nc  England  adult  0 01 W34  edu age gain    ms  Married civ  race Asian    30  age  37 adult  0 17 W35  edu gain    gain 5000 nc  Vietnam  adult  0 0008 Table 3  Workload query sets  W1  W2  W3  for Adult Approx Pruned Precise   0   2   4   6   8   10   12   14 NI RD TALOS NI RD TALOS NI RD TALOS Number of IEQs A1 A2 A3 RD NI TALOS   0   10   20   30   40   50   60   70   80 A1 A2 A3 Time  in sec   a   b  RD NI TALOS   0   5 000   10 000   15 000   20 000   25 000 A1 A2 A3 MDL RD NI TALOS   0   0 2   0 4   0 6   0 8   1 A1 A2 A3 F   measure  c   d  Fig  4  Comparison of TALOS  NI and RD   a  Number of IEQs  b  Running time  c  MDL metric  d  F measure metric 21RD NI TALOS   0   50   100   150   200   250   300   350 low   selectivity medium   selectivity high   selectivity Time  in sec  Approx Pruned Precise   0   2   4   6   8   10   12   14 NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS Number of IEQs W11 W12 W13 W14 W15  a   b  Approx Pruned Precise   0   2   4   6   8   10   12   14 NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS Number of IEQs W21 W22 W23 W24 W25 Approx Pruned Precise   0   2   4   6   8   10   12   14 NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS NI RD TALOS Number of IEQs W31 W32 W33 W34 W35  c   d  Fig  5  Comparison of TALOS  NI and RD for workload queries   a  Running time  b  Number of IEQs for W1  c  Number of IEQs for W2  d  Number of IEQs for W3 6 2 Comparing TALOS  NI  and RD In this section  we compare TALOS against the two static class labeling schemes  NI and RD  in terms of their e  ciency as well as the quality of the generated IEQs  Figures 4 a  and  b  compare the performance of the three algorithms in terms of the number of weak IEQs generated and their running times  respec  tively  using the queries A1 to A4  Note that Figure 4 only compares the perfor  mance for weak IEQs because as the Adult data set is a single relation database  all the tuples are necessarily bound when computing strong IEQs  Thus  the performance results for strong IEQs are the same for all algorithms and are therefore omitted  Similarly  the results for query A4 are also omitted from the graphs because it happens that all the tuples are bound for query A4  hence  the performance results are again the same for all three algorithms  The results in Figures 4 a  and  b  clearly show that TALOS outperforms NI and RD in terms of both the total number of  precise and approximate  IEQs computed 8 as well as the running time  In particular  observe that the number of precise IEQs from TALOS is consistently larger than that from NI and RD  This is due to the   exibility of TALOS s dynamic assignment of class labels for free tuples which increase its opportunities to derive precise IEQs  In contrast  8 For clarity  we have also indicated in Figure 4 a  the number of pruned IEQs  de  ned in Section 4 1  computed by each algorithm  Since the number of decision trees considered by all three algorithms are the same  the sum of the number of precise  approximate  and pruned IEQs generated by all the algorithms are the same  22the static class label assignment schemes of NI and RD are too restrictive and are not e  ective for generating precise IEQs  In addition  TALOS is also more e  cient than NI and RD in terms of the running time  The reason for this is due to the   exibility of TALOS s dynamic labeling scheme for free tuples which results in decision trees that are smaller than those constructed by NI and RD  Table 4 compares the decision trees constructed by TALOS  NI  and RD in terms of their average height and average size  i e   number of nodes   Observe that the decision trees constructed by TALOS are signi  cantly more compact than that by NI and RD  Average height Average size Query NI RD TALOS NI RD TALOS A1 14 9 19 8 2 1 5304 9360 4 7 A2 13 4 20 1 3 2 4769 4966 6 9 A3 16 1 21 8 6 5 3224 2970 19 2 Table 4  Comparison of decision trees for NI  RD  and TALOS Figures 4 c  and  d  compare the quality of the IEQs generated by the three algorithms using the MDL and F measure metrics  respectively  The results show that TALOS produces much better quality IEQs than both NI and RD  while the average value of the MDL metric for TALOS is extremely low  under 60   the corresponding values of both NI and RD are in the range of  4000  22000   For the F measure metric  the average value for TALOS is nearly 1  larger than 0 8   whereas the values for NI and RD are only around 0 3 and 0 4  respectively  Figure 5 compares the three algorithms for the three sets of workload queries  W1  W2  and W3  on the Adult data set  As the results in Figure 5 a  show  TALOS again outperforms both NI and RD in terms of running time  For both low and medium selectivity query workload  i e   W1 and W2   the results in Figures 5 b  and  c  show TALOS is able to   nd many more precise IEQs for all queries compared to NI and RD  The reason for this is because such queries have a larger number of free tuples which gives TALOS more   exibility to derive precise IEQs  Figure 5 d  shows the comparison for the high selectivity query workload  i e   W3   As the number of free tuples is smaller for highly selective queries  the   exibility for TALOS becomes reduced  but TALOS still obtains about 1 5 to 9 times larger number of precise IEQs compared to NI and RD  Our comparison results for the Baseball data set  not shown  also demon  strate similar trends with TALOS outperforming NI and RD in both the running time as well as the number and quality of IEQs generated  6 3 E  ectiveness of Optimizations Figure 6 examines the e  ectiveness of the optimizations by comparing the run  ning times of TALOS and TALOS  on both the Baseball and TPC H data sets  Note that the number and quality of IEQs produced by TALOS and TALOS  are 23the same as these qualities are independent of the optimizations  The results show that TALOS is about 2 times faster than TALOS   The reason is that the attribute lists accessed by TALOS  which correspond to the base relations  are much smaller than the attribute lists accessed by TALOS   which are based one J  For example  for query T1  the attribute list constructed by TALOS  for at  tribute  container  in part relation is 4 times larger than that constructed by TALOS  and for query T2  the attribute list constructed by TALOS  for attribute  acctbal  in customer relation is 10 times larger than that constructed by TALOS  In addition  the computation of Jhub by TALOS using join indices is also more e  cient than the computation of J by TALOS   For the queries B1  B2  and B3 on the Baseball data set  the number of IEQs  both precise and approximate  generated by TALOS is in the range  50  80  with an average running time of about 80 seconds  Thus  it takes TALOS about 1 2 seconds to generate one IEQ which is reasonable  Note that the comparison for query B4 is omitted because while TALOS takes only 37 6 seconds to complete  the running time by TALOS  exceeds 15 minutes  For the queries T1 and T2 on the TPC H data set  TALOS takes 34 34 seconds to compute six precise IEQs for T1  and takes 200 seconds to compute one precise and one approximate IEQs  T2 is more costly to evaluate than T1 because the decision trees constructed for T2 are larger and more complex  the average height and size of the decision trees for T2 are  respectively  2 and 4 times  larger than those for T1  Overall  even for the large TPC H data set  the running time for TALOS is still reasonable  TALOS TALOS      0   50   100   150   200   250   300 B1 B2 B3 Time  in sec  Fig  6  Comparison of TALOS and TALOS  6 4 Strong and Weak IEQs In this section  we discuss some of the IEQs generated by TALOS for the various queries  The sample of weak and strong IEQs generated from Adult data set are shown in Tables 5 and 6  respectively  Tables 7 show sample weak IEQs 24generated from Baseball and TPC H data sets  respectively 9   For each IEQ  we also show its value for the F measure or F est m metric  the latter is used only in Table 5 as all the IEQs shown in this table are precise  In Tables 6 and 7  the F measure metric values are shown in terms of their jpaj  jpbj and jpcj values  a IEQ is precise i   jpbj   0 and jpcj   0  We use Xi j to denote an IEQ for a query Xi   X 2 fA  B  Tg  Q IEQ F est m A1 1   gain 7298 ms  Married AF   adult  0 63 A1 2   edu  Preschool  race  Eskimo   adult  0 25 A1 3   loss 3770  adult  0 24 A2 1   loss 3683 edu num 10  adult  0 07 A2 2   age  24 nc  Hungary   adult  0 06 A3 1   p1 p2  adult  0 004 p1    age    85   hpw    1   edu   13  p2    age   85   edu    Master    hpw    40  Table 5  Weak IEQs on Adult Adult  In query A1  we want to know the native country of people whose occu  pation is in the Armed Force  The query result is  U S   From the weak IEQs  we learn that the people who is married to some one in the Armed Force and have high capital gain  A1 1  have the same native country  U S   or people with high capital loss    3770  also have  U S  as their native country  A1 3   In query A2  we want to know the education level of Asians who have a spouse working in the Air Force  The result shows that they all have bachelor degrees  From the weak IEQs  we know that Hungarians who are younger than 25 also have the same education level  A2 2   For the strong IEQs  we have some interesting characterizations  A2 3 shows that these Asians are from Philippines  while A2 4 shows that these Asians are wives whose age are at most 30 and work more than 52 hours per week  Such alternative queries provide more insights about query A2 on the Adult data set  In query A3  we want to   nd the occupation and education of white females who are never married with age in the range  64  68   and have capital gain   500  The query result has 5 records  The strong IEQ A3 2 provides more insights about this group of people  those in the age range  64  66  are highly educated  whereas the others in the age range  67  68  have high capital gains  Query A4 is a skyline query looking for people with maximal capital gain and minimal age  The query result includes four people  Both strong and weak IEQs return the same IEQs for this query  Interestingly  the precise IEQ A4 1 provides a simpli  cation of A4  the people selected by this skyline query are  1  very young  age    17  and have capital gain in the range 1055    27828  or  2  have very high capital gain    27828   work in the protective service  and whose race is classi  ed as  others   9 The weak IEQs shown in Table 7 actually turn out to be strong IEQs as well for the queries B1 to B4  25Q IEQ jpapjj bjjpcj A1 4   p1  adult  1 1 13 p1    48   hpw    50   race  6  Eskimo   Asian    6849   gain    7298   loss    0   edu num   14  A2 3   ms  Married AF  nc  Philippines   adult  1 0 0 A2 4   race  Asian  rs  Wife  hpw 52 age  30  adult  1 0 0 A3 2   p1 p2  adult  5 0 0 p1    63   age    66   edu   15   ms    NM   p2    66   age    68  ms    NM    gain   2993  A4 1   p1 p2  adult  4 0 0 p1    1055   gain    27828   age    17  p2    gain   27828   occ   P   race   O  6 Table 6  Strong IEQs on Adult Baseball  In query B1  we want to   nd all players who belong to team  ARI  in 2006 and have a high performance  HR   10   The result includes 7 players  From the IEQ B1 1  we know more information about these players  performance  G  RBI  etc    and their personal information  e g   birth year   In addition  from IEQ B1 2  we also know that one player in this group got an award when he played in  NL  league  In query B2  we want to   nd the set of high performance players who have very high total home runs    600   There are four players with these charac  teristics  The IEQ B2 1 indicates that some of these players play for  ATL  or  NY1  team  The IEQ B2 2 indicates one player in this group is highly paid and has a left throwing hand  Query B3 is a skyline query that looks for players with maximal number of home runs  HR  and minimal number of strike outs  SO   The result has 35 players  The IEQs provide di  erent characterizations of these players  Query B3 1 indicates that two players in this group are also the managers of  WS2  and  NYA  teams  while query B3 2 indicates that two players in this group are averagely paid  Query B4 is an interesting query that involves multiple core relations  This query asks for the managers of team  CIN  from 1983 to 1988  the year they managed the team as well as the rank that the team gained  There are 3 managers in the result  In this query  we note that TALOS found alternative join paths to link the two core relations  Manager and Team  The   rst alternative join path  shown by B4 1  involves Manager  Master  Batting  and Team  The second alternative join path  not shown  involves Manager  Master  Fielding  and Team  The IEQ B4 1 reveals the interesting observation that there is one manger who is also a player in the same year that he managed the team with some additional information about this manager player  TPC H  Query T1 retrieves the manufacturers who supply products of brand  brand 32   The result includes one manufacturer  Manufacturer 1   The IEQ T1 1 indicates that this manufacturer also supplies some parts at a high price  where their available quantity is in the range  332  1527   The IEQ T1 2 indicates 26Q IEQ jpapjj bjjpcj B1 1   p1 p2  Master    Batting  7 0 0 p1    team    ARI    G    156   70   RBI    79   year   1975  p2    team    ARI    G   156   BB    78  B1 2   lg  NL  year 12 71 height  72 nc  D R  6  Master    AwardsP layer  1 0 6 B2 1   p1 p2  Master    Batting  4 0 0 p1    BB    162   HR   46   team 2 f ATL    NY1 g   RBI    127  p2    BB   162  B2 2   salary 21680700 throws  L   Master    Salaries  1 0 3 B3 1    team  WS2  R  4   team  NYA  state  LA    Master    Manager  2 0 33 B3 2   p1 p2   Master    Salaries  2 0 33 p1    height    78   weight   229   country    DR    180000   salary   195000  p2    height   78   state    GA    salary   302500  B4 1   21 L  22 SB  0 67 W  70  Mananger    Master    Batting    T eam  1 0 5 T1 1   price 2096 99 331 avaiqty  1527  part  1 0 0 T1 2   avaiqty  1 container  SM bag   part    part supp  1 0 0 Table 7  Weak IEQs on Baseball and TPCH that this manufacturer also supplies others products in the container named  SM bag  with an available quantity of at most one  7 Related Work Although the title of our paper is inspired by Zloof  s in  uential work on Query by Example  QBE   27   the problem addressed by QBE  which is on providing a more intuitive form based interface for database querying  is completely di  erent from QBO  There are several recent work  2  3  5  15  that share the same broad principle of  reverse query processing  as QBO but di  er totally in the problem objectives and techniques  Binnig et al   2  3  addressed the problem of generating test databases  given a query Q and a desired result R  generate a database D such that Q D    R  Bruno et al   5  and Mishra et al   15  examined the problem of generating test queries to meet certain cardinality constraints  given a query Q  a database D  and a set of target cardinality constraints on intermediate subexpression in Q s evaluation plan  modify Q to generate a new query Q0 such that the evaluation plan of Q0 on D satis  es the cardinality constraints  An area that is related and complementary to QBO is intensional query an  swering  IQA  or cooperative answering  where for a given Q  the goal of IQA is to augment the query s answer Q D  with additional  intensional  informa  tion in the form of a semantically equivalent query 10 that is derived from the 10 Two queries Q and Q 0 are semantically equivalent if for every valid database D  Q D     Q 0  D   27database integrity constraints  7  16   While semantic equivalence is stronger than instance equivalence and can be computed in a data independent manner using only integrity constraints  ICs   there are several advantages of adopting instance equivalence for QBO  First  in practice  many data semantics are not explicitly captured using ICs in the database for various reasons  9   hence  the e  ective  ness of IQA could be limited for QBO  Second  even when the ICs in the database are complete  it can be very di  cult to derive semantically equivalent queries for complex queries  e g   skyline queries that select dominant objects   By being data speci  c  IEQs can often provide insightful and surprising characterizations of the input query and its result  Third  as IQA requires the input query Q to be known  IQA therefore cannot be applied to QBO applications where only Q D   but not Q  is available  Thus  we view IQA and our proposed data driven approach to compute IEQs as complementary techniques for QBO  More recently  an interesting direction of using Pr  ecis queries  13  20  has been proposed  The idea is to augment a user s query result with other related information  e g   relevant tuples from other relations  and also allow the re  sults to be personalized based on user speci  ed or domain requirements  The objectives of this work is orthogonal to QBO  and as in IQA  it is a query driven approach that requires the input query to be known  In the data mining literature  a somewhat related problem to ours is the problem of redescription mining introduced by Ramakrishnan  18   The goal in redescription mining is to   nd di  erent subsets of data that a  ord multi  ple descriptions across multiple vocabularies covering the same data set  At an abstract level  our work is di  erent from these methods in several ways  First  we are concerned with a   xed subset of the data  the output of the query   Second  none of the approaches for redescription mining account for structural  relational  information in the data  something we explicitly address   Third  redescription mining as it was originally posited requires multiple independent vocabulary descriptions to be identi  ed  We do not have this requirement as we are simply interested in alternative query formulations within an SQL context  Finally  the notion of  at least one  semantics described in our work is some  thing redescription mining is not concerned with as it is an artifact of the SQL context of our work  8 Conclusion In this work  we have introduced a new data driven approach called query by output  QBO  targeted at improving the usability of database management sys  tems  The goal of QBO is to discover instance equivalent queries  Such queries can shed light on hidden relationships within the data  provide useful information on the relational schema as well as potentially summarize the original query  We have developed an e  cient system called TALOS for QBO  and our experi  mental results on several real database workloads of varying complexity highlight the bene  ts of our approach  28Although our discussions focus primarily on QBO where Q is known  to iden  tify core relations   extending to the case without Q is feasible too  it requires an additional preprocessing phase to map each column of Q D  to a set of relation attributes by comparing the column contents  In addition  our approach also could be adapted to handle duplicates in Q D  by using the at least k semantics instead of at least one semantics  As part of our future work  we plan to explore a hybrid approach that includes an o   line phase to mine for soft constraints in the database and an online phase that exploits both the database contents as well as mined constraints  Another interesting direction to be explored is to increase the expressiveness of IEQs  e g   SPJ   union queries   References 1  P  Andritsos  R  J  Miller  and P  Tsaparas  Information theoretic tools for mining database structure from large data sets  In SIGMOD  pages 731 742  2004  2  C  Binnig  D  Kossmann  and E  Lo  Reverse query processing  In ICDE  pages 506 515  2007  3  C  Binnig  D  Kossmann  E  Lo  and M  T  Ozsu  QAGen  generating query aware    test databases  In SIGMOD  pages 341 352  2007  4  S  Borzsonyi  D  Kossmann  and K  Stocker  The skyline operator  In ICDE  pages 421 430  2001  5  N  Bruno  S  Chaudhuri  and D  Thomas  Generating queries with cardinality constraints for dbms testing  IEEE TKDE  18 12  1721 1725  2006  6  P  Buneman  J  Cheney  W  C  Tan  and S  Vansummeren  Curated databases  In PODS  pages 1 12  2008  7  T  Gaasterl  P  Godfrey  and J  Minker  An overview of cooperative answering  Journal of Intelligent Information Systems   2  123 157  1992  8  L  Getoor  B  Taskar  and D  Koller  Selectivity estimation using probabilistic models  In SIGMOD  pages 461 472  2001  9  P  Godfrey  J  Gryz  and C  Zuzarte  Exploiting constraint like data characteriza  tions in query optimization  In SIGMOD  pages 582 592  2001  10  H  V  Jagadish  A  Chapman  A  Elkiss  M  Jayapandian  Y  Li  A  Nandi  and C  Yu  Making database systems usable  In SIGMOD  pages 13 24  2007  11  T  Johnson  A  Marathe  and T  Dasu  Database exploration and bellman  26 3  34 39  2003  12  V  Kabanets and J  yi Cai  Circuit minimization problem  In ACM Symposium on Theory of Computing  STOC   2000  13  G  Koutrika  A  Simitsis  and Y  Ioannidis  Pr  ecis  The essence of a query answer  In ICDE  page 69  2006  14  M  Mehta  R  Agrawal  and J  Rissanen  SLIQ  A fast scalable classi  er for data mining  In EDBT  pages 18 32  1996  15  C  Mishra  N  Koudas  and C  Zuzarte  Generating targeted queries for database testing  In SIGMOD  pages 499 510  2008  16  A  Motro  Intensional answers to database queries  IEEE TKDE  6 3  444 454  1994  17  M  P N  Tan and V Kumar  Introduction to Data Mining  Addison Wesley  2006  2918  N  Ramakrishnan  D  Kumar  B  Mishra  M  Potts  and R  F  Helm  Turning cartwheels  An alternating algorithm for mining redescriptions  In KDD  pages 266 275  2004  19  J  Rissanen  Modeling by shortest data description  Automatica  14 465 471  1978  20  A  Simitsis  G  Koutrika  and Y  E  Ioannidis  Generalized pr  ecis queries for logical database subset creation  In ICDE  pages 1382 1386  2007  21  G  B  Thomas and R  L  Finney  Calculus and Analytic Geometry  Addison Wesley  1991  22  P  Valduriez  Join indices  ACM Trans  Database Syst   12 2  218 246  1987  23  C  J  van Rijsbergen  Information Retireval  Butterworth  1979  24  W  Wu  B  Reinwald  Y  Sismanis  and R  Manjrekar  Discovering topical structures of databases  In SIGMOD  pages 1019 1030  2008  25  X  Xiao and Y  Tao  Output perturbation with query relaxation  Proc  VLDB Endow   1 1  857 869  2008  26  K  Yang  Integer circuit evaluation is pspace complete  In Journal of Computer and System Sciences  2001  27  M  M  Zloof  Query by example  In AFIPS NCC  pages 431 438  1975  A Proving the hardness of the variants of QBO problem In this discussion  we use X  P Y to denote that the problem X could be reduced to the problem Y in polynomial time  For expository simplicity  let us formally recall the de  nition of Minimization Circuit Size Problem  MCSP  and Integer Circuit Evaluation  ICE  before proving our theorems  De  nition 1  MCSP   Given a truth table of a Boolean function fn and a natural number sn  The question is whether fn is computable by a Boolean circuit of size at most sn  MCSP is reduced to QBOS in proving Theorem 1  One truth table example  Tn  of MCSP is given in Figure 7 a   In this truth table  we have three gates corresponding to x1  x2 and x3 variable  The values of function f x1  x2  x3  are given in the fourth attribute  f  values  One possible minimization circuit corresponding to this truth table is f   x1   x3  x1 x2 x3 f 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 A1 A2 A3 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1  a   b  Fig  7  Converting MCSP to QBOS  a  The truth table Tn b   The corresponding relation R in database D 30De  nition 2  ICE   An integer circuit  IC  takes singleton sets  each contain  ing one integer  as input and has three types of set operations as gates  the union gate  A S B  the pair wise multiplication gate  A N B   fa    bja 2 A  b 2 Bg  and the pair wise addition gate  A L B   fa   bja 2 A  b 2 Bg  ICE is given an integer X  a circuit  and its inputs to determine whether or not X is contained in the set output by the circuit  ICE is reduced to QBOG in proving Theorem 3  One integer circuit example is given in Figure 8  The circuit C receives 5 inputs A1 to A5  It calculates U12   A1 S A2  M34   A3 N A4  P345   M34 N A5 and   nally calculates the output M12345   U12 N P345  Given one instance of ICE  C  A1   f1g  A2   f2g  A3   f3g  A4   f5g  A5   f6g  X   21 then ICE returns yes because X belongs to the output of the circuit with that set of inputs  In contrast  if X   21 6   X   42  i e  6 X is not in the output of C  then ICE will return no  1 2 3 5 6 U X   X   1  2     15     21     21  42   U  Union X  times    plus A1 A2 A3 A4 A5 Fig  8  An integer circuit Theorem 1  Given an input query Q  we de  ne QBOS to be the problem to   nd the output query Q0 where Q0 is a conjunctive query that involves only projection and selection  with predicates in the form  Ai op c   Ai is an attribute  c is constant and op 2 f           6        g  such that  1   Q0  D     Q D  and  2  the number of operators  AND  OR and NOT  used in the selection condition is minimized  Then QBOS is unlikely to be in P  Proof  Let us   rst consider the decision form QBODS of QBOS  Informally  QBODS is similar to QBOS except that we require the number of operators in the selection clause of Q0  denoted by s sel Q0     is at most k instead of mini  mizing s sel Q0    where k is a user de  ned parameter  It is easy to reduce from the decision problem  QBODS  to the search form  QBOS   We then reduce the Minimization Circuit Size Problem to the decision problem QBODS as follows  31Given a truth table Tn of a Boolean function fn which includes n binary variables x1             xn  we create a database D which contains only one relation R A1             An   Relation R has n attributes where each attribute Ai corresponds to the variable xi   The variable xi is called the mapping variable of attribute Ai   Similarly  that attribute Ai is called the mapping attribute of variable xi   All of these attributes are binary  i e   the domain contains only two values 0 or 1   Every row in the truth table Tn is inserted into R in exactly the same order  This process is executed in polynomial times  i e  O N  where N is the number of rows in Tn   We then give a query  Q     A1         An    C R    4  The selection condition clause C is given by C   Ci1   Ci2            Cik where each Ci corresponds to the i th row in the truth table with fn   1  Example 9  The truth table Tn given in Figure 7 a  is converted into the corre  sponding relation R in database D as shown in Figure 7 b   The user query in this case is Q     A1 A2 A3    C1 C2  R    where C1   A1  A2 A3  C1   A1 A2 A3  Assume that QBODS   nds other queries Q0 that contain only selection and projection clause such that Q0  D    Q D  and the size of selection clause of Q0 is at most sn  Because D has only one relation R then the query Q0 must involve R  In addition  the projected attributes will also among attributes Ai   The selection condition of Q0 will only consist of AND  OR and NOT operators  according its de  nition   We convert the selection clause of Q0 into a circuit Cn in MCSP as follows  For any term in the form Ai   1  we convert into xi   In contrast  for any term in the form Ai   0  we convert into  xi   For any term in the form Ai   c  c  2 f0  1g  we convert into 0  Any AND  OR  NOT  is converted into the corresponding AND  OR  NOT  gate  It is clearly that every operand  term  in IEQ is converted into exactly one  input  to the circuit  Because the size of the circuit is the number of gates used in it  Then in this case  the size of circuit is equal to the total number of operators used in sel Q0    Therefore  the circuit Cn is exactly the circuit that we need to   nd in MCSP  For example  to create the circuit C from a query Q as shown in Figure 9  we convert  A1   1  into  x1    A3   0  into  x3    A2     1  into  1  and  A3     5  into  0   In addition  if QBODS could not   nd such Q0 that have selection clause size at most sn then  MCSP will also not   nd such a circuit Ci that has the size at most sn  Let us prove this property by contradiction  Let assume QBODS could not   nd any queries Q0 with size sel Q0       sn whereas MCSP   nds a circuit Cc with circuit size at most sn  We transform Cc into a selection condition Sc  Any occurrence of variable xi is transformed into the term Ai   1  Any occurrence of variable  xi is transformed into the term Ai   0  Transform any AND  OR  NOT  gate into the corresponding AND  OR  NOT  operator  We also reduce the circuit with the occurrence of constant 0 and 1 as follows    0   Ci    Ci  or 1   Ci    Ci   We recursively convert circuit Ci into the corresponding selection condition  32x1 x3 1 0 Fig  9  The circuit corresponding the query Q     A1 A2 A3    A1 1   A3 0   A2   1 A3   5  R    0  Ci    0  We transfer into  A1   1   A1   0   It derives that the number of operators in this case  equal to 1  is less than or equal to the number of gates used in 0   Ci     1   Ci    1  We transfer into  A1   1     A1   0   It is clearly that Sc has the size at most sn  A new query Q00 derived from Sc also satis  es the condition of QBODS  Q 00     A1         An    CcR   5  It means that we could   nd one IEQ  Q00   with the size of the selection clause is at most sn  This contradicts to our assumption  In summary  we have MCSP  P QBODS  P QBOS  It has been proven that MCSP is unlikely in P  12   Thus QBOS is also unlikely in P     Theorem 2  Given an input query Q  we de  ne QBOU to be the problem to   nd an output query Q0 of the form Q0   Q1 UNION Q2          UNION Qk where each Qi is in the SPJ form and the select clause refers to only attributes in the schema such that Q0  D    Q D  and k is minimized  Here SPJ form denotes the SQL queries with o</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ud2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09ud2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_understanding_data_and_queries"/>
        <doc>Enabling Privacy in Provenance Aware Work   ow Systems ### Susan B  Davidson  Sanjeev Khanna  Sudeepa Roy  Julia Stoyanovich  Val Tannen University of Pennsylvania  Philadelphia  USA fsusan  sanjeev  sudeepa  jstoy  valg seas upenn edu Yi Chen Tova Milo Arizona State University  Tempe  USA Tel Aviv University  Tel Aviv  Israel yi asu edu milo post tau ac il ### 1  INTRODUCTION A new paradigm for creating and correcting scienti   c analyses is emerging  that of provenance aware work   ow systems  In such systems  repositories of work   ow speci   cations and of provenance graphs that represent their executions will be made available as part of scienti   c information sharing  This will allow users to search and query both work   ow speci   cations and their provenance graphs  Scientists who wish to perform new analyses may search work     ow repositories to    nd speci   cations of interest to reuse or modify  They may also search provenance information to understand the meaning of a work   ow  or to debug a speci     cation  Finding erroneous or suspect data  a user may then ask provenance queries to determine what downstream data might have been a   ected  or to understand how the process failed that led to creating the data  With the increased amount of available provenance information  there is a need to e   ciently search and query scienti   c work   ows and their executions  However  work   ow authors or owners may wish to keep some information in the repository con   dential  For example  intermediate data within an execution may contain sensitive information  such as a social security number  a medical record  or    nancial information about an individual  Although users with the appropriate access level may be allowed to see such con   dential data  making it available to all users  even for scienti   c purposes  is an unacceptable breach of privacy  Beyond data privacy  a module itself may be proprietary  and hiding its description may not be enough  users without the appropriate access level should not be able to infer its behavior if they are allowed to see the inputs and outputs of the module  Finally  details of how certain modules in the work   ow are connected may be proprietary  and so showing how data is passed between modules may reveal too much of the structure of the work   ow  There is thus an inherent tradeo    between the utility of the information provided in response to a search query and the privacy guarantees that This article is published under a Creative Commons Attribution License  http   creativecommons org licenses by 3 0    which permits distribution and reproduction in any medium as well allowing derivative works  provided that you attribute the original work to the author s  and CIDR 2011  authors owners desire  Scienti   c work   ows are gaining wide spread use in life sciences applications  a domain in which privacy concerns are particularly acute  We now illustrate three types of privacy using an example from this domain  Consider a personalized disease susceptibility work   ow in Fig  1  Information such as an individual   s genetic make up and family history of disorders  which this work   ow takes as input  is highly sensitive and should not be revealed to an unauthorized user  placing stringent requirements on data privacy  Further  a work   ow module may compare an individual   s genetic makeup to pro   les of other patients and controls  The manner in which such historical data is aggregated and the comparison is made  is highly sensitive  pointing to the need for module privacy  Finally  the fact that disease susceptibility predictions are generated by    calibrating    an individual   s pro   le against pro   les of others may need to be hidden  requiring that work   ow structure be kept private  As recently noted in  8      You are better o    designing in security and privacy     from the start  rather than trying to add them later     1 We apply this principle by proposing that privacy guarantees should be integrated in the design of the search and query engines that access provenance aware work   ow repositories  Indeed  the alternative would be to create multiple repositories corresponding to di   erent levels of access  which would lead to inconsistencies  ine   ciency  and a lack of    exibility  a   ecting the desired techniques  This paper focuses on privacy preserving management of provenance aware work   ow systems  We consider the formalization of privacy concerns  as well as query processing in this context  Speci   cally  we address issues associated with keyword based search as well as with querying such repositories for structural patterns  To give some background on provenance aware work   ow systems  we    rst describe the common model for work   ow speci   cations and their executions  Sec  2   We then enumerate privacy concerns  Sec  3   consider their e   ect on query processing  and discuss the challenges  Sec  4   2  MODEL Work   ow speci   cations are typically represented by graphs  with nodes denoting modules and edges indicating data   ow between modules  Work   ow speci   cations may be hierarchical  in the sense that a module may be composite and itself 1While the context for this statement was the use of full body scanning in airports  where the privacy issues are obvious   it is equally valid in provenance systems  1I O Determine Genetic Susceptibility M1 M2 Evaluate Disorder Risk Consult  External Databases  M3 Expand SNP Set M4 Generate Database Queries Query OMIM Combine Disorder Sets M5 M6 M8       W1 W2 W4    Query PubMed M7 SNPs  ethnicity lifestyle  family  history  physical  symptoms disorders prognosis SNPs disorders disorders query query Reformat M9 Generate  Queries Search PubMed Central Search Private Datasets Update  Private Datasets Summarize Articles W3 Combine notes and summary M11 M10 M12 M13 M14 M15 query query notes summary notes result result result Figure 1  Disease Susceptibility Work   ow Speci   cation I  O  M1  S1  M2  S8  d2  d3 d4  d19  d0 d1  d10  Figure 2  View of Provenance Graph W1 W3 W2 W4 Figure 3  Expansion Hierarchy contain a work   ow  Composite modules are frequently used to simplify work   ow design and allow component reuse  For example  the work   ow in Fig  1 estimates disease susceptibility based on genome wide SNP array data  The input to the work   ow  whose top most level is given by the dotted box labeled W1  is a set of SNPs  ethnicity information  lifestyle  family history  and physical symptoms  The    rst module in W1  M1  determines a set of disorders the patient is genetically susceptible to based on the input SNPs and ethnicity information  The second module  M2  re   nes the set of disorders for which the patient is at risk  based on lifestyle  family history  and physical symptoms  Fig  1 also contains    labeled edges that give the de   nitions of composite modules  which we call expansions  For example  M1 is de   ned by the work   ow W2  M2 by the work   ow W3  and M4 by the work   ow W4  Hence W2 and W4 are subwork   ows of W1  and W3 is a subwork   ow of W2  The   expansions  subwork   ow relationships  naturally yield an expansion hierarchy as shown in Fig  3  Pre   xes of the expansion hierarchy can be used to de   ne views of a work   ow speci   cation  2 Given a pre   x  the view that it de   nes is given by expanding the root work   ow so that composite modules in the pre   x are replaced by their expansions  For example  consider the expansion hierarchy in Fig  3 and its pre   x consisting of fW1  W2g  This pre   x determines a view of the speci   cation in Fig  1  which is the 2 Recall that a pre   x of a rooted tree T is a tree obtained from T by deleting some of its subtrees  i e   some nodes and all their descendants   I O S1 M1 begin S3 M4 begin S4 M5 S5 M6 S2 M3 S6 M7 S7 M8 S1 M1 end S8 M2 begin S9 M9 S10 M12 S8 M2 end S3 M4 end d0 d1 d0 d1 d5 d5 d7 d6 d8 d10 d10 d10 d2 d3 d4 d10 d2 d3 d4 d9 S11 M13 S12 M14 S13 M10 S14 M11 S15 M15 d11 d12 d13 d15 d17 d18 d19 d14 d16 d19 Figure 4  Disease Susceptibility Work ow Execution I O M2 Evaluate Disorder Risk M3 Expand SNP Set Generate Database Queries Query</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10cc1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10cc1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Cloud_computing_and_internet_scale_computing"/>
        <doc>Data Consistency Properties and the Trade offs in Commercial Cloud Storages  the Consumers    Perspective ### Hiroshi Wada   y   Alan Fekete z   Liang Zhao y     Kevin Lee   and Anna Liu   y   National ICT Australia     NICTA y School of Computer Science and Engineering  University of New South Wales z School of Information Technologies  University of Sydney       rstname lastname  nicta com au z     rstname lastname  sydney edu au ABSTRACT A new class of data storage systems  called NoSQL  Not Only SQL   have emerged to complement traditional database systems  with rejection of general ACID transactions as one common feature  Di   erent platforms  and indeed di   erent primitives within one NoSQL platform  can o   er various consistency properties  from Eventual Consistency to single entity ACID  For the platform provider  weaker consistency should allow better availability  lower latency  and other bene   ts  This paper investigates what consumers observe of the consistency and performance properties of various o   erings  We    nd that many platforms seem in practice to o   er more consistency than they promise  we also    nd cases where the platform o   ers consumers a choice between stronger and weaker consistency  but there is no observed bene   t from accepting weaker consistency properties ###  1  INTRODUCTION Cloud computing is attracting interest through the potential for low cost  unlimited scalability  and elasticity of cost with load  7  8  11  33   A wide variety of o   erings are typically categorized as Infrastructure as a Service  IaaS   Platform as a Service  PaaS   and Software as a Service  SaaS   IaaS is exempli   ed by Amazon Web Services  AWS   and provides the capability to execute existing programs on a virtual machine that is essentially the same as a standard box with a standard operating system  The consumer has control over the virtual resources  Each PaaS system o   ers a distinctive set of functionalities as an API  that allow programs to be written specially to execute in the cloud  Google AppEngine  GAE  is an example of this approach  In PaaS systems  a persistent and scalable storage platform is a crucial facility  In an IaaS environment  one could simply install an existing database engine such as MySQL in one   s virtual machine instance  but the limitations  performance  scale  and fault tolerance  of this approach are wellknown  and the traditional database systems can become This article is published under a Creative Commons Attribution License  http   creativecommons org licenses by 3 0    which permits distribution and reproduction in any medium as well allowing derivative works  provided that you attribute the original work to the author s  and CIDR 2011  CIDR   11 Asilomar  California  January 2011 a bottleneck in a cloud platform  2  27   thus novel storage platforms are commonly o   ered within IaaS clouds too  These storage platforms operate within the cloud platform  and take advantage of the scale out from huge numbers of cheap machines  they also internally have mechanisms to tolerate the faults that are inevitable with so many unreliable machines  Examples include Amazon SimpleDB 1   Microsoft Azure Table Storage 2   Google App Engine datastore 3   and Cassandra 4   A term often applied to these storage platforms is NoSQL  Not Only SQL   NoSQL database systems are designed to achieve high throughput and high availability by giving up some functionalities that traditional database systems o   er such as joins and ACID transactions  NoSQL data stores may o   er weaker consistency properties  for example eventual consistency  32   A client of such a store may observe values that are stale  not from the most recent write  This design feature is explained by the CAP theorem  which states that a partition tolerant distributed system can guarantee only one of the following two properties  data consistency  or availability  17   Many of NoSQL database systems aim for availability and partition tolerance as their primary focus and thus they relax the data consistency constraints  It is a new challenge for developers to write applications that use storage o   ering weak consistency  For example  recent work by Hellerstein  19  has identi   ed a class of monotonic programs that give correct results on eventual consistent data  The application designer therefore tries to express their computational task using only monotonic operations  Further complicating the programmer   s task  there are variant consistency properties that may or may not be provided  such as read your writes  monotonic reads  or session consistency  each changing the set of possible situations  and thus what the code must be written to handle  The e   ort of coding for a weak consistency model is typically justi     ed by pointing to corresponding tradeo   s  such as better availability  lower latency  etc  16   We have experimentally investigated these issues  from the view of the consumer of the storage facilities  That is  we try to see what kinds of inconsistency are seen in the results returned from operations  and how frequently these situations arise  This contrasts with research on cloud based 1 aws amazon com simpledb  2 www microsoft com windowsazure  3 code google com appengine  4 cassandra apache org  134storage platforms  9 10 12  that takes the view of the platform owner and focuses on algorithms and the properties of the data held in various replicas within the platform  Our main contributions are detailed measurements over several storage platforms  that show how frequently  and in what circumstances  di   erent inconsistency situations are observed  and what impact the consumer sees on performance properties from choosing to operate with weak consistency mechanisms  The overall methodology of our experiments  for measuring consistency as seen by a consumer  is another contribution  In Section 2 we report on the experiments that investigate how often a read sees a stale value  For several platforms  data is always  or nearly always  upto date  For one platform  SimpleDB   we often see stale data  and so in Section 3 we investigate more deeply the consistency properties of this platform  covering issues such as consistency among multiple data elements  and cases where operations on one element impact on reads of another element  Section 4 then explores the performance of di   erent consistency options  in particular  we investigate whether the consumer is o   ered any tradeo    in cost or performance  to compensate for using weak consistency operations  Section 5 discusses some limitations to generalising our results  In Section 6 we connect and contrast our work with other research related to this topic  Section 7 gives some conclusions and suggests directions for further study  2  STALENESS OF DATA We    rst investigate the probability of a consumer observing stale data in an item  Figure 1 illustrates the architecture of the benchmark applications in this study  There are three cloud deployed roles  the data store  and two computations  writer and reader  A writer repeatedly writes 14bytes of string data into a particular data element  the value written is the current time  so that we can easily check which write is observed in a read  In most of the experiments we report  writing happens once every three seconds  A reader role repeatedly reads the contents from the data element and also notes the time at which the read occurs  in most experiments reading happens 50 times every second  In some of our experiments  we use one thread for the writer role and one or multiple threads each implementing the reader role  in other experiments we have a single thread that takes both roles  We refer to one    measurement    of the experiment as running the writing and reading for 5 minutes  doing 100 writes and 15 000 reads  We repeated the measurement once every hour  for at least one week  in October and November 2010  In a post processing data analysis phase  each read is determined to be either fresh  if the value observed has a timestamp from the closest preceding write operation  based on the times of occurrence  or stale  also each read is placed in a bucket based on how much clock time has elapsed since the most recent write operation  By examining all the reads within a bucket  from a single measurement run  or indeed aggregating over many runs  we calculate the probability that a read which occurs a given time after the write  will observe the freshest value  Repeating the experiment through a week ensures that we will notice any daily or weekly variation in behavior  2 1 Amazon SimpleDB SimpleDB is a distributed key value store o   ered by AmaWriter Reader Prepare and  send a req  Receive and  parse a resp  Read data  from a DB Write data  into a DB NoSQL time t time t x Deployed in a process  within a data center  or across data centers Figure 1  The Architecture of Benchmark Apps zon  Each key has associated a collection of attributes  each with a value  For these experiments  we take a data element to be a particular attribute kept for a particular key  a key identi   es what SimpleDB calls an item   SimpleDB supports  among other calls  a write operation  PutAttributes  and two types of read operations  distinguished by a parameter in the call to GetAttributes  eventual consistent read and consistent read  The consistent read is supposed to ensure that the value returned always comes from the most recently completed write operation  while an eventually consistent read does not give this guarantee  Our study investigates how these di   erences appear to the consumer of data  SimpleDB is currently operated in four geographic regions independently  i e   US West  US East  Ireland and Singapore  and each of them o   ers a distinct URL as its access point  For example  https   sdb us west 1 amazonaws  com is the URL of SimpleDB operated in US West  We used this as the data store for our experiments  Writer and reader are implemented in Java and run in EC2  they access SimpleDB in US West through its REST interface  2 1 1 Accessing from a Single Thread In the    rst study  we run the writer and reader in the same single thread on an m1 small instance provided by EC2 with Ubuntu 9 10  The writer reader process is deployed in US West  We cannot be sure that the SimpleDB data store will be in the same physical data center as the computation  5   but using the same geographic region is the consumer   s best mechanism to reduce network latency  We executed a measurement run once every hour for 11 days from Oct 21  2010  In total 26 500 writes and 3 975 000 reads were performed  Since we use only one thread in this study  the average throughput of reading and writing are 39 52 per second and 0 26 per second  respectively   Each measurement runs at least    ve minutes   The same set of measurements was performed with eventual consistent read and with consistent read  2 1 1 1 Read Your Write Consistency  Figure 2 and Table 1 show the probability of reading the fresh value plotted against the time interval that elapsed from when the write begins  to the time when the read is submitted  Each data point in the graph is an aggregate over all the measurements for a particular bucket containing all time intervals that agree to millisecond granularity  in the Table we aggregate further  placing all buckets whose time is in a broad interval together  and here we also show actual numbers as well as percentages  With eventual consistent read the probability stays about 33  from 0ms to 450ms  It 135jumps up sharply between 450ms and 500ms  and it reaches 98  at 507ms  A spike and a valley in the    rst 10ms are perhaps random    uctuation due to a small number of data points  With consistent read  the probability is 100  from about 0ms onwards  0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 0  0 200 400 600 800 1000 Prob  to read freshest values Time elapsed from completing write until starting read  ms  Consistent Read Eventual Consistent Read Figure 2  Probability of Reading Freshest Value Table 1  Probability of Reading Freshest Value Time Elapsed from Starting Write Until Starting Read Eventual Consistent Read Consistent Read  0  450  33 40   168 908 505 821  100 00   482 717 482 717   500  1000  99 78   1 192 541 062  100 00   509 426 509 426  A relevant consistency property is    read your writes     which says that when the most recent write is from the same thread as the reader  then the value seen should be fresh  As we    nd that stale eventual consistent reads are possible with SimpleDB within a single thread  so we conclude that eventual consistent reads do not satisfy read your writes  however  consistent reads of course do have this property  We now consider the variability of the time when freshness is possible or highly likely  among di   erent measurement runs  For eventual consistent reads  Figure 3 shows the    rst time when a bucket has freshness probability that is over 99   and the last time when the probability is less than 100   Each data point is obtained from a    ve minutes measurement run  so there are 258 data points in each timeseries  The median of the time to exceed 99  is 516 17ms and coe   cient of variance is 0 0258  There does not seem to be any regular daily or weekly variation  rather the outliers seem randomly placed  Out of the 258 measurement runs  2 runs  0 78   and 21 runs  8 14   show a non zero probability of stale read after 4000ms and 1000ms  respectively  Those outliers are considered to be generated by network jitter and similar e   ects  2 1 1 2 Monotonic Read Consistency  One consistency property that has been considered important  32  is    monotonic read     where a following operation sees data that is at least as fresh as what was seen before  This property can be examined across multiple data elements  or for a single element as we consider here  We    nd that consistent reads are monotonic as they should be  since each read should always see the most recent value  However  eventual consistent reads are not monotonic  and indeed the 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 Thu  Oct 21 Fri  Oct 22 Sat  Oct 23 Sun  Oct 24 Mon  Oct 25 Tue  Oct 26 Wed  Oct 27 Thu  Oct 28 Fri  Oct 29 Sat  Oct 30 Sun  Oct 31 Time elapsed from starting write  ms  First Time P    99  Last Time P   100  Figure 3  Time to See Freshness  Eventual Consis  tent Read  freshness of a successive operation seems essentially independent of what was seen before  Thus eventual consistent reads also do not have stronger properties like causal consistency  Table 2 shows the probability of observing fresh or stale values in each pair of successive eventual consistent reads performed during the range from 0ms to 450ms after the time of a write  The table also shows the actual number of observations out of 475 575 of two subsequent reads performed in this measurement study  The monotonic read condition is violated  that is  the    rst read returns a fresh value but the second read returns a stale value  in 23 36  of pairs  This is reasonably close to what one would expect of independent operations  since the probability of seeing a fresh value in the    rst read is about 33  and the probability of seeing a stale value in the second read is about 67   The Pearson correlation between the outcomes of two successive reads is 0 0281  which is very low  and we conclude that eventual consistent reads are independent from each other  Table 2  Successive Eventual Consistent Reads h First hh Read hhhhhh Second Read Stale Fresh Stale 39 94   189 926  21 08   100 1949  Fresh 23 36   111 118  15 63   74 337  2 1 2 Accessing from Multi Threads and Processes In the previous results  all read and write requests originate from the same thread  We did measurements for four other con   gurations  1  A writer and a reader run in di   erent threads in the same process  2  A writer and a reader run in di   erent processes on the same virtual machine in the same geographic domain as the data storage  US West   3  A writer and a reader run on di   erent virtual machines in US West  or 4  A writer and a reader run on di   erent virtual machines  one in US West and one in Ireland  In the    rst two cases  read and write requests are originated from the same IP address  In the third case  requests are originated from di   erent IP addresses but from the same 136geographical region  In the last case  requests are originated from di   erent IP addresses in di   erent regions  Each experiment was run for 11 days as well  In all four cases the probability of reading updated values shows a similar distribution as in Figure 2  We conclude that consumers of SimpleDB see the same data consistency model regardless of where and how clients are placed  2 2 Amazon S3 A similar measurement study was conducted on Amazon S3 for 11 days  In S3  storage consists of objects within buckets  so our writer updates an object in a bucket with the current timestamp as its new value  and each reader reads the object  In this experiment  we did measurements for the same    ve con   gurations as SimpleDB   s case  i e   a write and a reader run in a single thread  di   erent threads  di   erent processes  di   erent VMs or di   erent regions  Amazon S3 two types of write operations  standard and reduced redundancy  A standard write operation stores an object so that its probability of durability is at least 99 999999999   while a reduced redundancy write aims at giving at least 99 99  probability of durability  The same set of measurements was performed with standard write and reduced redundancy write  Documentation states that Amazon S3 buckets provide eventual consistency for overwrite put operations  4   however  no stale data was ever observed in our study regardless of write redundancy options  It seems that staleness and inconsistency might be visible to a consumer of Amazon S3 only in executions such that there is a failure in the particular nodes of platform where the data is stored  during the time of their access  this seems a very low probability event  2 3 Azure Table and Blob Storage The experiment was also conducted on Windows Azure table and blob storages for eight days  Since it is not possible to start more than one process on a single VM  Web Role in this experiment   we did measurements for four con   gurations  a write and a reader run in a single thread  di   erent threads  di   erent VMs or di   erent regions  On Azure table storage a writer updates a property of a table and a reader reads the same property  On Windows blob storage a write updates a blob and a reader reads it  The measurement study observed no stale data at all  It is known that all types of Windows Azure storages support strong data consistency  24  and our study con   rms it  2 4 Google App Engine Datastore Similar to SimpleDB  Google App Engine  GAE  datastore keeps key accessed entities with properties  and it offers two options for reading  strong consistent read and eventual consistent read  However  the behavior we observed for eventual consistent read in GAE datastore is completely di   erent from that of SimpleDB  It is known that the eventual consistent read of GAE datastore reads from a secondary replica only when a primary replica is unavailable  18   Therefore  it is expected that consumer programmers see consistent data in most reads  regardless of the consistency option they choose  We ran our benchmark application coded in Java and deployed in GAE  In GAE applications are not allowed to create threads  a thread automatically starts upon an HTTP request and it can run no more than 30 seconds  Therefore  each measurement on GAE runs for 27 seconds and we run measurements every 10 minutes for 12 days  The same set of measurements was performed with strong consistent read and eventual consistent read  Also  GAE o   ers no option to control the geographical location of applications  Therefore  we did measurements for two con   gurations  a writer and a reader run in the same application  i e   thread   or a writer and a reader run in di   erent applications  Each measurement consists of 9 4 writes and 2787 9 reads on average  and in total 3 727 798 reads and 12 791 writes happened on average for each con   guration  With strong consistent read no stale value was observed  With eventual consistent read and both roles in the same application  no stale value was observed  However 11 out of 3 311 081 readings  3 3E 4   observed stale values when a writer and an eventual consistent reader are run in di   erent applications  We cannot conclude for certain whether stale values might sometimes be observed when a writer and a reader run in the same application  however  it suggests the possibility that GAE o   ers read your writes eventual consistency  In any case  consistency errors are very rare  3  CONSISTENCY MODEL OF SIMPLEDB We do not have a public description of the implementation approach used by SimpleDB  However our data from Section 2 shows that eventual consistent read of SimpleDB does not support monotonic reads or read your writes  This section investigates in more detail  what the consumer can determine about the data consistency model of SimpleDB eventual consistent read  Section 3 1 investigate the support of monotonic write consistency and Section 3 2 investigates the consistency among multiple data elements  This section discusses the observations obtained in various studies  we speculate in Section 4 3 on mechanisms that might lead to these observations  3 1 Monotonic Write Consistency Vogels  32  has advocated the importance of the    monotonic write    property  because programming is notoriously hard if this is missing  The monotonic write property guarantees to serialize the writes by one process  It is not clear whether a consumer can test this  through looking at the values received in reads  To gain some insight  we ran a similar benchmark to the one which is described in Section 2  except it has only one thread which performs 100 repetitions of a small cycle  each of which updates a data element twice in a row and then reads the element repeatedly for three seconds  each read is placed in a bucket depending on the time interval from the start of the cycle till the read is submitted  We ran a measurement once every hour for nine days  and aggregated all the buckets from a given time after the cycle starts  We refer to the value in the element before the cycle starts as v0  the value placed there in the    rst write as v1 and then v2 is written immediately afterwards  Figure 4 shows the probability of reading v0  v1 or v2 against the time from the start of the cycle  The total probability of reading v0  v1 or v2 at times between 50ms to 400ms are 0 097   66 339  and 33 564   respectively  It appears that the second write is enough to ensure that no replica any longer contains the value from before the cycle started  even though we are still well below the period of 500ms from the    rst write  which our earlier data showed was the time till that write would be 137 visible in all replicas  We say that the second write       ushes    the    rst write to consistency  When we modify the experiment to write three di   erent values v1  v2 and v3 consecutively  the probability of reading v0  v1  v2 or v3 is 0 051   0 028   66 650  and 33 272   respectively  Again  each write except the last seems to have been    ushed  and this measurement suggests that each replica almost always contains a value that is no more than one write behind the latest  0 0 0 2 0 4 0 6 0 8 1 0  0 100 200 300 400 500 600 700 Probability Time invoking read op  after an invocation of the first write op   ms  Reading a previous value  v0  Reading first write  v1  Reading second write  v2  Figure 4  Consecutive Writes to One Item Another study shows even more complexity in the consistency model that the consumer sees for SimpleDB  because    ushing behavior varies depending on the content being written  This is just like the previous experiment  except that the same value is written in both writes of a cycle  that is v1   v2  Figure 5 shows the probability of reading v0 or v1 over time  The total probability of reading v0 or v1  through the period between 50ms to 400ms after the write  is 66 526  and 33 474   respectively  Note that this is quite di   erent from what one would see if one just combined the curves for v1 and v2 in Figure 4  In particular  when v1   v2  after the second write some replicas clearly continue to hold the value from before the cycle  which is not so when v1     v2  This suggests that the second write is ignored  and does not cause a    ush of previous writes  if v1   v2  0 0 0 2 0 4 0 6 0 8 1 0  0 100 200 300 400 500 600 700 Probability Time invoking read op  after an invocation of the first write op   ms  Reading a previous value  v0  Reading a new value  v1  Figure 5  Writing the Same Value Twice The SimpleDB data model is comprised of domains  items  attributes and values  5   A domain is a set of items  An item is a set of attribute value pairs  The write operation of SimpleDB can update multiple attribute value pairs in an item at once  We use this fact to explore more closely the cases when a second write    ushes the value in an earlier write  Our experiment is just like the previous one  except that the    rst write puts v1 in two attributes  A1 and A2  at once  and the second write puts v2 in only A1  As in the previous experiments  one experiment writes di   erent values  v1     v2  in the    rst and second writes  The other experiment writes the same value  v1   v2  in both writes  Figure 6 shows the probability of reading v1 or v2 when v1     v2  The probability of reading a previous value  v0  is not zero  however  they are very small  less than 0 1   and not shown in this    gure  A1 shows similar probability to the one shown in Figure 4  However  A2 shows the complexity in the consistency model  Although A2 is updated only once  by the    rst write  the probability of reading v1 is very close to 100   It indicates that the second write  i e   writing v2 on A1  a   ects the data consistency of A2     ushing the earlier write  despite the fact that it does nothing in A2  0 0 0 2 0 4 0 6 0 8 1 0  0 100 200 300 400 500 600 700 Probability Time invoking read op  after an invocation of the first write op   ms  Reading first write  v1  from A1 Reading second write  v2  from A1 Reading first write  v1  from A2 Figure 6  Writing in Two Attributes then in One When v1   v2  the probability of reading v1 from A1 and that of reading v1 from A2 draw similar curves as the one in Figure 5  This suggests that the second write is ignored as it is redundant  delivering a subset of the information in the    rst write  3 2 Inter Element Consistency NoSQL database systems usually provide limited support of transactions  in particular  there is typically avoidance of two phase commit  and so the platform will typically not allow transactional update to elements that might be stored on di   erent physical nodes  We explore this by having a writer thread modify two data elements  placing the current timestamp in each   and each reader examines those two locations  The SimpleDB data model is comprised of domains  items  attributes and values  We explore the e   ect of how closely the elements are related in the data model  they might be two attributes within one item  or attributes that are in di   erent items within the same domain  or they might be in di   erent items in di   erent domains  SimpleDB provides several operations to write and read values from elements  we also explore the e   ect of using various combinations of these to do writing and reading      PutAttributes updates attribute value pairs in a certain item      BatchPutAttributes performs multiple PutAttribute operations in a single call      GetAttributes returns all attribute value pairs in a certain item      Select returns a set of attribute value pairs in a certain domain that match a query statment  Table 3 shows the probability observed under di   erent ways to write read the values in two attribute value pairs X 138Table 3  Write and Read Two Elements in SimpleDB two GetAttributes one GetAttributes one Select A     B     C     D     A     B     C     D     A     B     C     D     Values in same item two PutAttributes 34 459 65 142 0 138 0 262 33 593 66 246 0 000 0 161 33 559 63 808 0 000 2 633 one PutAttributes 12 050 21 820 22 554 43 576 33 859 0 000 0 000 66 141 33 756 0 000 0 000 66 244 one BatchPutAttributes 11 789 21 964 22 507 43 740 33 649 0 000 0 000 66 351 33 610 0 000 0 000 66 390 Values in di    items in a domain two PutAttributes 34 443 65 138 0 149 0 271 N A N A N A N A 32 908 66 832 0 000 0 260 one BatchPutAttributes 11 872 21 918 22 471 43 738 N A N A N A N A 33 763 0 000 0 000 66 237 Values in di    domains two PutAttributes 14 097 24 882 20 740 40 282 N A N A N A N A N A N A N A N A two BatchPutAttributes 14 187 24 924 20 740 40 149 N A N A N A N A N A N A N A N A and Y  N A means this combination is infeasible  for example  it is impossible to read two values from di   erent domains by one GetAttributes call  For each approach we show 4 probabilities  A for when the values of X and Y are both fresh  B when X is fresh but the value of Y is stale  C for the case where the value of X is stale but the value of Y is fresh  and    nally D when both X and Y are stale  We run a measurement once every hour for seven days and obtained the total probability for reads that occur from 0ms to 450ms after the time of a write  There are three distinct patterns observed  The    rst pattern is that the probabilities of A  B  C and D are about 12   22   22  and 44   respectively  The second pattern is that the probabilities are about 34   0   0  and 66   The third pattern is that the probabilities are about 34   66   0  and 0   The    rst pattern is what one would expect given independence between the items  based on the 33  probability for a single read seeing a fresh value  For example  when updating two attribute value pairs with a single call of PutAttributes or BatchPutAttributes and then reading them with two consecutive calls of GetAttributes  the probability of reading fresh X and Y is about 12   close to 33     33    The second pattern shows interaction between the items  both X and Y are stale or both are fresh  For example  it is observed when updates are done with one PutAttributes  and reading with a single call of GetAttributes or Select  The third pattern holds when two operations are used to do the writing  no matter how the reading is done  we see here that the    rst item has almost 100  chance of freshness  and the second has about 34  chance of freshness  This is the same phenomenon observed in Figure 6 with data no more than one write behind the current value  4  TRADE OFF ANALYSIS OF SIMPLEDB The previous sections show the behavior of SimpleDB for di   erent read options  For the platform provider  there are added costs for stronger consistency options  less availability  higher latency   1   We wish to see however what the consumer experiences  as this is what will guide the users of SimpleDB make a well informed decision about which consistency option to ask for when reading  We used the benchmark architecture described in Section 2  The measurement ran between 1 and 25 virtual machines in US West to write and read one attribute  which is a 14bytes string data  from an item in SimpleDB  Each virtual machine runs 100 threads  i e   emulated clients  each of which executes one read or write request every second in a synchronous manner  Thus  if all requests    response time is below 1 000ms  the throughput of SimpleDB can be reported as 100  of the potential load  Three di   erent read write ratios were studied  99  read and 1  write  75  read and 25  write  and 50  read and 50  write cases  We run a measurement  which runs for    ve minutes with a set number of virtual machines  once every hour for one day  4 1 Response Time and Throughput The bene   t of eventual consistent read in SimpleDB is explained as follows  5   The eventually consistent read option maximizes your read performance  in terms of low latency and high throughput   Since a consistent read can potentially incur higher latency and lower read throughput it is best to use it only when an application scenario mandates that a read operation absolutely needs to read all writes that received a successful response prior to that read  To test this advice  we investigated the di   erence in response time  throughput and availability of the two options  as o   ered load increased  Figure 7 shows the average  95 percentile and 99 9 percentile response time of eventual consistent reads and consistent reads at various levels of load  The result is obtained from the case of 99  read ratio and all failed requests are excluded  The result shows no visible di   erence in average response time  however  consistent read slightly outperforms eventual consistent read in 95 percentile and 99 9 percentile response time  0 0 2 0 4 0 6 0 8 0 10 0 12 0 14 0  0 500 1000 1500 2000 2500 Read RTT  sec  Number of Emulated Clients  Threads  Average 95 0 percentile 99 9 percentile Consistent Read Eventual Consistent Read Figure 7  Response Time of Read on SimpleDB Figure 8 and 9 show the average response time of reads and writes at various read write ratios  plotted against the number of emulated clients  We conclude that changing the level of update intensity does not have a marked impact  Figure 10 shows the absolute throughput  the average number of processed requests per second  We also place whiskers surrounding each average with the corresponding 1390 0 0 2 0 4 0 6 0 8 1 0 1 2 1 4 1 6 1 8 2 0  0 500 1000 1500 2000 2500 Average Read RTT  sec  Number of Emulated Clients  Threads  Consistent   99  read Consistent   75  read Consistent   50  read Eventual Consistent   99  read Eventual Consistent   75  read Eventual Consistent   50  read Figure 8  Response Time of Read on SimpleDB 0 0 0 5 1 0 1 5 2 0 2 5 3 0  0 500 1000 1500 2000 2500 Average Write RTT  sec  Number of Emulated Clients  Threads  Consistent   99  read Consistent   75  read Consistent   50  read Eventual Consistent   99  read Eventual Consistent   75  read Eventual Consistent   50  read Figure 9  Response Time of Write on SimpleDB minimum and maximum throughput  Similar to what we saw for response time  the result that consistent read slightly outperforms eventual consistent read  though the di   erence is not signi   cant  Figure 11 shows the throughput as a percentage of what is possible with this number of clients  As the response time increased  each client sent less than one request every second and  therefore  the throughput percentage decreased  0 0 200 0 400 0 600 0 800 0 1000 0 1200 0 1400 0  0 500 1000 1500 2000 2500 Absolute Throughput  req sec  Number of Emulated Clients  Threads  Consistent Read Eventual Consistent Read Figure 10  Processed Requests of SimpleDB We observed that SimpleDB often returns exceptions  with status code 503     Service is currently unavailable     under heavy load  Figure 12 shows the average failure rates of eventual consistent reads and consistent reads  each data point has whiskers to the corresponding maximum and minimum failure rates  Clearly the failure rate increased as o   ered load increased  but again we    nd that eventual consistent read does less well than consistent read  though the di   erence is not signi   cant  40 0 50 0 60 0 70 0 80 0 90 0 100 0  0 500 1000 1500 2000 2500 Throughput Percentage     Number of Emulated Clients  Threads  Consistent Read Eventual Consistent Read Figure 11  Throughput Percentage of SimpleDB 0 0 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 4 5  0 500 1000 1500 2000 2500 Failure Rate     Number of Emulated Clients  Threads  Consistent Read Eventual Consistent Read Figure 12  Request Failure Rate of SimpleDB 4 2 Financial Cost Another way people sometimes view the consistency choice in cloud platforms is as a trade o    against    nancial cost  22   In US West region  SimpleDB charges  0 154 per SimpleDB machine hour  which is the amount of SimpleDB   s server capacity used to complete requests  and which can vary depending on factors such as operation types and the amount of data to access  We compared the    nancial cost of two read consistency options for the runs described above  Amazon reports the SimpleDB machine hour usage  so one can calculate the    nancial charge incurred for each request  The cost of read operations is constant  at  1 436 per 10 6 requests  regardless of the consistency options or workload  Also  the cost of write operations is constant at  3 387 per 10 6 requests as well  4 3 Implementation Ideas While our study takes a consumer view of the storage  we have ideas about the implementation based on our experiments  It seems feasible that SimpleDB maintains each item stored in 3 replicas  one primary and two secondaries  We suspect that an eventually consistent read chooses one replica at random  and returns the value found there  while a consistent read will return the value from the primary  This aligns with our experiments showing the same latency and computational e   ort for the two kinds of read  We are told  James Hamilton  personal communication  that an update is sent synchronously to all replicas  We conjecture that the update is applied to the data immediately at the primary replica  but that it remains bu   ered at the secondaries for a while  explaining the 66  probability of seeing a stale value in an eventual consistent read  Perhaps a write that 140has been bu   ered at a replica is applied immediately when any subsequent write operation arrives  even one that is for a di   erent data element   or when a timeout expires  usually 500ms after the write itself   This would explain the experiments of section 3 1 and 3 2  if we assume that all items within one domain are replicated at the same physical nodes  and items in di   erent domains are replicated elsewhere  Further  maybe the system has an optimization that detects redundant write operations  and suppresses them  This must be sophisticated enough to detect not only repeated put requests  but also cases where one put is merely a subset of the previous update  We also suggest  from Table 3  that when multiple attributes are modi   ed within a single operation such as PutAttributes or BatchPutAttributes  the activity happens with a single message  since in these cases we do not see any immediate application of the writes  but we do see that a following read always observes the same freshness status for each attribute  5  CAN CONSUMERS RELY ON OUR RESULTS  Our paper reports on the properties and performance of various cloud based NoSQL storage platforms  as we observed them during some experiments  A natural concern is whether our results can be extrapolated to predict what the consumer will experience when using one of the platforms  We really can   t say  All the usual caveats of benchmarks measurements apply to us  For example  the workload may be unrepresentative for the consumer   s needs  perhaps because in our tests the size of the writes is so small  and the number of data elements is small  Similarly  the metrics quoted may not be what matters to the consumer  the consumer   s sta    may be more or less skilled in operating the system than we were  perhaps the experiments were not run for long enough and the    gures might re   ect chance rather than system fundamentals  etc  As well  there is a particular issue when measuring cloud systems  the vendor might change any aspect of hardware or software without notice to the consumer  For example  even if the algorithm used by a platform currently provides read your writes  the vendor could shift to a di   erent implementation that lacked this guarantee  As another example  a vendor that currently places all replicas within a single data center might implement geographical distribution  with replicas stored across data centers for better reliability  Such a change could happen without notice to the consumers  but it might lead to a situation where eventual consistent reads have observably better performance than consistent reads  Similarly  the background load on the vendor   s systems might have a large impact  on latency or availability or consistency  but the consumer cannot control or even measure what that load is at any time  29   For all these reasons  our observations that eventual consistent reads are no better for the consumer  might not hold in the future  The observations reported in this paper were mainly obtained in October and November  2011  We had conducted similar experiments in Ma</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10cc2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10cc2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Cloud_computing_and_internet_scale_computing"/>
        <doc>Low Overhead Concurrency Control for Partitioned Main Memory Databases ### Evan P  C  Jones MIT CSAIL Cambridge  MA  USA evanj csail mit edu Daniel J  Abadi Yale University New Haven  CT  USA dna cs yale edu Samuel Madden MIT CSAIL Cambridge  MA  USA madden csail mit edu ABSTRACT Database partitioning is a technique for improving the performance of distributed OLTP databases  since    single partition    transactions that access data on one partition do not need coordination with other partitions  For workloads that are amenable to partitioning  some argue that transactions should be executed serially on each partition without any concurrency at all  This strategy makes sense for a main memory database where there are no disk or user stalls  since the CPU can be fully utilized and the overhead of traditional concurrency control  such as two phase locking  can be avoided  Unfortunately  many OLTP applications have some transactions which access multiple partitions  This introduces network stalls in order to coordinate distributed transactions  which will limit the performance of a database that does not allow concurrency  In this paper  we compare two low overhead concurrency control schemes that allow partitions to work on other transactions during network stalls  yet have little cost in the common case when concurrency is not needed  The    rst is a light weight locking scheme  and the second is an even lighter weight type of speculative concurrency control that avoids the overhead of tracking reads and writes  but sometimes performs work that eventually must be undone  We quantify the range of workloads over which each technique is bene   cial  showing that speculative concurrency control generally outperforms locking as long as there are few aborts or few distributed transactions that involve multiple rounds of communication  On a modi   ed TPC C benchmark  speculative concurrency control can improve throughput relative to the other schemes by up to a factor of two  Categories and Subject Descriptors H 2 4  Database Management   Systems General Terms Experimentation  Performance Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     10 00 ###  1  INTRODUCTION Databases rely on concurrency control to provide the illusion of sequential execution of transactions  while actually executing multiple transactions simultaneously  However  several research papers suggest that for some specialized databases  concurrency control may not be necessary  11  12  27  26   In particular  if the data    ts in main memory and the workload consists of transactions that can be executed without user stalls  then there is no need to execute transactions concurrently to fully utilize a single CPU  Instead  each transaction can be completely executed before servicing the next  Previous work studying the Shore database system  14  measured the overhead imposed by locks  latches  and undo bu   er maintenance required by multi threaded concurrency control to be 42  of the CPU instructions executed on part of the TPC C benchmark  1   This suggests that removing concurrency control could lead to a signi   cant performance improvement  Databases also use concurrency to utilize multiple CPUs  by assigning transactions to di   erent threads  However  recent work by Pandis et al   20  shows that this approach does not scale to large numbers of processing cores  instead they propose a data oriented approach  where each thread    owns    a partition of the data and transactions are passed to di   erent threads depending on what data they access  Similarly  H Store  26  also has one thread per data partition  In these systems  there is only one thread that can access a data item  and traditional concurrency control is not necessary  Data partitioning is also used for shared nothing systems  Data is divided across n database servers  and transactions are routed to the partitions that contain the data they need to access  This approach is often used to improve database performance  Some applications are    perfectly partitionable     such that every transaction can be executed in its entirety at a single partition  In such a case  if the data is stored in main memory  each transaction can run without concurrency control  running to completion at the corresponding partition  However  many applications have some transactions that span multiple partitions  For these transactions  some form of concurrency control is needed  since Network stalls are necessary to coordinate the execution of these multi partition transactions  and each processor must wait if there is no concurrency control  This paper focuses on these    imperfectly partitionable    applications  For these workloads  there are some multipartition transactions that impose network stalls  and some single partition transactions that can run to completion without disk  user  or network stalls  The goal is to use a concur rency control scheme that allows a processor to do something useful during a network stall  while not signi   cantly hurting the performance of single partition transactions  We study two such schemes  The    rst is a speculative approach which works as follows  when a multi partition transaction t has completed on one partition  but is waiting for other participating partitions to complete  additional speculative transactions are executed  However  they are not committed until t commits  Speculation does not hold locks or keep track of read write sets   instead  it assumes that a speculative transaction con   icts with any transaction t with which it ran concurrently  For this reason  if t aborts  any speculative transactions must be re executed  The second scheme is based on two phase locking  When there are no active multi partition transactions  single partition transactions are e   ciently executed without acquiring locks  However  any active multi partition transactions cause all transactions to lock and unlock data upon access  as with standard two phase locking  We compare the strengths and limitations of these two concurrency control schemes for main memory partitioned databases  as well as a simple blocking scheme  where only one transaction runs at a time  Our results show that the simple blocking technique only works well if there are very few multi partition transactions  Speculative execution performs best for workloads composed of single partition transactions and multi partition transactions that perform one unit of work at each participant  In particular  our experimental results show that speculation can double throughput on a modi   ed TPC C workload  However  for abort heavy workloads  speculation performs poorly  because it cascades aborts of concurrent transactions  Locking performs best on workloads where there are many multi partition transactions  especially if participants must perform multiple rounds of work  with network communication between them  However  locking performs worse with increasing data con   icts  and especially su   ers from distributed deadlock  2  ASSUMPTIONS ON SYSTEM DESIGN Our concurrency control schemes are designed for a partitioned main memory database system similar to H Store  26   This section gives an overview of the relevant aspects of the H Store design  Traditional concurrency control comprises nearly half the CPU instructions executed by a database engine  14   This suggests that avoiding concurrency control can improve throughput substantially  H Store was therefore designed explicitly to avoid this overhead  2 1 Transactions as Stored Procedures H Store only supports executing transactions that have been pre declared as stored procedures  Each stored procedure invocation is a single transaction that must either abort or commit before returning results to the client  Eliminating ad hoc transactions removes client stalls  reducing the need for concurrency  2 2 No Disk Today  relatively low end 1U servers can have up to 128 GB of RAM  which gives a data center rack of 40 servers an aggregate RAM capacity of over 5 TB  Thus  a modest amount of hardware can hold all but the largest OLTP databases in memory  eliminating the need for disk access during normal operation  This eliminates disk stalls  which further reduces the need for concurrent transactions  Traditional databases rely on disk to provide durability  However  mission critical OLTP applications need high availability which means they use replicated systems  H Store takes advantage of replication for durability  as well as high availability  A transaction is committed when it has been received by k   1 replicas  where k is a tuning parameter  2 3 Partitioning Without client and disk stalls  H Store simply executes transactions from beginning to completion in a single thread  To take advantage of multiple physical machines and multiple CPUs  the data must be divided into separate partitions  Each partition executes transactions independently  The challenge becomes dividing the application   s data so that each transaction only accesses one partition  For many OLTP applications  partitioning the application manually is straightforward  For example  the TPC C OLTP benchmark can be partitioned by warehouse so an average of 89  of the transactions access a single partition  26   There is evidence that developers already do this to scale their applications  22  23  25   and academic research provides some approaches for automatically selecting a good partitioning key  4  21  28   However  unless the partitioning scheme is 100  e   ective in making all transactions only access a single partition  then coordination across multiple partitions for multi partition transactions cause network stalls and executing a transaction to completion without stalls is not possible  In this paper  we focus on what the system should do in this case  3  EXECUTING TRANSACTIONS In this section  we describe how our prototype executes transactions  We begin by describing the components of our system and the execution model  We then discuss how single partition and multi partition transactions are executed  3 1 System Components The system is composed of three types of processes  shown in Figure 1  First  the data is stored in partitions  a single process that stores a portion of the data in memory  and executes stored procedures using a single thread  For each partition  there is a primary process and k   1 backup processes  which ensures that data survives k     1 failures  Second  a single process called the central coordinator is used to coordinate all distributed transactions  This ensures that distributed transactions are globally ordered  and is described in Section 3 3  Finally  the client processes are end user applications that issue transactions to the system in the form of stored procedure invocations  When the client library connects to the database  it downloads the part of the system catalog describing the available stored procedures  network addresses for the partitions  and how data is distributed  This permits the client library to direct transactions to the appropriate processes  Transactions are written as stored procedures  composed of deterministic code interleaved database operations  The client invokes transactions by sending a stored procedure invocation to the system  The system distributes the work across the partitions  In our prototype  the mapping of work to partitions is done manually  but we are working on a query planner to do this automatically Clients H Store Central  Coordinator Multi Partition Node 1 Data  Partition 1 Data  Partition 2 Node 2 Data  Partition 3 Data  Partition 4 Node 3 Data  Partition 1 Data  Partition 4 Node 4 Data  Partition 3 Data  Partition 2 Single  Partition Fragment Fragment Client Library Client Library Client Library Replication Messages Primary Primary Primary Primary Backup Backup Backup Backup Figure 1  System Architecture Each transaction is divided into fragments  A fragment is a unit of work that can be executed at exactly one partition  It can be some mixture of user code and database operations  A single partition transaction  for example  is composed of one fragment containing the entire transaction  A multipartition transaction is composed of multiple fragments with data dependencies between them  3 2 Single Partition Transactions When a client determines that a request is a single partition transaction  it forwards it to the primary partition responsible for the data  The primary uses a typical primary backup replication protocol to ensure durability  In the failure free case  the primary reads the request from the network and sends a copy to the backups  While waiting for acknowledgments  the primary executes the transaction  Since it is a single partition transaction  it does not block  When all acknowledgments from the backups are received  the result of the transaction is sent to the client  This protocol ensures the transaction is durable  as long as at least one replica survives a failure  No concurrency control is needed to execute single partition transactions  In most cases  the system executes these transactions without recording undo information  resulting in very low overhead  This is possible because transactions are annotated to indicate if a user abort may occur  For transactions that have no possibility of a user abort  concurrency control schemes that guarantee that deadlock will not occur  see below  do not keep an undo log  Otherwise  the system maintains an in memory undo bu   er that is discarded when the transaction commits  3 3 Multi Partition Transactions In general  multi partition transaction can have arbitrary data dependencies between transaction fragments  For example  a transaction may need to read a value stored at partition P1  in order to update a value at partition P2  To ensure multi partition transactions execute in a serializable order without deadlocks  we forward them through the central coordinator  which assigns them a global order  Although this is a straightforward approach  the central coordinator limits the rate of multi partition transactions  To handle more multi partition transactions  multiple coordinators must be used  Previous work has investigated how to globally order transactions with multiple coordinators  for example by using loosely synchronized clocks  2   We leave selecting the best alternative to future work  and only evaluate a single coordinator system in this paper  The central coordinator divides the transaction into fragments and sends them to the partitions  When responses are received  the coordinator executes application code to determine how to continue the transaction  which may require sending more fragments  Each partition executes fragments for a given transaction sequentially  Multi partition transactions are executed using an undo bu   er  and use two phase commit  2PC  to decide the outcome  This allows each partition of the database to fail independently  If the transaction causes one partition to crash or the network splits during execution  other participants are able to recover and continue processing transactions that do not depend on the failed partition  Without undo information  the system would need to block until the failure is repaired  The coordinator piggybacks the 2PC    prepare    message with the last fragment of a transaction  When the primary receives the    nal fragment  it sends all the fragments of the transaction to the backups and waits for acknowledgments before sending the    nal results to the coordinator  This is equivalent to forcing the participant   s 2PC vote to disk  Finally  when the coordinator has all the votes from the participants  it completes the transaction by sending a    commit    message to the partitions and returning the    nal result to the application  When executing multi partition transactions  network stalls can occur while waiting for data from other partitions  This idle time can introduce a performance bottleneck  even if multi partition transactions only comprise a small fraction of the workload  On our experimental systems  described in Section 5  the minimum network round trip time between two machines connected to the same gigabit Ethernet switch was measured using ping to be approximately 40   s  The average CPU time for a TPC C transaction in our system is 26   s  Thus  while waiting for a network acknowledgment  the partition could execute at least two singlepartition transactions  Some form of concurrency control is needed to permit the engine to do useful work while otherwise idle  The challenge is to not reduce the e   ciency of simple single partition transactions  The next section describes two concurrency control schemes we have developed to address this issue  4  CONCURRENCY CONTROL SCHEMES 4 1 Blocking The simplest scheme for handling multi partition transactions is to block until they complete  When the partition receives the    rst fragment of a multi partition transaction  it is executed and the results are returned  All other transactions are queued  When subsequent fragments of the active transaction are received  they are processed in order  After the transaction is committed or aborted  the queued transactions are processed  In e   ect  this system assumes that all transactions con   ict  and thus can only execute one at a time  Pseudocode describing this approach is shown in Figure 2 Transaction Fragment Arrives if no active transactions  if single partition  execute fragment without undo bu   er commit else  execute fragment with undo bu   er else if fragment continues active multi partition transaction  continue transaction with fragment else  queue fragment Commit Abort Decision Arrives if abort  undo aborted transaction execute queued transactions Figure 2  Blocking Pseudocode 4 2 Speculative Execution This concurrency control scheme speculatively executes queued transactions when the blocking scheme would otherwise be idle  More precisely  when the last fragment of a multi partition transaction has been executed  the partition must wait to learn if the transaction commits or aborts  In the majority of cases  it will commit  Thus  we can execute queued transactions while waiting for 2PC to    nish  The results of these speculatively executed transactions cannot be sent outside the database  since if the    rst transaction aborts  the results of the speculatively executed transactions may be incorrect  Undo information must be recorded for speculative transactions  so they can be rolled back if needed  If the    rst transaction commits  all speculatively executed transactions are immediately committed  Thus  speculation hides the latency of 2PC by performing useful work instead of blocking  Speculation produces serializable execution schedules because once a transaction t has    nished all of its reads and writes and is simply waiting to learn if t commits or aborts  we can be guaranteed that any transaction t     which makes use of t   s results on a partition p will be serialized after t on p  However  t     may have read data that t wrote  such that we will have to abort t     to avoid exposing t   s dirty  rolled back  data in the event that t aborts  The simplest form of speculation is when the speculatively executed transactions are single partition transactions  so we consider that case    rst  4 2 1 Speculating Single Partition Transactions Each partition maintains a queue of unexecuted transactions and a queue of uncommitted transactions  The head of the uncommitted transaction queue is always a non speculative transaction  After a partition has executed the last fragment of a multi partition transaction  it executes additional transactions speculatively from the unexecuted queue  adding them to the end of the uncommitted transaction queue  An undo bu   er is recorded for each transaction  If the non speculative transaction aborts  each transaction is removed from the tail of the uncommitted transaction queue  undone  then pushed onto the head of the unexecuted transaction queue to be re executed  If it commits  transactions are dequeued from the head of the queue and results are sent  When the uncommitted queue is empty  the system resumes executing transactions non speculatively  for transactions that cannot abort  execution can proceed without the extra overhead of recording undo information   As an example of how this works  consider a two partition database  P1 and P2  with two integer records  x on P1 and y on P2  Initially  x   5 and y   17  Suppose the system executes three transactions  A  B1  and B2  in order  Transaction A is a multi partition transaction that swaps the value of x and y  Transactions B1 and B2 are single partition transactions on P1 that increment x and return its value  Both partitions begin by executing the    rst fragments of transaction A  which read x and y  P1 and P2 execute the fragments and return the values to the coordinator  Since A is not    nished  speculation of B1 and B2 cannot begin  If it did  the result for transaction B1 would be x   6  which is incorrect  The coordinator sends the    nal fragments  which write x   17 on partition P1  and y   5 on partition P2  After    nishing these fragments  the partitions send their    ready to commit    acknowledgment to the coordinator  and wait for the decision  Because A is    nished  speculation can begin  Transactions B1 and B2 are executed with undo bu   ers and the results are queued  If transaction A were to abort at this point  both B2 and B1 would be undone and re executed  When P1 is informed that transaction A has committed  it sends the results for B1 and B2 to the clients and discards their undo bu   ers  This describes purely local speculation  where speculative results are bu   ered inside a partition and not exposed until they are known to be correct  Only the    rst fragment of a multi partition transaction can be speculated in this way  since the results cannot be exposed in case they must be undone  However  we can speculate many multi partition transactions if the coordinator is aware of the speculation  as described in the next section  4 2 2 Speculating Multi Partition Transactions Consider the same example  except now the system executes A  B1  then a new multi partition transaction C  which increments both x and y  then    nally B2  The system executes transaction A as before  and partition P1 speculates B1  Partition P2 can speculate its fragment of C  computing y   6  With local speculation described above  it must wait for A to commit before returning this result to the coordinator  since if A aborts  the result will be incorrect  However  because A and C have the same coordinator  partition P2 can return the result for its fragment of C  with an indication that it depends on transaction A  Similarly  partition P1 can speculate its fragment of C  computing and returning x   17  along with an indication that this depends on A  It also speculates B2  computing x   18  However  it cannot send this result  as it would go directly to a client that is not aware of the previous transactions  because single partition transactions do not go through the central coordinator  Once the multi partition transactions have committed and the uncommitted transaction queue is empty  the partitions can resume non speculative execution  After the coordinator commits A  it examines the results for C  Since C depends on A  and A committed  the speculative results are correct and C can commit  If A had aborted  the coordinator would send an abort message for A to P1 and P2  then discard the incorrect results it receivedfor C  As before  the abort message would cause the partitions to undo the transactions on the uncommitted transaction queues  Transaction A would be aborted  but the other transactions would be placed back on the unexecuted queue and re executed in the same order  The partitions would then resend results for C  The resent results would not depend on previous transactions  so the coordinator could handle it normally  The pseudocode for this scheme is shown in Figure 3  This scheme allows a sequence of multi partition transactions  each composed of a single fragment at each partition to be executed without blocking  assuming they all commit  We call such transactions simple multi partition transactions  Transactions of this form are quite common  For example  if there is a table that is mostly used for reads  it may be bene   cial to replicate it across all partitions  Reads can then be performed locally  as part of a single partition transaction  Occasional updates of this table execute as a simple multi  partition transaction across all partitions  Another example is if a table is partitioned on column x  and records are accessed based on the value of column y  This can be executed by attempting the access on all partitions of the table  which is also a simple multi partition transaction  As a third example  all distributed transactions in TPC C are simple multi partition transactions  26   Thus  this optimization extends the types of workloads where speculation is bene   cial  There are a few limitations to speculation  First  since speculation can only be used after executing the last fragment of a transaction  it is less e   ective for transactions that require multiple fragments at one partition  Second  multi partition speculation can only be used when the multi partition transactions come from the same coordinator  This is necessary so the coordinator is aware of the outcome of the earlier transactions and can cascade aborts as required  However  a single coordinator can become a bottleneck  as discussed in Section 3 3  Thus  when using multiple coordinators  each coordinator must dispatch transactions in batches to take advantage of this optimization  This requires delaying transactions  and tuning the number of coordinators to match the workload  Speculation has the advantage that it does not require locks or read write set tracking  and thus requires less overhead than traditional concurrency control  A disadvantage is that it assumes that all transactions con   ict  and therefore occasionally unnecessarily aborts transactions  4 3 Locking In the locking scheme  transactions acquire read and write locks on data items while executing  and are suspended if they make a con   icting lock request  Transactions must record undo information in order to rollback in case of deadlock  Locking permits a single partition to execute and commit non con   icting transactions during network stalls for multi partition transactions  The locks ensure that the results are equivalent to transaction execution in some serial order  The disadvantage is that transactions are executed with the additional overhead of acquiring locks and detecting deadlock  We avoid this overhead where possible  When our locking system has no active transactions and receives a single partition transaction  the transaction can be executed without locks and undo information  in the same way as the blocking Transaction Fragment Arrives if no active transaction  if single partition  execute fragment without undo bu   er commit else  execute fragment with undo bu   er else if fragment continues active multi partition transaction  continue transaction by executing fragment if transaction is    nished locally  speculate queued transactions else if tail transaction in uncommitted queue is    nished locally  execute fragment with undo bu   er same coordinator     false if all txns in uncommitted queue have same coordinator  same coordinator     true if transaction is multi partition and same coordinator  record dependency on previous multi partition transaction send speculative results else  queue fragment Commit Abort Decision Arrives if abort  undo and re queue all speculative transactions undo aborted transaction else  while next speculative transaction is not multi partition  commit speculative transaction send results execute speculate queued transactions Figure 3  Speculation Pseudocode and speculation schemes  This works because there are no active transactions that could cause con   icts  and the transaction is guaranteed to execute to completion before the partition executes any other transaction  Thus  locks are only acquired when there are active multi partition transactions  Our locking scheme follows the strict two phase locking protocol  Since this is guaranteed to produce a serializable transaction order  clients send multi partition transactions directly to the partitions  without going through the central coordinator  This is more e   cient when there are no lock con   icts  as it reduces network latency and eliminates an extra process from the system  However  it introduces the possibility of distributed deadlock  Our implementation uses cycle detection to handle local deadlocks  and timeout to handle distributed deadlock  If a cycle is found  it will prefer to kill single partition transactions to break the cycle  as that will result in less wasted work  Since our system is motivated by systems like H Store  26  and DORA  20   each partition runs in single threaded mode  Therefore  our locking scheme has much lower overhead than traditional locking schemes that have to latch a centralized lock manager before manipulating the lock data structures  Our system can simply lock a data item without having to worry about another thread trying to concurrently lock the same item  The only type of concurrency we are trying to enable is logical concurrency where a new transaction can make progress only when the previous transactionis blocked waiting for a network stall     physical concurrency cannot occur  When a transaction is    nished and ready to commit  the fragments of the transaction are sent to the backups  This includes any data received from other partitions  so the backups do not participate in distributed transactions  The backups execute the transactions in the sequential order received from the primary  This will produce the same result  as we assume transactions are deterministic  Locks are not acquired while executing the fragments at the backups  since they are not needed  Unlike typical statement based replication  applying transactions sequentially is not a performance bottleneck because the primary is also single threaded  As with the previous schemes  once the primary has received acknowledgments from all backups  it considers the transaction to be durable and can return results to the client  5  EXPERIMENTAL EVALUATION In this section we explore the trade o   s between the above concurrency control schemes by comparing their performance on some microbenchmarks and a benchmark based on TPCC  The microbenchmarks are designed to discover the important di   erences between the approaches  and are not necessarily presented as being representative of any particular application  Our TPC C benchmark is intended to represent the performance of a more complete and realistic OLTP application  Our prototype is written in C   and runs on Linux  We used six servers with two Xeon 3 20 GHz CPUs and 2 GB of RAM  The machines are connected to a single gigabit Ethernet switch  The clients run as multiple threads on a single machine  For each test  we use 15 seconds of warm up  followed by 60 seconds of measurement  longer tests did not yield di   erent results   We measure the number of transactions that are completed by all clients within the measurement period  Each measurement is repeated three times  We show only the averages  as the con   dence intervals are within a few percent and needlessly clutter the    gures  For the microbenchmarks  the execution engine is a simple key value store  where keys and values are arbitrary byte strings  One transaction is supported  which reads a set of values then updates them  We use small 3 byte keys and 4 byte values to avoid complications caused by data transfer time  For the TPC C benchmarks  we use a custom written execution engine that executes transactions directly on data in memory  Each table is represented as either a B Tree  a binary tree  or hash table  as appropriate  On each benchmark we compare the three concurrency control schemes described in Section 4  5 1 Microbenchmark Our microbenchmark implements a simple mix of single partition and multi partition transactions  in order to understand the impact of distributed transactions on throughput  We create a database composed of two partitions  each of which resides on a separate machine  The partitions each store half the keys  Each client issues a read write transaction which reads and writes the value associated with 12 keys  For this test  there is no sharing  i e   potential con   ict across clients   each client writes its own set of keys  We experiment with shared data in the next section  To create a single partition transaction  a clients selects a partition at random  then accesses 12 keys on that partition  To create a multi partition transaction  the keys are divided evenly by accessing 6 keys on each partition  To fully utilize the CPU on both partitions  we use 40 simultaneous clients  Each client issues one request  waits for the response  then issues another request  We vary the fraction of multi partition transactions and measure the transaction throughput  The results are shown in Figure 4  From the application   s perspective  the multipartition and single partition transactions do the same amount of work  so ideally the throughput should stay constant  However  concurrency control overhead means this is not the case  The performance for locking is linear in the range between 16  and 100  multi partition transactions  The reason is that none of these transactions con   ict  so they can all be executed simultaneously  The slight downward slope is due to the fact that multi partition transactions have additional communication overhead  and thus the performance degrades slightly as their fraction of the workload increases  The most interesting part of the locking results is between 0  and 16  multi partition transactions  As expected  the performance of locking is very close to the other schemes at 0  multi partition transactions  due to our optimization where we do not set locks when there are no multi partition transactions running  The throughput matches speculation and blocking at 0   then decreases rapidly until 16  multipartition transactions  when there is usually at least one multi partition transaction in progress  and therefore nearly all transactions are executed with locks  The performance of blocking in this microbenchmark degrades steeply  so that it never outperforms locking on this low contention workload  The reason is that the advantage of executing transactions without acquiring locks is outweighed by the idle time caused by waiting for two phase commit to complete  Our locking implementation executes many transactions without locks when there are few multipartition transactions  so there is no advantage to blocking  If we force locks to always be acquired  blocking does outperform locking from 0  to 6  multi partition transactions  With fewer than 50  multi partition transactions  the throughput of speculation parallels locking  except with approximately 10  higher throughput  Since this workload is composed of single partition and simple multi partition transactions  the single coordinator can speculate all of them  This results in concurrent execution  like locking  but without the overhead of tracking locks  Past 50   speculation   s performance begins to drop  This is the point where the central coordinator uses 100  of the CPU and cannot handle more messages  To scale past this point  we would need to implement distributed transaction ordering  as described in Section 4 2 2  For this particular experiment  blocking is always worse than speculation and locking  Speculation outperforms locking by up to 13   before the central coordinator becomes a bottleneck  With the bottleneck of the central coordinator  locking outperforms speculation by 45   5 2 Con   icts The performance of locking depends on con   icts between transactions  When there are no con   icts  transactions execute concurrently  However  when there are con   icts  there is additional overhead to suspend and resume execution  To0 5000 10000 15000 20000 25000 30000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Speculation Locking Blocking Figure 4  Microbenchmark Without Con   icts investigate the impact of con   icts  we change the pattern of keys that clients access  When issuing single partition transactions  the    rst client only issues transactions to the    rst partition  and the second client only issues transactions to the second partition  rather than selecting the destination partition at random  This means the    rst two clients    keys on their respective partitions are nearly always being written  To cause con   icts  the other clients write one of these    con   ict    keys with probability p  or write their own private keys with probability 1     p  Such transactions will have a very high probability of attempting to update the key at a same time as the    rst two clients  Increasing p results in more con   icts  Deadlocks are not possible in this workload  allowing us to avoid the performance impact of implementation dependent deadlock resolution policies  The results in Figure 5 show a single line for speculation and blocking  as their throughput does not change with the con   ict probability  This is because they assume that all transactions con   ict  The performance of locking  on the other hand  degrades as con   ict rate increases  Rather than the nearly straight line as before  with con   icts the throughput falls o    steeply as the percentage of multi partition transactions increases  This is because as the con   ict rate increases  locking behaves more like blocking  Locking still outperforms blocking when there are many multi partition transactions because in this workload  each transaction only con   icts at one of the partitions  so it still performs some work concurrently  However  these results do suggest that if con   icts between transactions are common  the advantage of avoiding concurrency control is larger  In this experiment  speculation is up to 2 5 times faster than locking  5 3 Aborts Speculation assumes that transactions will commit  When a transaction is aborted  the speculatively executed transactions must be undone and re executed  wasting CPU time  To understand the e   ects of re execution  we select transactions to be aborted at random with probability p  When a multi partition transaction is selected  only one partition will abort locally  The other partition will be aborted during two phase commit  Aborted transactions are somewhat cheaper to execute than normal transactions  since the abort 0 5000 10000 15000 20000 25000 30000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Locking 0  conflict Locking 20  conflict Locking 60  conflict Locking 100  conflict Speculation Blocking Figure 5  Microbenchmark With Con   icts happens at the beginning of execution  They are identical in all other respects   e g   network message length   The results for this experiment are shown in Figure 6  The cost of an abort is variable  depending on how many speculatively executed transactions need to be re executed  Thus  the 95  con   dence intervals are wider for this experiment  but they are still within 5   so we omit them for clarity  Since blocking and locking do not have cascading aborts  the abort rate does not have a signi   cant impact  so we only show the 10  abort probability results  This has slightly higher throughput than the 0  case  since abort transactions require less CPU time  As expected  aborts decrease the throughput of speculative execution  due to the cost of re executing transactions  They also increase the number of messages that the central coordinator handles  causing it to saturate sooner  However  speculation still outperforms locking for up to 5  aborts  ignoring the limits of the central coordinator  With 10  aborts  speculation is nearly as bad as blocking  since some transactions are executed many times  These results suggest that if a transaction has a very high abort probability  it may be better to limit to the amount of speculation to avoid wasted work  5 4 General Multi Partition Transactions When executing a multi partition transaction that involves multiple rounds of communication  speculation can only begin when the transaction is known to have completed all its work at a given partition  This means that there must be a stall between the individual fragments of transaction  To examine the performance impact of these multiround transactions  we changed our microbenchmark to issue a multi partition transaction that requires two rounds of communication  instead of the simple multi partition transaction in the original benchmark  The    rst round of each transaction performs the reads and returns the results to the coordinator  which then issues the writes as a second round  This performs the same amount of work as the original benchmark  but has twice as many messages  The results are shown in Figure 7  The blocking throughput follows the same trend as before  only lower because the two round transactions take nearly twice as much time as0 5000 10000 15000 20000 25000 30000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Speculation 0  aborts Speculation 3  aborts Speculation 5  aborts Speculation 10  aborts Blocking 10  aborts Locking 10  aborts Figure 6  Microbenchmark With Aborts 0 5000 10000 15000 20000 25000 30000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Speculation Blocking Locking Figure 7  General Transaction Microbenchmark the multi partition transactions in the original benchmark  Speculation performs only slightly better  since it can only speculate the    rst fragment of the next multi partition transaction once the previous one has    nished  Locking is relatively una   ected by the additional round of network communication  Even though locking is generally superior for this workload  speculation does still outperform locking as long as fewer than 4  of the workload is composed of general multi partition transactions  5 5 TPC C The TPC C benchmark models the OLTP workload of an order processing system  It is comprised of a mix of    ve transactions with di   erent properties  The data size is scaled by adding warehouses  which adds a set of related records to the other tables  We partition the TPC C database by warehouse  as described by Stonebraker et al   26   We replicate the items table  which is read only  to all partitions  We vertically partition the stock table  and replicate the read only columns across all partitions  leaving the columns that are updated in a single partition  This partitioning means 89  of the transactions access a single partition  and the others are simple multi partition transactions  Our implementation tries to be faithful to the speci   cation  but there are three di   erences  First  we reorder the operations in the new order transaction to avoid needing an undo bu   er to handle user aborts  Second  our clients have no pause time  Instead  they generate another transaction immediately after receiving the result from the previous one  This permits us to generate a high transaction rate with a small number of warehouses  Finally  we change how clients generate requests  The TPC C speci   cation assigns clients to a speci   c  warehouse  district  pair  Thus  as you add more warehouses  you add more clients  We use a    xed number of clients while changing the number of warehouses  in order to change only one variable at a time  To accommodate this  our clients generate requests for an assigned warehouse but a random district  We ran TPC C with the warehouses divided evenly across two partitions  In this workload  the fraction of multi partition transactions ranges from 5 7  with 20 warehouses to 10 7  with 2 warehouses  The throughput for varying numbers of warehouses are shown in Figure 8  With this workload  blocking and speculation have relatively constant performance as the number of warehouses is increased  The performance is lowest with 2 partitions because the probability of a multi partition transaction is highest  10 7   versus 7 2  for 4 warehouses  and 5 7  for 20 warehouses   due to the way TPC C new order transaction requests are generated  After 4 warehouses  the performance for blocking and speculation decrease slightly  This is due to the larger working set size and the corresponding increase in CPU cache and TLB misses  The performance for locking increases as the number of warehouses is increased because the number of con   icting transactions decreases  This is because there are fewer clients per TPC C warehouse  and nearly every transaction modi   es the warehouse and district records  This workload also has deadlocks  which leads to overhead due to deadlock detection and distributed deadlock timeouts  decreasing the performance for locking  Speculation performs the best of the three schemes because the workload   s fraction of multi partition transactions is within the region where it is the best choice  With 20 warehouses  speculation provides 9 7  higher throughput than blocking  and 63  higher throughput than locking  5 6 TPC C Multi Partition Scaling In order to examine the impact that multi partition transactions have on a more complex workload  we scale the fraction of TPC C transactions that span multiple partitions  We execute a workload that is composed of 100  new order transactions on 6 warehouses  We then adjust the probability that an item in the order comes from a    remote    warehouse  which is a multi partition transaction  With TPC C   s default parameters  this probability is 0 01  1    which produces a multi partition transaction 9 5  of the time  We adjust this parameter and compute the probability that a transaction is a multi partition transaction  The throughput with this workload is shown in Figure 9  The results for blocking and speculation are very similar to the results for the microbenchmark in Figure 4  In this experiment  the performance for locking degrades very rapidly  At 0  multi partition transactions  it runs e   ciently without acquiring locks  but with multi partition transactions it0 5000 10000 15000 20000 25000 2 4 6 8 10 12 14 16 18 20 Transactions second Warehouses Speculation Blocking Locking Figure 8  TPC C Throughput Varying Warehouses 0 5000 10000 15000 20000 25000 30000 35000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Speculation Blocking Locking Figure 9  TPC C 100  New Order must acquire locks  The locking overhead is higher for TPCC than our microbenchmark for three reasons  more locks are acquired for each transaction  the lock manager is more complex  and there are many con   icts  In particular  this workload exhibits local and distributed deadlocks  hurting throughput signi   cantly  Again  this shows that con   icts make traditional concurrency control more expensive  increasing the bene   ts of simpler schemes  Examining the output of a sampling pro   ler while running with a 10  multi partition probability shows that 34  of the execution time is spent in the lock implementation  Approximately 12  of the time is spent managing the lock table  14  is spent acquiring locks  and 6  is spent releasing locks  While our locking implementation certainly has room for optimization  this is similar to what was previously measured for Shore  where 16  of the CPU instructions could be attributed to locking  14   5 7 Summary Our results show that the properties of the workload determine the best concurrency control mechanism  Speculation performs substantially better than locking or blocking Few  Con   icts Many  Con   icts Few Con   icts Many  Con   icts Many multipartition  xactions Speculation Speculation Locking Locking or  Speculation Few multipartition  xactions Speculation Speculation Blocking or  Locking Blocking Many multiround  xactions Locking Locking Locking Locking Few multiround  xactions Few Aborts Many Aborts Table 1  Summary of best concurrency control scheme for di   erent situations  Speculation is preferred when there are few multi round  general  transactions and few aborts  for multi partition transactions that require only a single round of communication and when a low percentage of transactions abort  Our low overhead locking technique is best when there are many transactions with multiple rounds of communication  Table 1 shows which scheme is best  depending on the workload  we imagine that a query executor might record statistics at runtime and use a model like that presented in Section 6 below to make the best choice  Optimistic concurrency control  OCC  is another    standard    concurrency control algorithm  It requires tracking each item that is read and written  and aborts transactions during a validation phase if there were con   icts  Intuitively  we expect the performance for OCC to be similar to that of locking  This is because  unlike traditional locking implementations that need complex lock managers and careful latching to avoid problems inherent in physical concurrency  our locking scheme can be much lighter weight  since each partition runs single threaded  i e   we only have to worry about the logical concurrency   Hence  our locking implementation involves little more than keeping track of the read write sets of a transaction     which OCC also must do  Consequently  OCC   s primary advantage over locking is eliminated  We have run some initial results that verify this hypothesis  and plan to explore the trade o   s between OCC and other concurrency control methods and our speculation schemes as future work  6  ANALYTICAL MODEL To improve our understanding of the concurrency control schemes  we analyze the expected performance for the multipartition scaling experiment from Section 5 1  This model predicts the performance of the three schemes in terms of just a few parameters  which would be useful in a query planner  for example   and allows us to explore the sensitivity to workload characteristics  such as the CPU cost per transaction or the network latency   To simplify the analysis  we ignore replication  Consider a database divided into two partitions  P1 and P2  The workload consists of two transactions  The    rst is a single partition transaction that accesses only P1 or P2  chosen uniformly at random  The second is a multi partition transaction that accesses both partitions  There are no data dependencies  and therefore only a single round of communication is required  In other words  the coordinator simply sends two fragments out  one to each partition  waitsfor the response  then sends the commit or abort decision  Each multi partition transaction accesses half as much data in each partition  but the total amount of data accessed is equal to one single partition transaction  To provide the best case performance for locking  none of the transactions con   ict  We are interested in the throughput as the fraction of multi partition transactions  f  in the workload is increased  6 1 Blocking We begin by analyzing the blocking scheme  Here  a single partition transaction executes for tsp seconds on one partition  A multi partition transaction executes for tmp on both partitions  including the time to complete the two phase commit  If there are N transactions to execute  then there are Nf multi partition transactions and 1 2N 1     f  single partition transactions to execute at each partition  The factor of 1 2 arises because the single partition transactions are distributed evenly between the two partitions  Therefore the time it takes to execute the transactions and the system throughput are given by the following equations  time   Nftmp   N 1     f  2 tsp throughput   N time   2 2ftmp    1     f tsp E   ectively  the time to execute N transactions is a weighted average between the times for a pure single partition workload and a pure multi partition workload  As f increases  the throughput will decrease from 2 tsp to 1 tmp   Since tmp   tsp  the throughput will decrease rapidly with even a small fraction of multi partition transactions  6 2 Speculation We    rst consider the local speculation scheme described in Section 4 2 1  For speculation  we need to know the amount of time that each partition is idle during a multi partition transaction  If the CPU time consumed by a multi partition transaction at one partition is tmpC  then the network stall time is tmpN   tmp     tmpC  Since we can overlap the execution of the next multi partition transaction with the stall  the limiting time when executing a pure multi partition transaction workload is tmpL   max  tmpN  tmpC   and the time that the CPU is idle is tmpI   max  tmpN  tmpC    tmpC  Assume that the time to speculatively execute a single partition transaction is tspS  During a multi partition transaction   s idle time  each partition can execute a maximum of tmpI tspS single partition transactions  When the system executes Nf multi partition transactions  each partition executes N 1   f  2 single partition transactions  Thus  on average each multi partition transaction is separated by  1   f  2f single partition transactions  Therefore  for each multi partition transaction  the number of single partition transactions that each partition can speculate is given by  Nhidden   min     1     f 2f   tmpI tspS    Therefore  the time to execute N transactions and the resulting throughput are  time   NftmpL    N 1     f      2NfNhidden  tsp 2 throughput   2 2ftmpL     1     f      2fNhidden tsp In our speci   c scenario and system  tmpN   tmpC  so we can simplify the equations  Nhidden   min     1     f 2f   tmp     2tmpC tspS    throughput   2 2f tmp     tmpC      1     f      2fNhidden tsp The minimum function produces two di   erent behaviors  If 1   f 2f     tmpI tspS   then the idle time can be completely utilized  Otherwise  the system does not have enough single partition transactions to completely hide the network stall  and the throughput will drop rapidly as f increases past tspS 2tmpI tspS   6 2 1 Speculating Multi Partition Transactions This model can be extended to include speculating multi partition transactions  as described in Section 4 2 2  The previous derivation for execution time assumes that one multi partition transaction completes every tmpL seconds  which includes the network stall time  When multi partition transactions can be speculated  this restriction is removed  Instead  we must compute the CPU time to execute multipartition transactions  and speculative and non speculative single partition transactions  The previous model computed the number of speculative single partition transactions per multi partition transaction  Nhidden  We can compute the time for multi partition transactions and speculative single partition transactions as tperiod   tmpC   NhiddentspS  This time replaces tmpL in the previous model  and thus the throughput becomes  throughput   2 2ftperiod     1     f      2fNhidden tsp 6 3 Locking Since the workload is composed of non con   icting transactions  locking has no stall time  However  we must account for the overhead of tracking locks  We de   ne l to be the fraction of additional time that a transaction takes to execute when locking is enabled  Since locking always requires undo bu   ers  we use tspS to account for the single partition execution time  Furthermore  the overhead of two phase commit must be added  so for multi partition transactions we use tmpC  The time to execute N transactions  and the resulting throughput are given by  time   NfltmpC   N 1     f  2 ltspS throughput   N time   2 2fltmpC    1     f ltspS 6 4 Experimental Validation We measured the model parameters for our implementation  The values are shown in Table 2  Figure 10 shows the analytical model using these parameters  along with the0 5000 10000 15000 20000 25000 30000 35000 0  20  40  60  80  100  Transactions second Multi Partition Transactions Model Spec  Model Local Spec  Model Blocking Measured Blocking Model Locking Measured Locking Measured Local Spec  Figure 10  Model Throughput measured throughput for our system without replication  As can be observed  the two are a relatively close match  This suggests that the model is a reasonable approximation for the behavior of the real system  These results also s</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10is1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10is1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Indexing_and_storage_management"/>
        <doc>Ef   cient Graph Similarity Joins with Edit Distance Constraints ### Xiang Zhao        Chuan Xiao     Xuemin Lin             Wei Wang         The University of New South Wales  Australia  xzhao  chuanx  lxue  weiw  cse unsw edu au     East China Normal University  China    NICTA  Australia Abstract   Graphs are widely used to model complicated data semantics in many applications in bioinformatics  chemistry  social networks  pattern recognition  etc  A recent trend is to tolerate noise arising from various sources  such as erroneous data entry  and    nd similarity matches  In this paper  we study the graph similarity join problem that returns pairs of graphs such that their edit distances are no larger than a threshold  Inspired by the q gram idea for string similarity problem  our solution extracts paths from graphs as features for indexing  We establish a lower bound of common features to generate candidates  An ef   cient algorithm is proposed to exploit both matching and mismatching features to improve the    ltering and veri   cation on candidates  We demonstrate the proposed algorithm signi   cantly outperforms existing approaches with extensive experiments on publicly available datasets ### I  INTRODUCTION Graphs have a wide range of applications and have been utilized to model complex data in biological and chemical information systems  multimedia  social networks  etc  There has been considerable interest in many fundamental problems in analyzing graph data  Various algorithms were devised to solve these problems  including frequent graph mining  32    6   graph containment search and indexing  33    12    7    38   etc  Due to the existence of noisy and inconsistent data  a recent trend is to study similarity matches among graphs  34    30    27    36    24    23    28   This body of work solves the problem of searching for graphs in a database that approximately contain or are contained by a query  Among the various graph similarity measures used in these studies  graph edit distance  4    22  has been widely accepted for representing distances between graphs  Compared with alternative distances or similarity measures  graph edit distance has three advantages   1  it allows changes in both vertices and edges   2  it re   ects the topological information of graphs  and  3  it is a metric and can be applied to any type of graphs  Due to these elegant properties  graph edit distance has been used in the context of classi   cation and clustering tasks in various applications domains  1    21   However  the expensive computation of graph edit distance poses serious algorithmic challenges  In order to tackle the NP hardness of the problem      Corresponding Author a few algorithms have been proposed to either convert it to binary linear programming and compute the bounds  13  or seek unbounded suboptimal answers with heuristic techniques  9   In this paper  we study the graph similarity join problem with graph edit distance constraints  a batch version of the graph similarity selection problem  It takes as input two sets of graphs  and returns pairs of graphs from each set such that their graph edit distances are no more than a given threshold  There are several studies on the graph similarity selection problem with edit distance constraints  i e   to    nd the graphs whose edit distances to the query are no larger than a threshold  These methods are either based on trees  28  or star structures  36   The    AT algorithm proposed in  28  borrows the qgram idea from the solution to string similarity problems  10   and de   nes a q gram as a tree consisting of a vertex along with all those that can be reached in q hops  A count    ltering condition on common q grams is established to qualify the candidate pairs that satisfy the graph edit distance constraint  However  it suffers from the looseness of the lower bound due to the huge impact of edit operations on common q grams  and therefore is only effective against sparse graphs  The choice of q gram length is also limited to a very small range  which usually consists of short q grams  resulting in poor selectivity and thus large candidate size  The star structure proposed in  36  is exactly the same feature as the 1 gram de   ned by    AT  Unlike    AT  it computes the lower and upper bounds of graph edit distance with a bipartite matching between the star representations of two graphs  For graph similarity join problem  it has to invoke bipartite matching for every pair of graphs  The time complexity will be O  R  S  V   3    where  R  and  S  are the dataset sizes  and  V   is the number of vertices in a graph  Distinct from the existing approaches  we explore a novel perspective of utilizing path based q grams  We    nd that the count    ltering condition of path based q grams is tighter than using trees  This enables us to perform similarity join on denser graphs as well as choose longer q grams for better selectivity  Another novelty is to exploit the valuable information provided by mismatching q grams that cannot be matched in a candidate pair  Two    ltering conditions are accordingly proposed so that the size of candidates can be substantially reduced  In addition  we elaborate how tospeed up graph edit distance computation by further utilizing the two    ltering conditions  The    ltering and veri   cation techniques constitute the GSimJoin algorithm  Its superior time ef   ciency to alternative methods is demonstrated through extensive experimental evaluation  Our contributions can be summarized as follows      We solve the graph similarity join problem by introducing a new notion of q grams based on paths  We develop the count    ltering condition regarding the number of matching q grams  which is tighter than using tree based q grams      We analyze mismatching q grams and develop two    ltering techniques to improve the performance of graph similarity join by optimizing both candidate generation and veri   cation      We propose a new algorithm  GSimJoin  that integrates the proposed    ltering and veri   cation methods  We conduct extensive experiments using two publicly available datasets from different application domains  The proposed algorithm has been demonstrated to outperform other approaches  The rest of the paper is organized as follows  Section II presents the problem de   nition and preliminaries  Section III introduces the de   nition of path based q gram on graphs and the corresponding count    lter on matching q grams  Sections IV and V present the two    ltering techniques exploiting mismatching q grams  Section VI elaborates the veri   cation of candidates  Experimental results and analyses are presented in Section VII  Section VIII summarizes related work  and Section IX concludes the paper  II  PRELIMINARIES A  Problem De   nition For ease of exposition  we focus on simple graphs in this paper  A simple graph is an undirected graph with neither self loops nor multiple edges 1   A labeled graph r can be represented as quadruple  V  E  lV   lE   where V is a set of vertices  and E     V   V is a set of edges  lV and lE are label functions that assign labels to vertices and edges  respectively  V  r  denotes the vertex set of r  and E r  denotes the edge set   V  r   and  E r   represent the number of vertices and edges in r  respectively  lV  u  denotes the label of u  and lE e u  v   denotes the label of an edge between u and v  u  v     V   A graph r is isomorphic to another graph s if there exists a bijection f   V  r      V  s  such that  1     u     V  r   f u      V  s      lV  u    lV  f u    and  2     e u  v      E r   e f u   f v       E s      lE e u  v     lE e f u   f v     A graph edit operation is an edit operation to transform one graph to another  4    22   It can be one of the following six operations      Insert an isolated vertex into the graph      Delete an isolated vertex from the graph      Change the label of a vertex      Insert an edge between two disconnected vertices  1Without loss of generality  our approach can be easily extended to directed graphs  C C C O 1 2 3 r C C C O 1 2 N 3 s Fig  1  Cyclopropanone and 2 Aminocyclopropanol     Delete an edge from the graph      Change the label of an edge  The graph edit distance between r and s  denoted by ged r  s   is the minimum number of edit operations that transform r to a graph isomorphic to s  It is shown that computing the graph edit distance between two graphs is NP hard  36   Example 1  Figure 1 shows the structure of cyclopropanone  r  and 2 aminocyclopropanol  s  molecules after omitting hydrogen atoms  They have been used in investigations of potential antiviral drugs  8   For ease of illustration  we add subscripts to carbon atoms  while C1  C2 and C3 correspond to the same label in real data  Single and double lines indicate different chemical bonds  represented in edge labels in real data  The graph edit distance between r and s is 3  We formalize the graph similarity join problem as follows  Given two sets of graphs R and S  a graph similarity join with edit distance threshold    returns pairs of graphs from each set  such that their graph edit distance is no larger than     i e     hr  si   ged r  s          r     R  s     S    Assuming there is a unique identi   er recorded in id for each graph  this paper will focus on the self join case  i e     hri   rj i   ged ri   rj              ri  id   rj  id  ri     R  rj     R    B  Tree based q gram Approach The string similarity problem with edit distance constraints has been extensively studied  10    31    14    15    35    37    29    18   Among them  many prevalent approaches are based on q grams  10    31    14    18   namely  substrings of length q  Since an edit operation will only affect a limited number of q grams  similar strings will have certain amount of overlap between their q gram sets 2   Based on this observation  these approaches essentially relax the edit distance constraint to a weaker count constraint on the number of common q grams called count    ltering  Inspired by the idea of q gram on string similarity   28  proposed    AT algorithm that de   nes the q grams on graphs based on trees  For each vertex u  a tree based q gram is a set of vertices that can be reached from u in q hops  represented in a breadth    rst search tree rooted at u  Example 2  Consider r in Figure 1 and q   1  There are four 1 grams of r  as shown in Figure 2  The    rst 1 gram appears twice  2 q grams in strings are accompanied by their starting positions in the string  and thus there is no duplicate C C C   2 C C O C   1 O C   1 Fig  2  Tree based q grams The maximum number of q grams that can be affected by an edit operation is shown as Dtree   1                 1  q     1        2   where    is the maximum vertex degree in the graph     AT algorithm exploits the constraint that two graphs r and s must share at least a number of common q grams if they are within graph edit distance     LBtree   max  V  r             Dtree r    V  s             Dtree s    A pair of graphs that satis   es the lower bound test is called a candidate pair  Note that it does not necessarily satisfy the graph edit distance constraint  Therefore  graph edit distance calculation will be invoked for every candidate pair that survives this count    lter     AT algorithm is associated with the drawback that the lower bound of common q grams is usually loose  It may become equal to or even less than zero if there is a vertex with high degree in the graph  and we call such phenomenon under   owing  This problem results in the following dilemma  We have to use very short q grams  e g   1 grams  to ensure the pairs of graphs to have at least one common q gram so that the all pair comparison due to under   owing can be avoided  however  short q grams suffer from poor performance problems as they are usually frequent and hence yield large candidate size  Consider the two graphs in Figure 1 and      1  the lower bound is only 1 if we use 1 grams  and becomes less than zero when longer q grams are applied  III  A PATH BASED q GRAM METHOD Seeing the drawback of the tree based q gram approach  we seek a new way of de   ning q grams on graphs  Since q grams de   ned on strings are sequences  we may choose paths in a graph as its q grams  as paths are convertible to    sequences     Next we formally introduce the de   nition of path based q grams on graphs  A  De   nition of Path based q gram De   nition 1  path based q gram   A path based q gram in a graph r is a simple path of length q     Simple    means that there is no repeated vertex in the path  Since a path has two ends  namely  start vertex and end vertex  two sequences can be formed by concatenating the vertex and edge labels from both ends  We only keep the lexicographically smaller as a q gram  Since the length of a path can be zero for the case of a single vertex  a 0 gram will be a single vertex  In the rest of the paper  we use    path based q gram    and    q gram    interchangeably when there is no ambiguity  Example 3  Consider the two graphs in Figure 1 and q   1  There are four 1 grams in r  C O   1  C C   3  and    ve 1 grams in s  C N   1  C O   1  C C   3  Before developing count    ltering condition for path based q grams  we    rst study the effect of an edit operation on a graph   s q grams  Let Qr denote the multiset of q grams in r  and Qu r denote the multiset of q grams that contain the vertex u  The following theorem shows how many q grams in Qr will be affected when an edit operation occurs in r  Theorem 1  An edit operation on r will affect at most  Dpath r    maxu   V  r   Qu r    q grams in Qr  Proof  We enumerate the effect of various edit operations      Insert an isolated vertex into the graph  No q gram in Qr will be affected      Delete an isolated vertex from the graph  The number of q grams affected is either 1 when q   0  or 0 otherwise      Change the label of a vertex  Suppose u   s label is changed  This will affect  Qu r       maxu   V  r   Qu r   q grams      Insert an edge between two disconnected vertices  No q gram in Qr will be affected      Delete an edge from the graph  Suppose e u  v  is deleted  The number of affected q grams is max  Qu r     Qv r        maxu   V  r   Qu r        Change the label of an edge  This will affect the same number of q grams as deleting an edge from the graph  According to Theorem 1  the count    ltering condition for path based q grams can be established as  Lemma 1  Count Filtering   Consider two graphs r and s  If ged r  s          r and s must share at least LBpath   max  Qr            Dpath r    Qs            Dpath s    1  common q grams  Example 4  Consider Figure 1       1  and q   1  Changing the label of C1 gives the maximum  Qu r     3 for both graphs  Therefore the lower bound of common q grams is max 4     3  5     3    2  Even when q is 2  the lower bound of common q grams is still above zero  as given by max 5     5  7     6    1 B  Comparison with Tree based q grams Now we compare the effect of edit operations on tree based and path based q grams      For q   1  consider r in Figure 1  All the tree based q grams can be affected by an edit operation on C1  while the path based q gram consisting of C2 and C3 will still be kept  This example showcases that path based q grams can preserve more common structural information than tree based q grams  excluding the affected part      For longer q grams  the number of vertices covered by a tree based q gram increases exponentially with the length q  Any edit operation resident on these vertices will make this q gram mismatched  On the contrary  the coverage of a path based q gram increases linearly with the length q  and therefore the probability being hit by an edit operation is much decreased  Compared with tree based q grams  path based q grams have the advantage of presenting tighter lower bounds  and this will deliver the chance of using longer q grams in seek of better selectivity and runtime performance  C  Pre   x Filtering An ef   cient way to    nd the pairs of graphs that satisfy the count    ltering condition is to use inverted index  2   An inverted index maps each q gram w to a list of identi   ers of graphs that contain w  With the inverted index built for all the graphs in the data collection  we can scan each graph r  probe the index using every q gram of r  and produce a set of candidates  Merging these candidates gives the actual intersection of q grams and the graph pairs that meet the lower bound  The main performance bottleneck in accessing inverted index is that the inverted lists of some q grams can be very long  For example  the carbon chain C     C     C exists in most chemical compounds in AIDS dataset  These long inverted lists will incur prohibitive overhead when accessed  In addition  a large number of candidate pairs will be produced if they share such q grams  Existing approaches to string similarity problem address this bottleneck by employing pre   x    ltering technique  5    31    18  to quickly    lter out the candidate pairs that are guaranteed not to meet the count    ltering condition  The intuition is that if two multisets of q grams meet the lower bound constraint  they must share at least one common q gram if we look into part of the q grams  Figure 3 illustrates the idea of pre   x    ltering  Suppose the q grams in two multisets are sorted in the same ordering  l is the total number of q grams in both multisets  The unshaded cells are pre   xes  If Qr and Qs have no common q gram in their pre   xes  the number of their common q grams is no more than LBpath     1  We formally state the pre   x    ltering principle for graph similarity joins in Lemma 2  Lemma 2  Pre   x Filtering   Consider two graphs r  s  their corresponding q gram multisets Qr  Qs  and a global ordering O of the q gram universe  Let Qr and Qs be sorted in the order of O  and the p pre   x be their    rst p elements  If  Qr    Qs      Q wa wb             r  Q wc wd             s  l     LBpath   1 LBpath     1 l Fig  3  Illustration of Pre   x Filtering     then the   Qr       1  pre   x of Qr and the   Qs       1   pre   x of Qs must have at least one common q gram  In order to achieve a small candidate size and fast execution speed  rare q grams are favored in pre   xes  Therefore we sort the multiset of q grams in each graph in ascending order of document frequency  the number of graphs that contain the q gram  D  Graph Join Algorithm Combining count    ltering for path based q grams and pre   x    ltering  we have the basic GSimJoin algorithm  Algorithm 1   The algorithm takes as input a collection of graphs  and follows an index nested loop join style  maintaining an in memory inverted index on the    y  It iterates through each graph r     R  For each q gram w in Qr   s pre   x  it probes the inverted index to    nd other graphs s that contain w in their pre   xes  The candidates will be sent into Verify  and checked by  1  count    ltering  and then  2  the expensive graph edit distance computation to tell if they are join results  According to Lemma 1 and 2  the pre   x length is      Dpath r  1 for each graph r  Line 6   In addition  the numbers of vertices and edges in r and s must have the difference within     This size    ltering is included in Line 9  In the next two sections  we will study how to exploit the information provided by mismatching q grams to gain further ef   ciency  Although the similar property also happens for strings and has been investigated in  31   the scenario on graphs is much more challenging  First  the q grams on strings have starting positions  and hence are easy to locate  while the q grams on graphs do not have such attribute  Second  the minimum edit operation problem on strings is of polynomial time complexity while it is NP hard on graphs  We will propose two non trivial techniques on graphs to reduce both index and candidate sizes  IV  MINIMUM EDIT FILTERING We    rst show an illustrative example  Example 5  Consider Figure 1       1  and q   1  The count    ltering lower bound is 2  while the two graphs share 3 q grams  see Example 3  and therefore will survive count    ltering  However  if we consider the two mismatching q grams in s  C O and C N  it can be seen that they are disjoint  see the two bounded regions in s   It takes at least two edit operations to affect them  Obviously  we can infer a lower bound of the graph edit distance between r and s to be 2 and hence prune this pair  This motivates us to    nd the minimum number of edit operations that can cause the observed mismatching q grams Algorithm 1  GSimJoin  R      Input   R is a collection of graphs     is a graph edit distance threshold  O is a global ordering of q grams  Output   S     hr  si   ged r  s            1 S          2 Ii          1     i      U        inverted index    3 for each r     R do 4 A     empty map from id to boolean  5 Qr     r   s q grams sorted in O  6 pr           Dpath r    1  7 for i   1 to pr do 8 w     Qr i   9 for each s     Iw such that abs  V  r        V  s      abs  E r        E s          and A s  has not been initialized do 10 A s      true      find a candidate    11 Iw     Iw       r        index for q gram w    12 S     S     Verify r  A   13 return S The above example evidences the implication of edit operations occur on disjoint q grams and the existence of redundancy within pre   xes  Since we choose increasing document frequency as the global ordering on q gram multisets  the rarest q grams reside in the beginning of pre   xes  while the end of pre   xes are relatively frequent q grams  Both index size and the candidates passing pre   x    ltering can be reduced if we are able to remove the redundancy and avoid frequent q grams with shortened pre   xes  A  Minimum Graph Edit Operations Example 5 illustrates the case where mismatching q grams are disjoint  To handle the general case where q grams may overlap  we formulate the minimum graph edit operation problem  Given a multiset of q grams Q     nd the minimum number of graph edit operations that can affect all the q grams in Q  Theorem 2  The minimum graph edit operation problem is NP hard  Proof   sketch  It can be shown the q grams affected by all the other edit operations are a subset of the q grams affected by changing vertex label  The minimum graph edit operation problem can be reduced from the set cover problem by treating q grams as elements and vertices as sets  Therefore the minimum graph edit operation problem is NP hard  Despite its NP hardness  the problem can be solved with an exact algorithm enumerating the positions of edit operations  since we only concern whether the answer is within or beyond     The time complexity is O  V         Q    where  V   is the number of vertices contained by the q grams in Q  To alleviate the problem of large  V    we may compute an approximate answer using the greedy algorithm 3 with an approximation ratio of ln  Q      ln ln  Q    0 78  25   The time complexity is reduced to O     V      Q   log  Q    3 The greedy algorithm for set cover problem chooses the set which contains the largest number of uncovered elements at each stage  C C C C C C O r C C C C C C C 1 2 N s Fig  4  Example of Minimum Edit Operation Algorithm 2 presents the approximate algorithm and it is guaranteed to return a lower bound of the exact answer  Algorithm 2  MinEditLowerBound  Q  Input   Q is a multiset of q grams  Output   A lower bound of the minimum edit operations that affect all the q grams in Q  1 edit     compute min     edit Q  with the greedy algorithm  2 return edit ln  Q    ln ln  Q  0 78 Algorithm 3  MinEdit  Q  Input   Q is a multiset of q grams  Output   The exact minimum edit operations that affect all the q grams in Q  1 edit     compute exact min     edit Q   2 return edit Example 6  Figure 4 shows the structure of phenol  r  and toluidine  s  molecules  Suppose q is 2  There are three mismatching q grams from s to r  C C C  C C N  and C C N  as bounded in the    gure  At least two minimum edit operations are needed to make these three q grams mismatch  e g   changing the vertex label of C1 and C2  It is noteworthy to mention the following two properties  which are essential to the    ltering techniques we are going to present in the rest of the paper  Let min edit Q  denote the minimum graph edit operations for a multiset of q grams Q  Proposition 1  Monotonicity   min edit Q      min edit Q           ged r  s      Q     Q         Qr Qs  Proposition 2  Disconnectivity   min edit Q1     Q2    min edit Q1  min edit Q2   if    qi     Q1  qj     Q2  qi   qj        B  Minimum Pre   x Length Recall Example 5  although the lower bound of common qgrams is LBpath  it is likely that the minimum edit operations that occur on mismatching q grams have already exceeded     and thus the candidate pair should be discarded  Based on this assumption  our task becomes seeking a minimum pre   x such that at least      1 edit operations are needed to affect all the q grams in the pre   x  In this case  r and s will be guaranteed not to meet the graph edit distance constraint if all the q grams in their pre   xes are mismatched Algorithm 4  MinPre   xLen  Qr  Input   Qr is a sorted multiset of q grams of graph r  Output   The minimum pre   x length of Q  1 left          1  right           Dpath r    1  2 while left   right do 3 mid      left   right  2  4 edit     MinEditLowerBound Qr 1     mid    5 if edit        then left     mid   1  6 else right     mid  7 right     left  left          1  8 while left   right do 9 mid      left   right  2  10 edit     MinEdit Qr 1     mid    11 if edit        then left     mid   1  12 else right     mid  13 return left The monotonicity  Proposition 1  enables us to    nd the minimum pre   x length for a multiset of q grams Qr with a binary search within the range of      1       Dpath r  1   as presented in Algorithm 4  It performs two rounds of binary search  In the    rst round  the greedy algorithm is called to    nd the lower bound of the answer to minimum graph edit operation problem  The result is used as the upper bound of the second round binary search  in which the exact algorithm is applied  Lemma 3  Minimum Edit Filtering   Denote the minimum pre   x length for the q grams of r and s as pr and ps  respectively  If ged r  s          Qr   s pr pre   x and Qs   s ps pre   x must have at least one common q gram  Lemma 3 states the minimum edit    ltering  To apply this    ltering in the join algorithm  we replace Line 6 in Algorithm 1 with    pr     MinPre   xLen Qr      Example 7  Consider s in Figure 1 and its    ve 1 grams sorted according to the order they are listed in Example 3  When    is 1  the minimum pre   x length is 2  while the pre   x length before using minimum edit    ltering is 4  V  LABEL FILTERING In this section  we introduce another approach of exploiting the labels in mismatching q grams  A  Exploiting the Labels in Mismatching q grams Although the minimum edit    ltering can estimate a lower bound of graph edit distance  it works in a pessimistic way assuming the edit operations are scattered  However  it is likely that several edit operations are clustered and incurred by the same mismatching q gram  Example 8  Consider Figure 1       1  and q   1  The two mismatching q grams are bounded in dashed lines  If we compare the labels in the mismatching q gram in the right bounding box with those in r  they already incur at least one edit operation because there is no nitrogen atom  N  in r  Motivated by this idea  we are able to establish a lower bound of graph edit distance from the labels in mismatching q grams  Denoting LV  r  the multiset of the vertex labels in C O C C C F F r O C C C C Cl Cl s Fig  5  Example of Local Label Filtering r  and LE r  the multiset of the edge labels in r  we state the local label    ltering for graph edit distance  Lemma 4  Local Label Filtering   If ged r  s           LV  r       LV  s      LE r       LE s          for any subgraph r     of r  Applying local label    ltering on whole graphs immediately yields global label    ltering  Lemma 5  Global Label Filtering   If ged r  s             LV  r   LV  s        LE r   LE s           where    A  B    max  A    B        A     B   B  Implementation of Local Label Filtering Although the local label    ltering can be applied on any subgraphs  we choose as a heuristic to use it on the subgraphs containing at least one mismatching q gram  since mismatching q grams may imply difference in vertex and edge labels  In addition  we employ minimum edit    ltering to enhance the power of local label    ltering  Recall the disconnectivity of minimum graph edit operations  Proposition 2   the observed mismatching q grams can be articulated and form a set of connected components  We may derive the lower bound of graph edit distance in the whole graph by computing that in each component and summing them up  Algorithm 5  LocalLabelFilter Q  s  Input   Q is a multiset of mismatching q grams from r to s  Output   A lower bound of ged r  s   1 C     the connected components formed by Q  2 total     0  3 for each ci     C do 4 edit loc     MinEdit ci   5 edit con      LV  ci  LV  s      LE ci  LE s    6 total     total   max edit loc  edit con   7 return total Algorithm 5 explains the implementation of the enhanced local label    ltering after including minimum edit    ltering  For each connected component consisting of one or more mismatching q grams  we compute the minimum edit operations that can result in these mismatching q grams using  1  minimum edit    ltering  and  2  local label    ltering  The larger one is then chosen as the ged lower bound within this component  and added up to the total ged lower bound  The time complexity of the algorithm is O  V       Q   E    In case of large V   we may calculate an approximate answer to the minimum edit operation problem with the greedy algorithm  and the time complexity will be O     V    Q   log  Q   E   Example 9  Consider the two graphs in Figure 5       2  and q   2  Global label    ltering yields a lower bound of 2  count    ltering requires the two graphs share at least 2 q grams  while they do share C C C and C C C  Minimum edit    ltering only gives a lower bound of 2 for both r and s  Therefore the pair can pass these three    lters  The two bounded regions in the    gure show the two components formed by jointing the mismatching q grams in r  The edit operations on the left component will be 1  from minimum edit    ltering   and 2 on the right component  from local label    ltering   Therefore the pair can be pruned  VI  VERIFICATION ALGORITHM Our veri   cation algorithm consists of two parts   1  multiple    lters that quickly prune unpromising candidates  and  2  computation of graph edit distance  We introduce both parts in detail  A  Integrating Multiple Filters The veri   cation algorithm for GSimJoin is shown in Algorithm 6  The candidates are veri   ed through three    lters in succession  global label    ltering  Lines 3     4   count    ltering  Lines 5     6   and local label    ltering  Lines 7     9   Those still survive will be veri   ed through the expensive graph edit distance computation  The CompareQGrams algorithm in Line 5 extracts the mismatching q grams from both r to s and s to r  returned in Q    r and Q    s respectively  In addition  we carefully compute the numbers of mismatching q grams in both directions without double counting  and return them in   2 and   3  Algorithm 6  Verify r  A  Input   r is a graph  A is map indicating r   s candidates  Output   S     hr  si   ged r  s            1 S          2 for each s such that A s    true do 3   1        LV  r   LV  s        LE r   LE s        global label filtering    4 if   1        then 5  Q     r  Q     s    2    3      CompareQGrams Qr  Qs       count filtering    6 if   2           Dpath r  and   3           Dpath s  then 7   4     LocalLabelFilter Q     r  s       local label filtering    8   5     LocalLabelFilter Q     s  r       local label filtering    9 if   4        and   5        then 10 edit     GraphEditDistance r  s   11 if edit        then 12 S     S       hr  si    13 return S B  Graph Edit Distance Computation Most widely used exact approaches for computing graph edit distance are based on A  algorithm  11   In this section  we brie   y review a state of the art approach for computing exact ged  20   and then see how our mismatch    ltering techniques can be employed to speed up the algorithm  A  explores the space of all possible vertex mappings between two graphs in a best    rst search fashion with a function  denoted f x   established to determine the order in which the search visits vertex mappings  f x  is a sum of two functions   1  the distance from the initial state to the current state  denoted g x    and  2  a heuristic estimate of the distance from the current state to the goal  denoted h x    A  maintains states in a priority queue  and guarantees the path to the goal is shortest when the goal is popped from the queue  if the h x  function is admissible  i e   h x  is lower than or equal to the real distance from the current to the goal  With no vertex mapped in the initial state  we form a new state in each step by mapping a vertex in r to either a vertex in s  or none to imply a vertex deletion  The goal is to map all the vertices in r  g x  is the graph edit distance between the two partial graphs corresponding to current vertex mapping  For h x    20  gives a lower bound of the graph edit distance between the remaining parts with bipartite matching  The original algorithm is designed for weighted graph edit distance  For our unweighted version  h x  becomes exactly the result of global label    ltering  g x    ged rp  sp   h x       LV  rq    LV  sq        LE rq   LE sq    rp consists of the vertices that have been mapped and the edges connecting them  while rq consists of the vertices unmapped yet as well as their resident edges  1  Exploiting Minimum Edit Filtering  Although A  algorithm adopts a best    rst search scheme to ef   ciently compute graph edit distance  it does not discuss the impact of search order on the ef   ciency of the algorithm  Due to the removal of unpromising candidates with multiple    lters  the pairs veri   ed by A  algorithm are very likely to resemble though they may not satisfy the graph edit distance constraint  The isomorphic part of the graphs do not incur any edit operations  and therefore the goal cannot be found until very late stage of the process if we start searching from this part  In contrast  the process ends more quickly if we start with the part that needs edit operations  Recall the mismatching q grams identi   ed by CompareQGrams algorithm  The mismatching q grams indeed contribute edit operations and hence should be favored  Algorithms 7 exploits this idea and determines the order of vertices to be mapped by the A  algorithm  The vertices contained by at least one mismatching q gram are put before the others  In the interest of connectivity  we break tie by mapping vertices in the order of spanning tree  so as to expedite the discovery of edge edit operations  Using such order leverages the connectivity of a graph and can quickly    nd edge edit operations  E g   assume in r  u and v are adjacent vertices in the spanning tree  and they are mapped to u     and v     in s  respectively  An edge edit operation will occur if there is no edge between u     and v     in s  2  Exploiting Local Label Filtering  Any lower bound of graph edit distance can serve as the heuristic estimate h x Algorithm 7  DetermineVertexOrder r  Q    r   Input   r is graph  Q     r is a multiset of mismatching q grams from r to s  Output   An array of vertices that the A  algorithm will    nd mapping in order  1 M         2 C     the connected components formed by Q     r  3 for each ci     C do 4 Insert vertices in ci into M in the order of spanning tree  5 Insert the vertices not contained by any mismatching q gram into M in the order of spanning tree  6 return M to render the A  algorithm admissible  We consider not only global label    ltering but also local label    ltering in h x   The mismatching q grams in the remaining graphs composed of unmapped vertices are    rst extracted  and then sent into local label    ltering to get lower bounds of graph edit distance between the two remaining graphs  Algorithm 8 provides the pseudocode of the algorithm  Note that we compute mismatching q grams from both rq to sq and sq to rq  and hence have two lower bounds from local label    ltering  The lower bound from global label    ltering is also considered  and the maximum of the three is returned as the result of heuristic estimate  Algorithm 8  EstimateDistance rq  sq  Input   rq and sq are two graphs consist of unmapped vertices Output   A lower bound of ged rq  sq  1   1        LV  rq   LV  sq        LE rq   LE sq    2  Q     r  Q     s      CompareQGrams rq  sq   3   2     LocalLabelFilter Q     r  sq   4   3     LocalLabelFilter Q     s  rq   5 h     max   1    2    3   6 return h VII  EXPERIMENTS In this section  we report experiment results and our analyses  A  Experiment Setup The following algorithms are used in the experiment      GSimJoin is our proposed algorithm that utilizes path based q grams         AT is a state of the art algorithm based on tree based q grams  28   We implemented this algorithm and applied size    ltering  pre   x    ltering  and global label    ltering successively to    nd the candidates that need graph edit distance veri   cation  as they also work for tree based q grams  We ran    AT algorithm with different q gram lengths and found q   1 yields the smallest candidate size as well as the best runtime performance under all our threshold settings  and consequently we choose q   1 for    AT algorithm      AppFull is another state of the art algorithm based on star structure  36   In order to make it support graph similarity joins  we run the the binary code in a nested loop join mode  It iterates through the dataset and selects each graph as a query  and the corresponding database contains all the graphs with smaller identi   ers  The    ltering time for each query is then summed up as the total    ltering time  Edge labels in datasets are omitted when comparing with AppFull as the binary code ignores edge labels  All experiments were carried out on a Quad Core Intel Xeon Processor X3330 2 66GHz with 4GB RAM  The operating system is Debian 5 0 6  All algorithms with source codes were coded in C    We compiled them using GCC 4 3 2 with  O3    ag  and all the algorithms were run in main memory  With respect to q gram storage  we assume the label of a vertex or an edge takes 1 byte  Since a q gram is a path of length q  2q   1 bytes are needed to store a q gram if we concatenate the labels of the vertices and edges in the path  In our implementation  we choose to hash a q gram into a 4 byte integer  This not only controls the index size  but also speeds up equality checking  The only downside is the existence of false positives within candidates due to hash collision  This will not affect correctness but only ef   ciency  We selected two publicly available real datasets with different data distributions      AIDS is the antivirus screen compound dataset from the Developmental Theroapeutics Program in NCI NIH 4   It contains 42 687 chemical compounds  We randomly sample 4 000 graphs to make up the dataset used in the experiment      PROTEIN is the protein database from the Protein Data Bank 5 and labeled with their corresponding enzyme class labels  It contains 600 protein structures  Vertices represent secondary structure elements and are labeled with their types  helix  sheet  or loop   Edges are labeled to indicate whether the two elements are neighbors along the amino acid sequence or neighbors in space within the protein structure  Statistics about the datasets are listed in Table I  AIDS is composed of sparse graphs while those in PROTEIN are denser  TABLE I STATISTICS OF THE DATASETS Dataset  R  avg  V   avg  E  avg  lV   avg  lE  AIDS 4 000 25 6 27 5 44 3 PROTEIN 600 32 6 62 1 3 2 We measure  1  the average length of pre   xes for GSimJoin and    AT   2  the index size for GSimJoin and    AT   3  the candidates formed after probing inverted index for GSimJoin and    AT  denoted Cand 1    4  the candidates that need graph edit computation for GSimJoin and    AT  and the pairs of graphs that pass both lower bound and upper bound tests of AppFull  denoted Cand 2   and  5  the running time  4 http   dtp nci nih gov docs aids aids data html 5 http   www iam unibe ch fki databases iam graph database  download the iam graph database 0  50  100  150  200  250  300  350  400  450  500  1 2 3 4 Prefix Length GED Threshold      PROTEIN Basic GSimJoin   MinEdit  a  PROTEIN  Pre   x Length  0  50  100  150  200  250  300  350  1 2 3 4 Index Size  kB  GED Threshold      PROTEIN Basic GSimJoin   MinEdit  b  PROTEIN  Index Size 10 2 10 3 10 4  1 2 3 4 Cand 1 GED Threshold      PROTEIN Basic GSimJoin   MinEdit  c  PROTEIN  Cand 1 10 0 10 1 10 2 10 3  1 2 3 4 Cand 2 GED Threshold      PROTEIN   MinEdit   Local Label Real Result  d  PROTEIN  Cand 2 10  2 10  1 10 0 10 1 10 2 10 3  1 2 3 4 GED Computation Time  s  GED Threshold      PROTEIN A    Improved Order   Improved h x   e  PROTEIN  GED Computation Time 10  1 10 0 10 1 10 2 10 3 BG ME LL BG ME LL BG ME LL BG ME LL    1    2    3    4 Running Time  s  GED Threshold      PROTEIN GED Computation Candidate Generation Index Construction  f  PROTEIN  Total Running Time 10 4 10 5 10 6 10 7  1 2 3 4 Cand 1 GED Threshold      AIDS 2 gram 3 gram 4 gram 5 gram 6 gram  g  AIDS  Cand 1 10 2 10 3 10 4 10 5 10 6  1 2 3 4 Cand 2 GED Threshold      AIDS 2 gram 3 gram 4 gram 5 gram 6 gram  h  AIDS  Cand 2 10  1 10 0 10 1 10 2 10 3 10 4  1 2 3 4 Running Time  s  GED Threshold      AIDS 2 gram 3 gram 4 gram 5 gram 6 gram  i  AIDS  Total Running Time Fig  6  Experiment Results   I B  Evaluating Filters In order to evaluate the effectiveness of our    ltering techniques  we use the term    Basic GSimJoin     for the GSimJoin algorithm without minimum edit or local label    ltering       MinEdit    denotes applying minimum edit    ltering to compute the pre   x length  and      Local Label    denotes further applying local label    ltering  i e   the complete GSimJoin algorithm  We    rst study the effect of minimum edit    ltering  Figure 6 a  shows the average pre   x lengths of Basic GSimJoin and   MinEdit on PROTEIN dataset with q   3 and varying edit distance threshold    Local Label has the same pre   x length of   MinEdit  so we do not show it in this    gure  The pre   x lengths from both algorithms grow steadily when the threshold increases  The pre   x length has been substantially reduced after applying minimum edit    ltering  and the reduction is more signi   cant when    is small  When      1  the pre   x length can be reduced by 95   As index size is in   uenced by pre   x length  we plot the memory consumed for indexing by the two algorithms in Figure 6 b   Both algorithms need small amount of memory and exhibit a similar trend as on pre   x length  The memory consumed by   MinEdit is only 76 6k when    is as large as 4  The number of Cand 1   s is also in   uenced by pre   x length  as plotted in Figure 6 c  in logarithmic scale  The Cand 1 size can be reduced by as much as 88  when    is 1  As for local label    ltering     gure 6 d  compares the number of Cand 2s produced by   MinEdit and   Local Label on PROTEIN  The number of real join results is also shown  Local label    ltering results in remarkable reduction on Cand 2s  which can be up to 62   C  Evaluating Graph Edit Distance Computation To evaluate the optimization in graph edit distance computation  we choose the candidate pairs generated by   Local Label with the parameters q   4       4  and verify them with different algorithms  The A  algorithm proposed in  20  is labeled as    A      Minimum edit    ltering is exploited to improve the search order  and the result algorithm is labeled      Improved Order     Local label    ltering is further applied to improve heuristic estimate h x  and labeled      Improved h x      Figure 6 e  reports the graph edit distance computation time for the three algorithms with varying     We observe that the optimizations can enhance the time ef   ciency of the ged computation  and the margin is more signi   cant with larger      s  Combining the    ltering algorithms and ged computation algorithms according to the techniques employed  we show the total running time and decompose it into different phases in Figure 6 f   The notations in the    gure denote the following combinations      BG  Basic GSimJoin   A       ME    MinEdit     Improved Order      LL    Local Label     Improved h x   BG has smaller index construction and candidate generation time  but becomes less competitive for large      s in terms of total running time due to its large Cand 2 size and lessef   cient ged computation  LL can be up to 2 1 times faster than ME and 31 4 times faster than BG  D  Evaluating q gram Length We ran GSimJoin algorithm on AIDS dataset with q gram length varying in the range  2  6   and plot the Cand 1  Cand 2  and running time in Figures 6 g     6 i   With respect to Cand 1 and Cand 2  the general trend is that the candidate size    rst drops with an increasing q gram length  reaches the bottom at a q of 3 or 4  and then rebounds  There are several factors contributing to this trend   1  Small q indicates a small q gram domain  and hence the inverted list of a q gram can be fairly long  This will lead to a large candidate size  especially when q is 2   2  Large q indicates a long pre   x length  We have to probe more inverted lists and hence it will increase the candidate size  The second factor explains why the candidate size rebounds for long q grams  It can be seen when q is 6  the candidate size is the actually the largest for most threshold settings  The trend of candidate size re   ects the running time under varying q  As can be expected from candidate size  q   3 or 4 will be the most competitive in total running time  The    gure shows the best runtime performance is achieved when q is 4 for         2  4   The only exception is  when      1  q   2 is the most time ef   cient setting  This is because the generation of q grams and index construction take most of running time for this threshold setting  In the rest of the experiment  we use q   4 on AIDS and q   3 on PROTEIN as they are the most time ef   cient  E  Comparing with    AT We compare GSimJoin with    AT algorithm on both datasets  The average pre   x lengths of both algorithms are shown in Figures 7 a  and 7 b   In spite of a longer pre   x on AIDS  GSimJoin has more average number of q grams in a graph  For example     AT   s pre   x length is 8 2 when    is 1 and the average number of q grams in a graph is 25 6  while GSimJoin   s pre   x length is 8 9 and the average number of q grams is 71 5  This means    AT requires two graphs have an average of 25 6     8 2   1   18 4 common q grams to become a candidate  while GSimJoin needs an average of 71 5   8 9   1   63 6 common q grams  Note that the q gram length is 1 for    AT  i e   the count    ltering of    AT is the tightest among all its q settings  On PROTEIN  the pre   x length of GSimJoin is even shorter than    AT under some parameter settings  Both algorithms are competitive in index sizes  as shown in Figures 7 c  and 7 d   Figures 7 e      7 h  give the Cand 1 and Cand 2 sizes of the two algorithms  GSimJoin performs better than    AT on both Cand 1 and Cand 2 sizes  There are three main factors   1  4 grams based on paths are more selective than 1 grams based on trees  This results in a less number of Cand 1s for GSimJoin   2  GSimJoin   s count    ltering constraint is stricter than    AT   s   3  GSimJoin employs local label    ltering to further prune candidates  The last two factors contribute to GSimJoin   s advantage on Cand 2 numbers  The running time of both algorithms are shown in Figures 7 i  and 7 j      AT    and    GS    are short for    AT and GSimJoin respectively      AT shows better index construction time and candidate generation time as GSimJoin needs minimum edit and local label    ltering to build index and prune candidates  However  GSimJoin is always better than    AT in terms of total running time  and the gap is more substantial under large    settings  The speed up on AIDS is 6 6x and 80 6x on PROTEIN  The latter one showcases the superior time advantage of GSimJoin on denser graphs  F  Comparing with AppFull We compare GSimJoin with AppFull on both datasets and plot the number of Cand 2s and running time in Figures 7 k      7 n      AF    and    GS    are short for AppFu</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10is2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10is2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Indexing_and_storage_management"/>
        <doc>Workload Aware Storage Layout for Database Systems ### Oguzhan Ozmen  Kenneth Salem University of Waterloo Waterloo  ON Canada  oozmen  kmsalem  uwaterloo ca Jiri Schindler  Steve Daniel NetApp  Inc   First Last  netapp com ABSTRACT The performance of a database system depends strongly on the layout of database objects  such as indexes or tables  onto the underlying storage devices  A good layout will both balance the I O workload generated by the database system and avoid the performance degrading interference that can occur when concurrently accessed objects are stored on the same volume  In current practice  layout is typically guided by heuristics and rules of thumb  such as separating indexes and tables or striping all objects across all of the available storage devices  However  these guidelines may give poor re  sults  In this paper  we address the problem of generating an optimized layout of a given set of database objects  Our lay  out optimizer goes beyond generic guidelines by making use of a description of the database system s I O activity  We formulate the layout problem as a non linear programming  NLP  problem and use the I O description as input to an NLP solver  Our layout optimization technique  which is in  corporated into a database layout advisor  identi es a layout that both balances load and avoids interference  We evaluate experimentally the e cacy of our approach and demonstrate that it can quickly identify non trivial optimized layouts  Categories and Subject Descriptors H 2  Database Management   Physical Design General Terms Performance ### 1  INTRODUCTION Database management systems  DBMS  rely on an un  derlying storage system for persistent storage of database objects such as tables  indexes  and logs  The storage sys  tem typically provides a set of RAID groups  groups of stor  age devices in some kind of RAID con guration  or individ  ual storage devices  such as disk drives or solid state drives  SSDs   We will refer to these as storage targets  by which we mean independent containers into which data can be stored  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   10 June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM     10 00  In this paper  we address the problem of laying out  or map  ping  database objects onto these targets  A good layout should result in a balanced load across the storage targets  otherwise  the most heavily loaded target may become a performance bottleneck while storage band  width available from other targets is wasted  It should also minimize interference between requests for di erent ob  jects that are laid out on the same target  For example  if two sequentially accessed database objects are laid out on the same target  interference among I O requests for those two objects may prevent the underlying storage sys  tem from exploiting the sequentiality  thus increasing I O service times  2  25   Finally  a good layout should re ect the di erent characteristics of the available targets  For example  RAID groups may vary in con guration  e g   in the RAID level used  or in the number of devices in the group  and hence in performance  Heterogeneity may arise as new storage devices are added to storage systems over time  since the more recently added devices are likely to be larger and faster  Heterogeneity may also occur by design  It is not uncommon to have mixed storage systems containing both enterprise class 15K RPM disk drives targeted for high throughput of small random I O requests and cost e ective nearline 7200 RPM disks  better suited for sequential ac  cesses  Similarly  a  ash memory SSD storage target will have much better random I O performance than one imple  mented using disk drives  The promise of good performance for database workloads with SSDs  13  suggests that storage systems will likely continue to include heterogeneous con g  urations with SSDs or high end disks for small random I Os and cost e ective disk drives for large sequential accesses  A layout that fails to account for these kinds of heterogeneity may result in poor performance  In current practice  layout is based primarily on heuris  tics and rules of thumb  For example  one strategy is to distribute every database object across all available targets  an approach that is known as stripe everything everywhere  SEE   18  22   For homogeneous storage systems this may indeed be a good way to balance load  but it ignores the interference problem and it is not clear how to apply it ef  fectively in the presence of heterogeneous targets  Another strategy is to lay out tables  indexes  and logs on separate targets in order to reduce interference among them  How  ever  this leaves unresolved the question of which target s  to use for each type of object  and poor choices may lead to lower performance due to load imbalances  In this paper we propose a technique for identifying opti  mized database layouts automatically  Our technique  which 939could be implemented as a standalone database storage lay  out advisor  makes use of information about the I O access patterns of each database object and employs storage target models that allow it to recognize the targets  performance characteristics  This paper makes the following contribu  tions    We formulate the database storage layout problem as a non linear optimization problem incorporating the im  portant characteristics of the storage workload and of the underlying storage targets    We propose a technique for solving the layout prob  lem to identify good layouts  Our technique exploits a generic non linear program  NLP  solver as well as heuristics speci c to the layout problem    We present an experimental evaluation of the e cacy and e ciency of our technique  The rest of the paper is structured as follows  Section 2 presents a scenario which illustrates the e ect of object lay  out on DBMS performance  Section 3 de nes the layout problem and Sections 4 and 5 describe our technique for lay  out optimization  Section 6 presents an empirical evaluation of the technique s e cacy and cost  Section 7 summarizes related work on the layout problem  and Section 8 concludes and brie y discusses some future directions for this work  2  LAYOUT EXAMPLE Before we present our layout advisor  we illustrate workload  aware database layout with an example of a PostgreSQL database system managing a small TPC H  8  database  Our system has four identical storage targets  each of which is a single disk drive  The TPC H database has 20 di erent objects  including tables  indexes  and a tablespace for tem  porary objects  that must be laid out on the four targets  The PostgreSQL database runs our OLAP1 63 workload  which consists of a sequence of TPC H benchmark queries  Section 6 provides more detailed descriptions of both the OLAP1 63 workload and the system  Figure 1 shows two di erent layouts of the database ob  jects across the four storage targets  For clarity  we only show the layouts of the eight most heavily accessed objects  which include four tables  three indexes two of which are de ned on the LINEITEM table and the other on the ORDERS table  and the tablespace for temporary objects  The illus  tration on the left shows the stripe everything everywhere  SEE  layout  which distributes each object evenly across all four of the disks  The illustration on the right shows the layout that is recommended by our layout advisor based on the I O workloads of each of the objects and the perfor  mance characteristics of the devices  In the optimized layout  the two most heavily accessed tables  LINEITEM and ORDERS  are isolated from one another  Both tables exhibit sequential access patterns  and they tend to be accessed concurrently  so isolating these tables from one another helps to avoid interference  LINEITEM is laid out on more targets than ORDERS because LINEITEM experiences a greater I O load  The next most heavily accessed object  I L ORDERKEY  is also isolated from both LINEITEM and OR  DERS to avoid interference with those objects  sequential ac  cess patterns  The TEMP SPACE  temporary table space  ob  ject also experiences a sequential workload  Had more stor  age targets been available  the layout optimizer may have TABLES INDEXES TEMPORARY SPACE ORDERS TEMP SPACE ORDERS PKEY PARTSUPP I L SUPPK PARTK PART Baseline  Stripe   Everything   Everywhere Advisor Recommended Layout LINEITEM ORDERS I L ORDERKEY LINEITEM I L ORDERKEY TEMP SPACE ORDERS PKEY PARTSUPP I L SUPPK PARTK PART Figure 1  SEE and Optimized Layouts of TPC H Database Objects  chosen to isolate it as well  Since this is not possible  TEMP SPACE is placed on the same target as ORDERS as these two objects are rarely accessed simultaneously in this workload  The remaining objects  which experience low I O loads  are placed on the least heavily loaded targets  On our system  the OLAP1 63 workload under the SEE layout completes in 40927 seconds  Under the optimized lay  out  the same workload completes in 31879 seconds  a 1 28x speedup  While this reduction may not seem dramatic  the scenario we have constructed is very friendly to SEE since the available storage targets are homogeneous  Even in this environment  an optimized layout can improve performance  The bene ts of an automatic  workload aware layout advi  sor are more signi cant in situations for which SEE or other similar heuristics are poorly suited  for example  when the storage targets are heterogeneous  Section 6 shows that the bene ts of layout optimization increase in these cases  3  PROBLEM FORMULATION We assume that the storage system provides M disjoint storage targets  and we use cj to denote the capacity of the jth target  The exact nature of the targets is not important to our layout advisor  It cares only that each storage target has an associated performance model  as we will describe shortly  and that the performance of each storage target is independent of the performance of the other targets  In an enterprise class storage system  10  12  17   a target might correspond to a RAID group  i e   a set of storage devices in a particular RAID con guration  In smaller scale settings  an individual storage device directly attached to a server could be a storage target  The layout advisor is given N disjoint database objects that are to be laid out on the available storage targets  We use si to denote the size of the ith object  Again  the exact nature of the database objects is not important  They could be tablespaces  individual tables  indexes  or logs  All of the objects may be part of the same database or they may originate from di erent databases  A layout L is an N   M matrix  where 0   Lij   1 represents the fraction of object objecti that is assigned to target targetj   We are interested only in valid layouts  which are layouts that satisfy two additional constraints  capacity constraint  PN i 1 siLij   cj   8j  1   j   M integrity constraint  PM j 1 Lij   1  8i  1   i   N The capacity constraint ensures that a target is not assigned more data than it can hold  and the integrity constraint 940Resources Storage Storage Target 1 Storage Target 2 Storage Target M Database Object 1 Object 2 Object N Objects       Figure 2  Laying out Objects onto Storage Targets  Symbol Description N Number of database objects si Size of ith object M Number of storage targets ci Capacity of ith target Wi I O workload for the ith object  j Utilization of the jth storage target Figure 3  Summary of Layout Problem Parameters ensures that each object is mapped in its entirety to the storage target s   Figure 2 illustrates the layout of database objects onto storage targets  The goal of the layout advisor is to identify a good lay  out from among all possible valid ones  Speci cally  the objective is to minimize the maximum utilization across all storage targets  This objective encourages the advisor to identify balanced layouts  since the most heavily utilized tar  get determines the quality of the layout  It also encourages the advisor to avoid interference  since interference increases I O request service times and hence storage target utiliza  tions  Finally  it encourages the advisor to consider the per  formance characteristics of the targets as it makes layout decisions  For example  with a mix of fast and slow storage targets in the system  good layouts will tend to place more load on the faster targets than on the slower ones  To determine the storage target utilizations resulting from a candidate layout  the advisor needs information about the I O workload  We assume that the layout advisor is given a workload description for each database object  and we use Wi to denote the workload description for the ith object  An object s workload description characterizes the the object s I O activity  For example  it describes the I O request rate for the object  the mix of reads and writes  the sequentiality of the requests  and other properties  Section 5 1 describes the I O workload model in more detail  To predict storage target utilizations for a given workload  the layout advisor also needs a performance model for each target  We use  j  W1          WN  L  to denote the model  predicted utilization of the jth storage target under the I O workloads W1          WN and layout L  To simplify our no  tation  we will refer to the utilization of the jth target as  j when the workloads and layout on which it depends are clear from the context  We defer further discussion of per  formance models and the calculation of  j to Section 5 2  Figure 3 summarizes the layout problem parameters  We can now state the layout problem  Definition 1  Database Object Layout Problem Given N database objects  M storage targets  and an I O workload description Wi for each object   nd a valid layout L that minimizes M max j 1  j  W1          WN  L  Once the layout advisor has recommended an optimized layout  a variety of mechanisms can be used to implement the layout  Most storage systems provide mechanisms for de ning logical volumes  which are mapped to underlying storage targets  A recommended layout L can be imple  mented by de ning logical volumes for database objects  or for groups of database objects that have the same layout in L  The storage system s mechanism for mapping logical volumes to the underlying RAID groups or devices can then be used to implement L  On the host side  operating system supported logical volume managers  LVMs  can be used to implement a layout in essentially the same way  Finally  at the application level  many database systems are capable of distributing database objects  e g   tablespaces  across con  tainers  e g    les or raw devices  provided by the underlying operating system  18   By de ning one or more containers for each object and placing them appropriately on the avail  able storage targets  a database administrator can use this mechanism to implement the recommended layout  Some layout mechanisms are more limited than others in the types of layouts that they can support  For example  some mechanisms use round robin striping of objects  How  ever  this always distributes an object evenly across some set of targets  A layout that maps  for example  20  of an object to one target  30  to a second target  and 50  to a third target is a valid mapping according to our de nitions  but it cannot be implemented by such a mechanism  For this reason  we provide the ability to optionally restrict the layout advisor to regular layouts  Definition 2  Regular Layout A regular layout is a valid layout in which  for every pair of elements Lij and Lik  either Lij   0 or Lik   0 or Lij   Lik  In a regular layout  each object is distributed evenly across one or more storage targets  For example  50  of the object may be placed on each of two targets  or 25  may be placed on each of four targets  4  LAYOUT OPTIMIZATION The two key elements of our technique for solving the layout problem are  i  its formulation as a non linear pro  gramming  NLP  problem and  ii  the use of a generic NLP solver  By using a generic solver  we can directly leverage the solver s techniques for exploring the optimization space  Explicitly formulating layout as a non linear programming problem makes it easy to incorporate additional constraints on the resulting layout  For example  if administrative con  straints require certain objects to be laid out onto particu  lar targets  we can easily add such constraints to the NLP problem before solving it  As discussed in Section 6  we also found the generic solvers to be fast and e ective x 4 1 Solver We use AMPL  11   a standard mathematical modeling language  to formulate the layout problem as a non convex NLP  There exist several general solvers designed to solve 941Looking for a regularized solution  no Final Solution Find a valid initial layout Regularize the solver   s layout Call an NLP solver no yes yes repeat  Figure 4  Layout Algorithm  non convex optimization problems with non linear objective functions and non linear constraints  We use the MINOS solver  16   because it supports the use of external  i e   non  AMPL   black box functions as part of the de nition of the objective function  We leverage this feature for our perfor  mance models as described in Section 5  In order to use a generic NLP solver like MINOS  we must address several issues  First  we need to provide a valid initial layout  This step is described in more detail in Section 4 2  Second  the solver will  nd a valid  optimized layout  but the resulting layout may not be regular  If a regular layout is required  then we need a way to regularize the solver s solution  This regularization step is described in Section 4 3  Figure 4 summarizes the behaviour of our layout optimizer  Many NLP solvers  like MINOS  are not guaranteed to identify a globally optimal solution  Moreover  the solution found by the solver may depend on the choice of the initial layout  Thus  it is possible to improve the quality of the op  timized layout  at the cost of additional optimization time  by repeating the optimization process using di erent initial layouts  The optimization procedure described in Figure 4 incorporates this iteration  A potential advantage of using multiple initial layouts is that they o er a convenient way of introducing the knowledge of domain experts into the opti  mization process  For example  if a knowledgeable database administrator expects that certain layouts might perform well  those layouts can be used as initial layouts  We have not yet explored this possibility in our layout advisor  all of the results presented in Section 6 use a single initial layout  The choice of a suitable initial layout is detailed below  4 2 Initial Layout Originally  we experimented with SEE as the initial layout as it is simple and balanced  However  it often represents a local minimum in the search space  and we found that MI  NOS at times had di culty breaking out of this minimum  Therefore  SEE is not an ideal choice if the layout advisor uses only a single initial layout  To avoid this problem  we switched to a simple heuristic for choosing an initial lay  out  placing database objects based on their sizes and total request rates  An object s total request rate is one of its workload characteristics  which we describe in more detail in Section 5  The initial con guration is chosen by laying out one object at a time  in decreasing order of request rate  Each object is assigned to the target that has the lowest total assigned request rate from among those targets that have su cient re  maining storage capacity to hold the object  This approach assigns each object to a single storage target  By assigning each object to the least loaded target  we hope to obtain an initial layout that is at least approximately balanced  How  ever  the heuristic does not make any attempt to minimize interference  nor does it take into account the performance characteristics of the storage targets  4 3 Regularization If non regular layouts can be handled by the storage sys  tem  operating system or database system that is respon  sible for implementing them  then the layout generated by the NLP solver can be implemented directly and there is no need for regularization  For systems that only support reg  ular layouts  the layout advisor needs to perform the  nal regularization step  One way to generate regular layouts would be to add reg  ularization constraints directly to the NLP problem formu  lation  However  such constraints would e ectively turn a continuous optimization problem  each of the decision vari  ables Lij is a continuous variable in the range 0   Lij   1  into a combinatorial problem  With the addition of regular  ization constraints  there are up to 2 M  1 possible layouts for each object  and hence O 2 MN   possible layouts for all objects  To solve the regularized problem e ectively  we should use a solver that is intended for combinatorial prob  lems  Instead  we use the solver to identify an optimized non regular layout and then apply a post processing step to regularize the optimized layout  The regularization algorithm starts with the non regular optimized layout produced by the solver and regularizes the layout of one object at a time  until all objects  layouts are regular  The algorithm regularizes the objects in decreasing order of the total storage system load  for object i  P j  ij   they impose on the storage system  By considering the ob  jects in this order  load imbalances introduced by regulariz  ing the initial objects can potentially be corrected by the reg  ularizations of subsequent objects  Load imbalances created by regularization of the  nal objects will be uncorrectable but relatively small  To choose a regular layout for a particular object  the algorithm generates a small set of candidate regular lay  outs  Two classes of candidates are generated  The  rst class consists of all regular layouts of the object that are consistent with the layout chosen by the solver  By con  sistent  we mean that if Lij   Lik in the original layout generated by the solver  then only regular layouts for which Lij   Lik will be candidates  For example  if there are 3 storage targets and the solver lays out an object as follows   47   35   18    then the regularization algorithm will con  sider only the following regular layouts   100   0   0     50   50   0    and  33   33   33    1 The second class of candidates consists of regular layouts that place the ob  ject on the least loaded targets  according to the current layout  Speci cally  the regularizer considers layouts that 1 In case Lij   Ljk in the solver s layout  the tie is broken arbitrarily  using target identi ers  942 Parameter Symbol Description Request Size B R i  B W i Avg  read write request sizes Request Rate   R i    W i Avg  read write request rates Run Count Qi Avg  number of requests in a sequential run Overlap Oi j  Temporal correlation of I O requests in this workload with I O requests from the jth workload  Figure 5  Parameters of ith Object s Workload  Wi  assign 100  of the object to the least loaded storage target  50  of the object to each of the two least loaded targets  and so on  We call these balancing layouts  since they tend to correct load imbalances that may arise from the regular  ization of previous objects  For each object  there will be 2M candidate regular layouts  M of each class  From this set of candidates  the regularizer eliminates any layouts that would result in violations of the targets  space constraints  and from the remaining valid layouts it chooses the one that minimizes the optimization objective  i e   the regular lay  out that minimizes the maximum utilization of the storage targets  It is possible that  as a result of space constraints  all of the regular layouts considered by the regularization algorithm for some object will be invalid  In this case  the regulariza  tion algorithm may fail to generate a regular layout  even if such a layout does exist   and manual intervention would be necessary  In practice  this is unlikely to be a problem unless space constraints are very tight  5  WORKLOAD AND TARGET MODELS As described in Section 3  the layout advisor relies on workload descriptions and storage target performance mod  els  We use models that are based on ideas that were de  veloped at HP Laboratories  and which were used there as the basis of a series of storage management tools  such as Minerva  3  and Ergastulum  5   We call them  Rome style  models as they use the Rome language  28  to describe work  loads and storage devices  For our purposes  we need to ensure that our Rome style models are compatible with the NLP solver  5 1 Workload Model The layout advisor s input includes a set of workload de  scriptions  Wi  one for each database object  Each workload is modeled as a stream  sequence  of block I O requests  The requests in the stream are characterized by a set of parameters summarized in Figure 5  The values of these parameters constitute the workload description  The run count parameter describes the sequentiality of the workload  It describes the number of sequential requests that occur be  tween non sequential jumps  so higher run counts indicate sequential workloads  The Overlap parameters characterize the level of temporal overlap between the workload s I O re  quests and the I O requests of other workloads  The value of the Overlap parameters ranges within  0 1   with Oi j    0 meaning that requests from Wi never overlap with those of Wj and Oi j    1 indicating that they always do  Overlap  ping streams are potentially subject to interference if they are laid out on the same storage targets  Wi1 WiM Target 1 Target M Wi W1j W2j WNj    1j    2j    Nj      j Layout Model Target Model j  a   b  Layout Figure 6  Storage System Model There are several ways to obtain these workload descrip  tions  We obtained them by collecting a trace of I O activity from the operational database system  We then isolate the requests related to a single object from that trace and de  termine values of the workload parameters for that object by  tting them to the observed characteristics of the traced requests  We used a trace analysis tool called Rubicon  26  to do the parameter  tting  Another possibility is to directly infer the storage work  load descriptions using knowledge of the database system and its workload and a tool called a storage workload esti  mator  19   This allows storage workload descriptions to be generated without actually running the workload and collecting traces  However  the resulting descriptions may be less accurate than those obtained using the trace based method  5 2 Storage System Model The layout advisor requires a model that can predict the utilization of a storage target  given descriptions of the work  loads and a candidate layout  In theory  any function that maps workloads and layout to utilization could be used  In practice  the models we use have a particular internal struc  ture  as illustrated in Figure 6  These models de ne the NLP solver s objective function  As depicted in Figure 6 a   a layout model takes as an input an object s workload  Wi  and the candidate layout  L  and produces a description of the workload that will be generated by that object on each storage target  We use Wij to denote the workload imposed by the ith object on the jth storage target under a given layout  The layout model  which is described in more detail in Section 5 2 1 describes the distribution of the object s workload that is implied by the candidate layout  The advisor applies the layout model to each object s workload  As a result  each storage target has a set of in  coming workloads  one from each object  as illustrated in Figure 6 b   Given these workloads  a target model  which is described in more detail in Section 5 2 2  is used to produce an estimate of the storage target utilization attributable to each input workload  There may be a di erent model for each target type or even for each individual target  The storage target s total utilization  which the solver seeks to minimize  is the sum of those loads  Recall that  ij denotes the utilization of the jth target that is attributable to the ith object  and  j represents the jth target s total utilization  One advantage of this structure is that the layout model is relatively simple and can be readily expressed in AMPL  This exposes the characteristics of this part of the model to the NLP solver  The target model  on the other hand  must 943B R ij   B R i B W ij   B W i   R ij     R i Lij   W ij     W i Lij Qij   8     Qi if QiBi   StripeSize QiLij if QiBi   StripeSize Lij StripeSize Bi otherwise Oij  k      Oi k  if Lij   0 and Lkj   0 0 otherwise Figure 7  Layout Model for LVM Using Striping  StripeSize is the LVM stripe size  and Bi is the request  rate weighted average of BR i and BW i   capture the potentially complex performance characteristics of the target  These models are not expressed in AMPL  but rather as external functions invoked by the MINOS solver  This allows us to  plug in  models for di erent targets with  out changing the AMPL formulation of the rest of the NLP problem  The remainder of this section describes the layout and target models in more detail  5 2 1 Layout Model The layout model describes how to transform an object workload into per target workloads  given a particular lay  out  The per target workload descriptions Wij are of the same form as the original workload descriptions Wi  Fig  ure 5   Thus  a layout model describes how to calculate each of the parameters  e g   RunCount  Request Rate  of Wij from the parameters of Wi and the layout  The layout model must capture the e ects of whatever system is responsible for implementing the layout  This may be a RAID controller  a logical volume manager in the host operating system  or an underlying storage system  For the experiments reported in Section 6  layouts were implemented by the logical volume manager  LVM  in the host operating system  As con gured  the LVM divides objects into  xed  size stripes and distributes the stripes round robin to the underlying storage targets  Our layout model for this LVM is shown in Figure 7  5 2 2 Target Model A target model estimates the storage target utilization imposed by each workload   ij    the sum of which is the total utilization of the storage target   j    We construct two request cost models for each type of target  one model for read requests and the other for write requests  We then estimate  ij using  ij     R ijCost R j     W ij Cost W j  1  where Cost R j and Cost W j are the read and write request costs for the jth storage target  The read and write request costs for workload Wij de  pend on the characteristics of the jth target storage device and the properties  request sizes  run count  of the work  load  Furthermore  because of the potential for interference among workloads  the request costs also depend on the other workloads Wkj that impinge on the same target  Thus  in general the cost model will depend on a large number of factors  To eliminate some of this complexity  we assume that the amount of interference with the requests from work  Figure 8  Cost Model for 8 KByte Read Requests load Wij depends on the number of competing requests from other workloads  and not on the properties  request size  se  quentiality  of those competing requests  This allows us to compute  for each workload Wij   a contention factor   ij   which captures the total amount of interference from all other workloads on target j   ij   P k 6 i    R kj     W kj  Oij  k     R ij    W ij    2  Essentially   ij is a measure of the number of temporally  correlated requests from other workloads  Wkj   per request from workload Wij   With this simpli cation  the per request costs Cost R j and Cost W j are functions of the properties of storage target j and three workload parameters  request size  run count  sequen  tiality   and the contention factor  To estimate these costs  we construct two cost models  one for reads  one for writes  for each type of device  parametrized by the three workload parameters  It is possible  but di cult  to build accurate an  alytic cost models  24  25   Instead  we construct black box models based on interpolation among tabulated measure  ments of storage target performance  Others have used sim  ilar techniques to build black box storage device models  4  15  27   We construct the models by subjecting the storage targets to calibration workloads with known request sizes  run counts  and degrees of contention and measuring the re  quest service times  which are then tabulated  To estimate Cost R j or Cost W j given a set of workload parameter values  we look up the tabulated cost from the appropriate model  interpolating among nearby calibration points if necessary  Although the behavior of storage devices can be com  plex and highly non linear  the generality of the tabula  tion interpolation approach allows us to model them accu  rately  Figure 8 shows one  slice  of the read request cost model for the SCSI disk drives used in our experiments  This slice corresponds to read requests of size 8 KBytes  The  g  ure shows request costs as a function of the contention factor  Each curve corresponds to a di erent run count  degree of sequentiality  in the workload  This slice of the model illus  trates several interesting e ects  When contention is low  se  quential requests are signi cantly faster than non sequential ones  as expected  The sequential advantage is preserved in the face of a small amount of contention because the device is able to track and read ahead on a small number of concur  rent sequential streams  However  the advantage collapses 944Number of Objects Total Temp DB Size Tables Indexes Space Log TPC H 9 4GB 8 11 1 0 TPC C 9 1GB 9 10 0 1 Figure 9  Databases Used in the Experiments Number of Concurrency Target Workload Queries Level Database OLAP1 21 21 1 TPC H OLAP1 63 63 1 TPC H OLAP8 63 63 8 TPC H OLTP n a 9 TPC C Figure 10  Query Workloads Used in the Experi  ments quickly completely when the contention factor reaches 2  The cost of non sequential requests  RunCount   1  gradu  ally decreases with increasing contention because disk head scheduling is more e ective when there is a larger request queue  6  EVALUATION In this section  we present an experimental evaluation of the layout algorithm  Our primary goal is to evaluate the quality of the optimized layouts that are recommended by the layout advisor  In addition  we consider the time required by the layout advisor to produce a recommenda  tion  Finally  we compare our NLP based layout advisor to a previously proposed technique  2  for laying out relational databases  6 1 Experimental Setup To evaluate the quality of the optimized layouts  we com  pare the performance of a database system that uses an optimized layout to the performance of the same database system using a baseline layout  The baseline layouts are simple heuristic layouts that make little or no use of work  load information  The objects to be laid out are individual database tables  indexes  logs  and tablespaces for tempo  rary objects  In most cases  our performance metric is the total elapsed  wall clock  time required to execute a partic  ular set of queries  In addition to the elapsed times  which are our primary metric  we also record the estimated stor  age target utilizations   j   that are used internally by the layout advisor to judge the quality of layout  We use these estimated utilizations to illustrate and explain the advisor s behaviour  In our experiments  we used the PostgreSQL database management system  version 8 0 6  We used two databases  a scale factor 5 TPC H database and a scale factor 90 TPC  C database  The characteristics of the objects in these databases are shown in Figure 9  We de ne four SQL work  loads running against these databases  as shown in Fig  ure 10  The OLAP1 21 workload consists of 21 of the 22 TPC H benchmark queries   TPC H query Q9 was excluded because of its excessive run time in our system   In OLAP1  21  the queries are executed sequentially in a randomly se  lected order  with no think times  OLAP1 63 is simply a longer variant of OLAP1 21 in which each TPC H query oc  Baseline  SEE  Optimized Execution Execution Workload Time  seconds  Time  seconds  Speedup OLAP1 63 40927 31879 1 28x OLAP8 63 16201 13608 1 19x Figure 11  Workload Execution Times for Baseline and Optimized Layouts on Homogeneous Storage Targets PARTSUPP PART CUSTOMER ORDERS PKEY ORDERS LINEITEM I L ORDERKEY TEMP SPACE Figure 12  Optimized Layout for the OLAP8 63 Workload curs three times in the mix  The entire mix is randomly per  muted and the queries are executed sequentially  OLAP8 63 is the same as OLAP1 63 except that queries are executed with a concurrency level of eight  That is  whenever a query  nishes  the next query in the sequence is started so that eight queries are active at all times  The OLTP workload is generated by nine simulated terminals  with no think time or keying time  We ran PostgreSQL on a Dell PowerEdge 2600 server with two 2 4GHz Intel Xeon processors and 4GB of main mem  ory  running SUSE 10 0 Linux with kernel version 2 6 21 7  We instrumented the kernel so that we were able to obtain the I O request traces that we used to build our workload models  as described in Section 5  The size of the database system s shared bu er is set to the maximum allowable value  2GB  for the three TPC H workloads  and to 1 5GB for the OLTP workload  The server has a 70GB 15K RPM SCSI disk which holds all system software  including PostgreSQL itself  In addition  it has four 18 4GB 15K RPM SCSI hard drives behind a con gurable Dell Perc 4Di RAID controller and a 32GB solid state drive  SSD  behind a 3Gb s Koutech SATA II controller  We lay out the TPC H and TPC C database objects on various combinations of the four 18 4GB hard drives and the SSD in our experiments  6 2 Layout Quality  Homogeneous Targets In this experiment  we compare DBMS performance un  der advisor recommended layouts with performance under a baseline stripe everything everywhere  SEE  layout  We used the OLAP1 63 and OLAP8 63 workloads  For each workload  the layout advisor is asked to recommend a layout of the TPC H database objects onto four identical storage targets  the four 18 4GB disk drives attached to our server  Figure 11 summarizes the results of this experiment  Al  though this scenario is well suited to SEE  the optimizer is able to obtain some performance improvement for both workloads  In Figure 1  Section 2  we illustrated the opti  94567 1  58 5  Figure 13  Estimated Utilizations Under the OLAP8 63 Workload 32 8  34 5  32 6  100  25 5  25 8  30 3  100  100  100  100  51 3  31 5  10 5  6 7  95 1  4 9  100  100  100  100  100  100  23 6  25 1  17 8  24 2  27 7  I L SK PK CUST  PART PARTSUPP ORDERS PKEY TEMP SPACE I L ORDERKEY ORDERS LINEITEM  a  OLAP1   63  b  OLAP8   63 Figure 14  Layouts Produced by the NLP Solver Under the OLAP1 63 and OLAP8 63 Workloads mized layout that the advisor recommends for the OLAP1  63 workload  Figure 12 shows the optimized layout for the OLAP8 63 workload  As was the case in Figure 1  the ob  jects are shown in decreasing order of request rate  and only the most heavily requested objects are shown  For both the OLAP8 63 workload and the OLAP1 63 workload  the rec  ommended layouts separate the two most heavily used ob  jects  LINEITEM and ORDERS   For OLAP8 63  the workload on LINEITEM is less sequential than it is under OLAP1 63  because of query concurrency  As a result  the performance penalty for interference with LINEITEM is lower under the OLAP8 63 workload  and LINEITEM is not completely iso  lated in the OLAP8 63 layout  Instead  the optimizer dis  tributes I L ORDERKEY and TEMP SPACE across all of the tar  gets to better balance the load  Figure 13 illustrates the behaviour of the layout advisor by showing the quality of the layouts that it considers at di erent stages of its execution  In the  gure  each group of bars shows the estimated utilization   j   of one of the four storage targets  The leftmost bar in each group shows the advisor s estimated utilization for the baseline SEE layout  for the purposes of comparison  The second bar in each group shows the estimated target utilization under the initial workload generated by the layout advisor as a starting point for the NLP solver  These initial layouts assign each object to a single storage target  They tend to be unbalanced  as is the case for both OLAP8 63 and OLAP1 63  because the advisor does not take interference  workload sequentiality  and other factors into account when generating them  SEE Improvement Baseline Optimized Optimized Workload Performance Performance vs  SEE OLAP1 21 24416 sec  17005 sec  1 43x OLTP 304 tpmC 360 tpmC 1 18x Figure 15  Consolidation Scenario Performance Un  der Baseline and Optimized Layouts on Homoge  neous Storage Targets STOCK  c  CUSTOMER  c  I CUSTOMER  c  I ORDERS  c  PK CUSTOMER  c  I L ORDERKEY  h  XactionLOG  c  PK STOCK  c  ORDERS  h  PK ORDER LINE  c  TEMP SPACE LINEITEM  h  Figure 16  Optimized Layouts of the TPC H  h  and TPC C  c  Objects for the Consolidated Workload on Homogeneous Storage Targets The third bar in each group shows the estimated uti  lization for the  non regular  layout identi ed by the NLP solver  The solver s layouts for both OLAP1 63 and OLAP8  63 are shown in Figure 14  These layouts are very balanced and they reduce utilizations relative to the default SEE lay  out  If the layout mechanism supports non regular layouts  then the layouts shown in Figure 14 can be implemented directly  If not  then the  nal regularization step is needed  The fourth bar in each group in Figure 13 shows the estimated utilizations under the  nal regularized layout that the ad  visor produces in its post processing step  In general  these layouts will be less balanced than those produced by the solver  as the regularization process disturbs the balanced layout produced by the solver  and it may be unable to com  pletely correct these disturbances  However  in the case of the OLAP8 63 workload  the solver s layout  Figure 14 b   is almost regular  and the resulting regularized layout  Fig  ure 12  is very close to the solver s  6 3 Layout Quality  Consolidation Scenario In these experiments  we considered a consolidation sce  nario in which two database system instances run on the server  One instance runs our OLAP1 21 workload and the second runs our OLTP workload  In this experiment  we measure the performance of the OLAP1 21 workload by measuring the wall clock time required to complete the workload queries  For the OLTP workload  we measure performance in TPC C New Order transactions per minute  tpmC   We run the OLTP workload until the OLAP1 21 workload  nishes  The reported tpmC rate is the average rate over the lifetime of the experiment  minus an initial warm up period of 1600 seconds  In this scenario  there are a total of 40 database objects to be laid out  including 20 from the TPC H database and 946Baseline Baseline Baseline Optimized Storage  SEE   isolate tables   isolate tables   indicies  Execution Speedup Target Execution Execution Execution Time  Optimized vs  Con g Time  seconds  Time  seconds  Time  seconds   seconds  SEE  3 1 18103 14507 n a 13317 1 36x 2 1 1 16922 n a 22359 13163 1 29x 1 1 1 1 16201 n a n a 13608 1 19x Figure 17  Workload Execution Times for Baseline and Optimized Layouts on Heterogeneous Storage Targets  for the OLAP8 63 Workload 20 from the TPC C database  We used the layout advisor to generate an optimized layout of the 40 objects onto the four 18 4GB disk drives  We compared this layout against a SEE baseline  which stripes all 40 objects across the four drives  Figure 15 summarizes the results of this experiment  and Figure 16 shows the regular layout recommended by the advisor for the 12 most heavily requested objects  Opti  mization boosts the performance of both the OLAP1 21 and OLTP workloads  This is achieved primarily by separating the TPC H LINEITEM table from the TPC C STOCK and CUS  TOMER tables  which see heavy non sequential workloads in our con guration  6 4 Layout Quality  Heterogeneous Targets In these experiments we asked the layout advisor to rec  ommend layouts for heterogeneous storage targets  In the  rst experiment  we used our server s RAID controller to cre  ate heterogeneous targets out of the four 18 4GB disk drives  In the  3 1  con guration  we created a 3 disk RAID0 tar  get from three of the disks and used the remaining disk as a standalone target  In the  2 1 1  con guration  we created a 2 disk RAID0 target and used each of the remaining two disks as standalone single disk targets  In practice  hetero  geneity may arise not only from storage system con gura  tion  but also from the presence of multiple types of devices or storage systems  In a second experiment  we asked the layout advisor to recommend layouts across the four 18 4GB disk drives plus the SSD  We varied the capacity of the SSD to test whether the advisor was able to exploit di erent mix  tures of SSD and disk storage  For these experiments  we used the OLAP8 63 workload  When the storage targets are heterogeneous  it is not obvious how to de ne baseline layouts against which to compare the layout advisor s recommendations  As our  rst baseline we used SEE  although it is clear that this will lead to load imbalance when the targets are heterogeneous  For the  3 1  con guration  we also considered a second baseline in which the TPC H tables are isolated on the large target and the remaining objects are placed on the small one  For the  2 1  1  con guration  we consider a second baseline that isolates the tables on the large target  the indexes on one of the small targets  and the temporary tablespace on the other small target  These are layouts that might be considered by a database administrator faced with these situations  Finally  for the experiments involving the SSD  we also considered a baseline in which all of the TPC H objects are placed on the SSD in those scenarios for which the SSD capacity was su cient to permit it  Figure 17 summarizes the results of the  rst experiment  Speedup SSD All objects Optimized  Optimized vs  Cap  SEE on SSD Layout SEE  32GB 6742 6182 1 96x 10GB 12145 6354 1 9x 6GB n a 6234 1 94x 4GB 8529 1 42x Figure 18  Workload Execution Times  in second  for Baseline and Optimized Layouts on Heteroge  neous Storage Targets  for the OLAP8 63 Workload which did not involve the SSD  In addition to the results for the heterogeneous 3 1 and 2 1 1 target con gurations  we have also included the results from Figure 11 for the same workload  OLAP8 63  under the homogeneous  1 1 1  1  con guration  The most important observation is that the layout advisor is able to identify a good layout regard  less of the storage target con guration  In fact  the layouts it found for the heterogeneous con gurations were actually slightly better than the one that it found for the homoge  neous targets  Not surprisingly  the baselines do not fare as well  The performance of SEE degrades with increasing disparity in storage target con guration  faring worse under 2 1 1 than under 1 1 1 1  and worse under 3 1 than under 2 1 1  Isolating tables on the large target improved per  formance in the 3 1 con guration  but not as much as the optimized layout  which used 3 disk RAID0 target for the LINEITEM table  the I L ORDERKEY  index  and part of the CUSTOMER  Isolating tables and indexes in the 2 1 1 con g  uration hurt performance signi cantly relative to the SEE baseline  which points to the di culty of using heuristic lay  out guidelines  In the 2 1 1 con guration  the optimized layout isolated the LINEITEM table on the 2 disk target and distributed the remaining objects across the 2 single disk targets  Figure 18 summarizes the results of the second experi  ment  which involved the four disk drives and the SSD  As expected  the SEE layout performed poorly because the dis  parity in the performance characteristics of the SSD and the disk drives  When the SSD capacity was su cient to allow it  placing all of the objects on the SSD resulted in much bet  ter performance  but of this layout fails to utilize the disk drives at all  The optimized layout distributed the objects across the disk drives and the SSD and achieved about a 10  speedup relative to the SDD only layout  More importantly  the layout advisor was able to determine good layouts even when the the SSD was too small to hold all of the objects  For example  even with only a 6GB SSD  the optimized lay  947Regular  Workload N M Solver ization TOTAL OLAP8 63 20 4 3 5 0 1 3 6 4 12 1 0 5 12 6 consolidation 40 10 55 2 2 57 2 20 120 9 129 40 200 26 226 2xconsolidation 80 10 47 12 59 3xconsolidation 120 10 340 40 380 4xconsolidation 160 10 590 72 662 Figure 19  Execution Time of the Layout Advisor  in seconds  out still performs better than the SSD only layout with a 32GB SSD  It is also instructive to compare execution times from the SSD experiments  Figure 18  to those from the disk only experiments  Figure 17   For example  the workload runs in 16201 seconds under SEE in the four disk  1 1 1 1  con   guration  with no SSD   or in 13608 seconds in the same con guration with an optimized layout  Figure 17   With the addition of a 4GB SSD to the four disks  the recom  mended layout allows the workload to  nish in 8529 seconds  Figure 18   almost twice as fast as the disk only SEE layout  Thus  the layout advisor is able to exploit a small amount of added SSD capacity to achieve a substantial boost in work  load performance  6 5 Optimization Time Figure 19 shows the time required to recommend regular  ized layouts  for several di erent workloads  For each layout we report the total time required  as well as an indication of how much of that time is spent in the NLP solver and how much is spent on the regularization step  The total time is the solver time plus the regularization time plus the time required to generate the initial con guration for the solver  which is very small  much less than a second   The  gure shows the cost for the OLAP8 63 workload and for the workload used in the consolidation scenario  which includes both the TPC H and TPC C database objects  In addition  we created additional synthetic workloads by tak  ing the workload descriptions  Wi s  of the 40 objects from the consolidation workload and replicating them  This gives workloads with 80  120 and 160 objects  labeled 2xconsol  idation  3xconsolidation  and 4xconsolidation in Figure 19  We measured the time required to generate an optimized layout for these replicated workloads on 10 storage targets  A few things are clear from these tests  First  for lay  out problems at these scales  10 s of targets  a few 100 s of objects  the layout advisor is quite fast  For the largest problem we gave it  the total time required to generate an optimized layout was about 10 minutes  The results also show that the total optimization time is dominated by the time required by the NLP solver  rather than the regular  ization post processing step  The solver timings reported in Figure 19 were obtained without any tuning of the solver  We have found that tuning can speed the solver up by a factor of two or three  6 6 AutoAdmin Comparison As part of the Microsoft AutoAdmin project  Agrawal et al  2  addressed a database layout problem that is similar 100  100  100  100  100  100  100  100  100  100  100  100  50  50  100  100  100  TEMP SPACE LINEITEM ORDERS I L ORDERKEY ORDERS PKEY PARTSUPP CUSTOMER PART  a  INITIAL SOLUTION  b  AFTER RANDOM IMPROVEMENT Figure 20  AutoAdmin Layout of TPC H Database Objects for the OLAP1 63 Workload to the one that we have considered  and they developed a tool for recommending layouts  Although that tool and our own layout advisor address similar problems  they use dif  ferent approaches to solve them  Here  we present a brief comparison of the two tools that is intended to highlight the di erences between their approaches  The AutoAdmin tool takes as input a set of SQL state  ments describing a database system s workload  In contrast  our layout advisor expects statistical I O workload param  eters for each object  Our approach is more general in that it is not limited to layout for database systems  but for database systems the AutoAdmin input is very natural and easy to generate  The AutoAdmin tool builds a graph rep  resentation of the workload  with nodes representing objects and weighted edges between nodes representing concurrent access to those objects by workload queries  This graph is input to a two step layout process  The  rst step separates heavily co accessed objects in order to minimize interference between them  The second step further distributes objects across targets to increase I O parallelism  In our terminol  ogy  the resulting layout is regular  The emphasis in the AutoAdmin work is on reducing inter  ference among concurrently access objects and on providing I O parallelism for individual objects  It relies on relatively simple workload and performance models  e g   it models neither workload concurrency nor performance di erences among di erent types of storage targets  In contrast  we use more expressive models  Unlike the AutoAdmin tool  I O parallelism for individual database objects is not an ex  plicit objective of our layout optimization process  although our optimizer often does distribute objects across multiple targets  Although the AutoAdmin layout technique was originally developed for Microsoft s SQLServer DBMS  we implemented it in PostgreSQL so that we could compare the layouts that it recommends with those recommended by our layout ad  visor  Figure 20 illustrates the layout generated by the AutoAdmin technique for our OLAP1 63 workload  Fig  ure 20 a  shows the layout that results from the  rst step  in which each object is placed on a single target  Figure 20 b  shows the  nal layout after the second step  which attempts to distribute objects to increase the potential I O paral  lelism  This  nal layout shares some of the features of the layout recommended by our advisor  e g   it separates the heavily used objects LINEITEM  ORDERS  and I L ORDERKEY from each other  and isolates the LINEITEM table  However  while our optimizer distributed LINEITEM across two stor  age targets  the AutoAdmin optimizer assigns LINEITEM to a single target  so that it is able to isolate the TEMP SPACE object on the remaining target  The reason for this di er  948ence is that the AutoAdmin tool relies in part on cardinal  ity estimates from the database system s query optimizer to estimate the I O load on objects  and these estimates are sometimes erroneous  The PostgreSQL query optimizer makes errors of multiple orders of magnitude in estimating the sizes of some intermediate objects produced by query execution plan for TPC H Q18  This leads the AutoAdmin tool to overestimate the importance of separating LINEITEM and TEMP SPACE  Of course  this particular cardinality esti  mation error is an artifact of PostgreSQL and may not occur in another database system  However  most query optimiz  ers are subject to some kinds of estimation errors  The layout shown in Figure 20 b  is less balanced than that of Figure 1  primarily because LINEITEM is placed on a single target  Despite this  AutoAdmin s layout provided about the same speedup  between 1 25x and 1 3x  in work  load execution time as our optimized layout did  The OLAP1  63 workload runs in 32634 seconds under AutoAdmin layout  as compared to 31789 seconds under the layout of Figure 1 and 40927 seconds under the SEE baseline layout  The im  balance of the AutoAdmin layout did not result in a signif  icant workload execution time penalty because the storage targets  even the one holding LINEITEM  are lightly utilized under this workload on our test platform  As noted previously  the AutoAdmin tool is oblivious to the concurrency level in the workload  As a result  Au  toAdmin layout tool gives exactly the same layout for the OLAP8 63 workload as it does for OLAP1 63 workload be  cause these two workloads are composed of exactly the same queries and di er only in their concurrency level  How  ever  as discussed in Section 6 2  the workload characteris  tics of the TPC H objects under OLAP1 63 and OLAP8 63 workloads are quite di erent  As a result  the AutoAdmin  recommended layout actually hurts performance  relative to the SEE baseline  under the OLAP8 63 workload  The OLAP8 63 workload runs in 19937 seconds on the layo</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lh1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lh1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Leveraging_hardware_for_data_management"/>
        <doc>FAST  Fast Architecture Sensitive Tree Search on Modern CPUs and GPUs ### Changkyu Kim      Jatin Chhugani       Nadathur Satish      Eric Sedlar     Anthony D  Nguyen      Tim Kaldewey     Victor W  Lee       Scott A  Brandt     and Pradeep Dubey     changkyu kim intel com     Throughput Computing Lab  Intel Corporation   Special Projects Group  Oracle Corporation   University of California  Santa Cruz ABSTRACT In memory tree structured index search is a fundamental database operation  Modern processors provide tremendous computing power by integrating multiple cores  each with wide vector units  There has been much work to exploit modern processor architectures for database primitives like scan  sort  join and aggregation  However  unlike other primitives  tree search presents signi   cant challenges due to irregular and unpredictable data accesses in tree traversal  In this paper  we present FAST  an extremely fast architecture sensitive layout of the index tree  FAST is a binary tree logically organized to optimize for architecture features like page size  cache line size  and SIMD width of the underlying hardware  FAST eliminates impact of memory latency  and exploits thread level and datalevel parallelism on both CPUs and GPUs to achieve 50 million  CPU  and 85 million  GPU  queries per second  5X  CPU  and 1 7X  GPU  faster than the best previously reported performance on the same architectures  FAST supports ef   cient bulk updates by rebuilding index trees in less than 0 1 seconds for datasets as large as 64M keys and naturally integrates compression techniques  overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs  Categories and Subject Descriptors H 2  Database Management   Systems General Terms Performance  Algorithms ### 1  INTRODUCTION Tree structured index search is a critical database primitive  used in a wide range of applications  In today   s data warehouse systems  many data processing tasks  such as scienti   c data mining  network monitoring  and    nancial analysis require handling large volumes of index search with low latency and high throughput  As memory capacity has increased dramatically over the years  many database tables now reside completely in memory  thus elimPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     10 00  inating disk I O operations  Modern processors integrate multiple cores in a chip  each with wide vector  SIMD  units  Although memory bandwidth has also been increasing steadily  the bandwidth to compute ratio is reducing  and eventually memory bandwidth will become the bottleneck for future scalable performance  In the database community  there is growing interest in exploiting the increased compute in modern processors  Recently  researchers have explored speeding up critical primitives like scan  sort  join and aggregation  30  11  22  12   However  unlike these primitives  index tree search presents signi   cant challenges in utilizing the high compute and bandwidth resources  Database search typically involves long latency for the main memory access followed by small number of arithmetic operations  leading to ineffective utilization of large number of cores and wider SIMD  This main memory access latency is dif   cult to hide due to irregular and unpredictable data accesses during the tree traversal  In this paper  we present FAST  Fast Architecture Sensitive Tree  search algorithm that exploits high compute in modern processors for index tree traversal  FAST is a binary tree  managed as a hierarchical tree whose elements are rearranged based on architecture features like page size  cache line size  and SIMD width of underlying hardware  We show how to eliminate the impact of latency with hierarchically blocked tree  software pipelining  and prefetches  Having eliminated memory latency impact  we show how to extract parallelism to achieve high throughput search on two high performance commodity architectures     CPUs and GPUs  We report the fastest search performance on both platforms by utilizing many cores and wide SIMD units  Our CPU search performance on the Core i7 with 64M1 32 bit  key  rid  pairs is 5X faster than the best reported number  achieving a throughput of 50M search queries per second  We achieve peak throughput even within a stringent response time constraint of 1 microsecond  Our GPU search on the GTX 280 is around 1 7X faster than the best reported number  achieving 85M search queries per second  Our high throughput search is naturally applicable to look up intensive applications like On Line Analytical Processing  OLAP   DSS and data mining  Also  we can re build our index trees in less than 0 1 seconds for 64M keys on both CPU and GPU  enabling fast bulk updates  We compare CPU search and GPU search and provide analytical models to analyze our optimized search algorithms for each platform and identify the compute and bandwidth requirements  When measured with various tree sizes from 64K elements to 64M elements  CPU search is 2X faster than GPU search for small trees where all elements can    t in the caches  but 1 7X slower than GPU search for large tree where memory bandwidth limits the perfor  1 1M refers to 1 million  339mance  We also evaluate FAST search algorithm on the Intel ManyCore Architecture Platform  MICA   a Larrabee  29  based silicon platform for many core research and software development  By exploiting wider SIMD and large caches  search on the MICA platform results in 2 4X   3X higher throughput than CPU search and 1 8X   4 4X higher than GPU search  Search on current GPUs is compute bound and not bandwidth bound  However  CPU search becomes close to memory bandwidth bound as the tree size grows  Compressing the index elements will reduce bandwidth consumption  thereby improving CPU search performance  We therefore propose compression techniques for our CPU search that seamlessly handles variable length string keys and integer keys  Our compression extends the commonly used pre   x compression scheme to obtain order preserving partial keys with low overhead of false positives  We exploit SSE SIMD execution to speed up compression  compressing 512MB string keys in less than 0 05 seconds for key sizes up to 100B on the Core i7  We integrate our fast SIMD compression technique into the FAST search framework on CPUs and achieve up to 6X further speed up for 100B keys compared to uncompressed index search  by easing the memory bandwidth bottleneck  Finally  we examine four different search techniques     FAST  FAST with compression  buffered scheme  sort based scheme     under the constraint of response time  For the response time of 0 001 to 25 ms  FAST on compressed keys provides the maximum throughput of around 60M queries per second while the sort based scheme achieves higher throughput for the response time of greater than 30 ms  Our architecture sensitive tree search with ef   cient compression support lends itself well to exploiting the future trends of decreasing bandwidth to compute ratio and increasing compute resource with more cores and wider SIMD  2  RELATED WORK B  trees  13  were designed to accelerate searches on disk based database systems  As main memory sizes become large enough to store databases  T trees  23  have been proposed as a replacement  speci   cally tuned for main memory index structure  While T trees have less storage overhead than B  trees  Rao et al   25  showed that B  trees actually have better cache behavior on modern processors because the data in a single cache line is utilized more ef     ciently and used in more comparisons  They proposed a CSS tree  where each node has a size equal to the cache line size with no child pointers  Later  they applied cache consciousness to B  trees and proposed a CSB  tree  26  to support ef   cient updates  Graefe et al   16  summarized techniques of improving cache performance on B tree indexes  Hankins et al   18  explored the effect of node size on the performance of CSB  trees and found that using node sizes larger than a cache line size  i e   larger than 512 bytes  produces better search performance  While trees with nodes that are of the same size as a cache line have the minimum number of cache misses  they found that TLB misses are much higher than on trees with large node sizes  thus favoring large node sizes  Chen at al   9  also concluded that having a B  tree node size larger than a cache line performs better and proposed pB  trees  which tries to minimize the increase of cache misses of larger nodes by inserting software prefetches  Later  they extended pB  trees  called    fpB  trees     to optimize for both disk I O and CPU caches for disk based DBMS  10   While using prefetches in pB  trees reduces cache misses in the search within a node  all published tree structured indexes suffer from a full cache miss latency when moving from each node to its child  Chen at al  argue that prefetching the children is not ef   cient because the tree node fanout is large and each child will be visited with the same probability  Instead of prefetching all  or a subset of   children speculatively  our scheme waits until we know the correct child to move to  and only prefetches the correct child  To increase the prefetch distance  i e   the number of cycles between a prefetch initiation and the use of the prefetched data   we use software pipelining  3   Note that we do not waste main memory bandwidth due to the issue of unnecessary prefetches speculatively   this is important because memory bandwidth is a critical resource  Another way of reducing TLB cache miss overheads is to amortize the penalties by processing data in batches  4  32   Buffers are attached to nodes or sub trees and queries are processed in batch  These batched search schemes essentially sacri   ce response time to achieve high throughput and can only be used for applications where a large number of query requests arrive in a short amount of time  such as stream processing or indexed nested loop join  Modern processors exploit SIMD execution to increase compute density  Researchers have used SIMD instructions to improve search performance in tree structured index  28  31   Zhou et al   31  employ SIMD instructions to improve binary search performance  Instead of comparing a search key with one tree element  SIMD instructions allow K  SIMD width  consecutive elements to be compared simultaneously  However  their SIMD comparison in a node only splits the search into two ranges just as in binary search  Overall  the total computation complexity is still log N K   Recently  P ary and K ary search have been proposed to exploit SIMD execution on GPUs  21  and CPUs  28   Using the GPU   s memory gather capability  P ary accelerates search on sorted lists  The SIMD comparison splits the tree into  K   1  ranges  thus reducing the total number of comparisons by a larger number of logK N   To avoid non contiguous memory accesses at each level  they linearize the K ary tree in increasing order of levels by rearranging elements  However  when the tree is large  traversing the bottom part of the tree incurs TLB cache misses at each level and search becomes latency bound  We explore latency hiding techniques for CPUs and GPUs to improve instruction throughput  resulting in better SIMD utilization  Another architectural trend affecting search is that main memory bandwidth is becoming a critical resource that can limit performance scaling on future processors  Traditionally  the problem has been disk I O bandwidth  Compression techniques have been used to overcome disk I O bottleneck by increasing the effective memory capacity  15  17  20   The transfer unit between memory and processor cores is a cache line  Compression allows each cache line to pack more data and increases the effective memory bandwidth  This increased memory bandwidth can improve query processing speed as long as decompression overhead is kept minimal  19  33   Recently  SIMD execution has been applied to further reduce decompression overhead in the context of scan operations on compressed data  30   While there is much research on handling numerical data in index trees  9  18  21  25  26  28   there are relatively few studies on handling variable length keys  5  7  8   Compression techniques can be used to shrink longer keys into smaller keys  For tree structured indexes  compressing keys increases the fanout of the tree and decreases the tree height  thus improving search performance by reducing cache misses  Binnig et al   7  apply the idea of buffered search  32  to handle variable size keys in a cache friendly manner when the response time is of less concern as compared to throughput and lookups can be handled in bulk  3  ARCHITECTURE OPTIMIZATIONS Ef   cient utilization of compute resources depends on how to extract instruction level parallelism  ILP   thread level parallelism 340 TLP   and data level parallelism  DLP  while managing usage of external memory bandwidth judiciously  3 1 ILP Most CPUs and GPUs are capable of issuing one or more instructions per cycle  The latency of memory instructions can prevent execution of other instructions  Every memory access must go through a virtual to physical address translation  which is in the critical path of program execution  To improve translation speed  a translation look aside buffer  TLB  is used to cache translation of most frequently accessed pages  If the translation is not found in the TLB  processor pipeline stalls until the TLB miss is served  Both last level cache  LLC  and TLB misses are dif   cult to hide because the miss penalty reaches more than a few hundred cycles  If the miss penalty cannot be hidden  a processor cannot fully utilize its compute resources and applications become memory latency bound  At this point  exploiting SIMD vector units is ineffective  One way to reduce the memory access latency is prefetches  However  hardware prefetcher is not effective for irregular memory accesses like tree traversal  Software prefetch instructions are also hard to insert for tree structured indexes  Prefetching tree nodes close to the current node results in very short prefetch distance and most prefetches will be too late to be effective  Tree nodes far down from the current node can create a large fan out and prefetching all tree elements down wastes memory bandwidth signi   cantly since only one of prefetches will be useful  3 2 TLP DLP Once memory latency impact is minimized  we can exploit high density compute resources in modern processors  which have integrated many cores  each with wider vector  SIMD  units  Parallelization and vectorization are two key techniques to exploit compute resources  Parallelization in search can be done trivially by assigning threads to different queries  Each core executes different search queries independently  As for exploiting SIMD  there are multiple approaches to performing search  We can use a SIMD unit to speed up a single query  assign different queries to each SIMD lane and execute multiple queries in parallel  or combine these two approaches by assigning a query to a subset of SIMD lanes  The best approach depends on the underlying hardware architectures such as the SIMD width and ef     ciency of gathering and scattering 2 data  In Section 5  we describe the best SIMD search mechanism for both CPUs and GPUs  3 3 Memory Bandwidth With the increased compute capability  the demand for data also increases proportionally  However  main memory bandwidth is growing at a lower rate than compute  27   Therefore  performance will not scale up to the number of cores and SIMD width if applications become bandwidth bound  To bridge the enormous gap between bandwidth requirements and what the memory system can provide  most processors are equipped with several levels of onchip memory storage  e g   caches on CPUs and shared buffer on GPUs   If data structures being accessed    t in this storage  no bandwidth is utilized  thus amplifying the effective memory bandwidth  However  if data structures are too big to    t in caches  we should ensure that a cache line brought from the memory be fully utilized before evicted out of caches  called    cache line blocking      A cache line with a typical size of 64 bytes can pack multiple data elements in it  e g   16 elements of 4 byte integers   The cache line blocking 2 In this paper  we use the term    gather scatter    to represent read write from to non contiguous memory locations technique basically rearranges data elements so that the subsequent elements to be used also reside within the same cache line  Finally  data compression techniques can be used to pack more data elements into a cache line and prevent performance to be bandwidth bound  In Section 6  we show integrating data compression into our index tree framework to improve performance besides gaining more effective memory space  4  ARCHITECTURE SENSITIVE TREE We motivate and describe our index tree layout scheme  We also provide analytical models highlighting the compute and memory bandwidth requirements for traversing the resultant trees  4 1 Motivation Given a list of  key  rid  tuples sorted by the keys  a typical query involves searching for tuples containing a speci   c key  keyq  or a range of keys   keyq1  keyq2    Tree index structures are built using the underlying keys to facilitate fast search operations     with run time proportional to the depth of the trees  Typically  these trees are laid out in a breadth    rst fashion  starting from the root of the tree  The search algorithm involves comparing the search key to the key stored at a speci   c node at every level of the tree  and traversing a child node based on the comparison results  Only one node at each level is actually accessed  resulting in ineffective cache line utilization  to the linear storage of the tree  Furthermore  as we traverse deeper into the tree  each access results in accessing elements stored in different pages of memory  thereby incurring TLB misses  Since the result of the comparison is required before loading the appropriate child node  cache line prefetches cannot be issued beforehand  On modern processors  a search operation typically involves a long latency TLB cache miss followed by small number of arithmetic operations at each level of the tree  leading to ineffective utilization of the processor resources  Although blocking for disk memory page size has been proposed in the past  13   the resultant trees may reduce the TLB miss latency  but do not necessarily optimize for effective cache line utilization  leading to higher bandwidth requirements  Cache line wide nodes  25  minimize the number of accessed cache lines  but cannot utilize SIMD instructions effectively  Recently  3 ary trees  28  were proposed to exploit the 4 element wide SIMD of CPUs  They rearranged the tree nodes in order to avoid expensive gather scatter operations  However  their tree structure does not naturally incorporate cache line page blocking and their performance suffers for tree sizes larger than the last level cache  LLC   In order to ef   ciently use the compute performance of processors  it is imperative to eliminate the latency stalls  and store access trees in a SIMD friendly fashion to further speedup the run time  4 2 Hierarchical Blocking We advocate building binary trees  using the keys of the tuple  as the index structure  with a layout optimized for the speci   c architectural features  For tree sizes larger than the LLC  the performance is dictated by the number of cache lines loaded from the memory  and the hardware features available to hide the latency due to potential TLB and LLC misses  In addition  the    rst few levels of the tree may be cache resident  in which case the search algorithm should be able to exploit the SIMD architecture to speed up the run time  In order to optimize for all the architectural features  we rearrange the nodes of the binary index structure and blocking in a hierarchical fashion  Before explaining our hierarchical blocking scheme in detail  we    rst de   ne the following notation  34115 7 23 3 11 19 27 0 2 1 5 9 13 17 21 25 29 4 6 8 10 12 14 16 18 20 22 24 26 28 30 0 1 2 3 6 9 12 15 16 4 5 7 8 10 11 13 14 17 18 19 20 21 22 23 24 25 26 27 28 29 30  a   b  SIMD Blocking Cache line Blocking Page Blocking Key1   Rid1 Keyn   Ridn                                     Index Tree  Only Keys  Node Array  Keys   Rids  Key2   Rid2      c  dP dN dL Depth of SIMD Blocking dK dK dL Depth of Cache Line Blocking dP Depth of Page Blocking dN Depth of Index Tree Figure 1   a  Node indices   memory locations  of the binary tree  b  Rearranged nodes with SIMD blocking  c  Index tree blocked in three level hierarchy        rst level page blocking  second level cache line blocking  third level SIMD blocking  E   Key size  in bytes   K   SIMD width  in bytes   L   Cache line size  in bytes   C   Last level cache size  in bytes   P   Memory page size  in bytes   N   Total Number of input keys  NK   Number of keys that can    t into a SIMD register  NL   Number of keys that can    t into a cache line  NP   Number of keys that can    t into a memory page  dK   Tree depth of SIMD blocking  dL   Tree depth of cache line blocking  dP   Tree depth of page blocking  dN   Tree depth of Index Tree  In order to simplify the computation  the parameters NP  NL and NK are set to be equal to the number of nodes in complete binary sub trees of appropriate depths 3   For example  NP is assigned to be equal to 2 dP  1  such that E 2 dP  1      P and E 2 dP  1  1    P  Similarly  NL   2 dL  1 and NK   2 dK  1  Consider Figure 1 where we let NL  31  dL   5 and dK   2  Figure 1 a  shows the indices of the nodes of the binary tree  with the root being the key corresponding to the 15 t h tuple  and its two children being the keys corresponding to the 7 t h and 23 rd tuples respectively  and so on for the remaining tree  Traditionally  the tree is laid out in a breadth    rst fashion in memory  starting from the root node  For our hierarchical blocking  we start with the root of the binary tree and consider the sub tree with NP elements  The    rst NK elements are laid out in a breadth    rst fashion  Thus  in Figure 1 b   the    rst three elements are laid out  starting from position 0  Each of the  NK   1  children sub trees  of depth dK   are further laid out in the same fashion  one after another  This corresponds to the sub trees  of depth 2  at positions 3  6  9 and 12 in Figure 1 b   This process is carried out for all sub trees that are completely within the    rst dL levels from the root  In case a sub tree being considered does not completely lie within the    rst dL levels  i e  when dL   dK    0   the appropriate number of levels  dL   dK   are chosen  and the elements laid out as described above  In Figure 1 b   since the 16 sub trees at depth 4 can only accommodate depth one subtrees within the    rst    ve  dL  levels  we lay them out contiguously in memory  from positions 15 to 30  After having considered the    rst dL levels  each of the  NL  1  children sub trees are laid out as described above  This is represented with the red colored sub trees in Figure 1 c   This process is carried out until the    rst dP levels of the tree are rearranged and laid out in memory  the top green triangle   We continue the same rearrangement with the sub trees at the next level and terminate when all the nodes in the tree have been rearranged to the appropriate positions  For e g   Fig  1 c  shows the rearranged binary tree  with the nodes corresponding to the keys stored in the sorted list of  key  3 By de   nition  tree with one node has a depth of one  rid  tuples  Our framework for architecture optimized tree layout preserves the structure of the binary tree  but lays it out in a fashion optimized for ef   cient searches  as explained in the next section  4 3 Compute and Bandwidth Analysis We    rst analyze the memory access pattern with our hierarchically blocked index tree structure  and then discuss the instruction overhead required for traversing the restructured tree  Let dN denote the depth of the index tree  Consider Figure 1 c   Assuming a cold cache and TLB  the comparison to the root leads to a memory page access and a TLB miss  and say a latency of lP cycles  The appropriate cache line is fetched from the main memory into the cache  incurring a further latency of say lL cycles  We then access the necessary elements for the next dL levels  elements within the top red triangle in the    gure   The subsequent access incurs a cache miss  and a latency of lL cycles  At an average   dP  dL  cache lines will be accessed within the    rst page  the top green triangle   Therefore  the total incurred latency for any memory page would be  lP    dP dL lL  cycles  Going to the bottom of the complete index tree would require  dN  dP  page accesses  for an average incurred latency of  dN  dP   lP    dP dL lL  cycles 4   To take into account the caching and TLB effect  say dC out of the dN levels    t in the last level cache  Modern processors have a reasonable size TLB  but with a random query distribution  it is reasonable to assume just the entry for the top page to be in the page table during the execution of a random search  Therefore  the average incurred latency will be  1 dC  dN    dN  dP  dP  dL lL    lP  dN  dP  1  cycles  ignoring minimal latency of accessing cache lines from the cache   The resultant external memory bandwidth will be L 1 dC  dN    dN  dP  dP  dL   bytes  As for the computational overhead  our blocking structure involves computation of the starting address of the appropriate SIMD chunk  cache line  page block once we cross the appropriate boundary  For each crossed boundary  the computation is simply an accumulated scale shift operation  multiply add followed by add  due to the linear address translation scheme  For example  when crossing the cache line block  we need to multiply the relative child index from the cache line with the size of each cache line and add it to the starting address of that level of sub trees  For a given element key size  E   the number of accessed cache lines  and memory pages  is provably minimized by the hierarchical tree structure  However  while performing a single search per core  the compute units still do not perform any computation while waiting for the memory requests to return  thereby under utilizing the compute resource  In order to perform useful computation during the memory accesses  we advocate performing multiple search queries simultaneously on a single core thread  We use software 4 Assuming a depth of dP for the bottom most page of the index tree  For a smaller depth  replace dP  dL with d   P  dL for the last page  where d   P is the sub tree depth for the last page  342pipelining  and interleave the memory access and computation for different queries  For example  while crossing cache line blocks  we issue a prefetch for the next cache line block to be accessed  for a speci   c query   and subsequently perform comparison operations for the other query ies   After performing the comparison for all the remaining queries  we access the cache line  the same address we issued prefetch for earlier   With adequate amount of gap  the cache line would have been brought into the cache by the prefetch  thereby hiding memory latency  This process ensures complete utilization of the compute units  Although modern GPUs provide large number of threads to hide the memory access latency  the resultant memory access and instruction dependency still expose the latency  which is overcome using our layout and software pipelining schemes  Section 5 2   In order to minimize the incurred latency during search operation  we want to increase dP  in order to reduce the number of TLB misses  and increase dL  to reduce the number of accessed cache lines   The only way to increase both is to reduce the element size  E   For in memory databases  the number of tuples is typically less than the range of 32 bit numbers  2 32    and hence the keys can be represented using 32 bits  4 byte  integers  We assume 4 byte keys and 4 byte rid   s for algorithm description in the next section  and provide algorithms for representing longer and variable length keys using 4 bytes in Section 6  32 bit keys also map well to SIMD on modern architectures  CPU and GPU   with native instruction support for 32 bit elements in each SIMD lane  5  CPU SEARCH VS  GPU SEARCH We describe in detail the complete search algorithm on CPUs and GPUs We discuss various parameters used for our index tree layout and the SIMDi   ed search code  along with a detailed analysis of performance and ef   ciency comparison on the two architectures  5 1 CPU Implementation Today   s CPUs like the Intel Core i7 have multiple cores  each with a 128 bit SIMD  SSE  computational unit  Each SSE instruction can operate simultaneously on four 32 bit data elements  E equals four bytes for this section  As far as exploiting SIMD is concerned  there are multiple ways to perform searches   a  Searching one key  and using the SSE instructions to speedup the search   b  Searching two keys  using two SIMD lanes per search   c  Searching four keys  one per SIMD lane  Both options  b  and  c  would require gathering elements from different locations  Since modern CPUs do not support an ef   cient implementation of gather  the overhead of implementing these instructions using the current set of instructions subsumes any bene   t of using SIMD  Hence we choose option  a  for CPUs  and set dK   2 levels  The cache line size is 64 bytes  implying dL   4 levels  The page size used for our experimentation is 2MB  dL   19   although smaller pages  4KB  dL   10  are also available  5 1 1 Building the Tree Given a sorted input of tuples  Ti   i     1  N    each having 4 byte  key  rid    we layout the index tree  T   by collecting the keys from the relevant tuples and laying them out next to each other  We set dN    log2 N     In case N is not a power of two  we still build the perfect binary tree  and assume keys for tuples at index greater than N to be equal to the largest key  or largest possible number   denoted as keyL  We iterate over nodes of the tree to be created  using index k initialized to 0   With current CPUs lacking gather support  we layout the tree by   a  computing the index  say j  of the next key to be loaded from 61 47 73 23 11 31 41 2 19 29 37 43 53 67 79 Child Index   2 Child Index   3 000 100 010 110 001 101 011 111 Lookup  Index 0 N A 1 2 N A N A N A 3 Child  Index Lookup Table Search Key   59 1 1 0 1 1 1 Key value in the  tree node mask bit value  set to 1  if keyq   keynode Use mask  value as  index  Figure 2  Example of SIMD SSE  tree search and the lookup table  the input tuples   b  loading in the key   key       T j  key  if j N   key      keyL    c  T k   key     k    This process is repeated till the complete tree is constructed  i e  k    2 dN  1    The tree construction can be parallelized by dividing the output equally amongst the available cores  and each core computing writing the relevant part of the output  We exploit SIMD for step  a  by computing the index for NK    3  keys within the SIMD level block simultaneously  We use appropriate SSE instructions and achieve around 2X SIMD scaling as compared to the scalar code  Steps  b  and  c  are still performed using scalar instructions  For input sizes that    t in the LLC  tree construction is compute bound  with around 20 ops 5 per element  for a total construction time of 20  2 dN ops per core  For N   2M  the total time is around 40M cycles per core  assuming the CPU can execute 1 instruction per cycle  When the input is too large to    t into the LLC  the tree construction needs to read data from memory  with the initial loads reading complete cache lines but only extracting out the relevant 4 bytes  To compute the total bandwidth required  let   s start with the leaf nodes of T   There are a total of 2 dN    1 leaf nodes  The indices for the nodes would be the set of even indices     0  2  and so on  Each cache line holds eight  key  rid  tuples of which four tuples have even indices  Hence populating four of the leaf nodes of T  requires reading one cache line  amounting to L 4 bytes per node  For the level above the leaf nodes  only two leaf nodes can be populated per cache line  leading to L 2 bytes per node  There are 2 dN    2 such nodes  For all the remaining nodes  2 dN    2  1   a complete cache line per node is read  Since there is no reuse of the cache lines  the total amount of required bandwidth  analytically  would be 2 dN    1 L 4   2 dN    2 L 2    2 dN    2  1 L      L 2 2 dN bytes  equal to 32 2 dN   bytes for CPUs  Depending on the available bandwidth  this may be compute bandwidth bound  Assuming reasonable bandwidth   1 6 bytes cycle core   our index tree construction is compute bound  For N as large as 64M tuples  the run time is around 1 2 billion cycles  for a single core   which is less than a 0 1 seconds on the Core i7  With such fast build times  we can support updates to the tuples by buffering the updates and processing them in a batch followed by a rebuild of the index tree  5 1 2 Traversing the Tree Given a search key  keyq   we now describe our SIMD friendly tree traversal algorithm  For a range query   keyq1  keyq2    keyq     keyq1  We begin by splatting keyq into a vector register  i e   replicating keyq for each SIMD lane   denoted by Vkeyq  We start the search by loading 3 elements from the start of the tree into the register Vtree  At the start of a page  page offset     0  Step 1  Vtree     sse load T    page offset   This is followed by the vector comparison of the two registers to set a mask register  5 1 op implies 1 operation or 1 executed instruction  343   T   starting address of a tree page address  starting address offset of a particular page blocking sub tree page offset  starting address offset of a particular cache line blocking sub tree cache offset  starting address offset of a particular SIMD blocking sub tree      m128i xmm key q    mm load1 ps key q                        xmm key q   vector register Vkeyq  Splat a search key  keyq  in Vkeyq    for  i 0  i number of accessed pages within tree  i      page offset   0  page address   Compute page address child offset   for  j 0  j number of accessed cachelines within page  j          Handle the first SIMD blocking sub tree   2 levels of the tree      m128i xmm tree    mm loadu ps T   page address   page offset      xmm tree  vector register Vtree  Load four tree nodes in Vtree     m128i xmm mask    mm cmpgt epi32 xmm key q  xmm tree       xmm mask  mask register Vmask  Set the mask register Vmask   index    mm movemask ps  mm castsi128 ps xmm mask       Convert mask register into index   child index   LookUp index      Likewise  handle the second SIMD blocking sub tree   2 levels of the tree    xmm tree    mm loadu ps T   page address   page offset   Nk child index   xmm mask    mm cmpgt epi32 xmm key q  xmm tree    index    mm movemask ps  mm castsi128 ps xmm mask    cache offset   child index 4   Lookup index   page offset    page offset 16   cache offset    child offset   child offset  2 dp    page offset       child offset is the offset into the input  Key  Rid  tuple  T     While  T child offset  key    keq q2  child offset   Figure 3  SSE code snippet for index tree search  Step 2  Vmask     sse greater Vkeyq  Vtree   We then compute an integer value  termed index  from the mask register  Step 3  index     sse index generation Vmask   The index is then looked up into a Lookup table  that returns the local child index child index   and is used to compute the offset for the next set of load  Step 4  page offset     page offset   NK   Lookup index   Since dK   2  there are two nodes on the last level of the SIMD block  that have a total of four child nodes  with local ids of 0  1  2 and 3  There are eight possible values of Vmask   which is used in deciding which of the four child nodes to traverse  Hence  the lookup table has 2 NK    8  entries  with each entry returning a number      0  3   Even using four bytes per entry  this lookup table occupies less than one cache line  and is always cache resident during the traversal algorithm  In Figure 2  we depict an example of our SSE tree traversal algorithm  Consider the following scenario when keyq equals 59 and  keyq   Vtree 0     keyq   Vtree 1   and  keyq   Vtree 2    In this case  the lookup table should return 2  the left child of the second node on the second level  shown in the    rst red arrow in the    gure   For this speci   c example  Vmask      1  1  0  and hence index would be 1 2 0     1 2 1     0 2 2     3  Hence Lookup 3      2  The other values in the lookup table are similarly    lled up  Since the lookup table returns 2  child index for the next SSE tree equals 2  Then  we compare three nodes in the next SSE tree and Vmask      1  1  1   implying the right child of the node storing the value 53  as shown with the second red arrow in the    gure  We now continue with the load  compare  lookup and offset computation till the end of the tree is reached  After traversing through the index structure  we get the index of the tuple that has the largest key less than or equal to keyq  In case of range queries  we do a linear scan of the tuples  till we    nd the    rst key greater than keyq2  Analysis  Figure 3 delineates our SSE tree traversal code  As 2 5 6 1 3 4 0 7 8 9 10 11 12 13 14 0 1 M N O P 1 1 1 0 0 0 0 A B C D E F K L G H I J Node Index j rest   Result of  Comparison 0 1 2 3 4 5 6 7 Node Index Common  Ancestor  Index Lookup Table 8 9 10 11 12 13 14 N A N A N A N A N A N A N A 3 1 4 0 5 2 6 N A res  j res  j 1  a   b  Figure 4  Example of GPU tree search and the lookup table  compared to a scalar code  we resolve dK    log2 NK  1   levels simultaneously  Hence theoretically  a maximum of 2X   dK   speedup is possible  in terms of number of instructions   We    rst analyze the case where the index tree    ts in the LLC  For each level of the index tree  the scalar code is  child offset     2  child offset    keyq   T  child offset   The above line of code performs 5 ops  load  compare  add  multiply and store   In comparison  the SSE code requires similar number of instructions for two levels  However  our blocking scheme introduces some overhead  For every 4 levels of execution  there are 2 ops  multiply add for load address computation   another 2 ops  multiply add for cache offset computation   and 2 ops for multiply add  for page offset computation   for a total of 6 ops for 4 levels  Thus the net number of ops per level of the SSE code is around    10 6  4    4   for a speedup of 1 25X   5 4   Since modern CPUs can execute multiple instructions simultaneously  the analytical speedup provides a high level estimate of the expected speedup  As far as tree sizes larger than the LLC are concerned  for each cache line brought into memory  the total amount of instructions executed is around 16 ops  The net bandwidth required would be 64 16   4 bytes cycle  assuming IPC 1   Even recent CPUs do not support such high bandwidths  Furthermore  the computation will be bandwidth bound for the last few levels of the tree  thereby making the actual SIMD bene   t depend on the achieved bandwidth  5 1 3 Simultaneous Queries For tree sizes larger than the LLC  the latency of accessing a cache line  lL  is the order of a few hundred cycles  The total amount of ops per cache line is around 16  To remove the dependency on latency  we execute S simultaneous queries  using the software pipelining technique  The value of S is set to be equal to eight for our experiments  since that covers up for the cache TLB miss latency  In addition  for small tree sizes that    t in the LLC  our software pipelining scheme hides the latency caused by instruction dependency  The search code scales near linearly with multiple cores  The Core i7 supports the total of eight threads  with four cores and two SMT threads per each core  Therefore  we need a total of 64 concurrent queries to achieve peak throughput  5 2 GPU Implementation The NVIDIA GPU architecture consists of multiple shared multiprocessors  or SMs   The GTX 280 has 30 such SMs  GPUs hide memory latency through multi threading  Each GPU SM is capable of having more multiple threads of execution  up to 32 on the GTX 280  simultaneously active  Each such thread is called a thread block in CUDA  24   Each GPU SM has multiple scalar processors that execute the same instruction in parallel  In this work  we view them as SIMD lanes  The GTX 280 has eight scalar processors per SM  and hence an 8 element wide SIMD  However  the logical SIMD width of the architecture is 32  Each GPU instruction works on 32 data ele  344ments  called a thread warp   which are executed in four cycles  GPUs provide hardware support for gather scatter instructions at a half warp granularity  16 lanes   24   and hence we explored the complete spectrum of exploiting SIMD   a  Searching 32 keys  one per SIMD lane  dK   1    b  Searching one key  and exploiting the 32 element wide SIMD  dK   5   Since the GPUs do not explicitly expose caches  the cache line width  dL  was set to be same as dK   This reduces the overhead of computing the load address by the various SIMD lanes involved in searching a speci   c key  As far as the TLB size is concerned  NVIDIA reveals no information of page size and the existence of TLB in the of   cial document  With various sizes of dP   we did not see any change in run time  Hence  dP is assigned equal to dN   5 2 1 Building the Tree Similar to the CPUs  we parallelize for the GPUs by dividing the output pages equally amongst the available SMs  On each SM  we run the scalar version of the tree creation algorithm on one of the threads within a half warp  16 lanes   Only that one thread per half warp executes the tree creation code  and computes the index  and updates the output page  This amounts to running two instances of tree creation per warp  with effective SIMD width of two  Running more than two instances within the same warp leads to gather  to read keys from multiple tuples in the SIMD operation   and scatter  to store the keys to different pages within the SIMD operation  from to the global memory  In our experiments  we measured a slow down in run time by enabling more than two threads per warp  We execute eight blocks on the same SM to hide the instruction memory access latency  We assign one warp per block for a total of 30 SMs  8   240  warps  As far as the run time is concerned  the number of ops per tree element is similar to the CPU     20 ops   therefore reducing to 20 2   10 SIMD ops per element  Each of the executed warp takes 4 cycles of execution per warp  Hence  total number of cycles is equal to 10  4   2 dN   cycles     40  2 dN cycles  per SM  Since GPUs provide a very high memory bandwidth  our tree creation is compute bound  For N as large as 64 million tuples  the run time is around 2 6 billion cycles  which is less than 0 07 seconds on GTX 280  5 2 2 Traversing the Tree dK equal to 1 is a straightforward implementation  with each SIMD lane performing an independent search  and each memory access amounting to gather operations  Any value of dK less than 4 leads to a gather operation within the half warp  and the search implementation is latency bound  Hence we choose dK   4  since it avoids gather operations  and tree node elements are fetched using a load operation  Since GPUs have a logical SIMD width of 32  we issue two independent queries per warp  each with dK   4  Although the tree traversal code is similar to the CPUs  the current GPUs do not expose explicit masks and mask manipulation instructions  Hence Steps 2 and 3  in Section 5 1 2  are modi   ed to compute the local child index  We exploit the available shared buffer in the GTX 280 to facilitate inter lane computation  After comparing the key with the tree node element  each of the NK SIMD lanes stores the result of the comparison  resi   0 1  in a pre allocated space  Consider the eight leaf nodes in Figure 4 a   assume NK   15   We need to    nd the largest index  say j   such that resj   1 and resj 1   0  For the    gure  j   10  Hence the child node is either H or I  The result depends on the comparison result of their common ancestor  node0   In case res0   0  keyq is less than the key stored at node0  and hence the node belongs to its left sub tree  thereby setting index     H  shown in Figure 4   In case    In the GPU code  we process two independent queries within a warp   simd lane   threadId x  16     16 threads are devoted for each search query query id   threadId x   16          query id  either 0 or 1 ancestor   Common Ancester Array  simd lane   base index   2  simd lane      13    shared   int child index  2      store the child index for two queries   shared   int shared gt  32   for  j 0  j number of accessed cachelines within page  j         Handle the SIMD blocking sub tree   4 levels of the tree    page address    2  4 j  1    page offset 15      consume 2 ops int v node    Td   page address   simd lane         consume 4 ops    This is actually SIMD load  Our SIMD level blocking enables this instruction to be  loading 16 consecutive values as opposed to loading 16 non consecutive values    int gt    keyq   v node                              consume 2 ops shared gt threadIdx x    gt                        consume 2 ops   syncthreads                                           consume 2 ops next gt   shared gt threadIdx x   1          consume 2 ops if  threadIdx x    7        consume 2 ops child index query id    0                      consume 2 ops   if  threadIdx x   7       consume 2 ops if  gt    next gt       consume 2 ops    res j   1    res j 1   0    child index query id    base index   shared gt ancestor       consume 5 ops       syncthreads                                         consume 2 ops page offset   page offset 16   child index query id      consume 3 ops   child offset   page offset     child offset is the offset into the input  Key  Rid  tuple  T     While  T child offset  key    keq q2  child offset   Figure 5  GPU code snippet for index tree search  res0   1  index     I  We pre compute the common ancestor node for each of the leaf nodes  with its successor node  into a lookup table  and load it into the shared buffer at the start of the algorithm  Figure 4 b  shows this lookup table for our speci   c example  and similar tables can be built for other values of dK   The total size of the lookup table is NK   15 for our example   Figure 5 shows the CUDA code for GPU search    syncthreads   is required to ensure that the shared buffer is updated before being accessed in the subsequent line of code  for inter lane communication   child index is also stored in the shared buffer so that the relevant SIMD threads can access it for subsequent loads  This requires another   syncthreads   instruction  We also show the number of ops for each line of code in the    gure  The total number of executed ops is 32  Since dK   4  the average number of executed ops per level is 8 ops for two queries within the 32 wide SIMD  5 2 3 Simultaneous Queries Although the GPU architecture is designed to hide latency  our implementation was still not completely compute bound  Therefore  we implemented our software pipelining technique by varying S from 1 to 2  This further reduced the latency  and our resultant run time were within 5    10  of the compute bound timings  In order to exploit the GPU architecture  we execute independent queries on each SM  In order to hide the latency  we issue one warp per block  with eight blocks per SM  for a total of 240 blocks on the GTX 280 architecture  Since we execute 2 queries in SIMD  and S queries per warp  a total of 480S queries are required  With S being equal to 2  we operate the GPU at full throttle with 960 concurrent queries  5 3 Performance Evaluation We now evaluate the performance of FAST on an quad core Core i7 CPU and an GTX 280 GPU  Peak    ops  computed as frequency    core count    SIMD width   peak bandwidth  and total frequency 345Platform Peak GFlops Peak BW Total Frequency Core i7 103 0 30 12 8 GTX 280 933 3 141 7 39 Table 1  Peak compute  GFlops   bandwidth  GB sec   and total frequency  Cores   GHz  on the Core i7 and the GTX 280  Figure 6  Normalized search time with various architectural optimization techniques  lower is faster   The fastest reported performance on CPUs  28  and GPUs  2  is also shown  for comparison    core count    frequency  of the two platforms are shown in Table 1  We generate 32 bit  key  rid  tuples  with both keys and rids generated randomly  The tuples are sorted based on the key value and we vary the number of tuples from 64K to 64M6   The search keys are also 32 bit wide  and generated uniformly at random  Random search keys exercise the worst case for index tree search with no coherence between tree traversals of subsequent queries  We    rst show the impact of various architecture techniques on search performance for both CPUs and GPUs and compare search performance with the best reported number on each architecture  Then  we compare the throughput of CPU search and GPU search and analyze the performance bottlenecks for each architecture  5 3 1 Impact of Various Optimizations Figure 6 shows the normalized search time  measured in cycles per query on CPUs and GPUs by applying optimization techniques one by one  We    rst show the default search when no optimization technique is applied and a simple binary search is used  Then  we incrementally apply page blocking  cache line blocking  SIMD blocking  and software pipelining with prefetch  The label of     SW Pipelining    shows the    nal relative search time when all optimization techniques are applied  We report our timings on the two extreme cases     small trees  with 64K keys  and large trees  with 64M keys   The relative performance for intermediate tree sizes fall in between the two analyzed cases  and are not reported  For CPU search  the bene   t of each architecture technique is more noticeable for large trees than small trees because large trees are more latency bound  First  we observe that search gets 33  faster with page blocking  which translates to around 1 5X speedup in throughput  Adding cache line blocking on top of page blocking results in an overall speedup of 2 2X  This reduction of search time comes from reducing the average TLB misses and LLC misses signi   cantly     especially when traversing the lower levels of the tree  However  page blocking and cache line blocking do not help small trees because there are no TLB and cache misses in the    rst place  in fact  cache line blocking results in a slight increase of instructions with extra address computations  Once the impact of latency is reduced  SIMD blocking exploits data level parallelism and provides an additional 20      30  gain for both small and large trees  6 64M is the max  number of tuples that    t in GTX 280 memory of 1GB  Figure 7  Comparison between the CPU search and the GPU search     CPU BW    shows the throughput projection when CPU search becomes memory bandwidth bound Finally  the software pipelining technique with prefetch relaxes the impact of instruction dependency and further hides cache misses  Our    nal search performance is 4 8X faster for large trees and 2 5X faster for small trees than the best reported numbers  28   As shown in Figure 6  ourscalar performance with page and cache line blocking outperforms the best reported SIMD search by around 1 6X  This emphasizes the fact that SIMD is only bene   cial once the search algorithm is compute bound  and not bound by various other architectural latencies  Applications that are latency bound do not exploit the additional compute resources provided by SIMD instructions  Also note that our comparison numbers are based on a single thread execution  for fair comparison with the best reported CPU number   When we execute independent search queries on multiple cores  we achieve near linear speedup  3 9X on 4 cores   The default GPU search  Fig  6  executes one independent binary search per SIMD lane  for a total of 32 searches for SIMD execution  Unlike CPU search  GPU search is less sensitive to blocking for latency  We do not report the number for cache line blocking since the cache line size is not disclosed  While the default GPU search suffers from gathering 32 tree elements  SIMD blocking allows reading data from contiguous memory locations thus removing the overhead of gather  Since the overhead of gather is more signi   cant for large trees  our GPU search obtains 1 7X performance improvement for large trees  and 1 4X improvement for small trees with SIMD blocking  Our GPU implementation is compute bound  5 3 2 CPU search VS  GPU search We compare the performance of search optimized for CPU and GPU architectures  Figure 7 shows the throughput of search with various tree sizes from 64K keys to 64M keys  When the tree    ts in the LLC  CPUs outperform GPUs by around 2X  This result matches well with analytically computed performance difference  As described in the previous subsections  our optimized search requires 4 ops per level per query for both CPUs and GPUs  Since GPUs take 4 cycles per op  they consume 4X more cycles per op as compared to the CPU  On the other hand  GPUs have 3X more total frequency than CPUs  Table 1   On small trees  CPUs are not bound by memory latency and can operate on the maximum instruction throughput rate  Unlike GPUs  CPUs can issue multiple instructions per cycle and we observe an IPC of around 1 5  Therefore  the total throughout ratio evaluates to around  1 5 4 3     2X in the favor of CPUs  As the tree size grows  CPUs suffer from TLB LLC misses and get lower instruction throughput rate  The dotted line  labeled    CPUBW    shows the throughput projection when CPU search becomes memory bandwidth bound  This projection shows that CPUs are compute bound on small trees and become closer to bandwidth bound on large trees  GPUs provide 4 6X higher memory bandwidth than CPUs and are far from bandwidth bound  In the next 346Throughput  million queries per sec  Small Tree  64K keys  Large Tree  16M keys  CPU 280 60 GPU 150 100 MICA 667 183 Table 2  Measured performance comparison across three different platforms     CPU  GPU  and MICA  section  we show how to further improve CPU search performance by easing bandwidth bottleneck with compression techniques  Recent work  21  has shown that GPUs can improve search performance by an order of magnitude over CPUs and combining Fig  6 and 7 con   rms that unoptimized GPU search outperforms unoptimized CPU search by 8X for large trees  However  proper architecture optimizations reduced the gap and CPUs are only 1 7X slower on large trees  and in fact 2X faster on smaller trees  Note that GPUs provide much higher compute    ops and bandwidth  Table 1   Thus optimized CPU search is much better than optimized GPU search in terms of architecture ef   ciency  5 3 3 Search on MICA We study how FAST would perform on the Intel Many Core Architecture Platform  MICA   a Larrabee  29  based silicon platform for many core research and software development  7 Larrabee is an x86 based many core processor architecture based on small in order cores that uniquely combines full programmability of today   s general purpose CPU architecture with compute throughput and memory bandwidth capabilities of modern GPU architectures  Each core is a general purpose processor  which has a scalar unit based on the Pentium processor design  as well as a vector unit that supports 16 32 bit    oat or integer operations per clock  Larrabee has two levels of cache  low latency 32KB L1 data cache and larger globally coherent L2 cache that is partitioned among the cores  Each core has a 256KB partitioned L2 cache  To further hide latency  each core is augmented with 4 way multi threading  Since Larrabee features a 16 wide SIMD  we set dK   4 levels  enabling sub tree traversal of four levels with SIMD operations  Unlike GPUs  Larrabee SIMD operation supports inter lane computation  Therefore  SIMD tree traversal code is similar to SSE tree traversal code  Figure 3   The only difference is that dK   4 levels is used while SSE has dK   2 levels and no separate cache line blocking is necessary since we set dL is assigned equal to dK   To compare search throughput with CPU GPU search  we measure the performance on the MICA platform for small trees  with 64K keys  and large trees  with 16M keys   Table 2 shows search throughput on three different platforms     CPU  GPU  and MICA  As shown in the table  Larrabee takes the best of both architectures     a large cache in CPUs and high compute bandwidth in GPUs  For small trees  Larrabee is able to store the entire tree within L2 cache  and is therefore compute bound  Compared to CPU and GPU  search on the MICA platform obtains a speed up of 2 4X and 4 4X respectively  The speedup numbers over CPUs are in line with the peak computational power of these devices  after accounting for the fact that search can only obtain a speedup of log K   using K  wide SIMD  The high speedup over GPUs comes from the inef     ciency of performing horizontal SIMD operations on GPUs  For large trees  16M entries     depth 24   the    rst 16 levels are cached in L2 and hence the    rst 16 level traversal is compute bound while the remaining 8 level traversal is bandwidth bound  We observe a speedup of 3X and 1 8X over CPUs and GPUs  respectively  7 All results reported in this section are based on Intel   s internal research con   gurations of the Intel Many Core Architecture Platform  MICA   which is designed by Intel for research and software development  5 3 4 Limited Search Queries Research on search generally assumes the availability of a large number of queries  This may be the case either when a large number of users concurrently submit searches or a database optimizer chooses multiple index probes for a join operation  However  in some cases  there are limited number of concurrent searches that are available to schedule  In addition  response time may become more important than throughput when search queries cannot be buffered beyond some threshold of response time  To operate on the maximum throughput  our CPU search requires 64 concurrent queries to be available while our GPU search requires 960 concurrent queries once the overheads of thread spawning and instruction cache miss have been amortized over a few thousand queries  6  COMPRESSING KEYS In the previous section  we presented run times with key size  E  equal to 4 bytes  For some databases  the keys can be much larger in length  and also vary in length with the tuples  The larger the key size in the index tree  the smaller the number of levels per cache line and per memory page  leading to more cache lines and pages being read from main memory  and increased bandwidth latency requirements per query  With the total number of tuples being less than the range of numbers represented by 4 bytes  it is possible  theoretically  to map the variable length keys to a    xed length of 4 bytes  for use in the index tree   and achieve maximum throughput  In this section  we use compression techniques to ease memory bandwidth bottleneck and obtain further speedup for index tree search  Note that we focus on compressing the keys in the index tree 8 for CPUs  Since CPU search is memory bandwidth bound for last few levels of large trees  compressing the keys reduces the number of accessed cache lines  thereby reducing the latency  bandwidth  translating to improved performance  The GPU search is compute bound for 4 byte keys  and will continue to be compute bound for larger keys  with proportional decrease in throughput  Although the compression scheme developed in this section is applicable to GPUs too  we focus on CPUs and provide analysis and results for the same in the rest of the section  6 1 Handling Variable Length Keys We    rst present computationally simple compression scheme that maps variable input length data to an appropriate    xed length with small number of false positives  and incurs negligible construction and decompression overhead  Our scheme exploits SIMD for fast compression and supports order preserving compression  leading to signi   cant reduction in run time for large keys  6 1 1 Compression Algorithm We compute a    xed length  Ep bits  partial key  pkeyi   for a given key  keyi    In order to support range queries  it is imperative to preserve the relative order of the keys  i e  if  keyj     keyk    then  pkeyj     pkeyk    The cases where  pkeyj   pkeyk   but  keyj   keyk   are still admissible  but constitute the set of false positives  and should be minimal to reduce the overhead of redundant key comparisons during the search  Although there exist various hashing schemes to map the keys to a    xed length  6   the order preserving hash functions require O N log2N   storage and compute time  14   However  we need to compute and evaluate such functions in constant  and small  time  We extend the pre   x compression scheme  5  to obtain order preserving partial keys  These schemes compute a contiguous com  8 The original tuple data may be be independently compressed using other techniques  1  and used in conjunction with our scheme  3471111 0000 1101 0110 1111 0010 1101 1001 1111 0101 1101 0000 1111 1100 1101 0110 1111 0100 1101 0010 key0 key1 key2 key3 key4 MSB LSB     g   4 bits                 0000 0110 Full Keys 8 bit partial key 0010 1001 0101 0000 1100 0110 0100 0010 MSB   Most Significant Byte LSB   Least Significant Byte Figure 8  Example of extracting 8 bit partial keys in    ve keys  mon pre   x of the set of keys  and store the subsequent part of the keys as the partial key  Instead  we compute a pre   x such that the bits at which the keys differ constitute the partial key  Let K represent the list of keys  We de   ne a granularity factor  G   a number between 1 to 32  The partial key for all the keys is initialized to 0  We start by extracting and comparing the    rst G bits from all the keys  In case all the keys have the same G bits  we consider the next G bits  and repeat the process  In case they are different  we append these G bits to the partial key computed so far  We repreat this process and keep appending the set of G bits until a Ep bit partial key is obtained  Note that by construction  the pre   x bits not appended to the partial key so far are the same for all the keys  Figure 8 shows an example for computing an 8  bit partial key for    ve keys with G   4  Note that our scheme is a generalization of computing the common pre   x     we extract a noncontiguous common pre   x  with the discarded intermediate Ep bit positions forming the partial key  Our scheme is order preserving since we start with the most signi   cant bit  It is indeed possible to get the same partial key for two different keys  Setting the value of G  Databases storing variable length keys usually store low entropy values in each byte of the key  For example  keys representing the telephone numbers consist of only ten unique values in each byte  Similarly the name address url has the range of alphabets  52 values  in each byte  As far as setting G is concerned  it is affected by two competing factors   a  the smaller the value of G  the higher the chances of forming partial keys with few false positives   b  the larger the value of G  the lower the cost of computing the partial key from the original key  Furthermore  too small a G may require high cost of bit manipulation functions  thereby increasing the overhead of compressing decompressing the keys  We choose G   4 bits  which provide the right balance between the two factors described above  Compression Cost  We exploit SSE instructions by loading 128  bits at a time for each key  We maintain a register Vres initialized to 0  We start by loading the    rst 128 bits of K0 into register VK0  For each key Ki   we load the relevant 128 bits  into VKi   and compute the bitwise exclusive or operation  Vxor    mm xor ps VK0  VKi    We then bitwise or the result with the Vres register  Vres    mm or ps Vxor  Vres   After iterating over all the keys  we analyze the Vres register by considering G bits at a time  If the G bits are all 0  then the result of all exclusive or operations was 0  and hence all the keys have the relevant G bits the same  In case any of the bits is 1  then that relevant chunk becomes part of the partial key  We maintain a mask  termed as pmask  that stores a 0 or 1 for each G bit  We iterate over the remaining key  16 bytes at a time  to compute the bits contributing towards the partial key  As far as the total cost is concerned  we require 3 ops for every 16 bytes     for a total of 3 E 16  ops  in the worst case   For 16 byte keys  this amounts to only 3 ops  and    21 ops per element for as long as 100 byte keys  All the keys now need to packed into their respective partial keys with the same pmask  Consider the    rst 16 bytes of the key  loaded into the register Figure 9  Relative increase in computation with respect to various alphabet sizes  2 entropy   per byte  For example  52 means that we generate random keys for each byte among 52 values  VKi   This consists of 32 4 bit chunks  with a 32 bit pmask  pmaski   1 implies the i t h chunk needs to be appended with the previously set 4 bit chunk  SSE provides for a general permute instruction   mm shuf   e eip8  that permutes the 16 1 byte values to any location within the register  using a control register as the second argument   However  no such instruction exists at 4 bit granularity  We however reduce our problem to permuting 8 bit data elements to achieve fast partial key computation  Each byte  say B7  0  of data consists of two 4 bit elements  B7  4 and B3  0   Consider B7  4  If chosen by the mask register  it can either end up as the 4 most signi   cant bits in a certain byte of the output or in the 4 least significant bits of one of the bytes  Similar observation holds for B3  0  Each of these 4 bit values within a byte can be 0 padded by 4 bits  at left or right  to create four 8 bit values  namely  B3  0 0000    0000 B3  0    B7  4 0000  and  0000 B7  4   We create four temporary registers for VKi   where each byte has been transformed as described  We can now permute each of the four registers  using the appropriate control registers   and bitwise or the result to obtain the desired result  Thus  the cost for partial key computation is 4 ops  for creating the temporary registers   4 ops for permute  and 4 for oring the result  for a total of 12 E 16  ops per key  Including the cost to compute the pmask  the total cost for compression is 15 E 16  ops  For a 512MB tree structure with 16 byte keys  our compression algorithm takes only 0 05 seconds on Core i7  6 1 2 Building the Compressed Tree We compute the partial keys for each page separately to increase the probability of forming effective partial keys with low false positives  As we traverse down the tree  it is important to have few  if any  false positives towards the top of the tree  since that increases the number of redundant searches expontentially  For example  say there are two leaf nodes  out of the    rst page  with the same partial key  and the actual search would have led through the second key  Since we only store the partial keys  the actual search leads through the    rst of these matches  and we traverse down that subtree and end up on a tuple that is 2  dN    dP   tuples to the left of the actual match     leading to large slowdowns  Although Bohannon et al   8  propose storing links to the actual keys  this leads to further TLB LLC misses  Instead  we solve the problem by computing an Ep that leads to less than 0 01  false positives for the keys in the    rst page  We start with Ep   32 bits  and compute the partial keys  and continue doubling Ep until our criterion for false positives is satis   ed  In practice  with Ep   128 bits  16 bytes   we saw no false postives for the    rst page of the tree  for E     100   For all subsequent pages  we use partial keys of length  Ep   32 bits   The keys are compressed in the same fashion and each page stores the pmask that enables in fast extraction of the partial key  Since the time taken for building the index tree is around 20 ops per key  Section 5 1 1   the total time for building compressed index tree increases to  20   15 E 16   ops per key  The compressed tree 348Figure 10  Throughput comparison between no compression and compression with key size from 4 to 128 bytes  construction is still compute bound  and only around 75  slower than the uncompressed case  for E     16   6 1 3 Traversing the Compressed Tree During the tree traversal  we do not decompress the stored partial keys for comparsion  Instead we compress keyq  the query key  for each new page  and compare it with the partial keys  which enables search on compressed tuples  The cost of compressing the query key is around 12 E 16  ops per page  The key length is a multiple of 16 bytes for the    rst page  and 4 bytes for all subsequent page accesses  For Steps 2 and 3 of the traversal algorithm  Section 5 1 2  on the    rst page  we need to compare in multiples of 16 byte keys and generate the appropriate index  We implement 16 byte key comparison in SSE using the native 8 byte comparison instruction   mm cmpgt epi64   and use the corresponding maskto index generation instruction   mm movemask pd  to generate index into a lookup table  The total cost of tree traversal for the    rst page increases to around 10 ops per level  For all subsequent pages  it is 4 ops per level  similar to analysis in Section 5 1   As far as the bandwidth requirements are concerned  for tree sizes larger than the LLC  we now access one cache line for four levels  In comparison  for E   16  we would have accessed one cache line for 2 levels  and hence the bandwidth requirement is reduced by    2X by storing the order preserving partial keys  For E     32  the bandwidth reduction is 4X and beyond  This translates to signi   cant speedups in run time over using long er  keys  6 1 4 Performance Evaluation We implemented our compression algorithm on keys generated with varying entropy per byte  chosen from a set of 1 value  1  bit  to 128 values  7 bits   including cases like 10  numeral keys  and 52  range of alphabets   The key size varied from 4 to 128 bytes  The varying entropy per byte  especially low entropy  is the most challenging case for effective partial key computation  The distribution of the value within each byte does not affect the results  and we report data for values chosen uniformly at random  within the appropriate entropy   We also report results for 15 byte keys on phone number data collected from CUSTOMER table in TPC H  In Figure 9  we compare our compression scheme  non contiguous common pre   x  NCCP  with the previous scheme  contiguous common pre   x  CCP  and report the relative increase in computaton for searching 16 byte keys with various entropies  The increase in computation is an indicative of the number of false positives  which increases the amount of redundant work done  and thereby increasing the run time  A value of 1 imples no false positives  Even for very low entropy  choices per key     4   we perform relatively low extra computation  with total work less than 1 9X as compared to no false positives  For all other entropies  we measure less than 5  excess work  signifying the effectiveness of our low cost partial key computation  A similar overhead of around 5 2  is obesrved for the TPC H data using our scheme  with the competing scheme reporting around 12X increase  Figure 10 shows the relative throughput with varying key sizes  and    xed entropy per byte  log2 10  bits   The number of keys for each case is varied so that the total tree size of the uncompressed keys is    1GB  All numbers are normalized to the throughput achieved using 4 byte keys  Without our compression scheme  the througput reduces to around 50  for 16 byte keys  and as low as 30  and 18  for key sizes 64 and 128 bytes respectively  This is due to the reduced effectiveness of cache lines read from main memory  and therefore increase in the latency bandwidth  In contrast  our compression scheme utlizes cache lines more effectively by choosing 4 byte partial keys  The throughput drops marginally to around 80  for 16 byte keys  drop is due to the increase in cost for comparing 16 byte keys in the    rst page  Section 6 1 3   and varies between 70  80  for key sizes varying between 16 and 128 bytes  The ov</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lh2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lh2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Leveraging_hardware_for_data_management"/>
        <doc>Di   erential Logging  An E   cient and DBMS independent Approach for Storing Data into Flash Memory ### Yi Reun Kim      Kyu Young Whang       Il Yeol Song           Department of Computer Science Korea Advanced Institute of Science and Technology  KAIST         College of Information Science and Technology Drexel University e mail       yrkim  kywhang  mozart kaist ac kr         songiy drexel edu Abstract Flash memory is widely used as the secondary storage in lightweight computing devices due to its outstanding advantages over magnetic disks  Flash memory has many access characteristics di   erent from those of magnetic disks  and how to take advantage of them is becoming an important research issue  There are two existing approaches to storing data into    ash memory  page based and log based  The former has good performance for read operations  but poor performance for write operations  In contrast  the latter has good performance for write operations when updates are light  but poor performance for read operations  In this paper  we propose a new method of storing data  called page di   erential logging  for    ash based storage systems that solves the drawbacks of the two methods  The primary characteristics of our method are   1  writing only the di   erence  which we de   ne as the page di   erential  between the original page in    ash memory and the up to date page in memory   2  computing and writing the page di   erential only once at the time the page needs to be re   ected into    ash memory  The former contrasts with existing page based methods that write the whole page including both changed and unchanged parts of data or from log based ones that keep track of the history of all the changes in a page  Our method allows existing disk based DBMSs to be reused as    ash based DBMSs just by modifying the    ash memory driver  i e   it is DBMSindependent  Experimental results show that the proposed method is superior in I O performance  except for some special cases  to existing ones  Speci   cally  it improves the performance of various mixes of read only and update operations by 0 5  the special case when all transactions are readonly on updated pages      3 4 times over the page based method and by 1 6     3 1 times over the log based one for synthetic data of approximately 1 Gbytes  The TPC C benchmark also shows improvement of the I O time over existing methods by 1 2     6 1 times  This result indicates the e   ectiveness of our method under  semi  real workloads ###  11 Introduction Flash memory is a non volatile secondary storage that is electrically erasable and reprogrammable  4  10   Flash memory has outstanding advantages over magnetic disks  lighter weight  smaller size  better shock resistance  lower power consumption  and faster access time  10  14  25   Due to these advantages  the    ash memory is widely used in embedded systems and mobile devices such as mobile phones  MP3 players  and digital cameras  14  15   Flash memory is much di   erent from a magnetic disk in structures and access characteristics  12   It is composed of a number of blocks  and each block is composed of a    xed number of pages  It does not have seek and rotation latency because it is made of electronic circuits without mechanically moving parts  12   Flash memory provides three kinds of operations    read  write  and erase  In order to overwrite existing data in a page  an erase operation must be performed before writing new data on the page  12  14   The write and erase operations are much slower than the read operation  14  18   Besides  the unit of the erase operation is a block  while the unit of the read and write operations is a page  25   There have been a number of studies  2  3  8  13  14  21  on the method of storing updated pages into    ash memory for    ash based storage systems  In this paper  we refer to such methods as page update methods  The page update methods are classi   ed into two categories  25      page based  3  13  and log based  2  14  21   Page based methods write the whole page into    ash memory when an updated page needs to be re   ected into    ash memory  e g   when the page is swapped out from the DBMS bu   er to the database   3  13  25   These methods actually read only one page when recreating a page from    ash memory  e g   reading it into a DBMS bu   er   Thus  they have good read performance  However  they have relatively poor write performance because they write the whole page including unchanged parts as well as changed parts of data  25   In order to overcome this drawback  log based methods have been proposed  25   These methods write only the changes  which we call an update log 1   in the page into the write bu   er  which in turn is written into    ash memory when the bu   er is full  2  14  21   Thus  1An update log contains the changes in a page resulted in a single update command  2compared with page based methods  log based ones have good write performance when updates are not heavy 2  25   Log based methods  however  have relatively poor read performance because they keep the history of all the changes  i e   multiple update logs  in a page  Whenever an update is done  they write an update log into the write bu   er  Thus  when updates are done multiple times  the update logs are likely to be written into multiple pages in    ash memory  Thus  log based methods need to read multiple pages when recreating a page from    ash memory  In this paper  we propose a page update method called page di   erential logging  PDL  for    ashbased storage systems  A page di   erential simply  a di   erential  is de   ned as the di   erence between the original page in the    ash memory and the up to date page in memory  This novel method is much di   erent from page based methods or log based ones in the following ways   1  We write only the di   erential of an updated page  This characteristic stands in contrast with page based methods that write the whole page including changed and unchanged parts of data or log based ones that keep track of the history of all the changes  i e   multiple update logs  in a page  Furthermore  we compute and write the di   erential only once at the time the updated page needs to be re   ected into    ash memory  The overhead of generating the di   erential is relatively minor because  in    ash memory  the speed of read operation is much faster than those of write or erase operations   2  When recreating a page from    ash memory  we need fewer read operations than log based ones do because we read at most two pages  the original page and the single page containing the di   erential   3  When we need to re   ect an updated page into    ash memory  we need fewer write operations than others do because we write only the di   erential  A side bene   t is that the longevity of    ash memory is also improved due to fewer erase operations resulted from fewer write operations   4  Our method is loosely coupled with the storage system while the log based ones are tightly coupled  The log based methods need to modify the storage management module of the DBMS because they must identify the changes in a page whenever it is updated  These changes can be identi   ed only inside the storage management module because they are internally maintained by the system  On the other hand  our method does not need to modify the module of the DBMS because it computes the di   erential outside the storage management module by 2 When pages are frequently updated  the log based methods could be poorer in performance as we see in the experiments in p  30  Figure 13  3comparing the page that needs to be re   ected with the original page in the    ash memory  We elaborate on this point later in Section 4  The contributions of this paper are as follows   1  we propose a new notion of    di   erential    of a page  Using this notion  we then propose a new approach to updating pages that we call page di   erential logging   2  Our method is DBMS independent   3  Through extensive experiments  we show that the overall read and write performance of our method is mostly superior to those of existing ones  Hereafter  in order to reduce ambiguity in this paper  we distinguish logical pages from physical pages  We call the pages in memory logical pages and the ones in    ash memory physical pages  For ease of exposition  we assume that the size of a logical page is equal to that of a physical page  The rest of this paper is organized as follows  Section 2 introduces    ash memory  Section 3 describes prior work related to the page update methods for    ash based storage systems  Section 4 presents a new page update method called page di   erential logging  Section 5 presents the results of performance evaluation  Section 6 summarizes and concludes the paper  2 Flash Memory Based on the structure of memory cells  there are two major types of    ash memory  6   the NAND type and the NOR type  The former is suitable for storing data  and the latter for storing code  16   In the rest of this paper  we use the term       ash memory    to indicate the NAND type    ash memory  which is widely used in    ash based storage systems 3   Figure 1 shows the structure of    ash memory  The    ash memory consists of Nblock blocks  and each block consists of Npage pages  A page is the smallest unit of reading and writing data  and a block is the smallest unit of erasing data  25   Each page consists of a data area used for storing data and a spare area used for storing auxiliary information such as the valid bit  obsolete bit  bad block identi   cation  and error correction check  ECC   16   3 In this paper  we focus on    ash memory but not on solid state disks  SSD   s   19   which have controllers with their own page update methods  4page block data area spare area flash memory Figure 1  The structure of    ash memory  We consider three operations  read  write  and erase  6       The read operation   returns all the bits in the addressed page     The write operation   changes a set of bits selected in the target page from 1 to 0     The erase operation   sets all the bits in the addressed block to 1 The operations in    ash memory are di   erent from those in the magnetic disk in two ways  First  all the bits in    ash memory are initially set to 1  Thus  writing to    ash memory means selectively changing some bits in a page from 1 to 0  Next  the erase operation in    ash memory changes the bits in a block back to 1  Each block can sustain only a limited number of erase operations before becoming unreliable  which is restricted to about 100 000 4  14  15   Due to the restriction of the write and erase operations  a write operation is usually preceded by an erase operation in order to overwrite a page  12  14   We    rst change all the bits in the block to 1 using an erase operation  and then  change some bits in the page to 0 using a write operation  We note that the erase operation is performed in a much larger unit than a write operation  i e   the former is performed on a block while the latter on a page  The speci   c techniques for overwriting a page depend on the page update method employed  These techniques are discussed in Section 3  Based on the capacity of memory cells  there are two types of    ash memory  12   Single Level Cell  SLC  type and Multi Level Cell  MLC  type  The former is capable of storing one data bit per 4 Due to this characteristic  there have been a number of studies on wear leveling  10  and bad block management  16   However  we do not address them in this paper  but these studies can be applied to the storage system independently of the page update methods discussed in this paper  5cell  while the latter is capable of storing two  or even more  data bits per cell  Thus  MLC type    ash memory has greater capacity than SLC type one and is expected to be widely used in high capacity    ash storages  12   Table 1 summarizes the parameters and values of MLC    ash memory we use in our experiments  We note that the size of a page is 2 048 bytes  and a block has 64 pages  In addition  the access time of operations increases in the following order  read  write  and erase  The read operation is 9 2 times faster than the write operation  which is 1 5 times faster than the erase operation  Table 1  The parameters and values of    ash memory       Symbols De   nitions Values Nblock the number of blocks 32  768 Npage the number of pages in a block 64 Sblock the size of a block  bytes     Npage    Spage  135  168  64    2  112  Spage the size of a page  bytes     Sdata   Sspare  2  112    2  048   64  Sdata the size of data area in a page  bytes  2  048 Sspare the size of spare area in a page  bytes  64 Tread the read time for a page    s  110 Twrite the write time for a page    s  1010 Terase the erase time for a block    s  1500     Samsung K9L8G08U0M 2 Gbytes MLC NAND    ash memory  18  3 Related Work The Page Based Approach In page based methods  3  13   a logical page is stored into a physical page  When an updated logical page needs to be re   ected into    ash memory  the whole logical page is written into a physical page  25   When a logical page is recreated from    ash memory  it is read directly from a physical page  These methods are loosely coupled with the storage system because they can be implemented in a middle layer  called the Flash Translation Layer  FTL   3   which maintains logical to physical address mapping between logical and physical pages as shown in Figure 2  The FTL can be implemented as hardware in the controller residing in SSD   s  or can be implemented as software in the operating system for embedded boards 5   5Commercial FTL   s for SSD   s or embedded boards typically use page based methods  1  6flash memory Flash Translation Layer  FTL  storage system a page based method logical to physical address mapping table Figure 2  The architecture of the page based method  In page based methods  there are two update schemes  15      in place update and out place update    depending on whether or not the logical page is always written into the same physical page  When a logical page needs to be re   ected into    ash memory  the in place update overwrites it into the speci   c physical page that was read  15   but the out place update writes it into a new physical page  4  25   In Place Update  As explained in Section 2  the write operation in    ash memory cannot change bits in a page to 1  Therefore  when overwriting the logical page l1 that was read from the physical page p1 in the block b1 into the same physical page p1  we do the following four steps   1  read all the pages in b1 except p1   2  erase b1   3  write l1 into p1   4  write all the pages read in Step  1  except l1 in the corresponding pages in b1  The in place update scheme su   ers from severe performance problems and is rarely used in    ash memory  15  because it causes an erase operation and multiple read and write operations whenever we need to re   ect a logical page into    ash memory  Out Place Update  Figure 3 shows a typical example of the out place update scheme  Figure 3  a  shows the logical page l1 read from the physical page p1 in the block b1  Figure 3  b  shows the updated 7logical page l1 and the two physical pages p1 and p2     the original page read and the new page written  In order to overcome the drawback of in place update  when we need to re   ect the logical page l1 into    ash memory  the out place update scheme    rst writes l1 into a new physical page p2  and then  sets p1 to obsolete 6   When there is no more free page in    ash memory  a block is selected and obsolete pages in it are reclaimed by garbage collection  6   which converts obsolete pages to free pages  The out place update scheme is widely used in    ash based storage systems  25  because it does not cause an erase operation when a logical page is to be re   ected into    ash memory  flash memory a page based method using out place update scheme storage system logical page physical page block b 1 p 1 l 1 p 2 block b 2 flash memory a page based method using out place update scheme storage system p 1 l 1 p physical 2 page block b 1 block b obsolete 2 V logical page  a  The logical page l1 read from  b  The updated logical page l1 and the physical page p1  the process of writing it into the physical page p2  Figure 3  An example of out place update  The Log Based Approach In log based methods  2  14  21   a logical page is generally stored into multiple physical pages  14   Whenever logical pages are updated  the update logs of multiple logical pages are    rst collected into a write bu   er in memory  25   When this bu   er is full  it is written into a single physical page  Thus  when a logical page is updated many times  its update logs can be stored into multiple physical pages  Accordingly  when recreating a single logical page  multiple physical pages may need to be read and 6We set a page to obsolete by changing the obsolete bit in the spare area of the page from 1 to 0 as in Gal et al   6   8merged  The log based methods are tightly coupled with the storage system because the storage system must be modi   ed to be able to identify the update logs of a logical page  Among log based methods  there are Log structured File system  LFS   17   Journaling Flash File System JFFS   21   Yet Another Flash File System YAFFS   2   and In Page Logging  IPL   14   In LFS  JFFS  and YAFFS  the update logs of a logical page can be written into arbitrary log pages in    ash memory while  in IPL  the update logs should be written into speci   c log pages  IPL divides the pages in each block into a    xed number of original pages and log pages  It writes the update logs of a logical page into only the log pages in the block containing the original  physical  page of the logical page  Therefore  when recreating the logical page  IPL reads the original page and only the log pages in the same block  When there is no free log page in the block  IPL merges the original pages with the log pages in the block  and then  writes the merged pages into pages in a new block  this process is called merging  14    The old block is subsequently erased and garbage collected  Consequently  IPL improves read performance by reducing the number of log pages to read from    ash memory when recreating a logical page because log pages do not increase inde   nitely  i e   is bound  due to merging  The performance of IPL is similar to other log based methods since IPL inherits the advantages and drawbacks of log based methods other than the e   ect of merging and bound read performance  Figure 4 shows a typical example of the log based methods  Figure 4  a  shows the logical pages l1 and l2 in memory  Figure 4  b  shows the update logs q1 and q2 of logical pages l1 and l2  respectively  and the process of writing them into    ash memory  Here  the update logs q1 and q2 are    rst written into the write bu   er  and then  the content of the write bu   er is written into the log page p3  Thus  the update logs q1 and q2 are collected into the same log page p3  Figure 4  c  shows a similar situation for the update logs q3 and q4 of logical pages l1 and l2  Figure 4  d  shows the logical page l1 being recreated from    ash memory  Here  l1 is recreated by merging the original page p1 with the update logs q1 and q3 read from the log pages p3 and p4  respectively  9a log based method storage system block b flash memory 1 p 1 p 2 p 3 p 4 logical pages l 1 l 2 update log q 1 physical page original pages write buffer update log q 1 a log based method storage system l 1 logical pages l 2 write buffer block b 2 update log q 2 log page update log q 2 update log q 2 update log q 1  a  The logical pages l1 and l2  b  The update logs q1 and q2 of logical pages l1 and l2  and in memory  the process of writing them into the log page p3 in    ash memory  a log based method storage system block b flash memory 1 p 1 p 2 p 3 p 4 logical pages l 1 l 2 update log q 3 update log q 4 physical page update log q 3 update log q 4 original pages log pages write buffer update log q 3 update log q 4 block b 2  c  The update logs q3 and q4 of logical pages l1 and l2  and the process of writing them into the log page p4 in    ash memory  10a log based method storage system block b flash memory 1 p 1 p 2 p 3 p 4 logical page l 1 update log q 1 update log q 3 physical page l 1 p 1   update log q 1 in p 3 original page   update log q 3 in p 4 original pages log pages write buffer  d  The logical page l1 being recreated from    ash memory  Figure 4  An example of the log based approach  4 The Page Di   erential Logging Approach In this section  we propose page di   erential logging  PDL  for    ash based storage systems  Section 4 1 explains the design principles  and then  presents PDL  which conforms to these principles  Section 4 2 and 4 3 present the data structures and algorithms  Section 4 4 discusses the strengths and limitations  4 1 Design Principles We identify three design principles for PDL in order to guarantee good performance for both read and write operations  These principles overcome the drawbacks of both the page based methods and the log based methods in the following ways      writing di   erence only   We write only the di   erence when a logical page needs to be re   ected into    ash memory      at most one page writing   We write at most one physical page when a logical page needs to be re   ected into    ash memory even if the page has been updated in memory multiple times      at most two page reading   We read at most two physical pages when recreating a logical page from    ash memory  11Page di   erential logging method conforms to these three design principles  In this method  a logical page is stored into two physical pages     a base page and a di   erential page  Here  the base page contains a whole logical page  which could be the old version  and the di   erential page contains the di   erence between the base page and the up to date logical page  A di   erential page can contain di   erentials of multiple logical pages  Thus  the di   erentials of two logical pages could be stored in the same di   erential page  The di   erential has the following advantages over the list of update logs in the log based methods   1  It can be computed without maintaining all the update logs  i e   it can be computed by comparing the updated logical page with its base page only when the updated logical page needs to be re   ected into    ash memory   2  It contains only the di   erence from the original page for the part that has been updated multiple times in a logical page  When a speci   c part in a logical page is updated in memory multiple times  the list of update logs contains all the history of changes while the di   erential contains only the di   erence between original data and the up to date data  For instance  let us assume that a logical page is updated in memory twice as follows      aaaaaa             bbbbba             bcccba      Here  the list of update logs contains two changes bbbbb and ccc while the di   erential contains only the di   erence bcccb  In PDL  when an updated logical page needs to be re   ected into    ash memory  we create a di   erential by comparing the logical page with the base page in    ash memory  and then  write the di   erential into the one page write bu   er  which is subsequently written into    ash memory when it is full  Therefore  it conforms to the writing di   erence only principle  We note that  when a logical page is simply updated  we just update the logical page in memory without recording the log  Instead  we defer creating and writing the di   erential until the updated logical page needs to be re   ected into    ash memory  Thus  our method satis   es the at most one page writing principle  Theoretically  the size of the di   erential cannot be larger than that of one page  However  practically  it could be larger if a large part of the page has been updated  This case can occur since the di   erential contains not only the changed data but also the meta data such as o   sets and lengths  In 12this case  we discard the created di   erential and write the updated logical page itself into    ash memory as a new base page in order to satisfy the at most one page writing principle   In this special case  PDL becomes the same as the page based method   When recreating a logical page from    ash memory  we read the base page and its corresponding di   erential page  and then  merge the base page with its di   erential in the di   erential page  However  we need to read only one physical page if the base page has not been updated  i e   there is no di   erential page   Thus  we need to read at most two physical pages  and accordingly  PDL conforms to the at most two page reading principle  When there is no more free page in    ash memory  obsolete pages are reclaimed by garbage collection  Here  we select one block for garbage collection  Since it may contain valid base or di   erential pages  before erasing the block  we move those valid pages into a new block  which is reserved for the garbage collection process  6   For di   erential pages  however  we move only valid di   erentials into a new di   erential page  i e   we do compaction here  Our method requires fewer write operations than page based or log based ones do because it satis   es the writing di   erence only and at most one page writing principles  Thus  our method invokes garbage collection less frequently than other methods do  Figure 5 shows an example of PDL  Here  we have base page p   di   erential page p   and di   erential p  for the logical page p  Figure 5  a  shows the logical pages l1 and l2 in memory  Figure 5  b  shows the updated logical pages l1 and l2  and the process of writing them into    ash memory  When l1 and l2 need to be re   ected into    ash memory  we perform the following three steps   1  read the base pages p1 and p2 from    ash memory   2  create di   erential l1  and di   erential l2  by comparing l1 and l2 with the base pages p1 and p2  respectively   3  write di   erential l1  and di   erential l2  into the write bu   er  which is subsequently written into the physical page p3 when the bu   er is full  We note that l1 and l2 from di   erent logical pages are written into the same di   erential page p3  Figure 5  c  shows the logical page l1 recreated from    ash memory by merging the base page p1 with di   erential l1  in p3 7   7 Conceptually  we require an assembly bu   er in order to merge the base page with the di   erential  But  in practice  we can use the logical page itself as the assembly bu   er  13flash memory the page differential logging method main memory block b 1 logical  page p 1 p 2 l 1 l 2 l 1 l 2 p 1 comp  differential l 1   p 2 base page l 1   base page l 1   physical page differential  write buffer p 3 differential l 1   differential l 2   base page base page l 2   differential l 2   differential l 1   differential page l 1   differential page logical page l 1 comp  differential l 2   base page l 2   logical page l 2 main memory logical page l 1 l 2  a  The logical pages l1 and l2  b  The updated logical pages l1 and l2  and the process of in memory  writing them into the di   erential page p3 in    ash memory  flash memory the page differential logging method main memory logical page p 1 p 2 l 1 l 1 p 1   differential l 1   in p 3 base page l 1   physical page p 3 differential l 2   differential l 1 block b   1 base page l 1   differential page l 1   base page base page l 2   differential page  c  The logical page l1 recreated from    ash memory  Figure 5  An example of the di   erential based approach  4 2 Data Structures The data structures used in    ash memory are base pages  di   erential pages  and di   erentials  A base page stores a logical page in its data area and stores the page   s type  physical page ID  and creation 14time stamp in its spare area  Here  the type indicates whether the page is a base one or di   erential one  and the physical page ID represents the unique identi   er of a page in the database  The creation time stamp indicates when the base page was created  A di   erential page stores di   erentials of logical pages in its data area and stores the page   s type in its spare area  A physical page ID and a creation time stamp are stored also in a di   erential to identify the base page to which the di   erential belongs and when the di   erential was created  Therefore  the structure of a di   erential is in the form of   physical page ID  creation time stamp    o   set  length  changed data       The three data structures used in memory are the physical page mapping table  the valid di   erential count table  and the di   erential write bu   er  The physical page mapping table maps a physical page ID into   base page address  di   erential page address    This table is used to indirectly reference a base and di   erential page pair in    ash memory because  in    ash memory  the positions of the physical pages can be changed by the out place scheme  The valid di   erential count table counts the number of valid di   erentials  i e   those that have not been obsoleted  in a di   erential page  When the count becomes 0  the di   erential page is set to obsolete and made available for garbage collection  The di   erential write bu   er is used to collect di   erentials of logical pages into memory and later write them into a di   erential page in    ash memory when it is full  The di   erential write bu   er consists of a single page  and thus  the memory usage is negligible  Figure 6 shows the data structures for PDL  4 3 Algorithms In this section  we present the algorithms for writing a logical page into    ash memory and for recreating a logical page from    ash memory  We call them PDL Writing and PDL Reading  respectively  Figure 7 shows the algorithm PDL Writing  The inputs to the algorithm are the logical page p and its physical page ID pid  The algorithm consists of the following three steps  In Step 1  we read base page pid  from    ash memory  In Step 2  we create di   erential pid  by comparing base page pid  15pid 1  p 1    p 3  physical page mapping table logical page differential write buffer  one page             differential 1 differential 2 physical page ID  base page address   differential page address  flash memory P1 P2 P3 base page pid 1   differential page pid 1   containing  the differential of base page pid 1   physical page block the page differential logging method  main memory  main memory p 3 3 valid differential count table differential  page address count of  valid differentials Figure 6  The data structures for PDL  with p given as an input  In Step 3  we write di   erential pid  into the di   erential write bu   er  If old di   erential pid  resides in the bu   er  we    rst remove the old one  and then  write the new one  Here  there are three cases according to the size of di   erential pid   First  when the size of di   erential pid  is equal to or smaller than the free space of the bu   er  Case 1   we just write di   erential pid  into the bu   er  Second  when it is larger than the free space of the bu   er but is equal to or smaller than Max Di   erential Size 8  Case 2   we execute the procedure writingDi   erentialWriteBu   er    in Figure 8  clear the bu   er  and then  write di   erential pid  into the bu   er  Here  Max Di   erential Size is de   ned as the the maximum size of di   erentials to be stored in di   erential pages  The procedure writingDifferentialWriteBu   er    consists of the following two steps  In Step 1  we write the bu   er   s contents into the di   erential page q that is newly allocated in    ash memory  In Step 2  we update the physical page mapping table ppmt and the valid di   erential count table vdct  For each di   erential d in the bu   er  we 8 In Section 4 1  for ease of exposition  we have explained PDL on the assumption that Max Di   erential Size   the size of one physical page  However  in practice  we can adjust it according to the workload  We will show the performance while varying Max Di   erential Size later in the experiment section  Section 5   16decrement the count for the old di   erential page dp in vdct by executing the procedure decreaseValidDi   erentialCount     Here  if the count becomes 0  we set the di   erential page to obsolete 9 and make it available for garbage collection  We then set di   erential page pid d  in ppmt to the new di   erential page q and increment the count for q in vdct  Here  pid d is the physical page ID of the base page to which the di   erential d belongs  Third  when it is larger than Max Di   erential Size  Case 3   we discard di   erential pid  and execute the procedure writingNewBasePage    in Figure 8  The procedure consists of the following two steps  In Step 1  we write the logical page p itself into the base page q that is newly allocated in    ash memory  In Step 2  we update ppmt and vdct  We set the old base page bp to obsolete making it available for garbage collection  We then decrement the count for the old di   erential page dp in vdct by executing the procedure decreaseValidDi   erentialCount    and set base page pid  and di   erential page pid  in ppmt to q and null  respectively  Figure 8 shows the procedures for the PDL Writing algorithm  Algorithm PDL Writing  Inputs   1   p    updated logical page     2  pid    physical page ID of p    Algorithm     Step 1  Reading the base page by looking up the physical page mapping table ppmt    bp    ppmt pid  base page  Read bp from flash memory     Step 2  Creating a differential    Create differential pid  by comparing bp read from flash memory with  the updated logical page p given as an input     Step 3  Writing the differential into the differential write buffer dwb    IF old differential pid  resides in dwb THEN Remove old differential pid   END    IF    IF the size of differential pid      free space of dwb THEN    Case 1    Write differential pid  into dwb  ELSE IF the size of differential pid    free space of dwb AND  the size of differential pid      Max Differential Size THEN    Case 2    Call writingDifferentialWriteBuffer     Clear dwb  Write differential pid  into dwb  ELSE IF the size of differential pid    Max Differential Size THEN    Case 3    Discard differential pid   Call writingNewBasePage     END    IF    Figure 7  Writing a logical page into    ash memory in PDL  9 For the spare area in a page  a write operation that changes a set of bits from 1 to 0 can be repeatedly performed up to four times without an erase operation  6   17Procedure writingDifferentialWriteBuffer     Input  dwb      differential write buffer    Algorithm     Step 1  Writing dwb into flash memory as a differential page    Write its contents into the physical page q that is newly allocated in flash memory     Step 2  Updating the physical page mapping table ppmt and the valid differential count table vdct    FOR EACH differential d in dwb DO  BEGIN pid d    physical page ID of the base page to which the differential d belongs  dp    ppmt pid d  differential page  IF dp     null THEN      if the differential page already exists    Call decreaseValidDifferentialCount dp      decrement the valid differential count for dp    END    IF    ppmt pid d  differential page    q     set the differential page containing d to the new differential page q    vdct q  count    vdct q  count   1     increment the valid differential count for q    END    FOR    Procedure decreaseValidDifferentialCount     Input  dp      differential page    Algorithm  vdct dp  count    vdct dp  count     1       decrement the valid differential count for dp    IF vdct dp  count   0 THEN Set dp to obsolete  END    IF    Procedure writingNewBasePage     Inputs   1  p      logical page     2  pid    physical page ID of p    Algorithm     Step 1  Writing p into flash memory as a new base page    Write p into the physical page q that is newly allocated in flash memory     Step 2  Updating the physical page mapping table ppmt and the valid differential count table vdct    bp    ppmt pid  base page  dp    ppmt pid  differential page  Set bp to obsolete  IF dp     null THEN Call decreaseValidDifferentialCount dp      decrement the valid differential count for dp    END    IF    ppmt pid  base page    q     set the base page for the logical page p to the new base page q    ppmt pid  differential page    null     set the differential page for p to null    Figure 8  The procedures for the PDL Writing algorithm in Figure 7  Figure 9 shows the algorithm PDL Reading  The input to PDL Reading is the physical page ID pid of the logical page to read  The algorithm consists of the following three steps  In Step 1  we read base page pid  from    ash memory  In Step 2  we    nd di   erential pid  of the base page pid   Here  there 18are two cases depending on the place where the di   erential pid  resides  First  when the di   erential pid  resides in the di   erential write bu   er  i e   when the bu   er has not been yet written out to    ash memory  we    nd it from the bu   er  Second  when we cannot    nd it from the bu   er  we read di   erential page pid  from    ash memory     nding di   erential pid  from it  In Step 3  we recreate a logical page p by merging base page pid  read in Step 1 with di   erential pid  found in Step 2  Algorithm PDL Reading Input  pid      physical page ID    Output  p      logical page    Algorithm     Step 1  Reading the base page  by looking up the physical page mapping table ppmt     bp    ppmt pid  base page  Read bp from flash memory     Step 2  Finding the differential    IF differential pid  resides in the differential write buffer THEN Find differential pid  from the buffer  ELSE dp    ppmt pid  differential page  IF dp     null THEN Read dp from flash memory  Find differential pid  from dp read from flash memory  ELSE Return bp as the result p       there is no differential page    END    IF    END    IF       Step 3  Merging the base page with the differential    Merge bp with differential pid  to make p  Return p  Figure 9  Recreating a logical page from    ash memory in PDL  4 4 Discussions PDL has the following four advantages   1  As compared with the page based methods  it has good write performance  i e   it requires fewer write operations  when we need to re   ect an updated logical page into    ash memory  This is due to the writing di   erence only principle   2  As compared with the log based methods  it has good write performance when a logical page is updated multiple times  This is due to the at most one page writing principle   3  As compared with the log based methods  it has good read performance when recreating a logical page from    ash memory  This is due to the 19at most two page reading principle   4  Moreover  it allows existing disk based DBMSs to be reused without modi   cation as    ash based DBMSs because it is DBMS independent  Figure 10 shows the DBMS architecture that uses    ash memory as a secondary storage  The logbased methods need to modify the storage management module of the DBMS so as to write the update log whenever the page is updated as shown in Figure 10  a   On the other hand  PDL does not need to modify the DBMS but to modify only the    ash memory driver 10 because it computes the di   erential by comparing the whole updated logical page with its base page  Thus  it can be implemented inside the    ash memory driver as shown in Figure 10  b  without a   ecting the storage manager of the existing DBMS  flash memory flash memory driver an existing disk based DBMS the log based method flash memory flash memory driver an existing disk based DBMS the page differential logging method  a  The log based methods   b  page di   erential logging  Figure 10  The DBMS architecture that uses    ash memory as a secondary storage  PDL  however  has the following minor drawbacks  First  when recreating a logical page from    ash memory  PDL has to read one more page than page based methods do  However  this drawback is relatively minor because the speed of read operation is much faster than that of write or erase operations  Furthermore  if a database is used for read only access  PDL reads only one physical page just like page based methods since a di   erential page does not exist  i e   the base page has not been updated   Thus  in this case  the read performance of PDL is as good as that of the page based methods  Second  the data size written into    ash memory in PDL could be larger than that in log based methods  It is because the di   erential contains all the di   erence between an updated logical page and its base 10This    ash memory driver corresponds to the FTL shown in Figure 2  20page  while the update log in the log based methods contains only the di   erence between an updated logical page and its immediate previous version  However  in spite of this drawback  PDL improves the overall performance signi   cantly because the advantages outweigh these drawbacks  We will show the performance advantages later in the experiment section  Section 5   Table 2 summarizes the di   erences between PDL and the log based ones  Table 2  Comparison of PDL with log based and page based ones  PDL log based methods page based methods data to be written an update log the whole page into    ash memory di   erential  changed parts only   changed and unchanged parts  time for writing data only when a logical page whenever a page is into the write bu   er needs to be re   ected updated no write bu   er into    ash memory time for writing data when a page needs into    ash memory when the write bu   er is full to be re   ected into    ash memory number of physical maximum two pages pages to read when  1     n     2  multiple pages one page recreating a logical page architecture loosely coupled tightly coupled loosely coupled  DBMS independent   DBMS dependent   DBMS independent  4 5 Crash Recovery A storage device with a cache normally supports a write through command that    ushes the data written into the cache immediately out to the device  When the write through command is called  PDL    ushes the di   erential write bu   er out into    ash memory  In    ash memory  the page writing is guaranteed to be atomic at the chip level  9   When a system failure occurs  we lose the physical page mapping table and the valid di   erential count table in memory  However  by one scan through physical pages in    ash memory  we can reconstruct those tables  Here  the tables are recovered to the state in which data were re   ected into    ash memory by the write through call or by    ushing the di   erential write bu   er  That is  the data retained in the 21write bu   er only but not written out to    ash memory are not recovered in the tables  This is analogous to the situation where data retained only in the    le bu   er but not written out to disk in a disk    le system are not recovered after a system failure  Thus  when persistency of data is required  a write through call must be used  If a system failure occurs when a base page  or the di   erential write bu   er  is written into    ash memory  but the old base page  or the di   erential page that does not contain any valid di   erential  has not yet been set to obsolete in Figure 7  the new base page  or di   erential page  and the old base page  or di   erential page  might co exist in    ash memory  Thus  to identify the most up to date base page  or di   erential page   we use the creation time stamp stored in a base page and in each di   erential in a di   erential page as in Chang et al   5   Figure 11 shows the algorithm for reconstructing the physical page mapping table ppmt and the valid di   erential count table vdct  For every physical page r in    ash memory  we read the spare area of r and update ppmt and vdct only if r is not obsolete  Here  there are two cases according to the type of r  First  when r is a base page  Case 1   we check whether ts r  is more recent than ts bp   where ts r  is the creation time stamp of r and ts bp  is that of the base page bp currently in ppmt  If so  r must be a more recent base page  Thus  we set base page pid  to r and set the old base page bp to obsolete  where pid is the physical page ID of r  We then check whether ts r  is more recent than ts dp  di   erential pid    which is the time stamp of di   erential pid  in the di   erential page dp currently in ppmt  If so  the di   erential pid  must be obsolete since we have a base page r that is more recent  Thus  we set di   erential page pid  to null and decrement the count for the old di   erential page dp by executing the procedure decreaseValidDi   erentialCount     If ts r  is not more recent than ts bp   we set r to obsolete  Second  when r is a di   erential page  Case 2   we read the data area of r  For each di   erential d in r  we check whether ts d  is more recent than both ts bp  and ts dp  di   erential pid d    where ts d  is the time stamp of d  ts bp  is that of the base page bp currently in ppmt  and ts dp  di   erential pid d   is that of di   erential pid d  in the di   erential page dp currently in ppmt  Here  pid d is the physical page ID of the base page to which the di   erential d belongs  If so  d must be a more recent di   erential of bp than di   erential pid d  currently in ppmt  Thus  we set di   erential page pid d  to r  decrement the 22count for the old di   erential page dp by executing the procedure decreaseValidDi   erentialCount     and increment the count for the new di   erential page r  If r does not contain any valid di   erential after processing all the di   erentials in r  we set r to obsolete  Algorithm PDL RecoveringfromCrash    Reconstructing the physical page mapping table ppmt and the valid differential count table vdct    Initialize ppmt and vdct  FOR EACH physical page r in flash memory DO  BEGIN Read the spare area of r from flash memory  IF IS OBSOLETE PAGE r  THEN CONTINUE  END    IF    IF IS BASE PAGE r  THEN      Case 1  r is a base page    pid    physical page ID of r  bp    ppmt pid  base page  dp    ppmt pid  differential page     ts x  y  returns the creation time stamp as follows   1  if x is a base page or a differential  returns the time stamp of x  here  y can be omitted   2  if x is a differential page  returns the time stamp of differential y in x    IF ts r    ts bp  THEN      r is a more recent base page    Set bp to obsolete  ppmt pid  base page    r       set the base page with pid to the new base page r    IF ts r    ts dp  differential pid   THEN      r is more recent than differential pid  in dp    Call decreaseValidDifferentialCount dp      decrement the valid differential count for dp    ppmt pid  differential page    null       set the differential page containing differential pid  to null    END    IF    ELSE      bp is a more recent base page    Set r to obsolete  END    IF    ELSE      Case 2  r is a differential page    Read the data area of r from flash memory  FOR EACH differential d in r DO  BEGIN pid d    physical page ID of the base page to which the differntial d belongs  bp    ppmt pid d  base page  dp    ppmt pid d  differential page  IF ts d    ts bp  AND ts d    ts dp differential pid d   THEN      d is more recent than bp and differential pid d  in dp    Call decreaseValidDifferentialCount dp      decrement the valid differential count for dp    ppmt pid d  differential page    r     set the differential page containing d to the new differential page r    vdct r  count    vdct r  count   1       increment the valid differential count for r    END    IF    END    FOR    IF vdct r   count   0 THEN       r does not contain any valid differential    Set r to obsolete  END    IF    END    IF    END    FOR EACH    Figure 11  The algorithm for reconstructing the physical page mapping table and the valid di   erential count table upon system failure  23In Figure 11  we set two kinds of useless pages to obsolete   1  base pages that are not recent but have not been set to obsolete and  2  di   erential pages that do not contain valid di   erential but have not been set to obsolete  These pages can occur in    ash memory when a system failure occurs if a base page  or the di   erential write bu   er  has been written into    ash memory  but the old base page  or the di   erential page that does not contain valid di   erentials  has not yet been set to obsolete  The algorithm PDL RecoveringfromCrash guarantees that recovery is normally performed even when a system failure repeatedly occurs during the process of restarting the system  The reason is that the algorithm does not change data in the    ash memory except setting the useless pages  i e   the pages that are no longer used  but have not been set to obsolete  to obsolete  Setting useless pages to obsolete does not a   ect the recovery process of reconstructing the physical page mapping table and the valid di   erential count table  Since scanning the entire    ash memory of 1 Gbytes takes approximately 60 seconds  derived from Table 1 in Section 2   the scan time can be practically accommodated  To recover the physical page mapping table without scanning all the physical pages in    ash memory  we have to log the changes in the mapping table into    ash memory  We leave this extension as a further study  We note that we can implement the proposed PDL and recovery techniques in a DBMS that uses    ash memory to support transactional database recovery just as we do in a DBMS built on top of an O S    le system by using the write through facility whenever persistency of a write operation is required  e g   when writing the    transaction commit    log record   5 Performance Evaluation 5 1 Experimental Data and Environment We compare the data access performance of PDL proposed in this paper with those of the page based and log based methods discussed in Section 3  We use the wall clock time taken to access data from    ash memory  we call it the I O time  as the measure  Here  as the page based method  we use the one employing the out place update  OPU  scheme with the page level mapping technique  which is 24known to have good performance even though the method consumes memory excessively  9   We also compare with the in place update method  IPU   As the log based method  we use the in page logging method  IPL  proposed by Lee and Moon  14   We use the synthetic relational data of 1 Gbytes and update operations for comparing data access performance of the three methods  We de   ne an update operation as consisting of the following three steps   1  reading the addressed page   2  changing the data in the page  and  3  writing the updated page  The reading step  1  creates a logical page by reading physical pages from    ash memory  and the writing step  3  writes the updated logical page as one or more physical pages into    ash memory  The experiments are designed this way to exclude the bu   ering e   ect in the DBMS  Therefore  we can measure read  write as well as overall performance by executing only update operations  The I O time is a   ected by N updates till write and  ChangedByOneU Op  Here  N updates till write is the number of update operations applied to a logical page in memory from the time it is recreated from    ash memory until the time it is re   ected back into    ash memory   ChangedByOneU Op is the percentage of data changed in a logical page by a single update operation  Here  the portion of data to be changed is randomly selected  We also compare the performance of various mixes of read only and update operations varying the percentage of the update operations   UpdateOps   Besides  we measure the performance as we vary the performance parameters of    ash memory  i e   the I O times for read and write operations in Table 1   We also compare the longevity of    ash memory  Finally  we perform the TPC C benchmark  20  as a real workload  Table 3 summarizes the experiments and parameters  In each experiment  garbage collection is invoked whenever there is no more free page in    ash memory 11   Here  the cost  time  of garbage collection is amortized into that of the write operation because garbage collection is incurred by the accumulated e   ect of write operations  We repeatedly execute experiments so that garbage collection is invoked for each block at least ten times on the average after loading the database in order to make the database to reach a steady state  11 In IPL  garbage collection is invoked during the process of merging  25Table 3  Experiments and parameters  Experiments Parameters Exp  1 Read  write  and overall time per  ChangedByOneU Op 2 update operation N updates till write 1 Exp  2 Overall time per update operation  ChangedByOneU Op 2 as N updates till write is varied N updates till write 1     8 Exp  3 Overall time per update operation  ChangedByOneU Op 0 1     100 as  ChangedByOneU Op is varied N updates till write 1  5 Overall time per operation for the mixes  ChangedByOneU Op 2 Exp  4 of read only and update operations N updates till write 1  5 as  UpdateOps is varied  UpdateOps 0     100  ChangedByOneU Op 2 Exp  5 Overall time per update operation as N updates till write 1 the parameters of    ash memory are varied Tread 10     1500 Twrite 500  1000 Exp  6 Number of erase operations per update  ChangedByOneU Op 2 operation as N updates till write is varied N updates till write 1     8 Exp  7 I O time per transaction for TPC C data 1     100 Mbytes as the DBMS bu   er size is varied DBMS bu   er size  0 1     10   of database size  For the experiments  we have implemented an emulator of a 2 Gbyte    ash memory chip using the parameters shown in Table 1 12   We also have implemented the four methods  PDL  x   OPU  IPU  and IPL  y  13 14 for PDL and OPU  Here  x is Max Di   erential Size  de   ned in Section 4 3 in p  17   and y is the amount of log pages in each block  We used the Odysseus ORDBMS  23  24  as the storage system  Here  PDL  OPU  and IPU are implemented outside the DBMS  and IPL inside the DBMS  We conducted all experiments on a Pentium 4 3 0 GHz Linux PC with 2 Gbytes of main memory  We set the size of a logical page to be 2 Kbytes  which is the size of a physical page in    ash memory  We also test the case with a logical page of 8 Kbytes as was done by Lee and Moon  14   12 For each operation  the emulator returns the required time in the    ash memory  which is speci   ed in Table 1  while writing and reading the data to and from the disk  The data are in exactly the same format in disk as would be stored in    ash memory  Thus  access time using the emulator must be identical to that using the real    ash memory  13We set the size of log bu   er for each logical page to the size of a logical page    1 16 as was used by Lee and Moon  14   14 We do not use wear leveling in this paper  but the same wear leveling techniques can be applied to these methods  We use the same garbage collection method suggested by Woodhouse  21  265 2 Results of the Experiments Experiment 1  Figure 12 shows the read  write  and overall time per update operation for the six methods  IPL  18KB   IPL  64KB   PDL  2KB   PDL  256B   OPU  and IPU  For IPL  y   we have varied y from 8 Kbytes to 64 Kbytes  Among them  we select IPL  18KB  and IPL  64KB  because they have the best and worst overall time for update operations  respectively  For PDL  we select PDL  2KB  and PDL  256B  because the amounts of di   erential pages in them are similar to those of log pages in IPL  64KB  and IPL  18KB   respectively  Speci   cally  IPL  64KB  and PDL  2KB  use 50   of    ash memory for storing log di   erential pages  IPL  18KB  and PDL  256B  use 14 1   and 11 1   of    ash memory  respectively  Figure 12  a  shows that the I O time of the reading step per update operation is in the following order  IPL  64KB   IPL  18KB   PDL  2KB    PDL 256B   and OPU   IPU  This result is consistent with what was discussed in Sections 3 and 4  OPU and IPU require one read operation  PDL requires at most twice as many read operations  IPL requires multiple read operations  We note that  when we perform read only operations  we can also achieve the same result as is shown in Figure 12  a   Figure 12  b  shows that the I O time of the writing step is in the following order  IPU  OPU  PDL  2KB   IPL  18KB   IPL  64KB   and PDL  256B   Here  the slashed area indicates the I O time for garbage collection  The result is also consistent with the discussions in Sections 3 and 4  For an update operation  OPU requires two write operations  one for writing the updated page into    ash memory and another for setting the original page to obsolete  However  IPL requires only one write operation for writing the log bu   er into    ash memory  PDL  2KB  requires two write operations approximately for every two update operations  one for writing the di   erential write</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lsb1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lsb1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Location_and_sensor_based_data"/>
        <doc>Ef   cient Algorithms for Finding Optimal Meeting Point on Road Networks ### Da Yan  Zhou Zhao and Wilfred Ng The Hong Kong University of Science and Technology Clear Water Bay  Kowloon  Hong Kong  yanda  zhaozhou  wilfred  cse ust hk ABSTRACT Given a set of points Q on a road network  an optimal meeting point  OMP  query returns the point on a road network G    V   E  with the smallest sum of network distances to all the points in Q  This problem has many real world applications  such as minimizing the total travel cost for a group of people who want to    nd a location for gathering  While this problem has been well studied in the Euclidean space  the recently proposed state of the art algorithm for solving this problem in the context of road networks is still not ef   cient  In this paper  we propose a new baseline algorithm for the OMP query  which reduces the search space from  Q      E  to  V      Q   We also present two effective pruning techniques that further accelerate the baseline algorithm  Finally  in order to support spatial applications that involve large    ow of queries and require fast response  an extremely ef   cient algorithm is proposed to    nd a high quality near optimal meeting point  which is orders of magnitude faster than the exact OMP algorithms  Extensive experiments are conducted to verify the ef   ciency of our algorithms ###  1  INTRODUCTION Applications ranging from location based services to computer games require optimal meeting point  OMP  query as a basic operation  For example  a travel agency may issue this query to decide the location for a tourist bus to pick up the tourists  so that the tourists can make the least effort to get to the meeting point  This is also true for numerous other scenarios such as an organization that wants to    nd a place for its members to hold a conference  In strategy games like WorldofWarcraft  a computer player may need this query as part of the arti   cial intelligence program  to decide the routes of its warriors  There are two popular ways to de   ne the OMP of a set of points Q    q1  q2          qn   based on two commonly used cost functions      min sum  Find the point x   arg minx   i d qi  x   and     min max  Find the point x   arg minx maxi d qi  x   where d p1  p2  is the distance between the points p1 and p2  The metric of distance can be the Euclidean distance  for a Euclidean Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Articles from this volume were invited to present their results at The 37th International Conference on Very Large Data Bases  August 29th   September 3rd 2011  Seattle  Washington  Proceedings of the VLDB Endowment  Vol  4  No  11 Copyright 2011 VLDB Endowment 2150 8097 11 08      10 00  space  or the network distance  for a road network   The network distance between two points on a road network is the length of the shortest path connecting them  Figure 1 a  illustrates the idea of OMPs using a road network with six people at the six black points  who want to meet together at some location on the road network  The upward  left  triangle in Figure 1 a  is the min max OMP  and the downward  right  one is the min sum OMP                  a  Optimal Meeting Points                              b  Split Point Figure 1  Example of OMPs and Split Points  A min sum OMP minimizes the total travel distance of all the people  while a min max OMP minimizes the elapsed travel time  For the example in Figure 1 a   the person at the black point on the left has to walk for 9 km to reach the min sum OMP  and those on the right have to wait for him after they reach the meeting point  On the other hand  all the people will walk for 6 km to get to the minmax OMP  which is faster than the min sum one  Note that both types of OMPs may not be unique in general  For example  for two people at two different locations on a road network  the min sum OMP could be anywhere on the shortest path between them  As transportation is getting more and more convenient nowadays  min sum OMPs are often preferred over min max OMPs  Consider a multi national corporation that plans to hold a meeting to let all its executive of   cers in China report to its CEO from the headquarter in USA  The ideal location for the meeting is in China since most of the participants are in China  while the min max OMP may be within some European country on the path between USA and China  If the meeting is held in China  only the CEO from USA needs to set out earlier to    y to the meeting location  while the other participants can set out at a later time  and the travel cost is minimized  On the other hand  if the meeting is held in the min max OMP  all the participants have to set out early and the total travel cost is huge  Therefore  we study the min sum OMP query in this paper  and whenever OMP  or optimal meeting point  is mentioned later  we are referring to the min sum OMP  While the OMP query has been extensively studied in the Euclidean space  the state of the art algorithm of processing the query in road networks is still not ef   cient  In this paper  we identify an interesting property of this problem  which greatly prunes the search space compared with the best known technique  Two effec  968Table 1  Summary of Notations  Notation Meaning p1  p2 the shortest path between points p1 and p2 p1     p2 the line segment of an edge with endpoints p1 and p2  p1 and p2 are on the same edge  d    the length of the path segment   sd p  Q  the sum of distances of point p to the points in query set Q  Q is omitted when it is clear from the context  tive pruning rules are proposed to further accelerate query processing  Finally  in order to support spatial applications that require fast response  we propose another algorithm to    nd a high quality near optimal meeting point in considerably less time  The rest of this paper is organized as follows  Section 2 reviews the previous studies that are highly relevant to the OMP query  Then  we introduce our ef   cient algorithms  describe the underlying idea  and analyze the time complexity in Section 3  Extensive experiments are presented in Section 4 to show the ef   ciency of our algorithms for the OMP queries  Finally  we conclude our paper in Section 5  Table 1 summarizes the notations used throughout this paper  2  RELATED WORK Like the window query  1  and the various nearest neighbor queries  9  11  6  12  5   the OMP query is also fundamental in spatial databases  The studies of min sum OMP query in the context of Euclidean space date back to the 60s   70s  7  8  4  3   When the Euclidean distance is adopted as the metric of distance  the OMP query is called the Weber problem  7   and the OMP is called the geometric median of the query point set Q  Cooper  7  extended the Weber problem by posing the problem of minimizing the weighted sums of powers of the Euclidean distances  which was further generalized to handle radial cost functions by Reuven Chen  4   However  it has been shown that no closed form formula exists for the Weber problem and its generalizations  and these problems are usually solved by gradient descent methods  with initial point chosen as the center of gravity of the query point set Q  Fortunately  the sum of Euclidean distances is a convex function  since it is the composite of linear norm sum functions  all of which preserve convexity  2   As a result  the gradient descent method is able to approach the global minimum without the worry of being stuck at local minimal values  On the other hand  the OMP query is not well explored in terms of road networks  where the network distance is adopted as the distance metric  However  compared with the Weber problem  this is a more realistic scenario for location based services  Recently   19  proposed a solution to this problem by checking all the split points on the road network  For a point p on a road network  its split point on edge  u  v  is de   ned to be the point x such that d p  u    d u     x    d p  v    d v     x   see Table 1 for the notations   Figure 1 b  illustrates the idea of split point  where the dotted curves denote the shortest paths between the end points  The location marked by the triangle in Figure 1 b  is the split point x of p on edge  u  v   The shortest path from p to any point on the left  or right  of x on edge  u  v  passes through u  or v   It is proved in  19  that an OMP must exist among the split points  which leads to an algorithm that checks the split point of each query point in Q on each edge in the road network G    V   E   and picks the split point with the smallest sum of network distances as the OMP  As a result  the search space is  Q      E   which is huge  Although  19  includes a pruning technique to skip some split points that are guaranteed not to be an OMP  the search space after pruning is still very large  Therefore  a novel road network partitioning scheme is proposed in  19  to further prune the search space  based on the property that the OMP is strictly con     ned within the partition where all the objects in the query set Q are located  This leads to the algorithm which    rst obtains the smallest partition that encloses all the basic network partitions where the points in Q belong  and then checks the split points in this partition  A highly relevant but different type of query is the group nearest neighbor query  13  20   Given two sets of points P and Q  a group nearest neighbor  GNN  query retrieves the point s  of P with the smallest sum of distances to all the points in Q  GNN queries can be applied  for instance  when n users at locations Q    q1  q2          qn  want to choose a restaurant to have dinner together  among a set of restaurants at locations P    p1  p2          pm  in the city  The GNN query is different from the OMP query in that the candidate result locations of the former is the set P while the candidate result locations of the latter is all the possible locations on the road network  Therefore  the OMP query is more dif   cult than the GNN query due to its in   nite search space  The OMP query is also more general than the GNN query in that it does not require users to determine the kind of place to meet at in advance  For a travel agency that needs to decide the location for a tourist bus to pick up the tourists  the set P does not even exist  3  ALGORITHM It is proved in  19  that an OMP must exist among the split points  As a result   19  proposed to check all the  Q      E  split points for query set Q on the road network G    V   E   and to pick the one with the smallest sum of distances to all the points in Q as the OMP  However  the  Q      E  search space is still very huge  In Section 3 1  we improve the search space to  V      Q  by proving that V    Q must contain an OMP  and propose our baseline algorithm that only checks all the vertices in V and all the points in Q  and picks the one with the smallest sum of distances to all the points in Q as the OMP  Then  in Section 3 2  we improve the performance of our baseline algorithm by two online convex hull based pruning techniques  which restrains the search space to a small region of the whole road network  This region is always smaller than the partition obtained by  19  that uses the off line road network partitioning scheme  As a result  our pruning technique achieves better pruning effect  To further support spatial applications that involve simultaneous evaluation over many queries and require fast response  we propose another algorithm in Section 3 3 that    nds a high quality nearoptimal meeting point in considerably less time  3 1 Baseline Algorithm For a query point set Q  the baseline algorithm of  19  treats all the  Q      E  split points in the road network G    V   E  as candidates for the OMP  However  it is not necessary to compute all the split points and evaluate the sums of distances for all of them  In fact  it is suf   cient to consider only the vertices in V and the points in Q for the OMP  which we prove next  LE M M A 1  Given a query point set Q  let sd p  denote the sum of distances of point p to the points in Q  Suppose that no point in Q is on edge  u  v  except for the two end points u and v  then for any point x on edge  u  v   we have sd x      min sd u   sd v    PROOF  For a point x on edge  u  v   we denote Qu as the set of query points whose shortest paths to x pass through u  Accordingly  Qv   Q   Qu is the set of query points whose shortest paths 969            a                     b  Figure 2  Illustration of Lemma 1 and Theorem 1  to x pass through v  Without loss of generality  let us assume that  Qu       Qv   Figure 2 a  illustrates this scenario  where the hollow points are the query points and the dotted lines are part of their shortest paths to x  Now consider the point x   on edge  u  v  which is    closer to u than x  Let Qab  a  b      u  v   denote the set of query points that belong to Qa when the meeting point is x and belong to Qb when the meeting point is x     Therefore  we can classify the points in Q into four disjoint sets  Quu  Qvv  Quv and Qvu  For these four point sets  we have the following properties         p     Quu  d p  x      d p  x          Proof  d p  x      d p  u    d u     x       d p  u     d u     x             d p  u    d u     x            d p  x                 p     Qvv  d p  x      d p  x        Proof  d p  x      d p  v    d v     x       d p  v     d v     x           d p  v    d v     x          d p  x            Quv        Proof  For any p     Qu when the meeting point is x  we have d p  v   d v     x       d p  v     d v     x         d p  v    d v     x      d p  x    d p  u    d u     x    d p  u     d u     x             d p  u    d u     x      which implies that the shortest path from p to x   cannot pass through v  i e  p      Qv  when the meeting point is x            p     Qvu  d p  x        d p  x        Proof  d p  x        d p  v    d v     x       d p  v    d v     x         d p  x        Therefore  we have   q   Q d q x        q   Quu     q   Qvv     q   Quv     q   Qvu   d q x        q   Quu  d q x                q   Qvv     q   Qvu    d q x                  q   Quu     q   Qvv     q   Qvu   d q x         Quu       Qvv       Qvu   As Quv        we have   q   Quv d q x      0  Besides  since  Qu       Qv  when the meeting point is x  i e   Quu     Quv       Qvu     Qvv   we have  Quu       Qvv       Qvu          Quv    0  According to the above analysis    q   Q d q x          q   Quu     q   Qvv     q   Quv     q   Qvu   d q x        q   Q d q x    Thus  we can conclude that sd x         sd x  for arbitrary x  x   and      If we set x   to be u  we reach the conclusion that    x on edge  u  v   sd u      sd x   Due to the symmetry of u and v  if  Qv       Qu  we get     x on edge  u  v   sd v      sd x   To sum up     x on edge  u  v   min sd u   sd v       sd x   Intuitively  Lemma 1 shows that for any edge on the road network  one of the endpoints is at least as good as any other point on the edge in terms of the sum of distances value  Now  let us take into consideration the special case where there exist some query points on an edge  as illustrated by Figure 2 b   By using Lemma 1  we have the following theorem  TH E O R E M 1  Given an OMP query with query point set Q on a road network G    V   E   V     Q contains an OMP  PROOF  For each edge  u  v that contains some query points on it  but not at the end points u and v  let us denote these query points as qi1   qi2           qis  as illustrated in Figure 2 b   We introduce s dummy vertices pi1   pi2           pis on the edge  u  v   where each dummy vertex pij    j   1  2          s  is located at qij   After the introduction of the dummy vertices for all the edges that contain some query points on it but not at its end points  we obtain another road network G   such that all the query points in Q are at its vertices  Since the vertex set of G   is V     Q  we can conclude that V     Q contains an OMP according to Lemma 1  Theorem 1 is general enough for road networks of any topology  since its proof  including that of Lemma 1  relies only on the fact that the road network G is a graph  For example  the edge length can refer to the travel delay rather than physical distance  In fact  we can obtain the following more general statement  TH E O R E M 2  Given a point set Q    q1  q2          qn  on an arbitrary graph G    V   E   where each point qi is associated with a weight wi  If all the weights are integers or rational numbers  then V     Q must contain the point x   arg minx   i wi    d qi  x   PROOF  See Appendix A  Note that the idea of Theorem 1 to    nd the OMP among V     Q does not contradict the idea of  19  to    nd the OMP among the split points  See Appendix B for a detailed discussion on this point  Algorithm 1 Baseline Algorithm 1  given a query point set Q on a road network G    V   E  2  opt        N U LL 3  minCost             4  for each q     Q do 5  cost       sumOfDistance q  Q  minCost  6  if cost   minCost then 7  cost        minCost 8  opt        q 9  for each v     V do 10  cost       sumOfDistance v  Q  minCost  11  if cost   minCost then 12  cost        minCost 13  opt        v 14  return opt Based on Theorem 1  we design our baseline algorithm  Algorithm 1  to check all the points in Q and all the vertices in V   and pick the one with smallest sum of network distances to all the points in Q as the OMP  The function sumOfDistance in Lines 5 and 10 of Algorithm 1 computes the sum of network distances of q  or v  to all the points in Q  which is detailed in Algorithm 2  Lines 4   5 in Algorithm 2 return the partially computed value of the sum of distances for v if it is already larger than minCost  Let 970Algorithm 2 sumOfDistance v  Q  minCost  1  sum        0 2  for each q     Q do 3  sum        sum   d v q  4  if sum   minCost then 5  return sum 6  return sum minCost be the smallest sum of distances that is already found for the time being  then Lines 4   5 act as a pruning step to stop the computation for v since it cannot be the optimal point  Lines 8 and 11 in Algorithm 1 then automatically    lter out such points  A basic operation in all our algorithms is to obtain the length of the shortest path between two points p1 and p2  i e  d p1  p2    e g  Line 3 in Algorithm 2   Since shortest path computation is not our focus  we simply run Dijkstra algorithm for each vertex in the road network  and write all the obtained information into a index    le on the disk  For the details on the construction and organization of our index    le  please refer to Appendix C  After the shortest path index    le is constructed  the length of the shortest path between two vertices u  v     V on the road network G    V   E   i e  d u  v   can be obtained by only one I O operation  and the shortest path u  v    p0   u  p1  p2          p    v  can be obtained by   I O operations  Further  we can utilize the above operation to obtain d p  v  for any point p on the road network and any vertex v     V by two I O operations  and d p1  p2  for any two points p1 and p2 on the road network by at most four I O operations  The reasoning of the above statements is presented in Appendix D  To sum up  only O 1  I O operations are required to obtain the the length of the shortest path between two arbitrary points on the road network  while   I O operations are required to obtain a shortest path of length   between two points  As there are  V   vertices to check  Lines 9   13 in Algorithm 1   each of which requires O  Q   I O operations to compute the value of the sum of distances  Lines 2 and 3 in Algorithm 2   the total number of I O operations required is O  Q      V     Similarly  Lines 4   8 in Algorithm 1 take  Q     O  Q     O  Q  2   I O operations  To sum up  the time complexity of Algorithm 1 is O  Q      V      Q  2    3 2 Pruning Based on Convex Hull Compared with the split point based method in  19   our baseline algorithm in Section 3 1 has already signi   cantly reduced the search space of the OMP query  However  there is still room for the further pruning of the search space when the edge length of the road networks is based on the physical distance  For example  if all the query points are in California  then there is no need to check the vertices in Utah on the road network  Based on this rationale   19  cuts the whole road network into partitions  and checks only those split points that are in the smallest partition enclosing all the basic network partitions where the query points belong  Consider the partitioned road network shown in Figure 3 a   where the four black points are the query points   19  only checks the split points in the gray area  However  the correctness of that pruning technique relies on the assumption that whenever two roads cross with each other  there is an intersection vertex at the crossing point  This assumption may not be true in reality  e g  when one road is a viaduct or a tunnel  Appendix E shows an example road network where the pruning technique of  19  makes mistakes  Furthermore  the pruning effectiveness of that network partitioning scheme is still not suf   cient  Referring to Figure 3 a  again  it  a  Graph Partitioning          b  Counterexample of Alg  3 Figure 3  Illustration of Pruning Techniques  is intuitive that the optimal meeting point must appear in the region surrounded by the dotted curve  and it is not necessary to check the remaining part of the gray area  which is the area checked by the network partitioning scheme  Besides  there is no strict underlying principle on how to partition a road network mentioned in  19   Now  we propose two online convex hull based pruning techniques that are more effective  Although our online pruning techniques do not rely on a pre computed index  they provide higher ef   ciency since the dominating factor of the query processing time is the number of points vertices to check  and the time of convex hull computation required by our techniques is negligible  Before describing our pruning techniques  we    rst de   ne the format of a query point q in the query point set Q  Since a query point on a road network G    V   E  must be on some edge  u  v      E  we de   ne the format of a query point as follows  DE FI N I T I O N 1  Given a road network G    V   E  and a query point q on edge  u  v      E  we de   ne the format of q as the triplet  u  v      such that uq                uv         According to De   nition 1  q is at the vertex u when      0  and at v when      1  The    rst phase of our pruning techniques is to collect into a set P those end points of all the edges which the query points in Q are on  and then compute the convex hull of the point set P  Algorithm 3 details this process  where convexHull P  computes the convex hull of the point set P using Andrew   s Monotone Chain algorithm  15   which takes O  P  log  P   time  Algorithm 3 hullPhase1 Q  1  given a query point set Q on a road network G    V   E  2  P            3  for each q    u  v          Q do 4  P        P     u 5  P        P     v 6  return convexHull P  The    rst phase pruning simply checks the points in Q  and the vertices in the region surrounded by the convex hull computed by Algorithm 3 to    nd the optimal meeting point  However  this is not suf   cient  as we are going to illustrate  Consider the road network in Figure 3 b  where a bridge crosses over a river  The query points are marked by the triangles  The convex hull returned by Algorithm 3 is the one drawn with dotted lines  It is easy to check that v is the OMP  but it is outside of the region surrounded by the convex hull  In order to avoid such false negatives in search space pruning  we propose a second phase of convex hull computation  Suppose that the convex hull returned by hullPhase1 Q  is represented as H    h1  h2          h   h1  where the points hi on H are listed in clockwise order  we    nd the shortest path for each pair of neighboring points on H  insert all the points on these paths into a set S  971Algorithm 4 hullPhase2 H  1  given the convex hull H    h1  h2          h   h  1   h1  returned by hullPhase1 Q  2  S            3  for i   1 to   do 4  Get the shortest path L between hi and hi 1 5  for each vertex p on L do 6  S        S     p 7  return convexHull S  and then compute the convex hull of S  The process of the second phase pruning is detailed in Algorithm 4  where we use our index    le to obtain the shortest path between two vertices in Line 4  For the previous example in Figure 3 b   the OMP v is now in the region surrounded by the convex hull returned by Algorithm 4  since it is on the shortest path between a to b  which are the two neighboring vertices on the convex hull returned by Algorithm 3  Let  H  denote the number of vertices on H  and  Lmax  be the maximum number of points on the shortest path between two neighboring points on H  then Algorithm 4 takes O  H      Lmax     log  H      Lmax    time  Now  we present Algorithm 5 that    rst performs our two phase online convex hull computation  and then checks only the query points and the vertices in the region surrounded by the convex hull to    nd the OMP  Algorithm 5 Two Phase Online Convex Hull Based Pruning 1  given a query point set Q on a road network G    V   E  2  H       hullPhase1 Q  3  H        hullPhase2 H  4  opt        N U LL 5  minCost             6  for each q     Q do 7  cost       sumOfDistance q  Q  minCost  8  if cost   minCost then 9  cost        minCost 10  opt        q 11  for each v     V that is in the region surrounded by H  do 12  cost       sumOfDistance v  Q  minCost  13  if cost   minCost then 14  cost        minCost 15  opt        v 16  return opt If we use only the    rst phase of convex hull computation  Lines 2 and 3 in Algorithm 5 can be replaced by    H        hullPhase1 Q      To support ef   cient range query evaluation in Line 11 in Algorithm 5  we organize all the vertices in V by a kd tree  The query window is the minimum bounding box  MBR  of the convex hull H    and for the vertices in the MBR  a re   nement step is performed to obtain the vertices that are really in the region surrounded by H                   Figure 4  Points Inside Outside the Convex Hull  We check whether a vertex is inside the region surrounded by the convex hull using the following property  given three points p1    x1  y1   p2    x2  y2  and p3    x3  y3   let us de   ne ccw p1  p2  p3     x2     x1       y3     y1       x3     x1       y2     y1   or simply ccw   The angle p1p2p3 is in counter clockwise order if ccw   0  in clockwise order if ccw   0  and p1  p2 and p3 are on the same line if ccw   0  To judge whether a point p is inside or on the boundary of the region surrounded by a convex hull H    we have the following theorem  TH E O R E M 3  A point p is inside or on the boundary of the region surrounded by a convex hull H    if and only if for any edge  p0  p1  on H  where p0 and p1 are listed in clockwise order  ccw p0  p1  p      0  Since the result convex hull H  returned by Andrew   s Monotone Chain algorithm is a stack of points on H  that are pushed in counter clockwise order  for some point p  we pop each edge  p0  p1  on H   in clock wise order  and check ccw p0  p1  p   Figure 4 illustrates the process  As long as we    nd ccw   0 for an edge on H    p is outside the region surrounded by H  and thus pruned  Otherwise  we need to check p  Let  H    denote the number of vertices on H    then this check takes O  H     time  Algorithm 5 is able to    nd the OMP on almost all real road networks except for very unusual cases as illustrated in Appendix E  Actually  we have done extensive experiments on the road network datasets of 46 states in US  21   and the dataset of    city of Oldenburg     OL   22   Altogether 1700 randomly generated OMP queries are performed on each dataset  Algorithm 5 only fails to return the OMP in 5 of the 79900 queries in total  and the sum ofdistances values of these result meeting points are all within 0 1  more than the smallest sum of distances value  3 3 Fast Greedy Algorithm for OMP Queries Although the convexity property of the    sum of Euclidean distances    function no longer holds for the sum of network distances  the breach of this property is not signi   cant since both types of distances are de   ned over a metric space  0 5000 10000 0 5000 10000 0 2 4 x 10 4 Sum of Distances Y X Figure 5     Sum of Distances    Values at All Network Vertices  For example  Figure 5 shows the values of the sums of network distances at all the vertices  represented by  X  Y   coordinates  in the road network of the OL dataset for an OMP query  Warmer vertex color represents larger sum of distances value  From the shape of the function  we can observe that    convexity    is preserved to a great extent  One can easily obtain the same observation for arbitrary query point sets on road networks  Inspired by this observation and the gradient descent methods of  7  8   we propose a greedy algorithm as shown in Algorithm 6  where we denote the sum of distances value of a vertex u as sd u   and the set of neighboring vertices of u as N B u   972Algorithm 6 Greedy Algorithm 1  given a query point set Q on a road network G    V   E  2  Compute the center of gravity of Q as  xc  yc  3  Obtain the vertex vnn that is nearest to  xc  yc  by a nearest neighbor query on the vertex kd tree 4  opt        vnn 5  repeat 6  min        arg minu   N B opt  sd u  7  if sd min    sd opt  then 8  return opt 9  else opt        min Algorithm 6    rst computes the center of gravity  xc  yc  of the query point set Q  Line 2   which is the common choice of the initial point for the gradient descent methods of the Weber problem  As our initial point should be a vertex rather than an arbitrary point in the 2D space  Algorithm 6 picks the vertex that is closest to  xc  yc  as the initial point  Line 3  by a nearest neighbor query on the vertex kd tree  In each iteration  Algorithm 6    nds the neighbor min of the current vertex opt that has the smallest sum of distances among all the neighbors  Line 6   If the neighbor min has a smaller sum of distances than the current vertex opt  we update the current vertex to be min  Line 9   Otherwise  Algorithm 6 terminates and the current point is returned  Line 8   As we will see in Section 4  although Algorithm 6 may get stuck in a local optimal point  i e  all its neighbors have larger sums of distances   its sum of distances value is very close to the minimum value  More importantly  Algorithm 6 is often able to    nd the exact OMP and runs orders of magnitude faster than the algorithms described in Sections 3 1 and 3 2  Thus  the algorithm is extremely suitable for large scale query processing in real time  and the upper bound estimation of sum of distances for accelerating location constraint evaluation in applications such as those mentioned in  19   4  EXPERIMENTS In this section  we evaluate the performance of our algorithms for the OMP queries by using the road network datasets of 46 states in US from  21   and the datasets of    city of Oldenburg     OL  and    California     CA  from  22   Table 2 in the Appendix describes 31 of the datasets we use  which contains the number of nodes and edges in each dataset  The datasets of the remaining states in US from  21  are not used either because their sizes are too small  or because the datasets are composed of many small connected components  We require that all the vertices belong to the same component since we randomly generate OMP query points on the road networks  Appendix F details the experimental platform con   guration and the preprocessing of the datasets  For each dataset  we generate queries by imposing a rectangular window on the dataset  and all the query points are randomly generated on the part of the road network in the window  Let W denote the distance between the x coordinates of the leftmost vertex and the rightmost vertex on the road network  and H denote the distance between the y coordinates of the highest vertex and the lowest vertex on the road network  We parameterize the size of a window by the parameter      1  so that the window has size   W     H  Appendix G presents the details of our query generator  The con   guration of a query set Q can be represented as a triple      Np  Nw   where Nw denotes the number of windows used to generate the query set  Np denotes the number of query points generated in each window  and    decides the size of each window  which is   W      H   The total number of query points is  Q    Np    Nw  We introduce the parameter Nw to generate query sets whose query points belong to several groups  and the points in each group are close to each other  For each dataset and each query set con   guration  we randomly generate 100 query sets for evaluation  We implemented the split point checking algorithm proposed in  19   denoted as Split  SP   for experimental comparison  For fairness of comparison  we use our pre computed shortest path index for the shortest path computations required by Split  Besides Split  SP   in the sequel  we denote our baseline algorithm as Baseline  BL   the algorithm that uses only the    rst phase of convex hull based pruning as HullWindow  HW   the one that uses two phase convex hull based pruning as HullWindow2  HW2   and the greedy algorithm as Greedy  GD   To fully utilize the pruning in Lines 4   5 of Algorithm 2  we can use the result of Greedy to initialize the current minimum sum ofdistances value of the other algorithms  i e  Line 3 of Algorithm 1 and Line 5 of Algorithm 5  We denote the algorithm versions that use Greedy for initialization by appending an apostrophe to the original algorithm names  e g  Baseline becomes Baseline     BL      We evaluate the performance of our algorithms based on the following four criteria   1  query processing time   2  the sum ofdistances ratio of the other algorithms to Baseline   3  the number of steps  i e  iterations  of Greedy  and  4  the number of times that each algorithm    nds the exact OMP among the 100 queries  for each con   guration on each dataset   The reported value of the    rst three criteria are averaged over the 100 queries  Due to the space limitation  we only show our results on the    CA    and    OL    datasets from  22   and the overall results of the 47 datasets in this section  Appendix H shows part of the results on all the datasets  and the complete results are available online 1 2   4 1 Effect of the Window Size of Query Sets Figure 6 a   Figure 7 a   shows the average execution time of our algorithms  over 100 queries  for different window sizes  determined by      where we set Nw   1 and Np   20 for each query  We can see that Split takes the most time  Baseline the second most  HullWindow2 the third  HullWindow the fourth  and    nally Greedy  In all our experiments  Split is found to be stably around an order of magnitude slower than our most expensive algorithm Baseline  which is better than our expectation due to its pruning rule  Besides  initialization using Greedy does improve the performance of the algorithms  and the improvement on OL is more obvious than that on CA  While the query processing time increases with the increment of window size for the other algorithms  Greedy shows rather stable performance for all values of     and usually takes tens of milliseconds  On the average of the 47 datasets in this set of experiments  HullWindow2 runs 2 14 times faster than Baseline  and Greedy runs 142 02 times faster than HullWindow2  Let sd u  Q  denote the sum of distances value of vertex u for query set Q  As Baseline guarantees to return the OMP opt  for some other algorithm that returns the meeting point v  we de   ne its sum of distances ratio to be sd v Q  sd opt  Q      1  Figure 6 b   Figure 7 b   shows the average sum of distances ratio of our algorithms  We can see that HullWindow2 always returns the OMP  while HullWindow may return a near optimal meeting point when      40   The result of Greedy is less optimal but the sum ofdistances ratio is always within 3 5   Figure 6 c   Figure 7 c   shows the average number of iterations run by Greedy for different window sizes  and Figure 6 d   Fig  1 http   www cse ust hk    yanda datasets part1 pdf 2 http   www cse ust hk    yanda datasets part2 pdf 973 1  10  100  1000  10000  100000  20 30 40 50 60 70 80 90 100 Exec Time  ms         SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 005  0 01  0 015  0 02  0 025  0 03  0 035  20 30 40 50 60 70 80 90 100 Ratio        HW HW2 GD  b  Sum of Distances Ratio  12  13  14  15  16  17  18  19  20  21  20 30 40 50 60 70 80 90 100 Step        GD  c  Number of Steps of Greedy  0  10  20  30  40  50  60  70  80  90  100  20 30 40 50 60 70 80 90 100 OMP Percentage        HW HW2 GD  d  OMP percentage Figure 6  Effect of the Window Size of Query Sets on the CA Dataset   1  10  100  1000  10000  100000  20 30 40 50 60 70 80 90 100 Exec Time  ms         SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 002  0 004  0 006  0 008  0 01  0 012  0 014  0 016  0 018  20 30 40 50 60 70 80 90 100 Ratio        HW HW2 GD  b  Sum of Distances Ratio  6  6 2  6 4  6 6  6 8  7  7 2  7 4  7 6  20 30 40 50 60 70 80 90 100 Step        GD  c  Number of Steps of Greedy  20  30  40  50  60  70  80  90  100  20 30 40 50 60 70 80 90 100 OMP Percentage        HW HW2 GD  d  OMP percentage Figure 7  Effect of the Window Size of Query Sets on the OL Dataset   1  10  100  1000  10000  100000  1e 006  0 20 40 60 80 100 120 140 Exec Time  ms    of Query Points SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 01  0 02  0 03  0 04  0 05  0 06  0 20 40 60 80 100 120 140 Ratio   of Query Points HW HW2 GD  b  Sum of Distances Ratio  4  6  8  10  12  14  16  18  20  22  0 20 40 60 80 100 120 140 Step   of Query Points GD  c  Number of Steps of Greedy  10  20  30  40  50  60  70  80  90  100  0 20 40 60 80 100 120 140 OMP Percentage   of Query Points HW HW2 GD  d  OMP percentage Figure 8  Effect of the Number of Query Points on the CA Dataset   0 1  1  10  100  1000  10000  100000  1e 006  0 20 40 60 80 100 120 140 Exec Time  ms    of Query Points SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 005  0 01  0 015  0 02  0 025  0 03  0 035  0 04  0 045  0 05  0 20 40 60 80 100 120 140 Ratio   of Query Points HW HW2 GD  b  Sum of Distances Ratio  3 5  4  4 5  5  5 5  6  6 5  7  7 5  8  0 20 40 60 80 100 120 140 Step   of Query Points GD  c  Number of Steps of Greedy  20  30  40  50  60  70  80  90  100  0 20 40 60 80 100 120 140 OMP Percentage   of Query Points HW HW2 GD  d  OMP percentage Figure 9  Effect of the Number of Query Points on the OL Dataset   10  100  1000  10000  100000  20 25 30 35 40 Exec Time  ms         SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 005  0 01  0 015  0 02  0 025  0 03  0 035  20 25 30 35 40 Ratio        HW HW2 GD  b  Sum of Distances Ratio  10  12  14  16  18  20  22  24  26  20 25 30 35 40 Step        GD  c  Number of Steps of Greedy  10  20  30  40  50  60  70  80  90  100  20 25 30 35 40 OMP Percentage        HW HW2 GD  d  OMP percentage Figure 10  Effect of Multiple Windows on the CA Dataset  974ure 7 d   shows the number of times that the algorithms return the exact OMP among the 100 queries  HullWindow2 always returns the OMP  while HullWindow returns the OMP for over 90  of the queries  the percentage of which reaches 100  as    increases to 40   On the other hand  Greedy manages to return the OMP for only a small fraction of the queries  and the percentage goes down with the increment of     4 2 Effect of the Number of Query Points Figures 8  Figures 9   a     d  show the results of our algorithms for different query set sizes  Np   2 i   i      1  2          7    In this set of experiments  we set Nw   1 and      40   From Figures 8  Figures 9   a  and  d   we can obtain similar observations as in Section 4 1  We    nd that HullWindow2 can be tens of times faster than Baseline for small Np  but the gap reduces to only several times when Np becomes larger  On the average of the 47 datasets in this set of experiments  HullWindow2 runs 5 88 times faster than Baseline  and Greedy runs 54 times faster than HullWindow2  We can see from Figure 8 b   Figure 9 b   that the average sumof distances ratio of Greedy gets smaller as there are more query points in the query set  which stabilizes at between 1  and 1 5  when  Q      60  Figure 8 c   Figure 9 c   shows that Greedy requires more iterations when  Q  becomes larger  4 3 Effect of Multiple Windows In this set of experiments  we study the scenario where the query points belong to several groups  and the points in each group are close to each other  Speci   cally  we study the case where there are two groups and each group contains 10 query points  We set Nw   2  Np   10  and vary the parameter    to see the performance of our algorithms for different window sizes  Figures 10  Figures 11 in the Appendix   a     d  show the results of our algorithms for different query set sizes  determined by Np   We    nd that the performance of all the algorithms are quite stable  As    increases  the query processing time does not increase  the sum of distances ratio decreases  and the number of iterations of Greedy increases  On the average of the 47 datasets in this set of experiments  HullWindow2 runs 2 51 times faster than Baseline  and Greedy runs 129 9 times faster than HullWindow2  Among the OMP 79900 queries in total  HullWindow2 only fails to return the OMP for 5 queries  and the sum of distances ratio of these result meeting points are all within 0 1   Therefore  HullWindow2 is a good alternative to Baseline  especially for small query sets  Greedy is over two orders of magnitude faster than the other algorithms  and usually takes only tens of milliseconds  Besides  its sum of distances ratio is usually around 3   Therefore  the meeting point returned by Greedy is of high quality  and Greedy is the most practical method to support large scale meeting point queries on real world spatial database servers  5  CONCLUSION In this paper  we study the optimal meeting point query that returns the point on a road network G    V   E  with the smallest sum of network distances to all the query points in a given query set Q  Our baseline algorithm substantially reduces the search space of the OMP query from  Q      E  to  V      Q  according to the spatial property established in Theorem 1  We also design an effective two phase convex hull based pruning technique to further prune the search space  Finally  we develop an extremely ef   cient greedy algorithm to    nd a high quality near optimal meeting point instead of an exact OMP  The ef   ciency of this algorithm makes it the most practical choice for spatial applications that involve large    ow of queries and require fast response as the top priority  ACKNOWLEDGEMENTS  We are grateful to the anonymous reviewers for their insightful comments on this paper  This work is partially supported by RGC GRF under grant number HKUST 618509  6  REFERENCES  1  N  Beckmann  H  Kriegel  R  Schneider and B  Seeger     The R  Tree  An ef   cient and robust access method for points and rectangles     In SIGMOD  pp  322 331  1990   2  S  Boyd and L  Vandenberghe     Convex Optimization     Cambridge University Press  http   www stanford edu    boyd cvxbook bv cvxbook pdf  3  R  Chen     Location Problems with Costs Being Sums of Powers of Euclidean Distances     Computers   Mathematics with Applications  vol  10  issue 1  pp  87   94  Dec  1984   4  R  Chen     Solution of Location Problems with Radial Cost Functions     Computers   Mathematics with Applications  vol  10  issue 1  pp  87   94  Dec  1984   5  Z  Chen  H  T  Shen  X  Zhou and J  X  Yu     Monitoring Path Nearest Neighbor in Road Networks     In SIGMOD  pp  591   602  2009   6  H  Cho and C  Chung     An Ef   cient and Scalable Approach to CNN Queries in a Road Network     In VLDB  pp  865   876  2005   7  L  Cooper     An Extension of the Generalized Weber Problem     Journal of Regional Science  vol  8  issue 2  pp  181   197  Dec  1968   8  L  Cooper     The Mulitfacility Location Problem  Applications and Descent Theorems     Journal of Regional Science  vol  17  issue 3  pp  409   419  Dec  1977   9  G  Hjaltason and H  Samet     Distance Browsing in Spatial Databases     In TODS  pp  265   318  1999   10  H  Hu  D  L  Lee and V  C  S  Lee     Distance Indexing on Road Networks     In VLDB  pp  894   905  2006   11  M  R  Kolahdouzan and C  Shahabi     Voronoi Based k Nearest Neighbor Search for Spatial Network Databases     In VLDB  pp  840   851  2004   12  K  Mouratidis  M  L  Yiu  D  Papadias and N  Mamoulis     Continuous Nearest Neighbor Monitoring in Road Networks     In VLDB  pp  43   54  2006   13  D  Papadias  Q  Shen  Y  Tao and K  Mouratidis     Group Nearest Neighbor Queries     In ICDE  pp  301   312  2004   14  D  Papadias  J  Zhang  N  Mamoulis and Y  Tao     Query Processing in Spatial Network Databases     In VLDB  pp  802   813  2003   15  F  P  Preparata and M  I  Shamos     Computational Geometry  An Introduction     Springer Verlag  1985   16  H  Samet  J  Sankaranarayanan and H  Alborzi     Scalable Network Distance Browsing in Spatial Databases     In SIGMOD  pp  43   54  2008   17  Y  Tao  D  Papadias and Q  Shen     Continuous Nearest Neighbor Search     In VLDB  pp  287   298  2002   18  Z  Xu and H  A  Jacobsen     Ef   cient Constraint Processing for Location aware Computing     In MDM  pp  3   12  2005   19  Z  Xu and H  A  Jacobsen     Processing Proximity Relations in Road Networks     In SIGMOD  pp  243   254  2010   20  M  L  Yiu  N  Mamoulis and D  Papadias     Aggregate Nearest Neighbor Queries in Road Networks     In TKDE  pp  820   833  2005   21  http   data geocomm com catalog US  22  http   www cs fsu edu    lifeifei SpatialDataset htm 975 1  10  100  1000  10000  100000  20 25 30 35 40 Exec Time  ms         SP BL BL    HW HW    HW2 HW2    GD  a  Exec  Time  0  0 005  0 01  0 015  0 02  0 025  0 03  20 25 30 35 40 Ratio        HW HW2 GD  b  Sum of Distances Ratio  6  6 2  6 4  6 6  6 8  7  7 2  7 4  20 25 30 35 40 Step        GD  c  Number of Steps of Greedy  20  30  40  50  60  70  80  90  100  20 25 30 35 40 OMP Percentage        HW HW2 GD  d  OMP percentage Figure 11  Effect of Multiple Windows on the OL Dataset  Table 2  DataSet Description data vertex   edge   data vertex   edge   data vertex   edge   data vertex   edge   CA 21048 21693 FL 6879 7303 LA 5886 6133 NY 7503 7766 OL 6105 7035 GA 6757 7038 MA 1530 1595 OH 4738 4986 AR 7454 7695 IA 4833 5168 MD 1222 1270 OK 6877 7300 AZ 11050 11381 ID 8108 8310 ME 4536 4656 PA 6384 6619 CO 9796 10175 IL 6117 6520 MI 5928 6198 RI 203 212 CT 923 958 IN 4300 4592 MN 7718 8184 SC 2868 2966 DE 250 257 KS 5615 6054 MO 9500 9889 SD 6733 7165 DR 10513 10738 KY 4745 4952 MS 6022 6297 APPENDIX A  PROOF OF THEOREM 2 Theorem 2  Given a point set Q    q1  q2          qn  on an arbitrary graph G    V   E   where each point qi is associated with a weight wi  If all the weights are integers or rational numbers  then V     Q must contain the point x   arg minx   i wi    d qi  x   PRO O F  It is straightforward to convert the rational number weights into integer weights with the same weight distribution among all the points in Q  For example  suppose Q    q1  q2  q3   w1   0 15  w2   1 11 and w3   0 8  then we can re assign the weights to be w1   15  w2   111 and w3   80  Clearly  this transformation does not change the result point x  Now  let us assume that all the weights are integers  and we replace each point qi with wi new points at the same location of qi  each of which has weight 1  The resulting new query point set Q   can be treated as unweighted  and thus Q      V contains x according to Theorem 1  It is straightforward to see that the transformation from Q to Q   does not change the result point x  and the locations in Q   is exactly the locations in Q  Therefore  Theorem 2 is proved  B  DISCUSSION ON THEOREM 1 AND SPLIT POINT BASED OMP ALGORITHM Theorem 1 states that we can    nd an OMP among the vertices and query points  while  19  states that we can    nd an OMP among the split points  Although the two methods seems to be contradictory  they are both correct because  1 OMP may not be unique  and  2 OMP can be both a split point and a vertex  The    rst reason is easy to understand  consider a query point set Q    q1  q2   then any point on the shortest path p1  p2 can be the OMP  including all the split points and vertices on p1  p2  Now let us discuss the second reason  For a point p on a road network  its split point on edge  u  v  is de   ned by  19  to be the point s such that d p  u    d u     s    d p  v    d v     s   1                                                                  Figure 12  Four Cases for Split Points  However  this de   nition is only suitable for the scenario that p is not on  u  v   which can be further divided into three cases illustrated by Figure 12 a     c  where the dotted curves denote the shortest paths      Case 1  p  v does not pass through u  and p  u does not pass through v  Figure 12 a    In this case  the split point s exists due to the fact that the road network is a metric space  i e  d p  u    d u     v      d p  v  and d p  v    d u     v      d p  u   If we represent an arbitrary point x on  u  v  using lx   d u     x   we can see that d p  x  is a piecewise linear function to lx on  u  v  which consists of two different linear functions de   ned on  0  ls  and  ls  lv       Case 2  p  v passes through u  Figure 12 b    In this case  the split point is vertex v  as can be checked using Equation  1   For an arbitrary point x on  u  v   d p  x    d p  u   lx is a function linear to lx      Case 3  p  u passes through v  Figure 12 c    In this case  the split point is vertex u  as can be checked using Equation  1   For an arbitrary point x on  u  v   d p  x    d p  u     lx is a function linear to lx  When p is on  u  v   Figure 12 d    we de   ne p to be the split point s so that d p  x     ls    lx  is still a piecewise linear function delimited by s  Therefore  for each query point p and a point x on  u  v   d p  x  is a piecewise linear function delimited by the split point s  Since the sum of distances value   p   Q d p  x  is the sum of piecewise linear functions delimited by the split points  it achieves the mini  976mum or maximum at the split points on  u  v   This establishes the correctness of the split point checking method in  19   Note that for the cases illustrated by Figures 12 b     d   the split points belong to V     Q which are those checked by Theorem 1  In fact  our experiments show that the vast majority of the OMPs returned by the split point checking method in  19  belong to Cases 2 and 3  Although both methods are proved to be correct  our method checks  V      Q  candidate points  which is much smaller than the  Q      E  split points computed and checked by  19   C  SHORTEST PATH INDEX BETWEEN VERTICES Since the de   nition of OMP on a road network is directly built on shortest paths  we construct a disk based index    le to accelerate the shortest path computation between two vertices on the road network  Shortest path computation is actually the basic operation of many queries on the road network  19  16  10  14  5   While a lot of ef   cient online algorithms have been proposed  14  5  for shortest path computation on road networks   16  presents a nice off line index for shortest path computation on road networks  As road networks seldomly change  off line indices are usually viable and more effective in shortening query execution time  As shortest path computation is not our main focus  we simply run Dijkstra algorithm for each vertex in the road network  to compute d v1  v2  for each pair of vertices v1 and v2  The result is a 2D array  which is written to a    le  During query processing  to obtain d v1  v2   we set the    le pointer to the location where it is stored and read the value  Therefore  only one I O operation is required to obtain d v1  v2   To    nd the shortest path between two vertices  we maintain another disk based index which is also built by the Dijkstra algorithm  together with the shortest path length index mentioned above  Besides the length of the shortest path from vertex v1 to v2  the Dijkstra algorithm also reports the vertex before v2 on the shortest path from v1 to v2  which can be used to    nd the shortest path by going back iteratively  We store another 2D array P ath on the disk  which is appended at the end of shortest path length index in the index    le  where P ath v1  v2  stores the    rst vertex after v1 that is on the shortest path from v1 to v2  Algorithm 7 shows the way to    nd the shortest path L between v1 and v2  which takes   I O operations  where   denotes the length of L  Algorithm 7 shortestPath v1  v2  1  given two vertices v1 and v2 2  L            3  v        v1 4  while v    v2 do 5  Append v to L 6  v        P ath v  v2  7  Append v to L 8  return L Note that the more sophisticated methods for shortest path computation mentioned in the beginning of Appendix C can also be used  since our algorithms for the OMP query do not have speci   c requirement on the shortest path computation  D  SHORTEST PATH COMPUTATION BETWEEN ARBITRARY POINTS The length of the shortest path from a query point q on edge  u  v  to a vertex p is computed as d q p    min d q     u    d u  p   d q     v    d v p    which requires two I O operations from the index  The length of the shortest path from a query point q on edge  u  v  to another query point q   on edge  u     v     is computed as d q q       min d q     u    d u  u      d u       q      d q     u    d u  v       d v       q      d q     v    d v u      d u       q      d q     v    d v v       d v       q       if q and q   are on different edges  which requires four I O operations from the index  If q and q   are on the same edge  then according to the triangular inequality of the road network edges  the shortest path length d q q       d q     q      which requires no I O operation  E  COUNTEREXAMPLES                                                  Figure 13  Counterexamples  Figure 13 a  shows an example where the road network partitioning scheme of  19  fails to obtain the OMP  Let us assume that a query point is at each vertex in the set S    u  p1  p2  p3  p4  p5  v   and that the edge  u  v  cuts the graph into two partitions  Since all the query points belong to one partition  below  u  v    the partitioning scheme of  19  will not check the vertex w  above  u  v    However  by simple reasoning  we can see that w is the OMP  Our two phase online convex hull based pruning technique is able to    nd the OMP for the above example  This is because p1 and p2 are two neighboring points on the convex hull of the point set S  and the shortest path between them is  p1  w  p2   As a result  w will be included in the second phase for further checking  However  our technique may also have some limitation in some extreme cases  Consider the example road network in Figure 13 b   where we assume that a query point is at each vertex in the set S      u  p1  p2  p3  p4  v  s   In the    rst phase pruning  we obtain the convex hull of S     which contains u  s and v  In the second phase pruning  w will then not be included as none of the three shortest paths between the points in  u  v  s  goes through w  As a result  w is not checked by our technique  although it is easy to check that w is the OMP  We assume in the above example that the query point at u is represented as on an edge other than  u  w   See De   nition 1   e g   u  s   since otherwise  w will be included in the    rst phase  There is a similar assumption for v  Note that the intermediate point s  between pi and w is also important for the construction of the counterexample  since otherwise  pi is represented as on edge  pi  w  and w will be included  As real road networks are usually of regular structure rather than the weird topology as given in Figure 13 b   we claim in this sense that our two phase online convex hull based pruning technique is able to    nd the OMP on almost all real road networks  F  EXPERIMENTAL SETTING AND DATASET PREPROCESSING All the experiments are done on a computer with Intel R  Core TM  i5 CPU and 4GB memory  All our programs are written in JAVA  and run in Eclipse on Windows 7 Enterprise  977We use the road network datasets of 46 states in US from  21   and the datasets of    city of Oldenburg     OL  and    California     C</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lsb2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10lsb2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Location_and_sensor_based_data"/>
        <doc>Detecting Outliers in Sensor Networks using the Geometric Approach ### Sabbas Burdakis  1   Antonios Deligiannakis  2   Department of Electronic and Computer Engineering  Technical University of Crete  Greece 1 sbourdakis softnet tuc gr 2 adeli softnet tuc gr Abstract   The topic of outlier detection in sensor networks has received signi   cant attention in recent years  Detecting when the measurements of a node become    abnormal    is interesting  because this event may help detect either a malfunctioning node  or a node that starts observing a local interesting phenomenon  i e   a    re   In this paper we present a new algorithm for detecting outliers in sensor networks  based on the geometric approach  Unlike prior work  our algorithms perform a distributed monitoring of outlier readings  exhibit 100  accuracy in their monitoring  assuming no message losses   and require the transmission of messages only at a fraction of the epochs  thus allowing nodes to safely refrain from transmitting in many epochs  Our approach is based on transforming common similarity metrics in a way that admits the application of the recently proposed geometric approach  We then propose a general framework and suggest multiple modes of operation  which allow each sensor node to accurately monitor its similarity to other nodes  Our experiments demonstrate that our algorithms can accurately detect outliers at a fraction of the communication cost that a centralized approach would require  even in the case where the central node lies just one hop away from all sensor nodes   Moreover  we demonstrate that these bandwidth savings become even larger as we incorporate further optimizations in our proposed modes of operation ### I  INTRODUCTION Recent advances in microelectronics have enabled the development of large scale sensor networks for a variety of monitoring applications  ranging from wildlife monitoring  healthcare  traf   c monitoring  agriculture  production monitoring  battle   eld surveillance etc  In such applications  detecting events of interest may require monitoring whether sensors collect measurements that are deemed as    similar    to the measurements of nearby sensors  6   Detecting when the measurements of two nodes become dissimilar is interesting  because this event may help detect either  i  a malfunctioning node  or  ii  a node that starts observing a local interesting phenomenon  i e   a    re   The aforementioned detection process is often referred to as outlier detection  We need to note that  while several ways of classifying and detecting nodes as outliers exist  our work targets the important case where testing the similarity of measurements between different nodes is required by the outlier detection process  It  thus  is not applicable to other scenarios  for example when the measurements of a node may A  Deligiannakis was supported by the European Commission under ICTFP7 LIFT 255951  Local Inference in Massively Distributed Systems   be classi   ed as an outlier solely based on the node   s past measurements  For monitoring applications  it is often crucial to be able to detect interesting events with absolute accuracy  For example  in applications where sensors are deployed in order to detect natural phenomena  such as tsunamis  landslides or avalanches  a failure to quickly detect and report the offset of such phenomena may result in a failure to safely evacuate people from areas that are in danger  In other applications  extinguishing    res in forests is much easier when the    re is detected early  but extremely harder after the    re has escalated  In equipment monitoring  failure to accurately detect a malfunctioning part may result in the destruction of a much larger and expensive system  In all the above applications  it is crucial to have an outlier detection system that reports outlier nodes with 100  accuracy  Of course  to ensure the longevity of the sensor network  the requirement for accurate detection needs to coexist with data processing techniques that ef   ciently reduce the number of messages transmitted by sensor nodes  A signi   cant amount of effort  2    4    6    8    27    29    33  has been recently placed on detecting outliers  However  none of the existing techniques has tackled the general problem of being able to detect with 100  accuracy the similarity amongst any desired pair of sensor nodes  while at the same allowing  in some cases  sensor nodes to refrain from transmitting any information regarding their measurements  In a nutshell  existing outlier detection techniques suffer from one  or more  of the following drawbacks   i  they cannot identify the similarity of two nodes with 100  accuracy  assuming no message losses   or  ii  they require the transmission of information that is comparable in size to a centralized query evaluation  or  iii  they require nodes to perform transmissions at each epoch  or  iv  they are tailored to the evaluation of speci   c similarity functions and or cannot handle similarity functions over the measurements collected at different nodes  such as the correlation coef   cient  or the L    norm   In this paper we present an outlier detection framework that is based on the recently proposed geometric approach  21    22    23    24   The geometric approach allows us to accurately  and ef   ciently  monitor whether a complex  potentially non linear function  computed over the average of vectors maintained at all the sensor nodes  is above or below a speci   ed threshold  In a nutshell  each sensor is automatically assigned a monitoring zone  which is a subset of thedomain space  and examines whether the monitored function at any point within its monitoring zone may have crossed the threshold  Each sensor that detects a potential threshold violation makes a transmission  e g   to a coordinator   While monitoring zones of different shapes may be chosen  24   without loss of generality  in this work we adopt the simplest case where the shape of each monitoring zone is a sphere  22   What is guaranteed by the geometric approach is that  at any time  the true average vector  which is unknown to the sensor nodes  will lie within the union of the local monitoring zones examined by the sensor nodes  Thus  if no sensor signals a potential threshold violation  then the monitored function will not have crossed the threshold either  since the value of the monitored function for the true average vector has certainly been examined by at least one sensor node  As we demonstrate in this paper  several common similarity functions  depicted in Figure 2  can be transformed in a way that they allow the application of the geometric approach  Then  the similarity between any pair of nodes is simply expressed as a function whose value is desirable to lie above below a speci   ed threshold  For example  we may consider two nodes to be similar if their vector of measurements have an L1 distance that is below a threshold  or have a cosine similarity above a given threshold  Of course  the number of functions that we need to monitor in our application scenario may increase quadratically  in the worst case O   n 2      to the number n of sensor nodes  Our transformations allow us to utilize the geometric approach and develop a generic framework that   i  supports a variety of similarity functions   ii  performs the monitoring with 100  accuracy  and  iii  allows nodes to often refrain from any communication thus  as shown in our experimental evaluation  consuming only a fraction of the bandwidth that centralized techniques would require  This is in contrast to recently proposed outlier detection techniques  6    8  that require each node to perform a transmission at each epoch  Extensions to allowing the speci   cation of a minimum support  i e   how many nodes need to be similar to me  so that I am not considered an outlier   are also trivially incorporated in our framework  Our contributions can be summarized as follows      We demonstrate that several common similarity functions used in outlier detection can be transformed in a way that allows the application of the geometric approach      We propose a generic framework for outlier detection in sensor networks using the geometric approach and propose various modes of node operation that achieve the desired monitoring task  Our framework computes with absolute accuracy  assuming no message losses  the similarity of two nodes  a property that stems directly from the geometric approach transformation      We examine cases of monitoring the similarity amongst sensor nodes when the communication between these sensors is direct  or not  We also demonstrate how our framework allows the evaluation of minimum support queries      We perform an extensive experimental evaluation using real world data  Our analysis demonstrates that our algorithms can accurately detect outliers at a fraction of the communication cost that a centralized approach would require  even in the case where the central node lies just one hop away from all sensor nodes   Moreover  we demonstrate that these bandwidth savings become even larger as we incorporate further optimizations in our proposed modes of operation  The paper proceeds as follows  In Section II we present related work  Section III provides the necessary background on the geometric approach  Section IV details the setup that we consider in this paper  In Section V we demonstrate how several common similarity functions can be transformed in order to allow the application of the geometric approach  Section VI contains our framework and algorithms for monitoring the similarity of a node with any other sensor that our application deems necessary  Our experimental evaluation is presented in Section VII  while Section VIII contains concluding remarks and future directions  II  RELATED WORK In recent years  signi   cant effort has been placed on determining and designing the necessary primitives for data acquisition based on sensor networks  17    30   Multiple ways of organizing the network have been proposed  including hierarchical  i e   tree like  organizations such as the aggregation tree  17    26    32   clustered formations  3    9    20    31   or even completely ad hoc formations  1    13    16   Due to their inexpensive hardware  sensor nodes are prone to producing outlier readings  Thus  many techniques that seek to determine nodes with    abnormal    behavior have been have been proposed  34   In  10    11   a declarative data cleaning mechanism over data streams produced by the sensors is proposed  Similarly  the work of  7  introduces a data cleaning module designed to capture noise in sensor streaming data based on the prior data distribution and a given error model N 0     2    In  18  kalman    lters are adopted during data cleaning or outlier detection procedures  Nonetheless  without prior knowledge of the data distribution the parameters and covariance values used in these    lters are dif   cult to set  The data cleaning technique presented in  36  makes use of a weighted moving average which takes into account both recent local samples and corresponding values by neighboring motes to estimate actual measurements  A wavelet based value correction process is discussed in  35  while outliers are determined utilizing the Dynamic Time Warping  DTW  distance of neighboring motes    values  The work in  28  proposes a fuzzy approach to infer the correlation among readings from different sensors  assigns a con   dence value to each of them  and then performs a fused weighted average scheme  A histogram based method to detect outliers with reduced communication cost is presented in  25   The work in  4    29  addresses the problem of identifying faulty sensors using a localized voting protocol  However  localized voting schemes are prone to errors when motes that observe interesting events generating outlier readings are not in direct communication  6   Furthermore  the framework of  29  requires a correlation network to be maintained  TABLE I NOTATION Symbol De   nition Si The i th sensor node CNi The Comparison Neighborhood of Si  With which nodes does Si compute its similarity with  W Dimensionality of the measurements vector d Dimensionality of the local statistics vector T The similarity threshold  e The estimate vector  Its dimensionality is d  vi The local statistics vector of Si  Its dimensionality is d  v The true  not known by the sites  global statistics vector     vi The delta vector of Si  Calculated as the difference of the current local statistic vector from the last local statistic vector that Si has transmitted   ui The drift vector of Si  Equal to  e       vi B  e   ui  The sphere having  e and  ui as its diameter Conv  e   u1           un  The convex hull determined by vectors  e   u1           un minSupp The speci   ed minimum support In  15   the authors discuss a framework for cleaning input data errors using integrity constraints  while in  2    33  unsupervised outlier detection techniques are used to report the top k values that exhibit the highest deviation in a network   s global sample  However  these techniques provide no means of directly controlling the bandwidth consumption  thus often requiring comparable bandwidth to centralized approaches  10  for outlier detection  2   In  12   a probabilistic technique for cleaning RFID data streams is presented  In  27  the authors introduce a novel de   nition of an outlier  as an observation that is suf   ciently far from most other observations in the data set  A similar de   nition is adopted in  19  where a distributed outlier detection approach for dynamic data sets is presented  The framework of  6  is used to identify and remove outliers during the computation of aggregate and group by queries posed to an aggregation tree  5    17   The TACO  8  framework operates on top of a clustered network organizations and attempt to identify outliers based on compressed  LSH  representations of the collected data  The algorithms in  6    8    27  have signi   cant drawbacks compared to our proposed technique  Perhaps most important  they provide no strong guarantees of detecting outlier nodes  but rather follow a best effort approach   6    27    or at best offer  TACO  some probabilistic guarantees  which however are poor when the similarity of two sensors is close the similarity threshold  Furthermore  the work in  27  is tailored to different similarity functions and cannot directly handle the similarity functions that we target in this paper  The bandwidth savings of  6   compared to centralized alternative algorithms  are modest  while TACO cannot operate at each epoch  without being prohibitively expensive in bandwidth   but rather operates in tumbles  Both  6  and TACO require each node to perform a transmission at each epoch  On the contrary  our proposed algorithms  i  guarantee that the similarity  or not  of two nodes can always be performed with absolute certainty  assuming reliable communication    ii  can operate in a continuous fashion  thus computing the similarity of nodes at each epoch  and  iii  enable sensor nodes to safely refrain from transmitting in many epochs  thus achieving signi   cant bandwidth savings  The geometric approach was    rst presented in  22   The monitoring zones used in  22  were spheres  an approach that we also adopt in our work due to its lower computational requirements  The work in  14    24  demonstrated that using monitoring zones of different shapes  i e   triangles or ellipses  may be more bene   cial in terms of the number of transmitted messages  In Section VI we also exploit  for some functions such as the L     L1  L2 metrics  the notion of safe zones introduced in  14  in order to further reduce the number of transmitted messages   21  presents a framework for the ef   cient evaluation of threshold queries of general functions over distributed data  as opposed to distributed data streams settings used in  14    22    24    The work in  23  is the only work that we are aware of that applies the geometric approach over sensor networks  in order to answer aggregate threshold queries   In  23  the aggregate function is computed over the entire network  Besides the difference in the type of the monitored function with our problem  in this paper we are interested in evaluating multiple pair wise similarity functions  a setting in which  23  is not ef   cient  III  BASICS   THE GEOMETRIC APPROACH We now describe in more detail the geometric approach for function monitoring over a distributed system of n sites  Table I summarizes the most important notation used in this paper  The corresponding de   nitions appear at appropriate areas of the text  Figure 1 demonstrates the basic ideas of the geometric approach that we discuss in this section  Each site Si maintains a local d dimensional vector  termed as the local statistics vector  with the j th  j   1       d  element of the local statistics vector of Si denoted as  vj i   All sites contain a vector of the same dimensionality  i e   number of elements   The global statistics vector  v is computed as the average 1 amongst all local statistics vectors  Thus  the j th component of the global statistics vector  denoted as  vj is computed as   vj   1 n Pn i 1  vj i   For the framework to be applicable  any supported monitoring function f   Rd     R must be expressed over the global statistics vector  v  thus  over the average of all local statistics vectors   An important feature is the wide applicability of the geometric approach  as the threshold function can in general be non linear  Given a threshold T  the framework in  21    22    23    24  can safely determine whether f  v    T  The geometric approach decomposes the monitoring task into a set of constraints  one per site  that each site can monitor locally  To achieve this  during the operation of the algorithm  1 The same framework also applies when the global statistics vector is calculated as a weighted average of the local statistics vectors 2 u 3 u 4 Estimate Vector e Drift Vector u Global Vector v v e u 1 Area where f v    T u Fig  1  Local constraints using the Geometric Approach  Each node constructs a sphere with diameter the drift vector  u of the node and the estimate vector  e  The global statistics vector  v is guaranteed to lie in the convex hull of  e   u1   u2   u3   u4  The union of the local spheres covers the convex hull  each site Si maintains  i  the estimate vector  e  which is equal to the global statistics vector  v computed by the local statistics vectors transmitted by sites at certain times  and  ii  a delta vector     vi   denoting the difference of the current local statistic vector from the last local statistic vector that Si has transmitted  Based on these two quantities  Si calculates its drift vector  ui    e      vi   Additional optimizations have been developed in the framework  such as the ability to balance only a portion of the network in case of violations  In that case  an additional slack vector needs to be maintained and added in the calculation of the drift vector  The domain space Rd represents the potential locations of the global statistics vector at any time  Let all points in Rd where f  v     T be colored by the same color  i e   white in Figure 1   while the remaining points be colored by a different color  i e   green in Figure 1   Because the sites do not perform transmissions at each time period  the current global statistics vector  v is not known to the sites  However  what is guaranteed is that  v will always lie within the convex hull Conv  u1           un  of the drift vectors and  thus  within the convex hull Conv  e   u1           un  of the drift vectors and the estimate vector  Thus  if Conv  e   u1           un  is monochromatic  i e   either entirely below equal to the threshold  or entirely above to the threshold   then all sites are certain about the color of the function f    since this will coincide with the color of f  e   Of course  each node cannot compute Conv  e   u1           un   since it is not aware of the current drift vectors of other sites  However  an important observation  22  is that if each site monitors the sphere B  e   ui  constructed with diameter the estimate vector and its own drift vector  then the union of these spheres covers the convex hull  Thus  it suf   ces for each node to simply monitor whether its sphere is monochromatic  If all the spheres are monochromatic  then the convex hull is also monochromatic and  thus  f  v  has the same color as f  e   Otherwise  nodes transmit their local statistics vectors  and a new estimate vector is computed and made known to all nodes  IV  PROBLEM SETUP In this paper we are interested in the following general problem setup  A base station monitors the pair wise similarity of W dimensional vectors of measurements collected at sensor nodes  While our techniques are not restricted to the origin of these W values  two likely alternatives are the following   i  the vector contains the latest W readings regarding the same quantity  or  ii  the vector contains the current reading of W different quantities that the sensor monitors  Each sensor Si is asked to continuously compare its vector of measurements to a subset  which we will term as the comparison neighborhood  CN  of Si  of the other nodes in the network  We make no assumption on whether Si can directly communicate with all nodes in CNi   We  thus     rst discuss the general case  where each sensor may require a unicast  potentially multi hop  communication with all nodes in its CN  and then discuss further potential optimizations that can be applied when all the nodes in CNi are in direct communication with Si   The in between scenario where a node Si is in direct communication with only a fraction of the nodes in CNi can be trivially handled by partitioning the nodes in CNi in two sets   the    rst set contains those nodes that are in direct communication with Si   while the second set contains the remaining nodes   and applying the techniques developed for the appropriate scenario to each of these sets  The requirement of our application is for the base station to know with certainty  assuming no message losses  whether each node is similar  or not  to the other nodes in its CN  If a node Si detects that its similarity with a node Sj     CNi has changed  i e   Si and Sj were dissimilar similar up until the previous epoch  but they are now deemed similar dissimilar   then  exactly  one of these nodes needs to notify the base station  Obviously  these noti   cations from all sensor nodes may use for their propagation an interconnect such as the aggregation tree  17   Another interesting application involves monitoring whether the measurements in each node Si have a required minimum support minSupp  expressed as the number of nodes in CNi that are deemed similar to Si   We show in Section VI C that this case can also be easily handled in our framework  V  EXPRESSING SIMILARITY FUNCTIONS USING THE GEOMETRIC APPROACH We now show how several interesting similarity functions  shown in Figure 2  can be transformed in a way that they can be used in the geometric approach  Notation and Important Notes  Let us consider the similarity between two nodes with measurement vectors X and Y  correspondingly  Because some of the transformations that we need to perform do not treat X and Y in a symmetric way  we always assign X to the node  in the pair wise test  with the lowest identi   er  id   among the two nodes  In order to use the geometric approach  each similarity function must be expressed as a general function over theSimilarity  Metric  Function based on  vectors X  Y of two nodes Transforming the Function Calculation into a Function over       Average Quantities that each node may compute individually  Local Statistic Vectors   X      Y     may contain more  elements than X  Y    Value of function on any point Z     X   Y       dist X Y                                                                                                                     X   Y   1 dist X Y                                                                                                                     X   Y   2 dist X Y                                                                                                                        X   Y   k dist X Y                                                                                                                                    Cosine  Similarity                                                                                                                                                                                                                                                                   Extended  Jaccard  Coefficient                                                                                                                              0                                                        1         2                     4           2                     4                                                                  Correlation  Coefficient   55           6       7 7  0                                                               1     8    9      9               9          9               7    7             7      7                                      9     9       7  7                                          9     9       7  7                                                                                    Fig  2  Expressing common similarity functions between two nodes with W dimensional measurement vectors X and Y using the geometric approach  The function must be expressed as a general function over the average of local statistics vectors X    and Y     with the elements of X     Y     being computed based only on the elements of X  Y   X     Y     may have a different dimensionality than X  Y   average of local statistics vectors X    and Y     with the elements of X    Y    being computed based only on the elements of X Y  We need to emphasize the following points      X    Y    may have a different dimensionality than X Y      The dimensions of the sub space that each node monitors  i e   the spheres in Figure 1  correspond to the dimensionality of X    Y     and not on the dimensionality of X Y      During its monitoring  each node needs to calculate the value of the similarity function over any point Z that lies in its monitoring zone  i e   in the sphere that it constructs   Any such point Z represents a potential position of the global statistics vector  which is computed as the average of local statistics vectors   and should not be confused with how the function is computed on either X Y X    Y     The last column of Figure 2 demonstrates how to compute the value of the function over any point Z  Transformations for the L     L1  L2 and Lk norms  Let us    rst consider the simplest case of the L    norm  computed as the maximum difference maxi 1   W   Xi     Yi     Then  L    X  Y     max i 1   W   Xi     Yi      2 max i 1   W    Xi     Yi 2      2 max i 1   W    Xi       Yi  2    Thus  by setting X0   X and Y 0      Y   the overall L    X  Y   can be computed as a function on the average vector 1 2  X0   Y 0    Please recall that  as mentioned earlier in this section  the local statistics vector Y    corresponds to the node in the pair wise test with the highest id  The transformations for the L1  L2 and Lk norms are similar in principle  We  thus demonstrate the transformation only for the L2 norm  L2 X  Y     v tXW i 1  Xi     Yi  2   v t2 2XW i 1   Xi     Yi 2   2   2 v tXW i 1   Xi     Yi 2   2 Thus  similarly to the L    case  by using X0   X and Y 0      Y as the local statistics vectors at the two nodes  the L1  L2 and Lk norms  can be evaluated as shown in Figure 2  Transformations for the Cosine Similarity  In order to compute the cosine similarity of two vectors  we    rst need to express the inner product  which is equal to the summation of the products of corresponding vector elements  in a form that admits the geometric approach  Thus  X    Y   XW i 1 XiYi   1 2 XW i 1 2XiYi   1 2   XW i 1  Xi   Yi  2       XW i 1  Xi  2   XW i 1  Yi  2      2 XW i 1   Xi   Yi 2   2       X   2 2     Y    2 2 2Using a similar transformation for the quantity at the denominator of the cosine similarity    X  2  Y   2   2    X  2     Y   2 2   2       X   2 2     Y    2 2 2   Therefore  the overall cosine similarity cos    X  Y      XY   X  2  Y   2 can be computed as a function of average quantities  Thus  by maintaining X0    X1          XW     X   2 2     X  2  T  for the node with the lowest id in the comparison test   and Y 0    Y1          YW     Y    2 2     Y   2  T  for the node with the highest id   the overall cosine similarity can be computed over the average of the local statistics vectors maintained at the nodes  Figure 2   Please note that in this case the dimensionality of the local statistics vectors is W   2 and  therefore  larger than the dimensionality of the measurements vector  Transformations for the Extended Jaccard Coef   cient  The Extended Jaccard contains two quantities that need to be transformed   i  The inner product X  Y   which is transformed in the same way as in the cosine similarity  and  ii  the term   X   2 2     Y    2 2   which can be transformed to 2   X   2 2   Y    2 2 2   Thus  as shown in Figure 2  the Extended Jaccard Coef   cient can be transformed in the required format by maintaining as local statistics vectors X0    X1          XW     X   2 2   T  for the node with the lowest id in the comparison test   and Y 0    Y1          YW     Y    2 2   T  for the node with the highest id   Transformations for the Correlation Coef   cient  The most dif   cult transformation involves the computation of the Correlation Coef   cient  corr X  Y     cov X Y     X  Y   computed based on the covariance of the two vectors of measurements  and their standard deviations  The denominator can be computed based on the same approach that we have demonstrated    X  Y   2    X     Y 2   2        2 X      2 Y 2   We then observe that the covariance cov X  Y   can be computed as  cov X  Y     E XY     E X E Y    The product E X E Y   can be transformed as  E X E Y     2  E X    E Y   2   2      E X   2    E Y    2 2   For the term E XY   we obtain  E XY     1 W XW i 1 XiYi See inner product   2 W XW i 1   Xi   Yi 2   2     1 W   X   2 2     Y    2 2 2 Based on the above transformations  as shown in Figure 2  the Correlation Coef   cient can be transformed in the required format by maintaining as local statistics vectors X0    X1          XW     X   2 2   E X   E X   2     X     2 X  T  for the node with the lowest id in the comparison test   and Y 0    Y1          YW     Y    2 2   E Y    E Y    2     Y      2 Y   T  for the node with the highest id   VI  NODE OPERATION FOR SIMILARITY MONITORING We now propose different alternatives of node operation  so that each node can monitor with accuracy its similarity with any node in its CN set  For ease of presentation  we begin our discussion with a model of having just two nodes that want to compute their similarity  This model  on one hand  helps explain the various alternatives and  on the other hand  can be used for answering similarity queries between nodes that do not communicate directly with each other  the    rst scenario discussed in Section IV   while also being applicable to the second scenario  direct connectivity with all nodes in CN  as well  Besides  simply applying the same pair wise algorithms that we will present to all neighbors of a node  provides a solution to the monitoring task that we tackle in this paper  Each node  simply has to maintain statistics for each node in its CN set  and follow for each such node the process described in the two node communication model  A  A Two Node Communication Model In our initial model  we do not worry about whether the two nodes are in direct communication with each other  Thus  whenever we refer to a node Si transmitting a message to Sj   this message may involve a multihop communication  Of course  it is more natural to expect that the similarity between the readings of two nodes will be useful when these sensors are placed nearby and can  thus  either communicate directly  pending any obstacle that may block their communication   or are reachable within a few hops  All but  the last  one of the modes of operation that we propose in this paper do not require a    xed order  amongst the two nodes whose similarity is monitored  in which the sensor nodes will examine whether they have a local violation  However  for ease of presentation  let us simply assume that such an order  which need not be the same across all epochs  i e   as in a case where nodes alternate their turn  has been established  We need to note that a local violation occurs when a node deems that its local constraint is violated  i e  when the sphere that it monitors is bi chromatic   A global violation occurs when a node is certain that its similarity with another node has changed color  The node that detects a global violation noti   es the base station  The Simple Mode of Operation  Algorithm 1 presents the Simple mode of operation of each node  This mode of operation has no optimizations in its decisions and it is Algorithm 1 Node i  Operation under Simple Mode Require  Threshold T  Similarity Function F 1  Maintain   v 0 i  last transmitted local statistics vector 2  Maintain  e  Estimate vector 3  Maintain   v 0 j   last received local statistics vector from Sj 4  while Asked to Monitor Similarity to Sj do 5  Obtain new measurements and form local statistics vector  vi 6  Compute delta vector     vi    vi       v 0 i 7  Compute drift vector  ui    e       vi 8  if Acting Second in Pair then 9  if MessageWait Sj     v 0 j     e 0      true then 10  if   e 0 and  e are bi chromatic then 11  Notify base station about global violation 12  end if 13  Send local measurements vector to Sj 14    v 0 i    vi   e     e 0 15  Continue 16  end if 17  end if 18   Acting    rst  or acting second but did not receive a message  19  localViolation   checkIfViolation  e   ui  F  T  20  if localViolation    true then 21  Send local measurements vector to Sj 22    v 0 i    vi 23  MessageWait Sj     v 0 j     e 0    Will de   nitely arrive  24  Compute  e   1 2     v 0 j    vi  25  Continue  If global violation  Sj will send noti   cation  26  end if 27  if Acting    rst then 28  if MessageWait Sj     v 0 j     e 0      true then 29  goto 10 30  end if 31  end if 32  end while Algorithm 2 Node i  MessageWait Subroutine Require  Paired node Sj   last received local statistics vector   v 0 j   vector   e 0 1  Wait for message from Sj 2  if Message Arrived  containing vector of measurements then 3  Compute local statistics vector  vj of Sj 4    v 0 j    vj 5  Compute   e 0   1 2     v 0 j    vi  6  RETURN true 7  end if 8  RETURN false thus  presented only as a baseline solution for estimating the similarity of two nodes Si and Sj   One may consider this Simple mode as the most natural way of applying the geometric approach to our problem  However  while in prior work typically a coordinator node exists  in the Simple mode  as an optimization to reduce message transmissions  each node in a pair may act as a coordinator  Both nodes at each epoch update their vectors of measurements  Line 5   and compute their delta and drift vectors  Lines 6 7   There are three cases in which Si may receive a message from Sj       Lines 8 17  If Si acts second and receives a message from Sj  local violation at Sj    Then Si checks to see if a global Algorithm 3 Node i  checkIfViolation Subroutine Require  estimate vector  e  drift vector  ui  function F  threshold T 1  if F  e    T then 2  Find min value testVal in sphere B  e   ui  3  else 4  Find max value testVal in sphere B  e   ui  5  end if 6  if F  e  and testVal are monochromatic then 7  Return false 8  end if 9  Return true violation has occurred  In order to accomplish this  it does not need to construct any spheres using its drift vector  Please note that Si knows the measurement vector of Sj and also knows its own vector of measurements  Then Si knows the exact value of the global statistics vector   e 0  Algorithm 2  Line 5   and can simply check whether the similarity function at the estimate vector  e and   e 0 are bichromatic  If so  Lines 10 12   then a global violation has occurred  and the base station will be noti   ed by Si       Lines 20 26  If Si acts second  Sj has no local violation  but Si detects a local violation  using Algorithm 3   Then  Si will transmit its vector of measurements and will wait to receive the corresponding vector from Sj   in order to update its estimate vector      Lines 27 31  If Si acts    rst  Si detects no local violation  but Sj  which in this case acts second  detects a local violation  This case is identical to the    rst case that we described in this list  The Autobalance Mode of Operation  In the Simple mode operation either both nodes will not detect any local violation and will remain silent  or they will both transmit their local measurement vectors to the other node  The latter is a signi     cant drawback of the Simple mode  which we aim to improve upon with our Autobalance mode  In order to ensure accurate monitoring  the geometric approach requires that  i  both nodes always use the same estimate vector  and  ii  the average of the drift vectors is equal to the global statistic vector  i e   the true average of the local statistics vectors   In the Autobalance mode  the    rst node  in the pair  that detects a local violation      Modi   es its estimate vector to  e   1 2    v     Transmits its measurement vector  after the transmission      v   0       Awaits for a potential message from the other node  If a message is received  it updates its estimate vector by incorporating half of the delta vector that it calculates for the pairing node  On the other hand  the node that    rst receives a message  uses the following steps      Checks if the new global estimate vector is monochromatic with the estimate vector      Updates  note  the update must be done after the previous check  its estimate vector by incorporating half of the deltaT 2  T 2  Z1 Z2 T 2  T 2  Z1 Z2 T 2  T 2  Z1 Z2 Area in     2  where the  average vector may lie  such that                Area in     2  where the  average vector may lie  such that                 Area in     2  where the  average vector may lie  such that                 Fig  3  Depicting the shaded areas where the global statistic vector may lie such that the L     L2 and L1 distance between the two sensor nodes is not above the threshold T  for 2 dimensional vectors of measurements  The shaded areas are also convex in any dimension for these similarity measures  vector that it calculates for the pairing node      If the    rst step signaled a global violation  the node noti   es the base station  sets the estimate vector to the global statistics vector that it computed  and transmits its measurement vector to the pairing node  Otherwise  it continues using the estimate vector that it computed in the second step  The above procedure is similar in principle to the techniques developed in  22   where a slack vector may be added to all the computed drift vectors  with the requirement that the slack vectors cancel out  It is simple to verify that our procedure distributes the delta vector of the node with the violation to both nodes  both nodes modify their estimate vector and  thus  their drift vector by half of this delta  while the transmitting node also zeroes its delta vector   thus resulting in zero overall change in the computation  To understand why this is the case  assume that Si detected the local violation  Then the overall change in the drift vector of Si is      1 2    vi  the node zeroes its delta vector after the transmission  but half of that vector has been added to the estimate vector   while the overall change in the drift vector of Sj is 1 2    vi  due to the modi   cation of the estimate vector   Thus  the sum of the drift vectors before and after the autobalance operation remains the same  In the Autobalance mode  the node that    rst receives a message during an epoch does not necessarily transmits its own measurement vector  unless it detects a global violation  We  thus  expect the Autobalance mode to signi   cantly reduce the number of transmitted messages  However  since only the second message may be pruned in each epoch  this reduction may not exceed 50   when compared to the Simple mode  Exploiting the Convexity of Safe Zones  For the L     L1 and L2 similarity functions  we may further reduce the number of transmitted messages  by exploiting the notions of Safe Zones  SZ   14  and the convexity of the Safe Zones for the aforementioned similarity functions  According to  14      a node   s SZ consists of the set of vectors which satisfy the local constraints  and as long as the vectors remain in their SZs  no communication is required     Based on the above de   nition  for the L     L1 and L2 similarity functions we can de   ne the gray safe zones that are depicted  for W   2  in Figure 3  In higher dimensions  the corresponding safe zones are a hypercube  for L      a sphere  for L2  and the intersection of hyperplanes  for L1   We note that the safe zones that we depict are convex  Thus  as long as the estimate vector and the drift vectors remain within the safe zones  then the global statistics vector  which is the average of the drift vectors  will also lie inside the safe zone  Exploiting this fact  we may modify Algorithm 3 in the following way  If f  e      T  then simply check if f  u      T  i e   do not    nd the maximum value within a sphere  in order to test whether a local violation exists  In the case where f  e    T  the existing technique depicted in Lines 1 2 of Algorithm 3 is followed  We denote as Convex the variation of the Simple mode that exploits the convexity of safe zones for L     L1 and L2  We denote as Convex Autobalance the corresponding variation of the Autobalance mode  B  Neighbors in Direct Communication  In the case where all the neighbors of Si are in direct communication with Si   then any message transmission by Si   regarding its measurements vector  can be performed using broadcast communication  Due to space constraints  we do not enumerate all possible combinations of the broadcast communication with the Simple  Convex  Autobalance and Convex Autobalance modes  but rather focus on the most ef   cient one  namely the Proactive Broadcast Convex mode  This mode is based on the Convex Autobalance mode  In this mode  the nodes within each CN must be ordered  so that each node knows who acts before it  or not  Let us consider the operation of node Si       Si waits for messages from nodes that act before it      For each received message  Si updates its estimate vector  due to autobalancing  with the corresponding neighbor  and checks if a global violation regarding that neighbor occurs  If yes  then Si will later broadcast its measurement vector  so we skip the next step      Si examines if a local violation exists regarding itself and nodes that act after Si   Si marks all such nodes with which it observes a local violation      If either of the steps 2 3 necessitate a transmission  Si broadcasts its measurement vector  Si will update  due to autobalancing  its estimate vector for all nodes   i  that act after it  and  ii  for which a global violation was detected in Step 1  Please note that in this case Si proactively modi   es its estimate vector for nodes that act after it  even if it does not detect a local violation for their similarity      Si then awaits messages regarding potential violations  It ignores received messages from any node Sj that act after Si and which did not have a global violation with Si  i e   the received message was due to a violation that Sj had with another node   The latter is easy for Si to determine  based on the received measurements vector from Sj   C  Handling Minimum Support Queries In the case of queries involving a required minimum support  i e   number of nodes with similar measurements   the base station wants to be noti   ed about any sensor node that did nothave  did have  the required minimum support at the previous epoch  but reaches  drops below  the required minimum support at the current epoch  However  this problem is trivial to handle in our framework  since each node Si will only transmit a message to the base station at the end of the epoch  i e   after the monitoring process for all neighbors has been completed for this epoch  if its current support follows the previous condition  Please recall that each node knows with absolute certainty whether its desired distance from any node in its CN is above below the required threshold  We actually expect minimum support queries to be more bandwidth ef   cient  since our initial algorithms noti   ed the base station each time that their similarity status with any node in CN was modi   ed  However  as we demonstrate in our experimental evaluation  the overall bandwidth savings are restricted by the fact that the vast majority of the transmitted messages are between pairs of nodes tested for similarity  Thus  even though the transmissions to the base station are reduced  these transmissions were relatively few  to begin with  D  A Note on Message Losses All the presented algorithms assume  similarly to previous work that uses the geometric approach  the reliable delivery of messages  However  in sensor network applications messages are often lost due to con   icts  We now discuss the consequences of message loss when using the geometric approach  In case a message involving the noti   cation to the base station is lost  the base station is not aware whether two nodes are similar or not  In the case of minimum support queries  the base station will not know whether a node   s measurements have reached the required minimum support or not  This issue will be resolved in the next noti   cation transmitted by the same node to the base station  In versions of our algorithms that do not include the Autobalance mode  each node that    rst makes a transmission due to a local violation expects a reply  Thus  if a message is lost  the node will not receive a reply  will recognize this message loss and may resend its message  thus resolving this issue  On the other hand  in versions of our algorithms that include the Autobalance mode  it is possible that a node that exhibits a local violation and transmits a message will not receive a reply  In such a case  the node will not know whether this occurred because of a message loss  or because of a successful balancing operation at the pairing node  unless we use acknowledgments   In this case  the 100  guarantees of our algorithms are no longer valid  as the two corresponding pairing nodes will end up using different estimate vectors  Let us now consider when this problem will be resolved  Let X denote the node in the pair whose transmitted message was not received  The problem will only be resolved in either  i  the next epoch that X exhibits a local violation  when monitoring the same pair wise similarity   if the message of X is not lost again  or  ii  the next epoch that the pairing node    rst detects a local violation and X detects a threshold violation  as X will then transmit its measurements vector to its pairing node  E  Handling the Addition and Removal of Nodes When nodes are added removed from the network  for example  due to node failures   then each sensor simply needs to monitor its similarity to more fewer sensor nodes  In most cases  this does not require any additional effort by the remaining sensor nodes  In the case of minimum support queries  the removal of sensor nodes may cause some nodes to notify the base station  as their minimum support may drop below the required threshold  The only minor complication that arises involves the broadcast case described in VI B  any new node added in the network  after determining its comparison neighborhood CNi   will have to pick determine its order  when to act  within CNi   so that it knows which other sensors act before after it  VII  EXPERIMENTS In our experiments we utilized two real world data sets  The    rst data set  termed Intel Lab Data  includes temperature  humidity and light measurements collected by motes in the Intel Research  Berkeley Lab 2   We selected the measurements of the following nodes  in the speci   ed order  that lie in nearby lab locations  38  39  40  41  43  37  35  36  In experiments where we vary the number of used nodes  any experiment containing K nodes  contains measurements from the K    rst nodes in the above list  for 30000 epochs  The second data set  termed as Weather Data  includes air temperature  relative humidity and solar irradiance measurements from the station in the University of Washington and for the year 2002 3   We used these measurements to generate readings for up to 9 motes for a period of 2000 epochs  To avoid presenting experiments for the same quantity in both data sets  in the Intel Lab data set we present the results for the temperature and light data  while in the Weather data we present the results for the humidity and the solar irradiance data  Our simulator was written in Java  Techniques and Parameter Settings  We compare the performance of our Simple  Convex  Autobalance  Convex Autobalance and Proactive Broadcast Convex techniques in different data sets  when we vary several parameters  such as the used threshold  the similarity function  the minimum support  the dimensionality of the measurements vector  or the number of the sensor nodes  In order to be able to test the Convex alternatives  we focused on the L     L1 and L2 similarity functions  Moreover  these functions have a closed form solution that helps us determine in a simple way their minimum and maximum values within any sphere  We also consider in our discussion  but do not depict in our graphs due to its high bandwidth consumption compared to our techniques   a baseline method  termed NAIVE  in which the sensor nodes transmit their measurement vectors to the base station at each epoch  To favor this NAIVE algorithm  we generated clusters of k sensor nodes  the default value of k was 5   all in direct communication to each other and to the base station  Even though the competitive algorithm 2Data available at  http   db csail mit edu labdata labdata html 3Data available at  http   wwwk12 atmos washington edu k12 grayskies0 0 Similarity Threshold 5 10 15 20 25 10000 20000 30000 40000 50000 60000 70000 80000 90000  Transmitted Messages Convex Simple Convex Autobalance Autobalance Proactive Broadcast Convex            Intel Labs Data  Temperature  Nodes in Cluster   5  Epochs   30000 Fig  4  Intel Lab   Messages vs Threshold  Temperature  L1 0 0 600 Similarity Threshold 1200 1800 2400 3000 3600 5000 10000 15000 20000 25000 30000 35000 40000 45000   Transmitted Messages Convex Simple Convex Autobalance Autobalance Proactive Broadcast Convex                Intel Labs Data  Light   Nodes in Cluster   5  Epochs   30000 Fig  5  Intel Lab   Messages vs Threshold  Light  L1 0 0 10 20 30 40 Similarity Threshold 50 60 70 80 250 500 750 1000 1250 1500 1750 2000 2250 2500 2750 3000  Transmitted Messages Simple Autobalance Convex Convex Autobalance Proactive Broadcast Convex          Weather Data  Humidity  Nodes in Cluster   5  Epochs   2000 Fig  6  Weather   Messages vs Threshold  Humidity  L1 0 0 250 500 750 Similarity Threshold 1000 1250 1500 250 500 750 1000 1250 1500 1750 2000 2250 2500 2750 3000 3250 3500 3750 4000 4250  Transmitted Messages Simple Autobalance Convex Convex Autobalance Proactive Broadcast Convex         Weather Data  Solar Irradiance  Nodes in Cluster   5  Epochs   2000 Fig  7  Weather   Messages vs Threshold  Solar Irradiance  L1 0 0 Similarity Threshold 5 10 15 20 25 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000 110000   Transmitted Messages Convex Simple Convex Autobalance Autobalance Proactive Broadcast Convex           Intel Labs Data  Temperature   Nodes in Cluster   5  Epochs   30000 Fig  8  Intel Lab   Messages vs Threshold  Temperature  L    0 0 500 Similarity Threshold 1000 1500 2000 2500 3000 3500 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 55000 60000   Transmitted Messages Convex Simple Convex Autobalance Autobalance Proactive Broadcast Convex                 Intel Labs Data  Light   Nodes in Cluster   5  Epochs   30000 Fig  9  Intel Lab   Messages vs Threshold  Light  L    0 0 10 20 30 40 Similarity Threshold 50 60 70 80 500 1000 1500 2000 2500 3000 3500 4000 4500  Transmitted Messages Simple Autobalance Convex Convex Autobalance Proactive Broadcast Convex           Weather Data  Humidity  Nodes in Cluster   5  Epochs   2000 Fig  10  Weather   Messages vs Threshold  Humidity  L    0 0 200 400 Similarity Threshold 600 800 1000 1200 1400 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500  Transmitted Messages Simple Autobalance Convex Convex Autobalance Proactive Broadcast Convex     Weather Data  Solar Irradiance  Nodes in Cluster   5  Epochs   2000 Fig  11  Weather   Messages vs Threshold  Solar Irradiance  L    is termed as NAIVE  the setup that was provided favors it extremely  since at each epoch each node transmits its measurements vector over a single hop  Please also note that  as mentioned in Section II  recent approaches  6    8  require each node to perform a transmission at each epoch and  thus  cannot perform better than our baseline NAIVE algorithm  Contrary to the NAIVE algorithm  our algorithms transmit to the base station only when a global violation is detected  Thus  our algorithms  contrary to NAIVE  would not be severely impacted in cases where the base station is several hops away from the sensor nodes  Table II depicts the default values used for our parameters  A  Varying the Monitored Function In Figures 4 7 we demonstrate the total number of transmitted messages of our techniques in our data sets when varying the used threshold  with the L1 similarity function  The cluster contained 5 nodes  which means that the total number of transmitted messages for NAIVE  not depicted  was 150 000 in the Intel Lab data  and 10 000 in the Weather data  We make two interesting observations      Our techniques can provide signi   cant bandwidth savings compared to NAIVE  For example  by using a modest 5 degree threshold for L1 in Figure 4  the Simple mode reduces the number of messages by a factor of 3 3  compared to NAIVE   while our Broadcast mode achieves about a 10 fold reduction  When considering these improvements  recall that we have selected a setup that signi   cantly favors the NAIVE algorithm      In the controlled environment of the Intel Lab  the maximum number of transmitted messages occurs in very small threshold values  which implies small differences on the sensor measurement vectors  This is encouraging  as it seems less likely that one would pick such low threshold values in order to test whether the readings of some sensors are    abnormal     On the other hand  the Weather data set contains measurements from outdoor sensors  thus resulting in signi   cant differences between their measurements  This explains why the maximum number of transmitted messages in the Weather data set is at higher threshold values  Please note that using the geometric approach  a node is less likely to have a local violation if its estimate point is far away from the threshold surface  indeTABLE II PARAMETERS AND DEFAULT VALUES Parameter Default Value k  cluster size 5 W  vector dimensionality 3 minSupp  minimum Support unspeci   ed  epochs Intel Lab  30000  Weather 20000 0 Similarity Threshold 5 10 15 20 25 500 1000 1500 2000 2500 3000 3500 4000 4500   Violations MinSupp   1 without MinSupp MinSupp   3 MinSupp   2 MinSupp   4         Intel Labs Data  Temperature   Nodes in Cluster   5  Epochs   30000 Fig  12  Intel Lab   Violations vs Threshold  varying minimum support  L2  Proactive Broadcast Convex 0 0 20 40 Similarity Threshold 60 80 20 40 60 80 100 120 140 160 180 200 220 240 260   Violations MinSupp   1 without MinSupp MinSupp   3 MinSupp   2 MinSupp   4            Weather Data  Humidity   Nodes in Cluster   5  Epochs   2000 Fig  13  Weather   Violations vs Threshold  varying minimum support  L2  Proactive Broadcast Convex 0 0 1 2 3 4 Similarity Threshold 5 6 7 8 9 10 5000 10000 15000 20000 25000 30000 35000 40000 45000   Transmitted Messages Dimension   7 Dimension   9 Dimension   3 Dimension   5 Dimension   1           Intel Labs Data  Temperature   Nodes in Cluster   5  Epochs   30000 Fig  14  Intel Lab   Messages vs Threshold  varying Dimension  L2  Proactive Broadcast Convex 0 0 10 20 30 40 Similarity Threshold 50 60 70 80 250 500 750 1000 1250 1500 1750   Transmitted Mes</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pd1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pd1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_data__fuzzy_data_and_data_provenance"/>
        <doc>Optimizing Probabilistic Query Processing on Continuous Uncertain Data ### Liping Peng  Yanlei Diao  Anna Liu    Department of Computer Science     Department of Mathematics and Statistics University of Massachusetts  Amherst ABSTRACT Uncertain data management is becoming increasingly important in many applications  in particular  in scienti   c databases and data stream systems  Uncertain data in these new environments is naturally modeled by continuous random variables  An important class of queries uses complex selection and join predicates and requires query answers to be returned if their existence probabilities pass a threshold  In this work  we optimize threshold query processing for continuous uncertain data by  i  expediting joins using new indexes on uncertain data   ii  expediting selections by reducing dimensionality of integration and using faster    lters  and  iii  optimizing a query plan using a dynamic  per tuple based approach  Evaluation results using real world data and benchmark queries show the accuracy and ef   ciency of our techniques and signi   cant performance gains over a state of the art threshold query optimizer ###  1  INTRODUCTION Uncertain data management is becoming increasingly important in a wide range of applications  Much research in the literature has been motivated by traditional applications such as data integration  information extraction  and sensor networks  Recent studies have shown that uncertain data management also plays a key role in largescale scienti   c applications including severe weather monitoring  20  and computational astrophysics  17  18   The recent initiative to build professional data management and analytics software for scienti   c research  9  has further con   rmed that almost all scienti   c data that results from real world measurements is uncertain  and hence capturing uncertainty from data input to query output is a key component of the scienti   c data management system  A Motivating Application  Let us consider computational astrophysics for a concrete example  The Sloan Digital Sky Survey  SDSS  benchmark  18  and the ArrayDB benchmark show the following characteristics of the uncertain data management problem  Continuous uncertain data  Most attributes that resulted from scienti   c measurements or their data cooking processes are uncertain  These attributes are naturally modeled by continuous random vari      This work was supported in part by the National Science Foundation under the grants IIS 0746939 and IIS 0812347  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Articles from this volume were invited to present their results at The 37th International Conference on Very Large Data Bases  August 29th   September 3rd 2011  Seattle  Washington  Proceedings of the VLDB Endowment  Vol  4  No  11 Copyright 2011 VLDB Endowment 2150 8097 11 08      10 00  ables  Gaussian distributions are the most commonly used distributions  17  18  while more complex distributions such as asymmetric and bimodal distributions can be useful in special domains such as tornado detection  20   As a concrete example  the Galaxy table in the SDSS archive has 297 attributes  out of which 151 attributes are uncertain   The schema is partially shown in Appendix A   Complex selection and join predicates  An important class of queries employs a Select From Where SQL block using a wide variety of predicates  Queries Q1 and Q2 from the SDSS benchmark  shown in Appendix A  involve four types of predicates   i  predicates on deterministic attributes   ii  range predicates on a single uncertain attribute  e g   the    rst selection predicate in Q1   iii  multivariate linear predicates on uncertain attributes  e g   the second and third join predicates in Q2   iv  multivariate quadratic predicates on uncertain attributes  e g   the last selection predicate in Q1 and the last join predicate in Q2  Each query can have an arbitrary mix of these types of predicates  Ef   cient processing of threshold queries  Given uncertainty of input data  the user would want to retrieve query answers of high con   dence  re   ected by high existence probabilities of these answers  A common practice is that the user speci   es a threshold so that only those tuples whose existence probabilities pass the threshold are    nally returned  In many scenarios  such threshold queries need to be processed ef   ciently  for instance  in near real time to detect dynamic features  transient events  and anomalous behaviors  or with short delay to support interactive analysis where scientists issue explorative queries and wait for quick answers online  Most work on probabilistic databases models uncertain data using discrete random variables and evaluates queries based on the possible worlds semantics  e g    3  10  21    Recent work has argued for using new techniques natural to continuous random variables  17   and showed signi   cant performance gains of such techniques over discretization or Monte Carlo simulation for evaluating relational operators  20  and for ranking  12   C L A RO  19  20  and O R I O N  14  16  are two state of the art systems that provide native support of queries on continuous uncertain data  without using discretization or sampling   C L A RO  however  does not consider the threshold in query processing  a naive extension that applies the threshold    lter at the end of query processing wastes a lot of computation on nonviable answers  O R I O N has general evaluation strategies for selection projection join queries  but lacks optimizations of queries with complex predicates such as those in Q1 and Q2  Furthermore  its query optimizer uses simple static query plans and results in inef   cient execution  which we show later in this paper  Contributions  In this paper  we address ef   cient threshold query processing on continuous uncertain data  We support selection joinprojection queries with a threshold on the existence probabilities of query answers  We propose to optimize such query processing 1169using a suite of new techniques grounded in statistical theory and a new design of the query optimizer  Our contributions include  Selections    3   Selections with complex predicates on continuous uncertain data often involve high dimensional integrals such as with Q1 and Q2  In this work  we propose optimizations to reduce dimensionality of integration  We further develop fast    lters for a wide range of predicates which ef   ciently compute an upper bound of the probability that a tuple satis   es the predicates  Hence  these    lters can be used to prune tuples quickly  Joins    4   Joins on continuous uncertain data have traditionally used the strategy of a cross product followed by a selection  This strategy can be highly inef   cient as it may generate a large number of intermediate tuples  A join index can potentially prune many intermediate tuples  However  the design of a join index for continuous uncertain data is challenging because the index not only stores continuous distributions  but also takes a search condition based on the distribution from the probing tuple  which is not deterministic  and returns all candidates that can potentially produce join results that pass the threshold    lter  The  only  relevant type of join index  7  8  is based on primitive statistical results and has limited    ltering power  We propose several new indexes based on much stronger statistical results and support a range of join predicates  Query optimization    5   Query optimization for continuous uncertain data fundamentally differs from traditional query optimization because selectivity becomes a property of each tuple that carries a distribution  Depending on the attribute distribution  the optimal plan for one tuple can be bad for another tuple  This change dictates per tuple based query planning  Furthermore  in data stream systems the selectivity of operators cannot be estimated until the tuple arrives  and the selectivity of post join operators cannot be estimated until the join results with new joint distributions are produced  Hence  selectivity estimation and query planning need to be performed during query execution  We design a query optimizer that supports such dynamic  per tuple based planning at a low cost  Evaluation    6   Using real data and benchmark queries from SDSS  we demonstrate the accuracy and ef   ciency of our join indexes  selection    lters  and optimization technique  Our results further demonstrate remarkable performance gains over the state ofthe art join indexes  7  8  and optimizer for threshold queries  14   2  BACKGROUND In this section  we present a data model for probabilistic query processing on continuous uncertain data  This model provides a technical context for our discussion in later sections  Probability distributions  A Gaussian Mixture Model  GMM  describes a probability distribution using a convex combination of Gaussian distributions  A multivariate Gaussian Mixture Model  multivariate GMM  naturally follows from the de   nition of multivariate Gaussian distributions  We include their de   nitions in Appendix B  GMMs offer two key bene   ts to uncertain data management  First  theoretical results have shown that GMMs can approximate any continuous distribution arbitrarily well  11   Hence  they are suitable for modeling complex real world distributions  Second  GMMs allow ef   cient computation of relational operators based on Gaussian properties and advanced statistical theory as shown in our prior work  20  and later sections in this paper  Data model  We consider an input data set that follows the schema Ad     Ap   The attributes in Ad are deterministic attributes  like those in traditional databases  The attributes in Ap are continuousvalued uncertain attributes  such as the location of an object and the luminosity of a star  In each tuple  Ap is modeled by a vector of continuous random variables  X  that has a joint pdf  fAp  x   According to the schema  Ap can be partitioned into independent groups of correlated attributes  Each group of correlated attributes can be modeled by a  multivariate  GMM denoted by f j  xj    Then the joint distribution for Ap can be written as    j f j  xj    For simplicity  we use A to refer to uncertain attributes when our discussion focuses on uncertain attributes only  This model can be extended to address inter tuple correlation by leveraging existing work  16   for details  see Appendix B   3  OPTIMIZING THRESHOLD SELECTION In this section  we consider probabilistic threshold selections over a relation on a set of continuous uncertain attributes  Our goal is to support ef   cient evaluation of such selections  especially when they contain complex predicates  De   nition 3 1  Probabilistic Threshold Selection  A probabilistic threshold selection           over a relation T is de   ned as          T     t   Pr R    X           t     T    where    is the selection condition on continuous uncertain attributes A     is the probability threshold  and R   is the selection region de   ned as  a a     R A         a    true   For each tuple t  X is the random vector for t A  and Pr R    X   is the probability for X to satisfy the selection condition  i e   Pr R    X       R   fX x dx  A basic evaluation strategy for the selection follows the de   nition above  using an integral of the joint attribute distribution fX for each tuple  For instance  the W H E R E clause in Q2  in Appendix A  speci   es a condition    involving ten uncertain attributes  Assume that a cross product is    rst performed  Then the selection with condition    involves a ten dimensional integral for each tuple  An improvement is to factorize this integral into lower dimension integrals based on independence  16   Suppose that the schema indicates that the uncertain attribute set A can be partitioned into attribute groups that are independent of each other  Denote this partitioning using a set system S    A1   A2        AG   Then consider each predicate in the condition     if the predicate involves attributes from different groups  merge these groups into one  After doing so for all predicates  we obtain a new set system S      A  1   A  2        A  G     Now we can rewrite the big integral as the product of the integrals for the attribute groups in S     Revisit Q2  The SDSS schema shows that S     G1  u    G1  g    G1  r    G1  rowc  G1  colc    G2 u    G2 g    G2 r    G2 rowc  G2 colc    The predicates in W H E R E yields S       G1  u  G1  g  G1  r  G2 u  G2 g  G2 r    G1  rowc  G1  colc  G2 rowc  G2 colc    Hence for each tuple  we will perform a 6 dimensional integral plus a 4 dimensional integral  As can be seen  the basic evaluation approach can be expensive or even intractable when the integral has a high dimensionality and a complex shape of the selection region  In this section  we propose two classes of optimization techniques grounded in statistical theory  the    rst class reduces the dimensionality of integration  while the second class ef   ciently    lters  most of  tuples whose probabilities fall below the threshold without using integrals  3 1 Reducing Dimensionality of Integration We    rst propose to reduce the dimensionality of integration by leveraging the following result  15   Linear Transformation  Let X     Nk           For a given l    k matrix B of constants and a l dimensional vector b of constants  Y   BX   b     Nl  B     b  B  B T    The result states that a linear transformation of multivariate normal random vector still has a multivariate normal distribution  It is natural to extend linear transformation to a GMM  we simply perform a linear transformation of each mixture component separately  1170To apply the above result  given a selection condition     we de   ne a transformed selection region R        y y Bx b   x   R      If there exists a transformation matrix Bl  k  l  k  such that Pr R    X    Pr R       Y    we can reduce the dimensionality of integration  i e     R   fX x  dx   Pr R    X     Pr R       Y       R      fY y  dy   1  Given a condition    on a set of continuous uncertain attributes  we can construct B and b by taking the following steps   1  Partition attributes into independent groups based on the schema and     as described at the beginning of the section   2  For each group of attributes  de   ne a random vector X  Find maximum linear subexpressions relevant to X from     and denote them using a new vector Y  Rewrite each variable in Y as the product of a row vector and X  plus a constant  Let B be the matrix that contains all the row vectors and b be the column vector that contains all the constants   3  If B does not have full row rank  remove rows from B  one at a time  until it has full row rank  Remove elements from b accordingly  The correctness of the procedure is shown in Appendix C  Denote the matrix returned as Bl  k   Since it has full row rank  l     k  If l   k  we can apply Eq   1  to transform the integration from the space for X to that for Y  If l   k  linear transformation does not help to reduce the dimensionality of integration  Example 3 1 For Q2  we obtain two independent groups of attributes after step  1   which is the factorization described at the beginning of the section  In step  2   let us consider the    rst group  S   1    G1  u  G1  g  G1  r  G2 u  G2 g  G2 r   Let X be the random vector for S   1   There are two maximum linear subexpressions in    for S   1   Let y1    G1 u   G1 g     G2 u   G2 g   y2    G1 g   G1 r     G2 g   G2 r   Then  we have  Y    y1 y2       1    1 0    1 1 0 0 1    1 0    1 1   X    0 0    BX b Since B has full row rank  step  3  is omitted  We can get the pdf of Y using linear transformation from X  Finally based on Eq  1   the integral dimensionality is reduced from 6 to 2               R   6     i 1   fXi  xi   dxi     0 05      0 05 0 05      0 05 fY y1   y2  dy1 dy2 3 2 A Filtering Framework without Integrals Although linear transformation can improve performance by decreasing the dimensionality of integration  it still requires the use of one integral for each tuple  In this section  we propose a    lter operator        that computes an upper bound  p    of the true probability  p  that a tuple satis   es the selection condition without using integrals  When such upper bounds are tight enough  most tuples that fail to pass the threshold selection can be removed by the    lter p         In  rare  cases that p          p    the original selection operator      with exact integration is needed to compute the true probability  Hence  a key issue in designing the    lter is how to derive a tight upper bound at a cost much lower than the integration cost  3 2 1 A General Filtering Technique We    rst propose a general    ltering technique that leverages the multidimensional Chebyshev   s inequality  and explores its relationship with a selection region in a high dimensional space  Let X be a k dimensional random vector with expectation    and covariance matrix     If    is an invertible matrix  then for any real number a   0  multidimensional Chebyshev   s inequality states that  Pr  X         T       1  X           a 2       k a 2    2  0 5  0 5  0 5 0 5 q r u r  a  q r 2  u r 2  0 25 0 2  0 2  0 2 0 2 u g  b   u  0 2 and  g  0 2 Figure 1  Illustration of the predicate region  shaded in gray  and a tuple   s chebyshev region  shaded in stripes   To leverage the above result  we transform threshold selection evaluation into a geometric problem  Besides the predicate region R       Rk from De   nition 3 1  we also de   ne a geometric region speci   c to each given random vector X of size k and a threshold     De   nition 3 2  Chebyshev region  A Chebyshev region R   X  for a given    and a random vector X with mean    and variance    is  R   X     x   x         T       1  x           k        3  Geometrically  the Chebyshev region is an ellipse  in R2   or ellipsoid  in Rk where k     3  centered at     According to Eq   2   we can see that Pr R   X     1         That is  for the random vector X  the Chebyshev region covers the probability mass of more than 1         Therefore  when the Chebyshev region R   X  for a given tuple does not overlap with the predicate region R     it is easy to bound the probability mass of this tuple in the predicate region  Pr R    X       1     Pr R   X         We can then safely    lter the tuple  As such  the threshold selection problem is transformed into the geometric problem of judging whether the predicate region and a tuple   s Chebyshev region are disjoint in a k dimensional space  Example 3 2 For a bivariate random vector X with mean    and covariance     the Chebyshev region is a region bounded by an ellipse centering at     shown as the areas shaded in stripes in Fig  1 a  and Fig  1 b   The predicate    q r 2  u r 2  0 25    in Q1 is marked by the grey area outside the circle with center  0  0  and radius 0 5 in Fig  1 a   The predicate region of     u    0 2 and  g  0 2    is a square shown by the grey area in Fig  1 b   Detecting disjoint regions  The R   and R   X  regions are disjoint in the space Rk if they satisfy two conditions   1  the center of R   X   which is     falls outside of R      2  the boundary of R   is outside R   X   When R   has a simple shape  e g   a rectangle as shown in Fig  1 b   condition  2  is satis   ed if none of the edges intersects with the boundary of R   X  and the center of R   lies outside R   X   Generally  to test condition  2   we can minimize  X         T      1  X         on R     If the minimum is larger than k     condition  2  is satis   ed  While constrained optimization in general can be a dif   cult problem  in many common cases it can be solved ef   ciently  For example  when the region R   is an intersection of ellipsoids  the constrained optimization becomes the so called Quadratically Constrained Quadratic Program  QCQP   which can be solved as easily as the linear programs  5   When the boundary of R   can be readily de   ned by equalities  the minimization can be done on the boundary and solved using the Lagrange multiplier  For example  when R   is an ellipsoid  the Lagrange multiplier leads to a system of linear equations  In summary  for those common predicates whose regions are of simple shapes or whose boundary can be de   ned by linear or quadratic equalities  our general technique based on the multidimensional Chebyshev   s inequality can provide ef   cient    ltering  11713 2 2 Fast Filters for Common Predicates For several common types of predicates  we can devise fast    lters for threshold selection evaluation by exploiting known statistical results  Consider the following predicates on the attribute set A   1  One dimensional   A  1  R      n i 1  ai   bi    where ai  bi   1   An example is    r a 2   1      whose selection region can be written as             1       1          2  Multi dimensional quadratic forms   A  1  R     a a T   a o p      where    is an  A  dimensional symmetric matrix  o p is         or          An example is    q r 2  u r 2  0 25    in Q1  where    is the identity matrix   3  Predicates that can be reduced to category  1  or  2  above by applying linear transformation  Consider      G1  u   G1  g      G2 u   G2 g   0 05    in Q2  By letting z    G1  u   G1  g      G2 u   G2 g   we have     z    0 05     which belongs to category  1   Next consider     G1  rowc   G2 rowc  2    G1  colc    G2 colc  2   4E6    in Q2  With z1   G1  rowc   G2 rowc and z2   G1  colc   G2 colc  we have    z 2 1  z 2 2   4E6     which belongs to category  2   One dimensional predicates  We exploit the following statistical results to devise fast    lters  Markov   s inequality states that for a random variable X  Pr  X     a    E X  a for any real number a   0  It can be applied to predicates R      n i 1  ai   bi    if the point 0 does not lie in the predicate region  otherwise we will get a trivial upper bound of value 1   Chebyshev   s inequality and Cantelli   s inequality  Chebyshev   s inequality states that for a random variable X with expected value    and standard deviation     Pr  X             a        1 a 2 for any real number a   0  Cantelli   s inequality  known as the one sided version of Chebyshev   s inequality  provides a tighter bound on each side of the distribution  Pr X       a      1  1 a 2    Pr X          a        1  1 a 2    Both inequalities can be applied to predicates  n i 1  ai   bi   if    does not reside in the predicate region  We derive upper bounds using the above three inequalities for predicates in category  1   as listed in Appendix C  Multi dimensional quadratic forms  If X follows a GMM  its quadratic form X T   X yields a new random variable for which we can compute the mean and variance  This allows us to apply Chebyshev   s inequality and Cantelli   s inequality similarly as above  See Appendix C for the details of the upper bounds  4  OPTIMIZING THRESHOLD JOIN In this section  we consider probabilistic threshold joins of relation R and relation S on a set of continuous uncertain join attributes  De   nition 4 1  Probabilistic Threshold Join  A probabilistic threshold join of R and S on continuous uncertain attributes A is  R        S     r s   Pr R    Xr  Xs          r     R s     S    where    is the join predicate     is the probability threshold  Xr and Xs are the random vectors for r A and s A  R   is the predicate region in R2 A    and Pr R    Xr  Xs   is the probability for Xr and Xs to satisfy the join condition  When the input relations R and S are independent  Pr R    Xr  Xs        R   fXr  xr fXs  xs dxrdxs  If R and S tuples are correlated  we can compute the joint distribution using history  16   A default evaluation strategy for the threshold join R          S is to perform a cross product R    S followed by a threshold selection with the condition    R A  S A  and the threshold     The crossproduct can create a large number of intermediate tuples  hence highly inef   cient  In this section  we propose new join indexes to implement a    ltered cross product  denoted by R         S  which returns a superset of true join results but a subset of the cross product results  Then R          S           R         S   that is  the true join results are produced by further applying a threshold selection  Designing a join index for continuous random variables is much more dif   cult than its counterpart for deterministic values  Consider R    R A   S A    S  and we want to build an index on S  First  the design of the join index needs to answer two questions   1  what is the search key of the index   2  given a R tuple  how do we form a query region over the index  In a traditional database  S A has a deterministic value and naturally forms the search key of the index  Given a R tuple  the join predicate is instantiated with R A   v  which naturally yields a query region  S A   v         on the index  Now consider the join where R A and S A are random variables and each follows a distribution  To build an index on S A  it is not clear which aspects of the distribution of S A can be used as the index key  and given an R tuple  how we use the distribution of R A to form the query region over the index  Second  the index for probabilistic threshold join needs to take into account the probability threshold     For each tuple r in R  probing the index should return all those tuples s in S that can possibly satisfy Pr R    Xr  Xs           called candidate tuples  In other words  we want to ignore other tuples s   for which we know for sure Pr R    Xr  Xs            hence improving performance  Our main idea is that if we can    nd a necessary condition for Pr R    Xr  Xs           then the negation of the necessary condition identi   es all those tuples s   that can be ignored in the index lookup  To improve the index   s    ltering power  we seek necessary and suf     cient conditions if possible  or necessary conditions that are    tight    enough  Furthermore  to have real utility for index design  the necessary condition has to meet two requirements   1  In the necessary condition  the quantities concerning S can be used to form the search key of a common index structure such as an R tree  2    2  Given an R tuple  after the necessary condition is instantiated with all the quantities concerning R  it should yield a query region that can be easily tested for overlap with the index entries  Existing join indexes for continuous uncertain attributes  7  8  make simplifying assumptions about attribute distributions  and use a    loose    necessary condition in index design  resulting in poor performance as we will show in   6  Below we derive tighter necessary conditions for common join predicates  including a necessary and suf   cient condition  and develop new indexes based on them  4 1 Band Join of General Distributions We start with the simple case of a single join attribute  A band join uses the predicate    a R A     S A b     Given a tuple r from relation R  we use Xr to denote the random variable of its join attribute  which follows a univariate GMM  Similarly  Xs denotes the random variable for the join attribute in an s tuple  again following a GMM  We denote the mean  variance  and pdf of Xt using   t      2 t   f   t     2 t  xt    respectively  t   r s   A necessary condition  As shown in   3 1  the linear transformation Z   Xr   Xs can transform the original band shaped integral region into a single interval  Pr a Z    Xr     Xs  b      b a f  r     s     2 r     2 s  z  dz  For a single variable Z  the following theorem provides a necessary condition for Pr a   Z   b          Our proof is based on Cantelli   s inequality as shown in Appendix D 1  Theorem 1 Given a range  a  b   if a random variable Z with mean    and variance    2 satis   es the condition Pr a Z b          then            1                a and              1                b  1172Since Z   Xr    Xs     N        2    plug        r      s     2      2 r     2 s back to the inequalities in the theorem  Then we obtain a necessary condition for Pr a Xr   Xs  b            r       s     1               2 r      2 s       a   4a    r       s       1               2 r      2 s       b   4b  Index construction and retrieval  We now design an index on the S relation to provide ef   cient support for the    ltered cross product R   a R A   S A b     S  In Eq   4a  and  4b     s and    2 s are the quantities from relation S  We use them to form the search key of an index  for each tuple s  we insert the pair    s     2 s   together with the tuple id into an R tree index  2   This index essentially indexes points in a two dimensional space in the leaf nodes and groups them into minimum bounding rectangles in non leaf nodes  All existing R tree construction methods can be used  For each probing tuple r  the query region is naturally formed by instantiating   r and    2 r in Eq   4a  and  4b   However  this query region has a nonstandard shape  so we re implement the overlap method in the R tree  which returns True when a minimum bounding rectangle in a tree node  denoted by RI   overlaps with the query region  denoted by RQ  Let  x  y  denote the search key of the index  that is  x     s and y      2 s   Then RI is a rectangle  x1   x2  y1   y2   The query region RQ has two conditions  By setting x     s and y      2 s in Eq   4a   we can rewrite the    rst condition as  RQ1    1 x     r   a  or  2 x   r   a and y      x     r a  2   1            2 r   It is not hard to see that RI overlaps with the union of region  1  and region  2  in RQ1 if its upper left vertex  x1   y2  lies in either region  We can rewrite the second condition from Eq   4b  and develop the test condition in a similar way  4 2 Band Join of Gaussian Distributions We next consider the most common distributions  Gaussian distributions  for continuous random variables  The known Gaussian properties allow us to    nd a suf   cient and necessary condition and hence design an index with better    ltering power  Theorem 2 Given a range  a  b   a normally distributed random variable Z     N        2   satis   es the condition Pr a   Z  b         iff there exists an         0  1       such that a        1                   b        1              where      1 is the inverse of the standard normal cdf  also called the quantile function   The proof is given in Appendix D 1  Given a range  a  b  and a threshold     Theorem 2 essentially identi   es all normally distributed random variables  i e   all the          pairs  that satisfy Pr a  Z   b          Let     denote this collection of           Formally      a  b          0        1                  a         1                   b         1              5  The     region will play a key role in the index design  in particular  representing the query region  Fig  4 in the appendix shows the shape of     when      0 7  where the x and y axes denote    and     respectively  In general     controls the shape of      and the a and b values determine the stretch along both dimensions  Index construction and retrieval  We next present a new index that exploits the above suf   cient and necessary condition and thus returns only the true matches for each probing tuple  Recall that the join predicate is    a   R A   S A   b     and Xr and Xs denote the join attribute of a r tuple and an s tuple  As in Section 4 1  we build an R tree index on S A  we take the mean   s and variance    2 s of the variable Xs for each tuple and insert them as a pair to the R tree  Given each probing tuple r  we next design the query region over the R tree  As before  consider two variables Xr and Xs  and let Z   Xr     Xs  Eq   5  has de   ned all possible distributions of Z that would satisfy Pr a Z b          Now plug        r       s and         2 r      2 s into Eq   5   Since for a particular probing tuple r    r and   r are simply constants  Eq   5  naturally yields a query region over all the distributions    s     2 s   in the R tree  Given the query region  the next task is to design the    overlap    routine that directs the search in the R tree by comparing the query region  RQ  with the minimum bounding rectangles  RI   in each non leaf node of the tree  However  the above query region has a complex shape and hence it is slow to test the overlap between RQ and RI   The    rst technique we use is to transform both RQ and RI to a different domain through a mapping  Letting  x  y       s     2 s   be the search key of the index  the mapping F has  x       r     x and y          2 r   y Each rectangle in the index RI    x1   x2  y1   y2  is transformed to  R   I     ur     x2  ur     x1        2 r   y1        2 r   y2   Finally  the query region becomes  R   Q     0        1         x     y      a         1     y      x       b         1        y       which is exactly      It is also known RI and RQ overlap if and only if R   I and R   Q overlap because F is a one to one mapping  Now our task becomes testing the overlap between R   I and R   Q        The overlap test involves several steps based on results of a detailed mathematical analysis  Due to space constraints  this analysis is described in Appendix D 2  Other types of join indexes  We also provide join indexes for  1  band joins of multivariate GMMs   2  other joins using linear predicates  and  3  proximity joins using Euclidean distance  Due to space constraints  we leave the details to Appendix D 3  5  PER TUPLE BASED PLANNING We next discuss threshold query processing that takes a selectionjoin projection query and returns tuples that satisfy the query with a probability over the threshold     i e   their tuple existence probabilities        A naive approach would be to perform probabilistic query processing as in earlier work and then apply the threshold    lter at the end of the processing  wasting a lot of computation on nonviable answers  To prune nonviable answers early  we push the threshold    earlier to each relational operator in the query plan  and apply our techniques from the previous sections as follows  R          S           R         S           T                       T    where R         S is the    ltered cross product using a join index    4              T  is the fast    lter that prunes tuples with a relaxed condition    3 2   and         T  is the exact selection that evaluates the condition using integrals but possibly with reduced dimensionality    3 1   For continuous uncertain attributes  projections do not involve duplicate elimination because there does not exist an    nite set of values to project onto  Hence  projections do not change tuple existence probabilities and are not further discussed in this work  Besides our techniques for joins and selections separately  there remains a query optimization issue  What is the most ef   cient way to arrange    ltered cross products  fast    lters for selections  and exact selections in a query plan  We consider both the cost and 1173Tuple Selectivity id r q r u r r   24 q r 2   u r 2   0 25 1 N 27 0  2 2  N 1 2  2 2  N 0 1  1 1  0 08 0 95 2 N 21 6  0 1  N 0 1  0 1  N    0 1  0 1  1 1 74    10   4 Table 1  Illustration of per tuple based selectivity with Q1 and two tuples  each tuple has three normally distributed attributes  r  q r  and u r  for each predicate in Q1  these tuples have different selectivities  selectivity of operators as in a traditional query optimizer  However  several key differences exist in the new context   1  Due to the use of integrals  exact selections can have high costs and should be treated as    expensive predicates      2  The selectivity of an operator captures its    ltering power on the input data  Under attribute uncertainty  selectivity needs to be de   ned on a per tuple basis   3  The above property further implies that the optimal order of evaluating operators also varies on a per tuple basis  Example 5 1 Consider Q1 and two tuples t1   t2 in Table 1  For predicate   1      r   24     let X 1 r and X 2 r be the random variables for t1  r and t2 r correspondingly  X 1 r     N 27  2 2   so Pr X 1 r   24    0 08  X 2 r     N 21 6  0 1  and Pr X 2 r   24      1  So t1 has a much lower probability of satisfying   1   hence more likely to be    ltered  To the contrary  for predicate   2     q r 2  u r 2   0 25     t2 has a much lower probability to pass   2 than t1   Thus  the optimal evaluation order is   1 followed by   2 for t1   and the reverse for t2  Due to the above reasons  we advocate a per tuple  dynamic query optimization approach with the following features   1  A query plan is determined for each tuple rather than a whole set   2  The query plan arranges all operators based on both cost and selectivity   3  Such planning is performed at a low cost for each tuple  Traditional query optimizers consider a static query plan for a set of tuples  6   hence not suitable for our problem  Data stream systems  1  4  can adapt query plans dynamically but only estimate selectivity for a set of tuples and lack support of uncertain attributes  Recent work on probabilistic threshold query optimization establishes algebraic equivalence for query optimization  but still uses static query plans and further ignores operator costs in query planning  14   In the rest of this section  we detail our new query optimization approach  We focus on the data stream setting  like in existing systems  1   some streams can have indexes built on while other streams are used to probe these indexes  We assume that the decision of which indexes to build has been made separately and focus on query optimization only  Our approach can be applied to stored data by viewing the result of a    le scan as a data stream  To begin with  we de   ne the selectivity  denoted by     of a selection on each tuple t and the selectivity of a    ltered cross product between a probing tuple t and a set S               t    Pr R    X t                  t  S    num  true matches from S  S  Query optimization requires the knowledge of both cost and selectivity of each operator  Our approach combines of   ine measurements of unit operation costs  which depend only on the types of predicates  and online selectivity estimation  which depends on the attribute distribution in each tuple  These techniques are fairly straightforward and hence left to Appedix E  5 1 Online Query Planning and Execution In our approach  online query planning and execution for each tuple interleaves selectivity estimation and ordering of operators in iterations  This is because while we can estimate the selectivity of predicates on a base tuple r  we cannot estimate the selectivity of the join predicate on r and s until the tuple including r and s is produced with the new joint attribute distribution  Moreover  we cannot afford    1 100 0 8 2    2 300 0 2 1    3 10 4 0 1 3 Estimated cost Selection predicates on R Selectivity Rank Estimated cost   1   2   3   4   5 S T Y Y N Y N 500 300 100 10 4 50 10 4 10 5 1 10 3   Join with     Join predicates Has index Num  of candidates Choose To process tuple pool             t 2    1    3    2 t 1   1   2   5             Figure 2  Illustration of tuple based query planning  to perform an exhaustive search of the global optimal plan as in earlier work  6  due to per tuple based planning  Therefore  we break a query into several blocks that each involve at most one join  For each query block  we repeat the following steps  Step 1  Estimate selectivities of selections  We take all predicates speci   ed on an input tuple t  and group these predicates into independent groups as described at the beginning of   3  For each independent group of predicates   i   we then allocate a selection operator   i  t   We estimate the selectivity of each selection by taking the average of its lower and upper bounds  Step 2  Rank and execute    lters and selections  We expand each selection with all possible    lters    i  t      i       i j                i1  t     If there exist fast    lters based on known statistical inequalities  we apply all of them as they have negligible costs  otherwise  we apply the    lter using constraint optimization  We rank    lters and selections in ascending order of selectivity over cost  Filters are ranked before the corresponding selection if they have a lower cost  otherwise  they are unnecessary and should be removed from the plan  Then we execute the    lters and selections in order  The tuple starts with the existence probability Ep   1 and a query threshold   q  A selection with the predicate    reduces the tuple existence probability to E   p   Ep   Pr R    X t     A    lter estimates an upper bound E    p   The tuple is dropped whenever E   p    q or E    p    q  Step 3  Choose a relation to join with  For all relations that have not been joined with t  we probe all available indexes and count the number of matches of t from each index  We then multiply the number of matches with the cost of an index lookup  and    nally choose the join index that yields the smallest value of the product  If we have exhausted join indexes  we simply choose the relation with the smallest size and use a full scan as the access method  Step 4  Execute the     ltered  cross product  Once we have chosen to join the tuple t with an relation S  we execute the    ltered cross product using the index on S if existent  or a cross product using a    le scan on S  Once a new tuple t   is emitted  we mark all join predicates relevant to t   as selection predicates  and repeat the above four steps for the next query block  Example 5 2 Figure 2 shows the planning for tuple t1 from relation R  t1 has to pass three selection predicates  a join with relation S with three join predicates  and a join with relation T with two join predicates  These predicates and their estimated costs are shown in the shaded rows of the tables  In step 1  the selectivities of three selections are entered into the top table  In step 2  the selections are ranked with   2    rst  then   1   and    nally   3  the    lters are not shown in this example   In step 3  we choose the join with S using the second predicate because there is a join index and the expected cost of retrieving matches is the lowest  In step 4  tuple t1 is paired with three matches from the join index  The three new tuples are sent back to the to process pool for further processing  For these tuples  1174  1   2 static static dynamic performance optimal order time  ms  time  ms  gain time  ms  20 0 2  1 2  0 6 0 181 70  0 177 20 0 5  1 2  0 6 0 068 89  0 067 20 1  2 1  9 6 0 050 99  0 048 22 0 2  2 1  18 2 7 216 60  7 007 22 0 5  2 1  13 9 1 515 89  1 482 22 1  2 1  9 6 0 351 96  0 348 24 0 2  2 1  18 2 15 613 14  15 287 24 0 5  2 1  14 4 6 390 56  6 334 24 1  2 1  9 6 2 264 76  2 236 Table 2  Static planning vs Dynamic planning for Q1  the join predicates associated with S become selection predicates  while those associated with T remain as join predicates  6  EXPERIMENTAL EVALUATION In this section  we evaluate our threshold query processing and optimization techniques  To demonstrate our performance bene   ts  we compare to the state of the art techniques for indexing continuous uncertain data  7  8  and for optimizing threshold queries  14   Our evaluation uses real data and queries from the Sloan Digital Sky Survey  SDSS   18   See Appendix A for details about the dataset  6 1 Techniques for Optimizing Selections Expt 1  We    rst evaluate our general    ltering technique described in Section 3 2 1  We consider a selection    100 rowc 100     and 100 colc 100          which selects stars located in a square region anchored at the lower left vertex  100  100  and with side length     We can directly test the overlap between the selection region and each tuple   s Chebyshev region due to their regular shapes  We    rst set the threshold      0 7 and varied    from 200 to 2000  which approximately covers selectivities from 0  to 100   We report the time cost per tuple for evaluating the selection with and without    lters  baseline  in Fig  3 a   The baseline has a constant high cost because it computes a 2 dimensional integral for each tuple  no matter what    value is given  In contrast  using our    lter the per tuple cost is very low for small    values because most tuples can be    ltered without computing integrals  As    grows  more tuples pass the    lter and invoke integrals for exact evaluation  The two curves meet when      2000 and 98  tuples satisfy the predicate  We also varied    in  0 7  0 9      mainly affects the number of tuples that pass the selection  A larger    value results in fewer true matches  so our    lter can prune more tuples and hence improve the performance shown in Fig  3 a   More results are shown in  13   Expt 2  We also evaluate the effectiveness of the fast    lters from   3 2 2  Our results can be summarized as follows  For predicates on a single attribute  the Cantelli    lter provides tight upper bounds  For multivariate quadratic predicates  both the Cantelli    lter and general    lter work well  In the interest of space  the details are left to   F 2  6 2 Techniques for Optimizing Joins Expt3  Band Join of Gaussians  We    rst study the    ltering power and ef   ciency of our index for Gaussians  called GJ for short  and the state of the art indexing method called xbound  detailed in Appendix F 1   We consider a join     R u     S u          with the threshold      0 7 and varied     We    rst evaluate the join in the stream setting  A tumbling window of size W is applied to both R and S inputs  each window contains a set of tuples  An index is built in memory on the current window of S  Each tuple in the current R window probes the index after it is constructed  The retrieved  r s  pairs are    nally validated for true matches by computing an integral of the joint distribution  Hence  there are three cost components in this windowed join  index construction  index lookup  and validation using integrals  We ob    3   4 static static dynamic performance order time  s  time  s  gain 0 5 400  3 4  28 0 4 25 85  0 5 800  3 4  80 1 12 1 85  0 5 1600  3 4  142 22 9 84  1 400  4 3  149 16 7 89  1 800  3 4  105 49 3 53  1 1600  3 4  187 97 2 48  2 400  4 3  160 72 1 55  2 800  4 3  486 217 55  2 1600  3 4  487 432 11  Table 3  Static planning vs Dynamic planning for Q2  serve consistently that the validation step is the dominating cost as integration is indeed very expensive  Below we report results using W  500  other W sizes reveal similar trends   Fig  3 b  shows the number of candidates returned by our GJ index and the xbound index as well as the number of true matches  We can see that GJ returns exactly the true match set because it uses a suf   cient and necessary condition for the join predicate  In contrast  the xbound index returns much more candidates  The difference becomes smaller as    increases  because more tuples become true matches  when      100  almost all tuples in the indexed relation are true matches  Fig  3 c  shows the ef   ciency of the two indexes  GJ signi   cantly outperforms xbound because there is no need to validate the candidates returned from the index  We then evaluate the join in a disk setting  where indexes are precomputed and stored on disk  Due to the limited size of the real data  we replicated it to 500MB with 28 million tuples  The R tree took 1 3GB while the memory size was set to 1GB in our Java system  Since indexes are pre constructed  their construction costs are not reported  Fig 3 d  shows the number of candidates for each probing tuple  While the trend appears similar to that in the stream setting  the absolute number for the y axis is much larger  This determines the drastic difference between GJ and xbound in time cost shown in Fig 3 e   This difference comes from both the validation cost and the I O cost as xbound returns many false positives  Expt 4  Band Join of GMMs We then evaluate our join index for general distributions modeled as GMMs  called GMJ for short  and compare to xbound  As the SDSS uses Gaussians only  we generated a synthetic trace of GMMs for this experiment  The attribute u in each tuple has two Gaussian components  the coef   cient  mean and variance of each component are uniformly drawn from  0 1    0  100   and  0 10  respectively  We report the results using the stream setting with W 500  As Fig  3 f  shows  both indexes return more candidates than the true matches because they are both based on necessary conditions for the join predicate  But GMJ is always better than xbound until they meet at    100  where the selectivity is near 100   because xbound uses a    looser    condition as discussed in Appendix F 1  Since validation is the dominating cost  the time cost follows the same trend as the number of candidates in Fig  3 f   6 3 Per tuple Based Planning and Execution We    nally evaluate our dynamic per tuple planning technique  We compare it with static query planning  14   where a    xed plan is chosen for each query based on the selectivities of predicates over the entire data set   we give such full knowledge to the static query optimizer  hence showing its best performance  We design two query templates based on Q1 and Q2 from SDSS  We vary the parameters   1 and   2 to control selectivities of predicates for Q1 and   3 and   4 for Q2  The details of our setup are given in Appendix F 3  Expt 5  We    rst consider Q1 and vary   1 and   2  Table 2 shows the time cost per tuple for static  dynamic and optimal planning  The optimal planning loads the optimal plan for each tuple  generated of   ine  into memory before it runs  The plan space for dynamic 11750 0 2 0 4 0 6 0 8 1 0 1 2  0 500 1000 1500 2000 Time per tuple  ms     in the range predicate without filter with filter  a  general    lter vs exact integration rowc      100 100         colc      100  100     0 100 200 300 400 500 0 1 2 3 4 5 Candidates per probing tuple    in the range predicate xbound GJ exact  b  xbound vs GJ in    ltering power  stream    R u     S u        0 100 200 300  0 1 2 3 4 5 Time per window  ms     in the range predicate xbound GJ  c  xbound vs GJ in ef   ciency  stream    R u     S u        0 5 10 15 20 25 30 0 1 2 3 4 5 Candidates per probing tuple  x10 6      in the range predicate xbound GJ  d  xbound vs GJ in    ltering power  disk    R u     S u        0 10000 20000 30000 40000  0 1 2 3 4 5 Time per probing tuple  ms     in the range predicate xbound GJ  e  xbound vs GJ in ef   ciency  disk    R u     S u        0 100 200 300 400 500 0 20 40 60 80 100 Candidates per probing tuple    in the range predicate xbound GMJ exact  f  xbound vs GMJ in    ltering power  stream    R u     S u        Figure 3  Experimental results for selections and joins  planning is shown in Appendix F 3  Our dynamic query planning outperforms the static one in all cases  with over 50  gains in most cases and is very close to the optimal planning  The reasons are three fold   1  Each tuple is routed based on its distribution and resulting selectivities of predicates  A tuple may be sent to a predicate that is overall not selective but has a larger chance to    lter this tuple   2  The predicate cost is taken into consideration  It is possible for a tuple to be routed to a predicate with only a modest chance to    lter the tuple but has a very low cost   3  Our fast    lters can drop tuples earlier at a lower cost than using the exact integration to evaluate predicates  Expt 6  We next consider Q2 and vary   3 and   4   For this query  a join index is constructed for G2 on the    rowc    and    colc    attributes for each window of size W  Table 3 shows the time cost of joining W tuples from the input G1 with W tuples from G2  We make similar observations as before  The dynamic planning is better than the static one in all cases  As we increase   3 and   4   more tuples satisfy both predicates  So the difference between the two schemes decreases and is mainly due to the bene   t of using fast    lters  Due to space constraints  the plan space for dynamic planning and the detailed analysis are given in Appendix F 3  7  CONCLUSIONS We presented techniques to optimize threshold query processing on continuous uncertain data by  i  expediting joins using new indexes   ii  expediting selections by reducing dimensionality of integration and using faster    lters  and  iii  using dynamic  per tuple based plannig  Results using the SDSS benchmark show signi   cant performance gains over a state of the art indexing technique and its threshold query optimizer  In future work  we will extend threshold query optimization to a larger class of queries including group by aggregation  support user de   ned functions  and evaluate our techniques in broader applications  8  REFERENCES  1  R  Avnur and J  M  Hellerstein  Eddies  Continuously adaptive query processing  In SIGMOD  261   272  2000   2  N  Beckmann  H  P  Kriegel  et al  The R  tree  An ef   cient and robust access method for points and rectangles  In SIGMOD  322   331  1990   3  O  Benjelloun  A  D  Sarma  et al  Uldbs  Databases with uncertainty and lineage  In VLDB  953   964  2006   4  P  Bizarro  S  Babu  et al  Content based routing  different plans for different data  In VLDB  757   768  2005   5  S  Boyd and L  Vandenberghe  Convex Optimization  Cambridge University Press  2004   6  S  Chaudhuri and K  Shim  Optimization of queries with user de   ned predicates  In ACM TODS  24 2  177   228  1999   7  R  Cheng  S  Singh  et al  Ef   cient join processing over uncertain data  In CIKM  738   747  2006   8  R  Cheng  Y  Xia  et al  Ef   cient indexing methods for probabilistic threshold queries over uncertain data  In VLDB  876   887  2004   9  P  Cudre Mauroux  H  Kimura  et al  A demonstration of SciDB  A    science oriented dbms  PVLDB  2 2  1534   1537  2009   10  N  N  Dalvi and D  Suciu  Ef   cient query evaluation on probabilistic databases  VLDB J   16 4  523   544  2007   11  M  Geoffrey and P  David  Finite Mixture Models  Wiley  2000   12  J  Li and A  Deshpande  Ranking continuous probabilistic datasets  PVLDB  3 1  638   649  2010   13  L  Peng  Y  Diao  et al  Optimizing Probabilistic Query Processing  UMass Tech Report  http   claro cs umass edu pubs tr11 pdf   14  Y  Qi  R  Jain  S  Singh  et al  Threshold query optimization for uncertain data  In SIGMOD  315   326  2010   15  N  Ravishanker and D K  Dey  A    rst course in linear model theory  Chapman   Hall CRC  2002   16  S  Singh  C  May   eld  et al  Database support for probabilistic attributes and tuples  In ICDE  1053   1061  2008   17  D  Suciu  A  Connolly  et al  Embracing uncertainty in large scale computational astrophysics  In MUD Workshop  2009   18  A  S  Szalay  et al  Designing and mining multi terabyte astronomy archives  The sloan digital sky survey  In SIGMOD  451   462  2000   19  T  T  L  Tran  et al  Conditioning and aggregating uncertain data streams  Going beyond expectations  PVLDB  3 1  1302   1313  2010   20  T  T  L  Tran  L  Peng  et al  Pods  a new model and processing algorithms for uncertain data streams  In SIGMOD  159   170  2010   21  D  Z  Wang  E  Michelakis  et al  Bayesstore  Managing large  uncertain data repositories with probabilistic graphical models  In VLDB  340   351  2008  1176APPENDIX A  DETAILS OF THE SDSS DATABASE We illustrate the schema of the SDSS data set in Table 4  name type description O B J I D bigint SDSS identi   er with  run          eld  obj           rowc  rowc err  real  row center position  error term   colc  colc err  real  column center position  error term   q u  qErr u   real  stokes Q parameter  error term   u u  uErr u   real  stokes U parameter  error term   ra  dec  ra err  real  right ascension  declination  error in ra  dec err  ra dec corr  error in dec  ra dec correlation          Table 4  Schema of the Galaxy table in the Sloan Digital Sky Survey  SDSS   Attributes in italics are uncertain  We use two complex queries from the SDSS benchmark  18   where the attributes in the lower case are uncertain attributes  Q1  SELECT   FROM Galaxy G WHERE G r   22 AND G q r 2  G u r 2   0 25  Q2  SELECT   FROM Galaxy AS G1  Galaxy AS G2 WHERE G1 OBJ ID   G2 OBJ ID AND   G1 u G1 g   G2 u G2 g     0 05 AND   G1 g G1 r   G2 g G2 r     0 05 AND  G1 rowc G2 rowc  2   G1 colc G2 colc  2  1E4  The released data archive takes about 1GB  For experiments on selections and joins  we used the Star table  which has 57328 tuples and each column of uncertain attribute is about 1MB  We consider Q1 and Q2 for experiments on query planning  both of which involve the Galaxy table with 91249 tuples  B  DETAILS OF THE DATA MODEL A Gaussian Mixture Model  or distribution  is de   ned as follows  De   nition B 1 A Gaussian Mixture Model  GMM  for a continuous random variable X is a mixture of m Gaussian variables X1   X2             Xm  The probability density function  pdf  of X is  fX x    m     i 1 pi fXi  x   fXi  x    1   i     2   e      x      i   2 2   2 i  Xi     N   i      2 i     where 0     pi     1      m i 1 pi   1  and each mixture component is a Gaussian distribution with mean   i and variance    2 i   De   nition B 2 A multivariate Gaussian Mixture Model  multivariate GMM  for a random vector X naturally follows from the de   nition of multivariate Gaussian distributions  fX x    m     i 1 pi fXi  x   fXi  x    1  2    k 2    i   1 2 e     1 2  x      i   T       1 i  x      i    Xi     N    i     i     where k is the random vector size  and each mixture component is a k variate Gaussian with mean    i and covariance matrix   i   Correlation  To address inter tuple correlation in our data model  we adopt the use of history to capture dependencies among attribute sets as a result of prior database operations  16   The history  H  of an attribute set is de   ned as follows   1  For a newly inserted tuple t  H t A    t A   2  If a new set of attributes     t A     is derived from multiple attribute sets   t i  Ai  i   1  2          via a database operation  then H    t A          H t i  Ai    That is  the history of the new attribute set includes the base pdf   s that can be used to derive the joint pdf of this set of attributes  Finally  if two attribute sets intersect  they become correlated  Then a joint distribution of the two sets can be computed from their histories to capture correlation  C  DETAILS OF OPTIMIZING SELECTIONS Correctness of    nding the transformation matrix PROOF  Given a selection region R     it is known that for any nonsingular transformation matrix Dk  k   Pr R    X     Pr RD     YD     where RD    is the transformed region by matrix D  Below we show that given Bl  k and bl  1 obtained from the procedure in   3 1  we can    nd a nonsingular matrix Dk  k such that Pr RD     YD      Pr R B     Y B     i e   B gives the correct integration result  Since B has full row rank  we can apply elementary column operations and transform it into   B   l  l   0 l   k   l      where B   is nonsingular and 0 is the zero matrix  De   ne a new matrix and a new vector D  k  k     B   l  l 0 l   k   l  0  k   l   l I  k   l    k   l      d     bl  1 0  k   l   1     where I is the identity matrix  It can be proved that D  is nonsingular  We can then apply elementary column operations to D  such that the two upper blocks are transformed back to B  Denote the new matrix as D and obviously D is also nonsingular  Then we have  Y D  DX d    Bl  k I    k   l   k   X    bl  1 0  k   l   1       BX b I   X       Y B Y       Since there is no predicate on Y   in the selection condition     the integration interval on each dimension of Y   is                and that on each dimension of Y B does not vary with Y     So the integral of the joint pdf of YD over region RD    equals that of its marginal distribution on Y B over region R B      i e  Pr RD     YD      Pr R B     Y B     Then Pr R    X   Pr R B     Y B     One dimensional predicates  Fast    lters using Markov   s inequality  We consider three cases that apply Markov   s inequality based on the different relationships between the point 0 and the predicate region      0 a1   Pr R    X     Pr  a1              E X  a1     bn  0  Pr R    X     Pr          bn          E X  bn     bi   1  0 ai   Pr R    X     Pr          bi   1    Pr  ai              E X  min    bi   1   ai   The distribution of  X  can be computed as follows  When X follows a GMM with m components  each identi   ed by parameters  pi     i      2 i    we have E X        m i 1 piE Xi    where E Xi       i     2    exp       2 i     2   2 i         i   1     2        i   i     and    is the cdf of a standard normal distribution  Fast    lters using Chebyshev   s inequality and Cantelli   s inequality  Both inequalities can be applied to predicates  n i 1  ai   bi   if    does not reside in the predicate region  Again  we consider three cases below  In the    rst two cases  Cantelli   s inequality gives a tighter upper bound  In the third last case  we need to compute upper bounds using both inequalities and choose the smaller one         a1   Pr R    X   Pr  a1                2    2    a1         2     bn      Pr R    X   Pr          bn         2    2           bn  2     bi   1    ai   Pr R    X   Pr          bi   1    Pr  ai             min      2  min      bi   1  ai        2      2    2       bi   1   2      2    2  ai       2 0 1177Multi dimensional quadratic predicates  If X follows a GMM  its quadratic form X T   X yields a new random variable  We    rst derive the new distribution as follows  For X     N          we have E X T   X    tr            T      Var X T   X    2tr             4   T           where tr     denotes the trace of a matrix  For X follows a GMM with m components  each identi   ed by  pi      i     i    i   1           m   E X T   X    m     i 1 piE X T i   Xi   Var X T   X   E   1 X T   X 22       1 E X T   X  22   m     i 1 piE   1 X T i   Xi 22       1 E X T   X  22   m     i 1 pi   Var X T i   Xi    1 E X T i   Xi   22       1 E X T   X  22 Now suppose that the quadratic form X T   X yields a new random variable with mean   0 and variance    2 0   This allows us to apply Chebyshev   s inequality and Cantelli   s inequality as before  However  in the case of comparing a quadratic form with a constant  the predicate contains only one interval  either              or             Hence  when u0 lies outside the predicate region  Cantelli   s inequality always gives a tighter bound  Formally      If the predicate region is X T   X       when        0  Pr R    X          2 0        2 0      0         2       If the predicate region is X T   X       when        0  Pr R    X          2 0   1    2 0             0  2 2 D  DETAILS OF JOIN INDEXES D 1 Proofs of Theorems Proof for Theorem 1 PROOF  Suppose       3  1            a or         3  1             b  Then interval            3  1                     3  1             does not overlap with a  b   According to Cantelli   s inequality  Pr a   Z   b    1 1    1                  which is a contradiction  This completes the proof  Proof for Theorem 2 PROOF  If there exists an         0  1      such that a        1                  b        1              then we have a                    1       b                    1            Therefore Pr a Z b    Pr  a               Z               b                 Pr       1          Z                    1                  On the other hand  when Pr a  Z  b          de   ne          a            Apparantly         0  1         and     b                        Therefore  we found an    such that a        1                   b        1              D 2 Overlap Test in the Gaussian Join Index Recall that when search the R tree for the Gaussian index  our task is to test the overlap between R   I      1     2    1     2   an index entry  and R   Q        the query region   where                                             0       0       Figure 4  Two cases when R   I and     overlap          0        1                  a         1                   b         1               First  we show that          R   I   Since    is the standard deviation of a random variable Z         0  In the extreme case when      0  Z is reduced to a constant  so   min   0  where   min is the minimum value of    in      For a valid search key  it is impossible that   1   0  so          R   I   Given that          R   I   testing whether R   I and     overlaps is the same as to test whether there exists a point    0    0  on the edges of R   I   such that    0    0           De   ne a function g              1 b         2        1 a         2   where    is the cdf of the standard normal distribution  It is straightforward to see that                  iff g                 Then the problem is again transformed to checking whether the maximum value of g         on edges of R   I   denoted </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pd2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pd2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_data__fuzzy_data_and_data_provenance"/>
        <doc>Probabilistic String Similarity Joins ### Jeffrey Jestes Feifei Li Computer Science Department  FSU Tallahassee  FL  USA  jestes  lifeifei  cs fsu edu ABSTRACT Edit distance based string similarity join is a fundamental oper  ator in string databases  Increasingly  many applications in data cleaning  data integration  and scientific computing have to deal with fuzzy information in string attributes  Despite the intensive ef  forts devoted in processing  deterministic  string joins and manag  ing probabilistic data respectively  modeling and processing prob  abilistic strings is still a largely unexplored territory  This work studies the string join problem in probabilistic string databases  us  ing the expected edit distance  EED  as the similarity measure  We first discuss two probabilistic string models to capture the fuzzi  ness in string values in real world applications  The string level model is complete  but may be expensive to represent and process  The character level model has a much more succinct representa  tion when uncertainty in strings only exists at certain positions  Since computing the EED between two probabilistic strings is pro  hibitively expensive  we have designed efficient and effective prun  ing techniques that can be easily implemented in existing relational database engines for both models  Extensive experiments on real data have demonstrated order of magnitude improvements of our approaches over the baseline  Categories and Subject Descriptors H 2 4  Information Systems   Database Management   Systems  Subject  Query processing General Terms Algorithms Keywords Probabilistic strings  approximate string queries  string joins ### 1  INTRODUCTION Similarity join on string valued attributes in relational databases is an important operator with a large number of applications  Ex  amples include data cleaning  8   data integration  10   fuzzy key  word search  15  and many others  5  13  28   The edit distance Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     10 00  Zhepeng Yan Ke Yi Dept of Computer Science   Engineering HKUST  Hong Kong  China cs yzx stu ust hk  yike cse ust hk is the most commonly used similarity measure between two strings  7 8 13 17 19 28   and edit distance based string similarity join has been extensively studied in the literature  with a suite of techniques developed ranging from simple SQL statements  8  13  to more complicated ones that need to be implemented within the database engine  28   However  the increasing presence of probabilistic data in many applications  represented by the recent development in the TRIO  1   MayBMS  4   MystiQ  6   PrDB  25   MCDB  14  and Orion  26  projects  introduces new and interesting challenges for string joins in databases with probabilistic string attributes  which is the focus of this paper  Note that a probabilistic model is particularly useful for string attributes in the aforementioned applications  In data cleaning  un  certainty often arises in string values  due to data entry errors or simply typing mistakes  7   Similar scenarios also appear in the automatic recognition of scanned images for bank checks  23   In a scientific computing project involving strings  such as genome  sequencing and DNA sequencing  it is common that scientists have identified the values  one of the letters in     A        C        G        T      for 90  of the positions in a genome sequence  while there is some degree of uncertainty for the other 10   Finally  in a data integra  tion scenario  5  10   string values for describing the same object from different data sources could be different  In all these appli  cations  the    uncertainty    in strings is not    unknown     and certain knowledge  prior or posterior  is often available  11  12  22   Such knowledge is best modeled by probabilistic strings  Probabilistic string models  A natural way of modeling a proba  bilistic string S is the string level uncertainty model  in which all possible instances for S are explicitly listed and they form a prob  ability distribution function  pdf   Formally  Definition 1  string level model  Let    be an alphabet  A string  level probabilistic string is SP      1  p1              m  pm   where   i           pi     0 1 and mi 1pi  1  Sinstantiatesinto  i with probability pi  We also use S i  to denote its i th choice  i e   S i       i pi  for i   1     m  This model is complete  since it can represent any pdf of a prob  abilistic string  However  the uncertainty in a probabilistic string in many of the aforementioned applications often exists only at a few positions  Representing such probabilistic strings in the string  level model would be highly redundant  especially when each un  certain position has many choices or the string has a few uncer  tain positions  leading to an exponential blowup in the string level model representation  Indeed  this phenomenon  uncertain posi  tions  has been respected in existing relational databases by their explicit support for wildcards  For example          is a wildcard in SQL that can match any character  so    advis r    may represent either    adviser    or    advisor     Using wildcards assumes no knowledge for the uncertain positions  However  in most cases  c   1 i     2 j      1    1 i        2 j   0   1 i      2 j   where    i  is the ith character in a string     and the base case is d i j  i jwheni  j 0 InthisDP d i j storestheedit distance between substrings   1 1  i  and   2 1  j   thus d   1    2    d    1      2    This DP   s running time is clearly O    1        2    The edit distance of two probabilistic strings is a random vari  able  so its expectation is a natural measure of similarity between two probabilistic strings  Its formal definition uses the following possible worlds semantics  Let S1 and S2 be two probabilistic strings in either the string level or the character level model  Definition 3  possible worlds  The possible worlds    of S1 and S2 is the set of all possible combinations of instances of S1 and S2   A possible world s        is a pair     1  p1      2  p2    where   1    2  is an instance from S1  S2  with the probability p1  p2   The probability of s is w s    p1    p2   and the edit distance between S1 andS2 insisd s  d   1   2   Note that for both probabilistic string models  the possible worlds    defines a probability space  namely s      w s    1  Definition 4  expected edit distance  The expected edit distance some degree of knowledge is available for these positions  21 22   For example     advisur    is considered as a possible instance of    advis r     but most likely this is not what the user intends  Based on statistics  and common sense  we can reasonably postulate that this         is    o    with 80  probability and    e    with 20  probability  With merely a wildcard          we have essentially assumed a uniform distribution over the entire alphabet  which is far less accurate than the pdf constructed based on statistics  Generalizing this observation  a more concise model for many applications is the following character level uncertainty model  Definition 2  character level model  Let    be an alphabet  A char  acter level probabilistic string is S   S 1        S n   For i   1     n S i   P ci 1 pi 1       ci   i pi   i   whereci j        pi j      0  1  and   i pi j   1  Each S i  instantiates into ci j j 1 with probability pi j independently  We define a few notations  We use  X  to denote the absolute value of X if X is an integer  the number of elements in X if X is a set  and the length of X if X is either a deterministic or a prob  abilistic string  Note that if S is a probabilistic string in the string  level model   S  is a random variable  if S is a probabilistic string in the character level model   S  is a constant  We use size X  to denote the number of choices in X if X is a pdf  Hence  a string  P level probabilistic string S hasQsize S  choices  and a character  nb X level probabilistic string S has i 1 size S i   choices  The independence assumption in Definition 2 allows us to suc  cinctly represent exponentially many possible instances of a ran  dom string using the character level model  which would otherwise have to be stored explicitly in the string level model  This is also a reasonably good approximation of reality for many of the afore  mentioned applications  since in many applications the number of uncertain positions in a string is usually a small and isolated per  cent  In fact  similar models with independent uncertain positions have also been used for molecular biology  pattern matching  and similarity search in random sequences  see  21  and the references therein   If there is non negligible correlation between adjacent lo  cations  it is possible to consider a hybrid model that combines the string level model and the character level model  by making each S i  a string level probabilistic string instead of one single charac  ter  To keep the presentation clean we will focus on the string level model and the character level model in this paper  but the tech  niques could be extended to the hybrid model as well  With the wide presence of uncertainty in string data  probabilis  tic string similarity joins have become an important problem  Its role in probabilistic databases should be comparable to that of de  terministic string joins in traditional databases  which have been a focus of study in past years  8  13  17   19  27  28   Problem formulation  We extend the edit distance  the standard similarity measure for deterministic strings  to the probabilistic set  ting  by considering the expected edit distance  EED  between two probabilistic strings  Recall that for two deterministic strings   1 and   2   the edit distance d   1     2   is the minimum number of edit operations required to transform   1 to   2  where an edit operation is an insertion  a deletion  or a substitution of a single character  It is well known that d   1     2   can be computed by a simple dynamic program  the p8roof of its correctness is not trivial though       d i  j     1    1  insertion d i j  min d i   1 j  1  deletion s      Finally  the join problem is formally defined as  Definition5 probabilisticstringjoin  GiventablesRandT each with a probabilistic string attribute S  the string join between R and TonSistoreturnallpairsofrecords ri tj suchthatri    R tj     Although being a distance naturally defined between two proba  bilistic strings  the EED is very difficult to compute  Let us consider the character level model  One may be tempted to adapt the dy  namic program  1  to compute the EED  by changing c   1 i     2 j   to Pr S1 i      S2 j    which is the EED between S1 i  and S2 j   This is  unfortunately  wrong  Consider the following simple exam  ple  S1 is a single character string    a    or    b     each with probabil  ity 0 5  and S2 is    ab    with probability 1  namely  a deterministic string  There are two possible worlds     a        ab     and     b        ab      each with probability 0 5  It is clear that in both worlds  the edit distance is 1  thus the EED is also 1  However  the modified DP would give an incorrect answer 1 5  One may try other possible DP formulations  but any attempt turns out to be flawed  The fun  damental difficulty with any DP based approach is that it tries to relate the EED of a subproblem to the EED of a bigger problem  But the EED does not follow any optimal substructure property  the expectation of the minimum of two  or more  random variables is not equal to the minimum of their expectations  In fact  such DPs will only give upper or lower bounds on the EED  c f  Section 4 1 2 and 4 2 2   The only immediate method available to compute the EED exactly is to do so by enumerating all the possible worlds  which is clearly very expensive  especially in the character level model  we conjecture that this problem is  P complete   Thus we have an even more urgent need for effective and efficient pruning techniques for string joins in the probabilistic setting  which are the main contribution of this paper  Our contributions  In this paper we present a comprehensive in  vestigation on the problem of efficient string joins in probabilistic string databases  using the EED as the similarity measure  in both in which d i     1  j     1    c   1 i     2 j    substitution  1   EED  between S1 and S2 is d S1  S2    w s     d s    2  b T andd ri S tj S         where   issomeuser specifiedthreshold  the string level and character level models  In particular  we aim at performing such joins on top of a relational database engine  There are many benefits in realizing this goal  and in some cases it is critical  as it would be more cost effective to store and process large probabilistic data sets using existing mature relational data  bases  3   Indeed  for deterministic string joins  there have been a lot of efforts in trying to perform joins using mainly SQL and UDF in a relational database  8  13   More specifically      We introduce the string level and the character level proba  bilistic string models  and define the string join problem us  ing the EED in probabilistic string databases      We present efficient lower bound filters  based on what we call probabilistic q grams  to effectively prune string pairs that cannot possibly join under both models      We also present efficient upper bound filters to effectively report string pairs that must join in the character level model      We integrate the lower bound filters and the upper bound fil  ters  and give their implementation in relational databases      We conduct a comprehensive experimental evaluation for our algorithms in both models on a large number of real data sets  The results show that our efforts have lead to several orders of magnitude of performance improvement compared to the baseline approach  In the following we first provide some necessary background on q grams used in deterministic string joins in Section 2  Then we develop our techniques in Section 3 and 4  The experimental eval  uation is given in Section 5  We survey the related work in Section 6 and conclude in Section 7  2  BACKGROUND ON Q GRAMS A deterministic string join  with an edit distance threshold      on two string attributes A and B from two tables R and T in a relational database may be implemented as SELECT R id  T id FROM R  T WHERE d A B          Q1  where d A B  is a user defined function  UDF  that implements the dynamic program  1   Existing studies demonstrated that con  siderable improvements can be achieved using q grams  For a string     its q grams are produced by sliding a window of length q over the characters of     To deal with the special cases at the beginning and the end of    where there are fewer than q characters  we extend    by prefixing it with  q   1          and suffixing it with  q     1           where         and         are not in     Hence  each q gram for the string    has exactly q characters  A q gram also contains its positional information  Formally  Definition6 q gram  Aq gramforastring  isapair l g where l is the beginning position of the q gram  in the extended string      and g is the substring of length q in    beginning from l  Example 1 The 2 grams for the string advisor include   1   a    2  ad    3  dv    4  vi    5  is    6  so    7  or    8  r     Givenq1    l1 g1 andq2    l2 g2  q1   q2 ifg1   g2  i e   positions are ignored when comparing two q grams  Let G   be the set of all q grams in     For strings   1 and   2  define G  1     G  2     q1 q2    q1   q2 q1     G  1 q2     G  2   namely all the matchingpairsofq gramsfromG  1 andG  2 respectively  Clearly  a string    has         q     1  q grams  It has been ob  served that strings with a small edit distance share a large number of common q grams  This intuition has been formalized by  13 27   among others  Essentially  if we substitute a single character in   1 to obtain   2  then their q gram sets differ by at most q q grams  Similar arguments hold for insertions and deletions  Formally  Lemma 1   13  27   For two strings   1 and   2   we have  G  1     G  2       max    1      2       1     q d   1    2      1    3  Since G  1    G  2 canbecomputedefficiently Lemma1gives us a lower bound on d   1   2   if this lower bound is greater than      we can prune this pair from further consideration  Lemma 1 can be further improved with the positional informa  tion of the q grams  Intuitively  suppose d   1   2        then only those q grams within distance    could possibly match  So forq1   l1 g1 andq2   l2 g2 wedefineq1  k q2 ifg1  g2 and  l1     l2      k  and also define    k to be the set of matching pairs of q grams where the matching is done w r t   k   Lemma 2   13 27   For strings   1 and   2  we have  G  1    d   1   2  G  2       max    1      2       1     q d   1    2      1    4  Assuming d   1   2          by  4  we have d   1   2    1  1 max    1     2     1    G  1    d   1   2  G  2   q    1  1 max    1     2     1    G  1       G  2     5  q So if RHS of  5  is larger than      this will contradict the assumption that d   1   2          thus it must be d   1   2        Since  G  1       G  2     G  1    G  2  thisisatighterlowerboundthan 3   Finally  there is another straightforward observation  Lemma3  13 27   d   1   2        1       2    The implementation of these lemmas in SQL is possible and has been explored in  13   which shows a significant performance im  provement over the query  Q1   3  THE STRING LEVEL MODEL We represent a string level probabilistic string attribute in a re  lational database using five columns  id  cid  A  p  len  Consider a probabilistic string S       1  p1              m  pm    We convert it into m rows with each row representing one choice of S  All m rows share the same id  which uniquely identifies S  The cid column will take values 1          m  denoting the index of these m choices  The A column stores the m strings for these m choices  and p stores the corresponding probabilities  Finally  all m rows store the same len  which is the expected length E  S     P mi 1 pi   i   Please refer to Figure 1 for an example  The straightforward implementation of the string join over two tables R and T is to apply  2  in Definition 4 directly  with the help of the UDF d A B   the DP for two deterministic strings in the query  Q1    which leads to the following SQL query  probabilistic strings id S q grams id g 1     a 1 ad 1 dd 1 d  1  p 1 pl 1 lu cid l 1 1 1 1 2 2 2   1 2 3 4 1 2 3   1 2 id 1 1   add  0 8    plus  0 2     up  0 9    op  0 1   relational representation len 3 2 3 2 cid A p 1 2 1 2 add plus up op 0 8 0 2 0 9 0 1 2 2    22 Figure 1  String level probabilistic strings  their probabilistic q grams  q   2   and the relational representation  SELECT R id T id FROM R T GROUP BY R id T id HAVING SUM R p T p d R A T A           Q2  Clearly  this approach is very expensive  since it requires running a DP for every possible world on the attributes R A and T A for each pair of records in R and T  Our goal is to design efficient filters that can prune away most pairs of records that are not possible to match and execute as few DPs as possible  Note that one cannot improve this basic approach by directly applying the deterministic q gram based join in each possible world  since a threshold on the edit distance of each world would need to be given  which cannot be determined  Consider two string level probabilistic strings S1       1 1  p1 1          1 m1 p1 m1  andS2       2 1 p2 1         2 m2 p2 m2    The possible worlds    is the set that contains s    S1 i   S2 j         1 i p1 i     2 j p2 j  fori 1     m1 andj 1     m2  The first filter is a directXextension of the length filter  Lemma 3   for which the proof is trivial  Lemma 4 For any string level probabilistic strings S1 and S2  db S1 S2     w s     1 i       2 j     6  s      Inspired by the q gram based lower bounds on the edit distance of two deterministic strings  we propose effective lower bounds on the EED of S1 and S2 based on their probabilistic q grams  Definition 7  string level probabilistic q grams  Given a string  level probabilistic string S       1  p1              m  pm    a prob  abilistic q gram is a quadruple  i  p  l  g   where i is the choice index  cid   p is the choice probability  while l and g have the same meanings as those in deterministic q grams  The set of probabilistic q grams for S i   denoted as GS i   is  GS i     i pi lj gj    foralljs t  lj gj    G  i  GS is the set of string level probabilistic q grams for S  GS  GS 1             GS m   These probabilistic q grams can also be easily stored in a rela  tional table  as illustrated in Figure 1  For two probabilistic q grams   1    i  pi  lx  gx  and   2    j pj ly gy fromGS1 andGS2 respectively wedefine  1    2 ifgx  gy   1  k   2 ifgx  gy and lx    ly    k  Theopera  tors     and    k between GS1 and GS2 are defined similarly as in the deterministic case w r t  the   and  k operators  respectively  We also let p     represent the choice probability of     e g   p   1    pi and p   2    pj in the above example  Applying Lemma 2 in the possible world s    S1 i   S2 j    we have  The following theorem gives a lower bound on db S1   S2    Theorem 1 For any string level probabilistic strings S1 and S2     1   2    GS1   GS2 p   1 p   2  1     q   8  max E  S1     E  S2     d S1 S2      1  q bP PROOF  Let s denote a possible world  S1 i  S2 j   from     From  7  we have  X w s  G     G       S1 i  S2 j  s      w s  max    1 i      2 j        1     q d s      1     9  w s d s    q     max E  S1    E  S2        1     q db S1  S2      1   where the last step is by Lemma 5  The LHS of  9  is PPX s        E max  S1    S2        1     q    d S1  S2    q w s  X X X p   1 p   2  p   1 p   2    10  Rearranging the inequality  we obtain  8   Theorem 1 provides an efficient and effective way of pruning a pair of probabilistic strings  A pair will be pruned if the RHS of  8  is larger than      Given tables R and T  suppose their probabilistic q grams are stored in the format of Figure 1 in auxiliary tables Rq and Tq   Theorem 1 and Lemma 4 lead to the following query for the string join between tables R and T     1    2     GS1    GS2 1 2 3 4 5 6 7 EXCEPT 8 SELECT R id AS rid  T id AS tid FROM R  T 9 GROUP BY R id  T id 10 HAVING SUM R p T p ABS  R A   T A         11   AS L 12 WHERE L rid R id AND L tid T id 13 GROUP BY R id  T id 14 HAVING SUM R p T p d R A T A           Q3  In query  Q3   the pruning conditions in Theorem 1 and Lemma 4  as seen in the relation L  are first applied  The expensive calcu  lation of the exact EED based on  2  is only applied in the outer query block for those pairs of strings that cannot be pruned  As a result   Q3  is much more efficient than  Q2  in practice  In some database engines  max a  b   a  b are two real values  is not a built in function  In this case we can replace max a b  with  a   b    a     b   2  In addition  almost all engines evaluate the WHERE Rq g Tq g AND Rq id R id AND Tq id T id AND Rq cid R cid AND Tq cid T cid  GS1 i      GS2 j   The next lemma will be useful in deriving EED lower bounds  GROUP BY R id  T id  R len  T len HAVING1  max R len T len  SUM R p T p  1  q           GS1 i     d s  GS2 j    7      max    1 i     2 j     1   q d s    1   Lemma 5 For two non negative random variables X  Y   E max X  Y        max E X   E Y     s      Since s      w s    1  the RHS of  9  equals s      w s  max    1 i       2 j        1     q b Ps      w s       GS1 i      GS2 j   XX   s         1   2    GS1 i    GS2 j  s         1   2    GS1 i    GS2 j  SELECT R id  T id FROM R  T   SELECT R id AS rid  T id AS tid FROM R T Rq Tq PROOF  E max X Y     E   X  Y   X    Y    2 E X  E Y  E  X    Y    2 2 E X    E Y   E X     Y       2   2  E X   Similarly  E max X  Y        E Y    HAVING clause in a row by row fashion within each group  and multiple conditions in the HAVING clause will not be evaluated using short circuit evaluation  which explains our choice to place the exact EED query condition in an outer query block  The same reason also explains the forthcoming SQL statements in this paper  Theorem 1 can be seen as the counterpart of Lemma 1 in the probabilistic setting  which ignores the position information of the q grams  Considering the position information as in Lemma 2 potentially will give a tighter lower bound  but is unfortunately much more difficult in the probabilistic case than in the determin  istic case  We can follow the proof of Theorem 1 by replacing     with    d s  on the LHSPof  9   but the difficulty is that we can  not combine the two         in the last step as in  10   since d s  could be different for different possible worlds  Making things even worse  we do not know d s  as this is the very computation that we aim tPo avoid  We only know that its expectation  namely  db S1 S2   s     w s d s shouldbeatmost   Moreprecisely  from the second inequality in  7  and following the proof of Theo  rem 1  the RHS derivation stays the same   we obtain Xw s  GS1 i     d s  GS2 j   s      This leads to a tighter lower bound on db S1   S2    Theorem 2 For any string level probabilistic strings S1 and S2  max E  S1   E  S2      UB      1 d S1  S2      1   q       max E  S1    E  S2        1     q db S1  S2      1    11  b ComputingUB   requiresanaggregateUDFthattakesallprob  abilistic q grams for a pair of probabilistic strings and    as input  We omit the details on its implementation and let ub Rq cid  R p  Rq l  Rq g  Tq cid  T p  Tq l  Tq g     de  note this UDF  In some engines  an aggregate UDF may take only one input parameter  which could be a user defined data type  In these cases  we declare a user defined data type with nine elements as shown above  Since the aggregate UDF could be expensive to evaluate  our idea is to only invoke this pruning after the pruning by Theorem 1 and Lemma 4  The next query implements the string join with the pruning by Theorem 1 and Lemma 4 first  then Theo  rem 2  SELECT R id  T id FROM R  T    SELECT R id AS rid  T id AS tid FROM R T Rq Tq           AS L  same as lines 2 11 in Q3  WHERE L rid R id AND L tid T id AND Rq g Tq g AND Rq cid R cid AND Tq cid T cid AND Rq id R id AND Tq id T id GROUP BY R id  T id  R len  T len HAVING 1   1    max R len T len    1   q ub Rq cid R p Rq l Rq g Tq cid T p Tq l Tq g              AS L2           same as lines 12 14 in Q3  but with L2   Q4  4  THE CHARACTER LEVEL MODEL We can certainly represent a character level probabilistic string in the string level model  i e   explicitly store all possible strings it might instantiate into  But this approach incurs a huge space overhead  as there are exponentially many possible instances for a character level probabilistic string  The large size also implies a large processing cost when performing joins  To remedy this problem  we store a character level probabilistic string S as one string in a relational table  by representing each probabilistic character S i    s pdf as follows  We separate different choices in S i  with          and enclose each S i  with two            assum  ing that         and           do not belong to     For each choice in S i   we simply write down the character followed by its probability  If S i  is deterministic  i e   it has only one choice with probability 1  we just write it down by itself  If S is deterministic  i e   all characters are deterministic   it is thus stored as a normal string  We also store the length  S  of S  Hence  the table has three columns  id  A  len  Please refer to Figure 2 for an example  Next  we will assume d S1   S2          and under this assumption  relax the LHS of  11  by enlarging it so that it is easy to compute while  11  still holds  The relaxed  11  will give a lower bound on db   S 1   S 2     i f t h i s l o w e r b o u n d i s h i g h e r t h a n      t h a t w i l l c o n t r a d i c t the assumption that db S1   S2            thus  S1   S2   can be pruned  Let              be the subset of possible worlds s in which d s      2     Since db S1   S2            by the Markov inequality the probability mass of       is at most 1 2  We relax the LHS of  11  as LHS of  11      X w s  GS1 i      GS2 j     s         X w s  G    2     1 G   S1 i  S2 j  s              Xw s  GS1 i     2     1 GS2 j   s       Xw s   G    G      G    2     1G     s         Since we do not know d s   hence        we have to pessimistically assume that       is chosen so that the LHS of  11  is maximized  Denoting x s     GS1 i      GS2 j      P GS1 i     2     1 GS2 j    the problem becomes choosing       with     w s      1 2 to max  P s      S1 i  S2 j  S1 i  S2 j  imize s         w s x s   This is exactly a 0 1 knapsack problem  which is NP hard  However  since we are happy with just an up  per bound  we can consider this as a fractional knapsack problem  which always yields an  actually quite tight  upper bound on the id optimal solution of the 0 1 version  1 probabilistic strings   A  0 8    C 0 2      G  0 7    T  0 3   q grams g 1     A  C 1    AG AT 1 CG b l p S id 0 80 0 20 0 56 0 24 0 14 0 06 0 70 0 30   1 1 2 2 2 2 3 3   Specifically  we initialize             and sort all the possible worlds 2 in    by the    value weight    ratio w s x s  w s    x s   We take 3 pPossible worlds from    into       in the decreasing order of x s  until UB     w s  GS1 i     2     1 GS2 j     w s x s    w s    x s     s         w s    1 2    A  1      G  0 6    T 0 4   1   C  1      A  1      G  1   1 s         w s    1 2  Suppose s    is the last world taken  Then an upper bound on the LHS of  11  is id relational representation XX 121G  CT 2 21T  len 1 s      P 3 3   A    A0 8 C0 2      G0 7 T0 3    A   G0 6 T0 4    CAG w s     Figure 2  Character level probabilistic strings  their proba  s         bilistic q grams  q   2   and the relational representation  The straightforward implementation of the character level string join is to define a UDF ed A B  that computes the EED of two probabilistic strings by applying  2  in Definition 4 directly  It enu  merates all possible pairs of instances  possible worlds  from the two inputs  computes the edit distance for each such pair    1     2   by calling the UDF d   1    2  and sums up the edit distances for all pairs weighted by their probabilities  Given ed A B   a join with threshold    can be expressed in SQL as follows  SELECT R id T id FROM R T WHERE ed R A T A         Q5  Clearly   Q5  is very expensive  since there are exponentially many possible worlds  hence exponentially many calls to d   1     2    when the EED is to be computed between two probabilistic strings  as opposed to quadratic in the string level model  Thus we have an even more urgent need for efficient and effective filters to avoid calling ed  Below we present two kinds of filters  Section 4 1 presents lower bound filters that prune away pairs that cannot pos  sibly join  while Section 4 2 presents upper bound filters that as  certain that a pair must join  The expensive UDF ed is invoked only on those pairs for which neither type of filter works  For both kinds of filters we first present a fast  purely relational one based on probabilistic q grams which are defined below  followed by a more expensive one using dynamic programming  but formulated on the probabilistic strings directly   Note that for deterministic string joins a filter cannot use DP as that is the exact computation to save  but in this case  our goal is to avoid running exponentially many DPs  so it is still beneficial if running just one DP can avoid so many of them  We first define the character level probabilistic q grams  Definition 8  character level probabilistic q grams  For a char  acter level probabilistic string S   S 1        S n   we first prefix and suffix S with  q     1   deterministic  characters     1   and     1    A character level probabilistic q gram is a pair  l  S l  l   q     1    where l is the beginning position of the q gram and S l  l   q     1  is the probabilistic substring S l        S l   q     1  in S  The set of all probabilistic q grams for S is denoted as GS   A probabilistic q gram in the character level model could be viewed as a random variable  the probability space of a proba  bilistic q gram       l  S l  l   q     1   is the possible worlds for the probabilistic string S l  l   q     1   In contrast  a probabilis  tic q gram in the string level model can be actually considered as a weighted q gram  where the weight is the probability of the cor  responding deterministic string that generates this q gram  In the special case when S l  l   q     1  contains no probabilistic char  acters     becomes deterministic  We denote all such      s from GS as GdS   We also introduce the notation GdS1  S2   GdS1    GdS2 and GS1 S2  GS1   GS2 forconciseness  We represent GS in a relational table by explicitly listing all the instances for each        GS   together with their corresponding prob  abilities  We apply this technique for each probabilistic string S  leading to a q gram table  denoted Rq   with four columns id  p  l  g as shown in Figure 2  One may be concerned with the poten  tial exponential blowup of this representation  but we argue that this will not happen in practice  First  q is very small in practice  q   2 is usually recommended  13    so we expect at most a quadratic space overhead in size S i    Second  a character level probabilis  tic string S for most real life applications does not have many un  certain positions  And third  we only have a quadratic blowup when these uncertain positions are consecutive  Given any two probabilistic strings S1   S1  1        S1  n1   and S2   S2 1        S2 n2   the possible worlds    of S1 and S2 contains all possible combinations of deterministic strings instantiated from S1 and S2  As before we let s denote any possible world from     s is a pair     1   p1       2   p2     where   1 is instantiated from S1 with probability p1 and   2 is instantiated from S2 with probability pP2  Define d s    d   1   2   w s    p1    p2  It is clear that s      w s    1  We also define  X Pr S1   S2    w s   s        1    2 Foranytwoprobabilisticq grams  1   i S1 i  i q   1  and   2   j S2 j  j q   1  fromGS1 andGS2 respectively thedif  ference in positions between   1 and   2 is denoted by off    1     2      i     j   Note that this is not a random variable  We define  Pr   1    2  Pr S1 i  i q   1  S2 j  j q   1    In particular  for    1   2      GdS1 S2  Pr   1     2    1 if S1 i  i   q     1    S2 j  j   q     1   0 otherwise  In this case  wesimplywrite  1    2 ifandonlyifPr   1    2  1  Example 2 Let S1 and S2 be the two probabilistic strings for records 1 and 2 in Figure 2  The first probabilistic q gram of S1 is   1    1       1    A  0 8    C  0 2     and the first probabilistic q gram of S2 is   2    1       1    A  1     Pr   1     2    0 8  off   1   2  0 and  2   GdS2 but  1    GdS1   4 1 Lower bounds on the EED of S1 and S2 b The first filter is to lower bound d S1   S2   by their length differ  ence immediately by Lemma 3  since the length of a probabilistic string is fixed in the character level model  Lemma 6 For any character level probabilistic strings S1  S2  db   S 1   S 2           S 1         S 2       4 1 1 A probabilistic q gram based lower bound The lower bound below is a generalization of Lemma 1 to the probabilistic setting  Theorem 3 For any character level probabilistic strings S1  S2  max  S1   S2     1   P      G 1 S1 b   2    GS2 d S1  S2      1   q Pr   1    2   12  PROOF  Let s       1  p1      2  p2   be any possible world in the possible worlds    for S1 and S2  Applying Lemma 1 to all s      wehave X w s  G  1    G  2  s      X For any two probabilistic q grams   1    i  S1 i  i   q     1       GS1    2    j S2 j  j   q     1       GS2  and a random possible world s       1  p1      2  p2    we define   1  s   2 as the event that S1 i  i   q     1    S2 j  j   q     1  in s  We have Pr   1  s   2     w s   if  1 i  i q   1    2 j  j q   1   0  otherwise        w s  max    1      2       1     q d s      1   b s      max  S1    S2       1     q d S1  S2      1   Recall that  G  1    G  2   is the number of matching pairs of q grams inG  1 andG  2 so Xw s  G  1    G  2  X X Pr   1  s   2  s      s       1   G Theorem 4 For any character level probabilistic strings S1   S2           db S1 S2   b max  S1    S2       1 d S1  S2     1   q S1   2   GS2 P            X XPr   1  s   2   X Pr   1    2          1    2     GdS1  S2    1    2 min 1  off    1    2     1   GS1 s        1   GS1   2    GS2   2    GS2 rearranging the inequality completes the proof  Incorporating the q gram   s position information in each possible world using Lemma 2 to derive a tighter lower bound for db S1   S2   is more involved  We can follow the proof of Theorem 3  but ap  plying Lemma 2 instead  obtaining Pq    1   2    GS  S    GdS  S Pr   1     2     1212   15  q In  15   P   1   2    GS  S    GdS  S Pr   1     2  is the sum of 12 12 theprobabilitiesthattwoprobabilisticq gramsfromGS1 andGS2 equal if any one of them contains at least one probabilistic char  acter  and d min 1       is the sum of             G         off          Xb P w s  G  1    d s G  2       max  S1    S2     1   q d S1  S2    1   s      1 2 S1 S2 1 2 1 2 either 1  for two q grams from GS1 and GS2 if they are identical and neither contains any probabilistic character and        off    1     2    or    divided by their position difference if      off    1     2    Clearly   13  The difficulty is that d s  changes for different worlds and it is exactly the value that we avoid to compute for each possible world in a pruning method  Next  we show how to obtain an upper bound on the LHS of  13  that is efficiently computable  Lbemma 7 For any character level probabilistic strings S1  S2            d S1  S2   XX w s  G  1    d s  G  2       Pr   1     2  s         1    2      GS1  S2    GdS1  S2   X       X min 1 off   1   2      1     1    2     GdS  S      1    2     GdS  S     1   21 2   1   21 2 Hence  if we let C   1   max  S1   S2     1   then  q X              1    Pr   1    2              Gd      1   2       min1     12 S1 S2 d   1    2 GS1  S2    GS1  S2 X Pr             X Pr          12 12    1    2     GdS1  S2    1    2      GS1  S2    GdS1  S2    1   2    Gd         off   1   2       S1 S2 PROOF  Let   1 and   2 be any probabilistic q gram from GS 1 and GS respectively  For a random s  by the Markov inequality  2       E d s          X Pr   1    2   RHSof 12    C   q     1    2     GS1  S2 which means that Theorem 4 always provides a tighter lower bound  4 1 2 A DP based lower bound The succinct representation of the probabilistic string in the char  acter level model makes it possible to extend the DP formulation for the deterministic edit distance to the probabilistic setting to derive lower and upper bounds  but unfortunately not the exact EED as argued in the introduction   The lower bound DP for  mulation for the EED is very similar to the classic  deterministic edit distance DP as shown in  1   For two probabilistic strings S1   S1 1    S1 n1  and S2   S2 1    S2 n2   we will com  pute lower bounds on the EED between S1 1  i  and S2 1  j  for i   1     n1 and j   1     n2 inductively  The base cases are  RHS of  15      C     q XX 1 2 Pr d s      off   1    2       min 1  off   1    2   min 1  db S1 S2     min   1          14  off   1    2  off   1    2  where we define 1       if off   1    2    0  Next  s      off   1   2  Xw s  G  1    d s  G  2  X X Pr   1  s   2 and off   1    2      d s      1   2    GS1 S2 s      X X Pr   1  s   2 and off   1    2      d s      1    2     Gd s              In the last step  the second term is relaxed by removing the position b S1 S2 XXb8b   Pr   1  s   2 and off   1    2      d s   ld i  j    i   j for i    j   0  For other i  j  we set    1    2      s      X d    l b d   i   j     1     1   GS1 S2    GS1 S2 b  b     X Pr   1  s   2   1  Pr S1 i    S2 j     0  1  off   1    2     1   2     GS1 S2    GdS1 S2   s      lc S1 i   S2 j     0  Pr S1 i    S2 j     0  min ld i j  min ld i   1 j  1   16   by  14   ld i     1  j     1    lc S1 i   S2 j               1    2     GdS1  S2    1    2   X where We will show that ld i j  is a lower bound on the EED between constraint  This completes the proof  Lemma 7 and  13  lead to a tighter lower bound for db S1   S2    stronger result  S1 1  i  and S2 1  j  for all i  j  In fact  we will prove the following Theorem 5 For any two character level probabilistic strings S1   S1 1    S1 n1  and S2   S2 1    S2 n2   PROOF  This is derived by utilizing  17  in each possible world s        and summing over all worlds  4 2 2 A DP based upper bound It is possible to obtain a very tight upper bound for the EED with a DP formulation on the probabilistic characters directly  For any two probabilistic strings S1   S1  1        S1  n1   and S2   bb ld n1  n2    min d s       d S1  S2   s      PROOF  Let ds i  j  be the deterministic edit distance DP   s out  put in the possible world s       1  p1      2  p2          for   1 1  i  b and   2 1  j   We first show that ld n1  n2      ds n1  n2  for    s        and we prove this by induction  b c The base cases ld i  j    ds i  j  for i    j   0 are trivial  As          c sume that this claim holds for    i     i     j     j where the two  ud i j    1  1  cc equalities do not hold at the same time  Since lc S1 i   S2 j       u d   i   j     m i n    u d   i     1   j     1     1 8   c   1 i     2 j   for any s and by the induction hypothesis  the RHS c b b bc ld n1 n2      mins      d s    i   j for i    j   0  We will show that ud i j  thus computed is Next  we can show that it is possible to construct a possible world an upper bound on the EED between S1 1  i  and S2 1  j   We first s such that d s    ld n1   n2    by just following the choices of the present two technical results  bb S2 1        S2 n2   we set ud i  j  as 8 ud i     1  j     1    Pr S1 i      S2 j    bc of  16  is always at most the RHS of  1   thus we have ld i  j      ds i  j   In particular ld n1  n2      ds n1  n2  for    s         so for i      1  n1   j      1  n2   with the boundary condition ud i  j    ld i  j  in  16   So ld n1  n2    mins      d s    4 2 Upper bounds on the EED of S1 and S2 Lemma 9 For two non negative random variables X  Y   E min X  Y        min E X   E Y     PROOF  ObservingthatE min X Y   E X Y    X   Y   this 2 is proved by following a similar argument as in Lemma 5  Corollary 1 For three non negative random variables X  Y   Z  E min X  Y  Z       min E X   E Y    E Z    The lower bounds on the EED are useful to prune away many pairs that have expected edit distances larger than the join thresh  old  However  for string pairs which have their EED smaller than the threshold  lower bounds are not useful  To further reduce the cost of a string join  we introduce two upper bounds in this section  These upper bounds are particularly useful in efficiently identify  ing many string pairs that do satisfy the join condition  which helps avoid incurring the high cost of invoking ed        on these pairs  4 2 1 A probabilistic q gram based upper bound For any two deterministic q grams q1    l1 g1  and q2   cb  l2  g2  from two strings   1 and   2 respectively  we define q1     q2 ifandonlyifg1   g2 andl1   l2 and q1     q2 returns1if q1     q2  0 otherwise  Lemma 8 For any two strings   1 and   2  d   1   2    max    1     2   q   1    X  q1    q2  q1   G  1  q2   G  2  17  PROOF  Without loss of generality  assume that    1         2   Clearly  G  1       1  q   1andthereare   1  q   1po  sitions  including the prefix and suffix characters         and          for possible edit operations to transform   1 into   2  Without touch  ing any position in which the corresponding q grams q1       1 and q2       2 satisfy q1     q2  we can always transform   1 into   2 by editing all other positions in the worst case  which is    1     q     1     Pq1   G  1 q2   G  2 q1    q2 editoperations Sinced   1   2 isthe minimum number of edit operations required to make   1     2  this immediately implies  17   This observation directly leads to an upper bound for the EED of two probabilistic strings based on their probabilistic q grams  For any two probabilistic q grams   1    i  S1 i  i   q     1   and   2    j S2 j  j   q     1   for two character level probabilistic strings S1 and S2  we define that Pr   1      2  Pr   1    2 ifi j 0otherwise  Theorem 6 For any character level probabilistic strings S1  S2  Theorem 7 ud i  j      d S1 1  i   S2 1  j   for all i  j  PROOF  Weprovethistheorembyinduction Thetheoremclearly holds when i    j   0 since ud i  j    i   j   d S1 1  i   S2 1  j    Assumethatthetheoremholdsfor   i       i j       jwherethetwo b d S1 1  i   S2 1  j   26 8   d s   i     1   j     1 37   E4min ds i j   1  1 5  ds i     1  j     1    c   1 i     2 j   8b     min d S1 1  i   S2 1  j     1     1  by Corollary 1  d S1 1  i     1   S2 1  j     1     Pr S1 i      S2 j       min ud i  j     1    1  by induction  ud i     1  j     1    Pr S1 i      S2 j   c   c      uc cd   i     1   j     1 cb equalities do not hold at the same time  Let s       1   p1       2   p2    be a random world from     the edit distance DP formulation  1  in the world s is  8   d s   i     1   j     1   d s   i   j     m i n    d s   i   j     1     1   ds i     1  j     1    c   1 i     2 j    Taking expectation on both sides over all possible worlds  we get    db S  1  i     1   S  1  j     1 8 1 2 db S1 S2      max  S1   S2   q   1    X    1    2     GS1  S2 Pr   1       2    ud i  j   The theorem is therefore proved  4 3 Query implementation The next query implements the character level string join  by in  corporating the lower bounds  Lemma 6  Theorem 4 and 5  and the upper bounds  Theorem 6 and 7   The first part in the relation L before the EXCEPT  lines 2 9  is the probabilistic q gram lower bound in Theorem 4 with the length pruning in Lemma 6  line 4   In particular  line 6 is the first two terms in the RHS of  15   lines 7  8 is the third term in the RHS of  15   since FLOOR Rq p Tq p  equals 0 for    1    2      GS1 S2     GdS1 S2   and 1 for    1    2      GdS1 S2   note that max 1 ABS Rq l Tq l   in line 8 handles the special case when ABS Rq l Tq l  0  line 9 is the fourth term in the RHS of  15   since CEILING 1 Rq p Tq p  equals 1for   1   2    GS1 S2    GdS1 S2 and0for   1   2    GdS1 S2  The second part in the relation L after the EXCEPT  lines 11 15  is simply the probabilistic q gram upper bound in Theorem 6 with the length pruning in Lemma 6  Essentially  L contains those pairs that cannot be pruned by the length  or the probabilistic q gram lower and upper bounds  Finally  the outer block query joins tables R  T with L  For each pair in L  it performs the lower bound and upper bound DPs first  UDF ld        and ud        respectively   followed by the naive calculation with ed        as a last resort if a pair cannot be pruned by ld        and ud         Similarly to Q3 and Q4  max a  b  and min a  b  might not be defined for two real values a  b  they could be simply replaced by  a   b    a     b   2 and  a   b      a     b   2 if necessary  1  SELECT R id T id FROM R T  DBLP within edit distance 3 to     Note that A     contains dupli  cated names  Then we create a character level probabilistic string S based on    as follows  We identify the non ASCII characters in     we randomly pick a few if there are no such characters   and for each such position i  the pdf of S i  is generated based on the normalized frequencies of the letters in the i th position of all the strings in A      The other positions of S are deterministic and the same as in     A string level probabilistic string is created by simply taking all the distinct strings in A     as the possible choices  with the probabilities being proportional to their frequencies  To mimic the case where the same name may have drastically different forms  we also replace a few choices with randomly picked names outside A      We denote these datasets as Author1  string level  and Au  thor2  character level  accordingly  Note that Author1 and Author2 represent completely different information  The second dataset  Category  is from the Wikipedia database  using the category link string field in the category link table  Many category links refer to the same subject with different string values  which are best represented by the string level model  We generate its string level representation in the same way as for Author1  The third dataset  Genome  is based on the genome sequence database from the Rhodococcus project  Genome sequences may contain unknown positions  and they are best represented by the character level model  We broke a very long genome sequence into many shorter strings of controlled length  then generated character  level probabilistic strings in the same way as for the Author2 dataset  Finally  we create two tables R and T with a certain number of probabilistic strings  in either model   Each probabilistic string is created based on a random    from the corresponding data source  Setup  In the string level model  an interesting factor is the num  ber of choices  size S   that each probabilistic string S may have  Since many strings from our datasets have a large number of choices  we specify the maximum size S   denoted as C  If S has more than C choices  we only take its C most probable choices and normalize them  Similarly  in the character level model  we limit the number of choices that a probabilistic character S i  may have to     We also control the fraction of uncertain positions in a character level probabilistic string      as we generate the datasets  In both models  the string length obviously affects the join cost  In the string level model  we use   s to denote the average length of the deterministic strings from all choices of all probabilistic strings  To study its effect  we append each deterministic string to itself for      0  1  2        times  In the character level model  we use   c to denote the average length of all probabilistic strings  and to study its effect  we append each probabilistic string to itself    times  When      0  no self concatenation     s   13 6 and   c   14 3 for Author1 and Author2  and the string length distributions in both models follow approximately a normal distribution in the range of  5  36   for the Category dataset  string level model     s   19 9 and its string length distribution is  very roughly  a heavy tail dis  tribution in the range of  3  50   for the Genome dataset  character  levelmodel    c  14 9anditsstringlengthdistributionisroughly a uniform distribution in the range of  10  20   Since the basic methods are very expensive  we limit the sizes of the tables  The default sizes for R and T are  R    1  000 and  T    10  000  We study the scalability of all methods by varying the size of T  The default values for other parameters are  q   2     2 C 6    6    20  and   0  In what follows  we refer to the basic methods for string joins in the string level and character level models   Q2  and  Q5   as S BJ and C BJ  respectively  Our pruning based join methods are denoted as S PJ  Q4  and C PJ  Q6   respectively  In both 2 3 4 5 6 7 8 9 10  EXCEPT 11 SELECT R id AS rid T id AS tid FROM R T Rq Tq 12 WHERE Rq  g Tq  g AND Rq  l Tq  l AND R id Rq  id 13 AND T id Tq id AND ABS R len   T len         14 GROUP BY R id  T id  R len  T len 15 HAVING max R len T len  q 1 SUM Tq p Rq p        16   AS L 17 WHERE L rid R id AND L tid T id AND ld R A T A  18        AND ud R A T A      AND ed R A T A         Q6  Lastly  to be complete  results from Q6 need to be unioned with those pruned away by either the probabilistic q gram  lines 11 15  or the DP  ud  upper bound  It has been omitted in  Q6  for sim  plicity  and we use  Q6  to refer to this complete SQL query  5  EXPERIMENTS We implemented all methods in Microsoft SQL Server 2008  the Enterprise edition   All experiments were executed on a machine with an Intel Xeon E5405 2 00GHz CPU running Windows XP  We have 2GB memory allocated to SQL Server  Datasets  We created 4 datasets  2 for each model  using 3 real data sources  The first data source is the author names in DBLP  There are two types of ambiguities uncertainties in people   s names  The first is due to spelling errors or conversion between different char  acter sets  e g   the letter    a      is often converted to    a    in ASCII   and the second is due to the fact that a name may have different forms  The former can be represented by the character level model  while the latter requires the string level model  So we created two datasets  one for each model  as follows  For a name    in the DBLP database  we find its similar set A     that consists of all names in  SELECT R id AS rid T id AS tid FROM R T Rq Tq WHERE Rq  g Tq  g AND R id Rq  id AND T id Tq  id AND ABS R len   T len         GROUP BY R id  T id  R len  T len HAVING 1    max R len T len  1  q   SUM  FLOOR Tq p Rq p   min 1     max 1 ABS Rq l   Tq l      q   SUM  CEILING 1 Rq p Tq p  Tq p Rq p   q        104 104 3 104 5 10 104 103 10 102 103 103 1 102 S BJ 10 S BJ S BJ S PJ S PJ S BJ S PJ S PJ2 3 4 S PJ 102 S PJ2 13 6 27 2 40 8 54 4 average string length    s  d  vary avg string length    s  6 10 100 S PJ2 1 S PJ2 2 135710 15 20 102 4 6 8 10 101 2 size of table T    1000    T   a  vary the size ofT   T   max size S    C EED threshold      c  vary      b  vary max size S    C  Figure 3  String level model  running time analysis  Author1 dataset  105 105 105 4 10 5 10 4 10 103 S BJ S PJ 102 S PJ2 19 9 39 8 59 7 79 6 average string length    s  d  vary avg string length    s   T  increases  and they all incur higher costs in the Category dataset due to the longer strings it contains  In particular  on both datasets  S PJ2 is the best method and it is at least 1 order of magnitude faster than S BJ  S PJ is about 5 to 10 times better than S BJ  Effects of the maximum number of choices C  We next vary the maximum number of choices a probabilistic string may have  using C   max size S   from 2 to 10  The results from the Author1 and Category datasets are shown in Figures 3 b  and 4 b   All al  gorithms become more costly when C increases  However  on both datasets  we observe that S PJ2 and S PJ maintain their superior performance over S BJ in all cases  They are more than 10 times or approximately 5 times better than the basic method  respectively  Effects of the threshold      Figures 3 c  and 4 c  show the results on the Author1 and Category datasets when    changes from 1 to 4  Clearly  the cost of the basic method S BJ stays as a constant  whereas the costs of S PJ2 and S PJ increase when    becomes larger  Naturally  larger    values weakens the pruning power of our pruning techniques  Nevertheless  even in the worst case when      4  S PJ and S PJ2 are still 3 or 6 times more efficient than S BJ on both datasets  Effects of the string length  Our last experiment demonstrates the impact of the string lengths  The results from the Author1 and Category datasets  when    changes from 0 to 3  are shown in Fig  ures 3 d  and 4 d   Clearly  all algorithms require more time to fin  ish  For the S BJ method  the main driving force is the higher cost for each edit distance DP in every possible world  For the S PJ2 and S PJ methods  an additional factor is that they have to process more probabilistic q grams when string lengths increase  However  longer strings also imply a higher pruning power when    is fixed  which means that fewer strings will join  Hence  the performance gap between S BJ and the pruning based join methods actually has enlarged  For example  when   s   54 4 on the Author1 dataset  S PJ2  S PJ  becomes more than 30  10  times faster than S BJ  5 2 The character level model We performed string joins on our two character level datasets Author2 and Genome  A notable difference of these two datasets is their alphabet size  Author2 has an alphabet size of more than 52 while Genome only has 4 letters     A        T        G        C      103 2 10 104 104 33 10 10 10 S PJ S BJ 1 S BJ S BJ S PJ S PJ2 3 4 S PJ 100 S PJ2 2 S PJ2 2 135710 15 20 102 4 6 8 10 101 size of table T    1000    T  max size S    C 2 EED threshold      c  vary      a  vary the size ofT   T    b  vary max size S    C  Figure 4  String level model  running time analysis  Category dataset  models  we built clustered indices on the id colu</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pu1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pu1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_and_uncertain_databases"/>
        <doc>Ranking with Uncertain Scoring Functions  Semantics and Sensitivity Measures ### Mohamed A  Soliman   Greenplum San Mateo  USA mohamed soliman emc com Ihab F  Ilyas University of Waterloo Waterloo  Canada ilyas uwaterloo ca Davide Martinenghi Politecnico di Milano Milano  Italy davide martinenghi polimi it Marco Tagliasacchi Politecnico di Milano Milano  Italy marco tagliasacchi polimi it ABSTRACT Ranking queries report the top K results according to a user de ned scoring function  A widely used scoring func  tion is the weighted summation of multiple scores  Often times  users cannot precisely specify the weights in such functions in order to produce the preferred order of results  Adopting uncertain incomplete scoring functions  e g   us  ing weight ranges and partially speci ed weight preferences  can better capture user s preferences in this scenario  In this paper  we study two aspects in uncertain scor  ing functions  The  rst aspect is the semantics of ranking queries  and the second aspect is the sensitivity of computed results to re nements made by the user  We formalize and solve multiple problems under both aspects  and present novel techniques that compute query results e ciently to comply with the interactive nature of these problems  Categories and Subject Descriptors H 2 4  Database Management   Systems General Terms Algorithms  Design  Experimentation  Performance Keywords Uncertainty  Scoring  Top k  Ranking  Aggregation ### 1  INTRODUCTION Scoring  ranking  functions are among the most common forms of preference speci cation  A prominent application  Work has been done while the author was with University of Waterloo  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  SELECT R RestName  R Street  H HotelName FROM RestaurantsInParis R  HotelsInParis H WHERE distance R coordinates  H coordinates    500m RANK BY wR  R Rating   wH  H Stars LIMIT 5 Figure 1  A rank join query ID rating stars   1 2   6     2 7   5     3 4   7     4 5   2   Rank By  wR  rating wH  stars wR  wH  1 wR 0 0 167 0 4 0 571 0 833 1 0 Join Results    1      2      3      4      5     3     1   2   4   3     2   1   4   2     3   1   4   2     3   4   1   2     4   3   1 Figure 2  Possible orderings of the join results scenario is joining multiple data sources and ranking join results according to some score aggregation function  The class of queries captured by this scenario is usually referred to as rank join  8   where the objective is to compute the top K join results based on a given scoring function  The order of rank join results depends on the chosen score aggregation function  In the simplest but very common case  a linear aggregation function is adopted  which is speci ed as a weighted sum of scores  For example  Figure 1 shows a rank join query  where Restaurant Hotel join results are ranked based on a weighted sum of the rating and the num  ber of stars  while reporting only the top 5 join results  1 1 Motivation and Challenges Often times users cannot precisely specify the weights of the scoring function  e g   wR and wH in Figure 1  in order to produce the preferred order of results  This problem is usually handled either by the user in an interactive trial  and error manner  or by the machine through learning from user s feedback  e g   learning weights from user s prefer  ence judgment on object pairs  16    Both approaches have serious limitations  Trial and error is a tedious and a time  consuming process that can be very challenging especially to novice users  On the other hand  weight learning requires asu ciently large number of user provided training examples  which can be too demanding  In many scenarios  adopting an uncertain incomplete preference speci cation language can better capture user s preferences  For example  it is much easier for users to pro  vide approximate weights  e g   ranges  and other hints  e g   relative importance of the weights  than specifying exact weights  This does not change the goal  which is obtaining a representative ordering of results satisfying user s prefer  ences  but rather provides a  exible and more natural means of preference speci cation  Even when a user provides what are believed to be the right weights  it is crucial to analyze the sensitivity of the computed ordering with respect to changes in the given weights  This can provide signi cant help in data analy  sis in interactive and exploratory scenarios  For example  when  ne tuning the scoring function  a user may be inter  ested in quantifying the largest change in the weights that does not introduce perturbations in the computed ordering  We discuss the previous observations using Figure 2  which shows 4 join results for the query in Figure 1  To illustrate the consequences of uncertain weights  as  sume that the non negative weights wR and wH are uncer  tain and normalized to add up to 1  We adopt the convention that results are sorted in descending order of scores  There are 5 possible orderings   1             5 corresponding to all the possible weight vectors  we only show wR as wH   1  wR   While wR is continuous in  0 1   only a  nite number of pos  sible orderings exists  These orderings depend on the ranges of wR and wH as well as the scores of the join results  Each ordering has properties related to how robust the ordering is wrt  the weight space  and its relationship to other orderings  we give more details in Section 2 2   To illustrate the importance of quantifying the sensitivity of a speci c ordering or weight vector  assume that a user speci es  wR  wH  as  0 16  0 84   The corresponding top 2 results are thus h 3   1i  By changing the weights slightly to  0 17  0 83   i e   wR increased by only 6    the top 2 results become h 3   2i  Such sensitivity of the computed ordering to small weight changes may be important to quantify in interactive data analysis  We advocate the need to model and manage uncertainty in scoring functions  We envision uncertain scoring functions as a means to bridge the gap between imprecise speci ca  tions  which are more natural in describing user s prefer  ences  and score based ranking models  which are based on precise formulations of scoring functions  We identify two important problems pertinent to these settings    Finding a Representative Ordering  When weights are uncertain  a set of possible orderings is induced  The problem is  nding a representative ordering     with plausible properties that can be given to the user as an answer  Moreover  when relative preferences among weights are speci ed  e g   in Figure 2  the in uence of rating can be further emphasized by adding the con  straint wR   wH    nding a representative ordering satisfying such preferences is imperative    Quantifying Sensitivity  When weights are given as input  a corresponding ordering   can be computed  Two sensitivity analysis problems arise  The  rst prob  lem is quantifying the largest weights change  in the neighborhood of the input weight vector  that does not introduce perturbations in    The second problem is quantifying the likelihood of obtaining an ordering identical to    given a random weight vector  1 2 Contributions and Paper Organization Our key contributions are summarized as follows    We formulate four novel problems in the context of un  certain scoring functions  We propose multiple query semantics to allow users to reason about the reported answers  Section 2     We propose two approaches to compactly represent the set of possible orderings induced by an uncertain scor  ing function  We give a bound on the number of possi  ble orderings  and present several e cient techniques to compute representative orderings under di erent se  mantics  Section 3     We introduce a generalization of our methods to han  dle partially speci ed preferences on the scoring func  tion weights  Section 4     We present e cient techniques for quantifying the sen  sitivity of computed results with respect to changes in the input weights  Section 5   We also conduct an extensive experimental study  Sec  tion 6  on real and synthetic data to evaluate the e ective  ness and scalability of our solution  2  UNCERTAINTY MODEL AND PROBLEM DEFINITION Let R  represent the set of non negative real numbers  Consider a set of d relations R1          Rd  where each tuple ti 2 Ri includes a score s ti  2 R   Let   be an element of the form t1 1       1 td taken from the join O   R1 1       1 Rd of the d relations    is referred to as a join result 1   Let N denote the number of join results that can be formed  i e   N   jOj  The aggregate score F     of   is de ned as F       w1s t1            wds td    w T s      1  where w    w1          wd  T is a vector in R d    wi is the weight assigned to the score obtained from relation Ri  and s        s t1           s td   T is the score vector of join result     Given a weight vector w  it is possible to  nd a total order   of join results in O based on the aggregate score in  1   score ties are typically resolved using a deterministic tie breaker such as tuple IDs   We denote such operation as O w      Note that the order does not change if the weight vector is multiplied by a positive constant  Therefore  without loss of generality  we assume that Pd i 1 wi   1  2 1 Uncertainty Model To model weights uncertainty  let w be a random vari  able with probability density function p w  de ned over the standard  d  1  simplex  d1 given by  d1   fw 2 R d  j Pd i 1 wi   1g  Hence  Z  d1 p w dw   1  2  We assume that p w  is uniform over  d1   Each point on the simplex represents a possible scoring function  1 Our model is not limited to one score per input relation  We e ectively deal with the output  join  relation which may include any possible number of scores       Let  N denote a possible ordering of the N join results in O  where  N     indicates the position of   in  N  The uncertainty in the weights induces a probability distribution on a set of possible orderings  N  where each  N 2  N occurs with probability p  N   computed as follows  p  N    Z w2 d1  O  w  N p w dw  3  When the number of join results N is large  we might be interested only in the orderings of K   N join results  We denote with  K the set of possible top K answers  Note that each element of  K is a pre x of one or more orderings in  N  Whenever the ordering length is clear from the context  we drop the subscript and write    2 2 Problem De   nition Among the multiple possible ways to construct an order  ing from a set of possible orderings  we propose two problem de nitions  Problems 2 1 and 2 2  capturing the semantics of representative orderings  Problem 2 1   MPO  Given a depth K   nd the most probable ordering in  K  de ned as     MPO   arg  max  2 K p     2 The ordering     MPO is the distribution mode of  K  i e   the ordering that is most likely to be induced by a random weight vector   In Figure 2  for K   2  we have     MPO   h 2   3i  since it corresponds to the largest range of weights  The next problem de nition is based on measuring dis  tance between orderings  The most common among such measures assume orderings with exactly the same elements  and thus cannot be applied to pre xes of orderings  Problem 2 2   ORA  Given a distance function D   nd the optimal rank aggregation of  N  de ned as     ORA   arg  min   X  r2 N D     r     p   r    2 We adopt two widely used de nitions of the distance func  tion D    The Kendall tau distance  which counts the number of pairwise disagreements in the relative order of items in the two orderings  D   r     s     jf  i   j   2 O O     r   i      r   j      s   i      s   j  gj  4    The Spearman s footrule distance  which adds up the distance between the ranks of the same item in the two orderings  D   r     s     X  2O j  r        s    j  5  The ordering     ORA is the ordering with the minimum dis  tance summation to all orderings in  N  In Figure 2  we have     ORA     3 for either Kendall tau or Spearman s footrule distance  We next propose formulations of two sensitivity measures  stability of an ordering wrt  weights  Problem 2 3   and ordering likelihood  Problem 2 4   Problem d   2 d   3 d   3 MPO  average case  O N logN  K 1   O N logN  2K 1   O N bd 2c 1  logN   d1 K    x  MPO  worst case  O N 2 logN  O N 4   O N 2 d1    x  ORA  Kendall tau  O NlogN  NP Hard NP Hard ORA  Footrule  O N 2 5   O N 4   O N 2 d1    x  STB O N  O N  O dN  LIK O N  O N 2   O N 2 d2    x   x  Approximate solution  Figure 3  Solutions complexity Problem 2 3   STB  Given a depth K and a weight vec  tor w    where O w          nd the stability score of w    de ned as the radius  K w    of the maximal hypersphere  K w    cen  tered at w    such that for all w 2  K w     where O w      we have  K      K  2 In Problem STB  we compute the largest volume in the weights space  around an input weight vector w    in which changing the weights leaves the computed ordering unaltered at least up to depth K  In Figure 2  for w     0 2  0 8  and K   2  we have        2   The weight vector  0 167  0 833  is the furthest vector from w  that induces an ordering identical to    up to depth 2  Hence   2 w    is a circle centered at w  with  2 w      k 0 2  0 8    0 167  0 833 k   0 047  Problem 2 4   LIK  Given a depth K and a weight vec  tor w    where O w       N   nd the likelihood of    N up to depth K  de ned as  K    N    X  2 N  K    K p     2 In Problem LIK  we compute the probability of obtaining an ordering identical to    N up to depth K  In Figure 2  for w     0 5  0 5   we have    N     3   For K   2  we have  2   3     p   3     p   4    since   3 and   4 are identical up to depth 2  Figure 3 gives the complexity bounds of our proposed techniques  Our problem instances are con gured by three main parameters  d  N  and K  in uencing the complexity  We give worst case complexity bounds for each algorithm  In addition  for Problem MPO  we also give average case bounds under the assumption of uniformly distributed score vectors  As we show in the next sections   nding ordering probability requires computing a volume in a d dimensional space  For tractability  such volume can only be approxi  mated when d   3  3  REPRESENTATIVE ORDERINGS One possible approach to compute representative order  ings is to i  enumerate possible weight vectors  ii   nd the distinct orderings induced by these vectors  and iii  pick the required representative orderings  In addition to being very expensive  such approach can be also inaccurate since it needs to discretize the weights space  Problem MPO requires processing orderings  pre xes  while Problem ORA requires processing full orderings  Mo  tivated by this observation  we introduce two approaches    A Holistic Approach  We propose a succinct repre  sentation of full orderings as disjoint partitions of the weights space    An Incremental Approach  We propose a tree based representation that is incrementally constructed by ex  tending pre xes of orderings        In general  both approaches can be used for both prob  lems  However  based on our complexity analysis and ex  periments  we note the superiority of the holistic approach for computing ORA  and the superiority of the incremen  tal approach for computing MPO  Moreover  for the case of d   2  the holistic approach gives rise to useful properties that allow exact and e cient algorithms for both problems  3 1 A Holistic Approach Our holistic approach leverages the linearity of F to  nd  for each pair of join results   i   j    a hyper plane that divides the weights space into two partitions  such that F  i    F  j   in one partition  while F  i    F  j   in the other partition  By  nding all such hyper planes  we parti  tion the space into disjoint convex polyhedra corresponding to distinct orderings in  N  In Section 3 1 1  we describe our approach  while in Sections 3 1 2 and 3 1 3  we discuss e cient computation of the MPO and the ORA problems  3 1 1 Possible Orderings Representation We illustrate our approach for d   2  and then we show how it can be extended to address higher dimensions  Let si n be the n th entry of the score vector s  i   where 1   n   d  For d   2  let  i and  j be two distinct join results with score vectors s  i     si 1  si 2  T and s  j      sj 1  sj 2  T   respectively  Let  i j   p F  i    F  j     i e    i j is the probability of having  i ranked before  j    Let  i j n    si n  sj n   Then   i j can be expressed as follow   i j   p w1 i j 1   w2 i j 2   0   6  Due to linearity of F      we note that when s  i  domi  nates s  j    i e    si 1   sj 1 and si 2   sj 2  or  si 1   sj 1 and si 2   sj 2    we have  i j   1  That is  F  i    F  j   regardless of the value of the weight vector w  We thus only consider in the following the case where s  i  and s  j   are incomparable  i e    si 1   sj 1 and si 2   sj 2  or  si 1   sj 1 and si 2   sj 2    Lemma 3 1 formulates  i j in this case  Lemma 3 1  Let Ci j    j i 1  i j 2  Then   i j   p w2   Ci j 1 Ci j    or  equivalently   i j   p w1   1 1 Ci j    Proof  Since w1   w2   1  then we have  i j   p w2  i j 2   i j 1     i j 1   Without loss of generality  we assume that   i j 2   i j 1    0  and hence dividing by   i j 2   i j 1  does not change the inequality direction  note that if   i j 2   i j 1    0  we can always switch which join result we consider  i and which we consider  j    Then   i j   p w2    i j 1   i j 2   i j 1    Let Ci j    i j 1  i j 2  then  by rewriting  we get  i j   p w2   Ci j 1 Ci j    Based on Lemma 3 1  a geometrical representation for  i j for d   2 is given by Figure 4 a   which illustrates that  i j corresponds to the partition of  1 above the horizontal line w2   Ci j  1   Ci j   y   while  j i   1   i j corresponds to the partition of  1 below   i   j    For example  in Fig  ure 5 a   we have C1 4    s4 1  s1 1   s1 2  s4 2    3 4  and hence C1 4  1 C1 4    3 7 and  1 4   4 7  yWe only need to consider  i j 2  0  1   which implies that Ci j    sj 1  si 1   si 2  sj 2    0  and hence Ci j 1 Ci j 2  0  1   w2 w1 w2 w1 w3 1 0 1 0 1 0 1 0 1 0   i j Ci j  1 Ci j   Bi j Ai j   i j 1 Simplex    1   2 Simplex    2    a   b  Ci j         where   i j n   s i n      s j n i j 2   j i 1 Bi j      i j 3    i j 3        i j 2 Ai j      i j 3    i j 3        i j 1 Figure 4  Geometrical representation of  i j   p F  i    F  j    for  a  d   2  b  d   3 The horizontal line w2   Ci j  1 Ci j   is denoted as the switching line of   i   j    since it separates weights induc  ing F  i    F  j   from weights inducing F  j     F  i   When  i j   1  the switching line of   i   j   passes through the point  0  0   e g   the switching line of   2   4  in Fig  ure 5 a    We show how to extend Lemma 3 1 to handle higher di  mensions  By a similar analysis  we conclude that for d   3  the value of  i j  the shaded area in Figure 4 b   is the par  tition of  2 in front of the switching plane w1  i j 1 i j 3   w2  i j 2   i j 3     i j 3   0  In general  the switching hyper plane of   i   j   in a d  dimensional space is given by w1  i j 1   i j d    w2  i j 2   i j d         wd1  i j d1  i j d    i j d   0  Constructing all switching hyper planes  for all pairs of join results  divides the weights space into a set of disjoint convex polyhedra  where each minimal convex polyhedron  i e   a polyhedron that is not enclosed in another polyhedron  corresponds to a distinct ordering in  N  The reason is that the relative order of any two join results is  xed over all weight vectors inside a minimal polyhedron  while crossing a polyhedron s boundaries changes the relative order of the join results  since these boundaries are parts of the switching planes  Based on the constraint Pd i 1 wi   1  we can reduce the dimensionality of the simplex by eliminating one of the weight variables  This e ectively projects the simplex onto a plane with dimensionality smaller by one  For example  for d   2  Figure 5 a  shows the projection of  1 on the w2 axis  There are 5 possible orderings of the shown 4 join results  where each partition of the w2 axis enclosed be  tween two consecutive switching planes  indicated by dot  ted lines  induces a distinct ordering  Crossing a switching plane changes the relative order of  at least  one pair of join results  and hence a new ordering is induced  Figure 5 b  shows another example for d   3  where we project  2 onto the w1  w2 plane  and partition the simplex projection us  ing 2 dimensional switching planes  inducing 7 possible or  derings corresponding to 7 minimal convex polygons  Representing  N using switching hyper planes allows us to derive a bound on the number of possible orderings in  N  as we formalize in Theorem 3 2  Theorem 3 2  The number of possible orderings in  N is in O N 2 d1    Proof  Given a set of m points in a d dim space  the number of minimal convex polyhedra that can be created                            3    1    2    4              3    2    1    4              2    3    1    4              2    4    3    1              2    3    4    1         w2 1 1 6 3 7 3 5 5 6    3    1        2    4        3    4        1    4        3    2        1    2     ID   s1 s2   1 2   6     2 7   5     3 4   7     4 5   2   w2 1 w1 1 ID   s1 s2 s3   1 8   8   6     2 7   5   3     3 9   2   4     4 1   6   5    67  25      3    1    2    4          1    4    2    3          a   b    86  14     6  4   0  0 0   Join  Results   Join  Results    11  25    2  4             1    3    4    2              1    2    3    4           33  67        1    3    2    4              1    2    4    3           14  86        1    4    3    2           21  3   Figure 5  Switching Planes for  a  d   2  b  d   3 is in O m   Hence  a bound on j Nj is the same as the bound on the number of points created by the intersec  tions of O N 2   hyper planes  corresponding to pairs of join results  with each other and the simplex  The claim fol  lows from the fact that O N 2    d1  dim planes determine O N 2 d1   intersection points  which we show by induction  Base case  d   2    1 can be projected onto the 1 dim w2 plane  We have O N 2   switching lines  where each line generates only one intersection point with the projection of  1   leading to O N 2   intersection points  Inductive step  d   x   1    x can be projected onto a x dim plane  We have O N 2   switching x dim planes  Each two x dim planes may intersect at an  x  1  dim plane  thus determining O  N 2   2    x1  dim planes  By inductive hypothesis  O Y 2    x  1  dim planes determine O Y 2 x1   intersection points  so  by letting Y   N 2   the number of possible intersection points is O N 2 x    3 1 2 Computing MPO We  rst consider the case of d   2  We start with an ini  tial ordering  N corresponding to weights  1 0  0   and we scan the set of switching planes in ascending order of w2  We merge consecutive partitions of  1 that agree on the ordering of the top K join results  i e   we merge any two consecutive partitions if their boundary switching plane does not change the rank of any join result in the top K   We update ranks in  N at each switching plane  Eventually  the largest merged partition gives     MPO  The complexity of the algorithm is dominated by sorting O N 2   switching planes  and hence the overall complexity is O N 2 logN   We discuss in Section 3 3 an important optimization to signi cantly re  duce N in the previous complexity bound by pruning all K dominated join results  For example  in Figure 5 a   for K   2 we have     MPO   h 2   3i  since it corresponds to the partition  1 6  3 5   which is the largest partition inducing the same top 2 join re  sults  Similarly  for K   N we have     MPO   h 2   3   4   1i  For d   2  sorting the switching planes is not possible since  in contrast to the case of d   2  the planes are not 1  dimensional anymore  Computing     MPO in this case is done by progressively partitioning the space using the switching planes  while maintaining the polyhedra corresponding to di erent top K answers  Thus      MPO is given by the poly  hedron with the largest volume  we elaborate on volume computation in Section 3 2   Based on Theorem 3 2  the previous algorithm has complexity O N 2 d1     3 1 3 Computing ORA We  rst discuss computing     ORA for d   2  and then present a generalization for d   2 at the end of this section  ORA under Kendall tau Distance  d   2   We start by de ning the property of Weak Stochastic Transitivity  15  in our settings  Definition 3 3   Weak Stochastic Transitivity  F     is weak stochastic transitive i  8  i   j    k 2 O    i j   0 5 and  j k   0 5     i k   0 5  2 We show in Lemma 3 4 that weak stochastic transitivity always holds for d   2  Lemma 3 4  F     is weak stochastic transitive for d   2  Proof  Let  i   j    k be three join results  where  i j   0 5 and  j k   0 5  We show that we must also have  i k   0 5  Based on Lemma 3 1  we have  i j   p w2   Ci j 1 Ci j     0 5  As shown in Figure 4 a   the previous inequality holds if and only if Ci j 1 Ci j   0 5  or equivalently  Ci j   0 5 0 5Ci j   which yields Ci j   1 0  By expanding Ci j   we get the following expression  si 2  sj 2   sj 1  si 1  7  It is easy to show that the opposite also holds  That is   7  is a necessary and su cient condition for  i j   0 5  Speci cally  given si 2  sj 2   sj 1  si 1  we get Ci j   1 0  which yields Ci j 1 Ci j   0 5  and hence  i j   0 5  Similar to  7   using the given  j k   0 5  we get the following expression  sj 2  sk 2   sk 1  sj 1  8  Adding up  7  and  8  gives si 2 sk 2   sk 1 si 1  which implies that  i k   0 5  and hence weak stochastic transi  tivity holds  As was argued in the proof of Lemma 3 4   9  is a neces  sary and su cient condition for  i j   0 5    i j   0 5     si 1   si 2   sj 1   sj 2   9  That is  if  i precedes  j   in the order of scores  summa  tion  it is guaranteed that  i j   0 5  Assume that we would like to compute the total Kendall tau distance between an ordering    and all orderings in  N  Based on Problem ORA  an ordering   2  N con  tributes to the aggregated distance between    and  N  with a value of p     for each pair of join results with disagree  ing relative orders in    and    Let   i j N    N be the set of possible orderings where  i is ranked before  j   Then   i j   P  2  i j N p     and it follows that the objective of Problem ORA under Kendall tau distance can be restated as  nding an ordering that minimizes the total disagreements given by the following penalty function                pen        X   i  j       j       i   i j  10  Construction Algorithm  Assume an ordering    con  structed as follows  For two distinct join results  i and  j   let      i         j   if  i j   0 5 while breaking probability ties deterministically  Based on weak stochastic transitivity  such construction procedure does not introduce cycles in      Moreover  we show that such construction procedure indeed minimizes pen     Our result is the same as Theorem 3 in  13   and we include it here for completeness  z Theorem 3 5  If weak stochastic transitivity holds  then an ordering      with      i         j   if  i j   0 5  is     ORA under Kendall tau distance  2 It follows that     ORA is given by sorting join results using   i j   0 5  as the sort comparator  Based on  9   the sorting is achieved by ordering join results on  s  1   s  2   The total complexity is O NlogN   which is the complexity of conducting a regular sort  ORA under Footrule Distance  d   2   Given an or  derings space  N   nding     ORA under footrule distance is studied in  4   where a weighted bipartite graph G is used to connect each item   with each rank r using an edge weighted as P  2 N j       rj  or equivalently  as shown in  13    as PN i 1 p  i   ji  rj  where p  i is the probability summation of orderings in  N with   at rank i  Then      ORA is given by the minimum cost perfect matching of G  which can be found in O N 2 5    4   We show how to construct the graph G using our repre  sentation of  N  We maintain for each   a set fp  ig of N variables such that each variable p  i represents the current probability of having   at rank i  Starting from the initial ordering  N corresponding to weights  1 0  0   we scan the set of sorted switching planes in ascending order of w2  while updating ranks in  N at each switching plane  Whenever we access a partition of  1 with   at rank i  we add the length of that partition to the current p  i value  The  nal fp  ig values give the weights of edges incident to   in G  For exam  ple in Figure 5 a   for  3 we have p 3 1   0 4  p 3 2   0 433  p 3 3   0 167  and p 3 4   0  Since the number of switch  ing planes is in O N 2    the complexity of constructing G is O N 2 logN   and hence the overall complexity of  nding     ORA is dominated by the cost of perfect matching of G  which is O N 2 5   based on  4   Computing ORA in Higher Dimensions  Finding     ORA under Kendall tau distance is in general NP Hard  4   For d   2  weak stochastic transitivity allows us to give an O NlogN  algorithm  However  weak stochastic transitiv  ity does not hold for d   2 y   and hence      ORA can only be approximated in polynomial time using  for example  the Markov chain sampling method given in  4   We leave out the details in the interest of space  z The scoring model of  13  is di erent from ours  We prove that weak stochastic transitivity also holds under our scoring model  and hence     ORA can be computed e ciently  y A counterexample  For s  1     42  0  0   s  2     0  40  0   s  3     0  28  10   we have  1 2    51   2 3    54   1 3    48  ID   s1 s2 s3   1 0 1113   0 7752   0 4085     2 0 0676   0 0075   0 5034     3 0 7606   0 5537   0 0290     4 0 3112   0 2473   0 2962     5 0 8173   0 4005   0 9394     6 0 1549   0 3520   0 0416     7 0 4312   0 2521   0 2906     8 0 2388   0 1047   0 3881     1   0 135     3   0 043     5   0 822     3   0 062     5   0 073     1   0 008     5   0 035     1   0 334     2   0 057     3   0 421     7   0 010   Figure 6  Possible orderings tree  Computing     ORA under footrule distance for d   2 can be done by computing the minimum cost perfect matching of the graph G  as discussed for the case of d   2  However  the di erence is that for d   2  the cost of constructing G dominates the cost of solving the matching problem  which is O N 2 5     since G is given by materializing the orderings space  Based on Theorem 3 2  the overall complexity of computing     ORA is thus O N 2 d1    It is also known that     ORA under footrule distance is a 2 approximation of     ORA under Kendall tau distance  4   3 2 An Incremental Approach We represent possible orderings as a tree  where each node at depth k encodes a possible ordering pre x of size k  given by the path from the tree root to that node  A probability value is assigned to each node  We describe the details of tree construction in Section 3 2 1  while we discuss computing MPO in Section 3 2 2  3 2 1 Possible Orderings Representation For the sake of clarity  consider the problem of deter  mining the possible top 1 answers and the 3 dimensional score vectors space depicted in Figure 6  where N   8  By linearity of the aggregation function  we base our method on computing the convex hull of the score vectors s  i   i   1          N  The term  convex hull  usually refers to the boundary of the minimal convex set containing a set of vec  tors  In the following  when referring to the convex hull of the score vectors  we shall only retain those score vectors belonging to facets whose normal is directed towards the  rst orthant  This constraint stems from the fact that the weights are nonnegative  i e   wi   0  i   1          d  Intuitively  out of the  N 1   1    N join results  i e   sets of cardinality equal to 1   only the M1 join results whose score vector belongs to the convex hull can be selected as top 1 answers  i e     1 1   h 1i    2 1   h 3i and   3 1   h 5i  These join results are shown as nodes at depth k   1 in the tree de  picted in Figure 6  All the others are dominated by at least one score vector on the convex hull  regardless of the speci c linear aggregation function  This observation has been pre  viously made in  2   where onion indexes were designed with the goal of e ciently pre computing the answers of top K queries  Note that  for uniformly distributed score vectors in  0  1  3   the number M1 of score vectors on the convex hull increases asymptotically according to O  log N  2    5   thus it is typically much smaller than N  In order to compute p   r 1  we are interested in partition  ing  2 in M1 polygons     r 1   r   1          M1  where each polygon corresponds to the set of weights for which a join result  j is selected as top 1  i e     r 1   h j i  To this end  we consider the score vectors connected to s  j   on the convex hull  i e   those score vectors s  je    e   1          E  delimiting    0 0 5 1 0 0 5 1 0 0 2 0 4 0 6 0 8 1 s   1  s1 s   3  s   5  s2 s3  a  0 0 5 1 0 0 5 1 0 0 2 0 4 0 6 0 8 1 s   1  s1 s   2  s   7  s   3  s2 s3  b  Figure 7   a  Convex hull for top 1  b  Convex hull for top 2 when  5 is top 1 0 0 5 1 0 0 5 1 0 0 2 0 4 0 6 0 8 1 w1    h  1i     h  2i  w2 w3    h  3i   a  0 0 5 1 0 0 5 1 0 0 2 0 4 0 6 0 8 1 w1    h  3    1i       h  1    3i     h  1    5i     h  5    1i     h  3    5i  w2    h  5    2i     h  5    7i     h  5    3i  w3  b  Figure 8  Partitioning of  2  a  at depth K   1  b  at depth K   2 edges departing from s  j    and impose that their aggregate score does not exceed that of  j by writing wT s  j     wT s  j1          11  wT s  j     wT s  jE   together with the implicit constraint that w belongs to the simplex  i e   w 2  2   Thus  we can express     r 1      h j i  as the set fw 2  2 jAw   b A   A   r 1   b   b   r 1 g  which de nes a convex polygon  For instance  Figure 7 a  depicts the convex hull created using the possible top 1 join results  There are 3 join results on the hull surface  while other join results are inside the hull  Figure 8 a  illustrates the partitioning of  2 corresponding to the score vectors depicted in Figure 7 a   In order to  nd the M2 top 2 possible answers we pro  ceed as follows  For each of the possible top 1 orderings   r 1  r   1          M1  we recompute the convex hull after removing the score vector associated to the join result in   r 1  Let  j denote such a join result  i e     r 1   h j i  and  j l  l   1          M1 j   denote the join results whose score vectors are on the updated convex hull  For instance  Figure 7 b  depicts the modi ed convex hull obtained af  ter removing s  5   which includes s  1   s  2   s  3  and s  7   The candidate top 2 answers   r 2 can be expressed as h j    j li  j 2 fij9r   r 1   h iig  l   1          M1 j   For each candidate  we are interested in determining the poly  gon     r 2      h j    j li   which can be expressed by means of a set of linear constraints wT s  j     wT s  j l  wT s  j l    wT s  j l1          12  wT s  j l    wT s  j lE   Algorithm 1  buildTree K  MP O   s  1           s  N       Input  result size K  boolean MP O  score vectors matrix  s  1           s  N      Output  top K tree S    pruneDominatedObjects  s  1           s  N    K   if  MP O  then T ree    addNodesMPO empty tree  S  K  1     0   else T ree    addNodes empty tree  S  K  1      return T ree  Algorithm 2  addNodes T ree  S  K  k        Input  current tree T ree  score vectors matrix S  result size K     current depth k  current pre x      Output  top K tree if  k   K  then Remove from S all score vectors in    Compute convex hull H of remaining vectors  for each   j 2 H  Find objects connected to  j   Compute linear constraints and        j    if         j   is not empty  then Add  j to branch   in T ree  T ree    addNodes T ree  S  K  k   1       j    return T ree  together with the implicit constraint w 2  2   where s  j le    e   1          E  denotes the score vectors that belong to the convex hull obtained after removing s  j    which are con  nected to s  j l   For example  when h 5i is top 1  the follow  ing candidate top 2 answers can be obtained    1 2   h 5   1i    2 2   h 5   2i    3 2   h 5   3i and   4 2   h 5   7i  After that  computing  say    h 5   3i  requires to consider s  1  and s  7   since they are directly connected to s  3   Note that  for some candidate top 2 answers  the linear system of equations in  12  might not have a feasible solution and   h j    j li       This is due to the fact that there is no linear aggregation function for which the corresponding candidate answers can be selected as top 2  Thus  such candidates are pruned from the set of possible orderings  In the example given in Figure 6  the number of possible top 2 answers equals the number of leaves in the tree  Note that  out of  N K   K     8 2   2    56 orderings  only 8 of them are identi ed as possible top 2 answers  The determination of top K possible answers proceeds similarly  by recursively applying the same step individually to each leaf node of the top  K  1  tree  Algorithm 1 illustrates the procedure for constructing the tree of possible top K answers in detail  T ree represents the tree topology  whereas   indicates the current branch of the tree  i e   an input ordering for which the  rst k  1 elements have been computed  We also call depth the length of the pre x of   computed by the algorithm  After an initial pruning of all dominated objects  discussed in Section 3 3   the algorithm recursively calls addNodes  or addNodesMPO  if the tree is used to compute MPO  in a depth  rst fashion  until the desired depth K is reached  In the addNodes and addNodesMPO functions  reported in Algorithms 2 and 3  resp   we indicate as   the concatenation of an element to a sequence  namely  of a join result to a pre x of an ordering   The average time complexity of constructing the top K tree can be derived for the case of uniformly distributed score vectors  For K   N  each node has O  log N  2   chil  dren  Therefore  there are O  log N  2K   non leaf nodes for which the convex hull needs to be computed  For d   3  spe  cialized algorithms exist that compute the convex hull with    Algorithm 3  addNodesMPO T ree  S  K  k     pmax     Input  current tree T ree  score vectors matrix S  result size K     current depth k  current pre x    current MPO probability pmax    Output   partial  top K tree if  k   K  then Remove from S all score vectors in    Compute convex hull H of remaining vectors  for each   j 2 H  Find objects connected to  j   Compute linear constraints and        j    Compute p      j    for each   j 2 H in descending order of p      j    if  p      j     pmax  then Add  j to branch   in T ree  if  k   K  then pmax    p      j    else T ree    addNodesMPO T ree  S  K  k   1       j   pmax   return T ree  complexity O N log N    3   Hence  the overall asymptotic complexity is O N log N  2K 1    We discuss in Section 3 3 how to signi cantly reduce N in the previous bound by pruning all K dominated join results  Computing Nodes Probabilities  When the weight vec  tor w is uniformly distributed on  2   evaluating p    re  quires computing the area of the polygon       This can be done in three steps  i  Compute the coordinates of the ver  tices of the polygon on  2   i e    w v 1   w v 2   T   v   1          V   The vertex enumeration problem can be solved in O CV   time  1   where C denotes the number of linear constraints and V the number of vertices  The resulting asymptotic complex  ity is dominated by the top K tree construction  since the number of constraints is O  log N   2   and there might be up to O  log N   4   intersections  candidate vertices   ii  Com  pute the area by means of the Shoelace formula A   1 2 XV v 1 w v 1w v 1 2  XV v 1 w v 1 1 w v 2  13  where w V  1 i   w 1 i   iii  p      2A  For example  Figure 6 illustrates  for each set of possible orderings  1 and  2  the corresponding probabilities  Handling Higher Dimensions  The method proposed for d   3 continues to be applicable for larger values of d  as far as the enumeration of possible orderings is concerned  although the asymptotic complexity of computing the con  vex hull is O N bd 2c 1   instead of O N log N    3   Unfor  tunately  determining the probabilities p    requires com  puting the volume of convex polyhedra embedded in  d1   which is NP hard  It is still possible to determine a lower and upper bound on p    in polynomial time  by comput  ing  respectively  the maximum volume inscribed ellipsoid and the minimum volume enclosing ellipsoid  In order to  nd an approximation of p     we propose a Monte Carlo sampling method  For a space of orderings  K   f  1 K            m Kg  we maintain a counter cr for each ordering   r K 2  K  We sample the space of weights uni  formly at random  For each sample weight vector w    where O w      r K  we increment the counter of   r K by 1  Let the number of drawn samples be s  The value of p   r K  is ap  proximated as cr s  The approximation error is in O 1  p s   which is the error of Monte Carlo sampling  3 2 2 Computing MPO A na  ve method to compute     MPO consists of the follow  ing steps  i  construct the  full  top K tree  ii  compute the probability of each leaf node p  K   iii  select the node associated with the largest probability  We notice that  in order to speed up the computation of     MPO  some of the branches of the top K tree can be safely pruned without the need of reaching all leaf nodes  To this end  we interleave the tree construction with the computation of the proba  bilities of visited nodes  both intermediate and leaf nodes  and keep track of the current MPO candidate among the visited leaf nodes  We observe that each node is assigned a probability which cannot be greater than the probability of its parent  Therefore  it is possible to safely prune those branches rooted at a node whose probability is less than that of the current MPO candidate  The amount of pruning depends on the order followed to traverse the tree  Ideally  the algorithm should reach the leaf node corresponding to     MPO as early as possible in such a way that most of the nodes can be pruned  Therefore  at each level of the tree  nodes are explored in descending order of probability  This behavior is implemented in Algorithm 3  3 3 Pruning K Dominated Join Results When computing MPO at depth K  an important opti  mization is to start by pruning all K dominated join results  A join result  i is K dominated if there exists a set of   K join results whose score vectors dominate the score vector of  i  A K dominated join result does not appear in any possi  ble top K answer  and hence it can be safely pruned without a ecting the correctness of the computed MPO  Since K is typically a small number  the e ect of K dominance on re  ducing the number of join results is substantial  A na  ve algorithm to prune K dominated join results is to count  for each join result  i  the number of other join results dominating  i  and prune  i as soon as such count is K  The complexity of this algorithm is O N 2    We show that we can achieve the same goal with more optimized algorithms  For d   2  we adopt the DominatingSet algorithm in  14   which has complexity O NlogN   Extending this algorithm to handle d   2 requires maintaining intersections of sets of join results  which reduces to O N 2   complexity  We thus adopt a di erent algorithm  based on Fagin s Algo  rithm  FA   6   which  in practice  prunes the majority of K dominated join results when d   2  First  each join result   is assigned a unique identi er  Then  we create d sorted lists L1          Ld  where each Li maintains join results  identi ers in the order of s  i  The cost of this step is O dN log N   Let    x  i denote the iden  ti er of the join result at position x in Li  The sorted lists L1         Ld are scanned in parallel until a depth  x is reached such that at least K common identi ers are found among the d lists of seen identi ers  That is  jf   1  1               x  1 g         f   1  d               x  d gj   K  Let U denote the union of the seen identi ers  i e   U   f   1  1               x  1 g           f   1  d               x  d g  The top K join results are guaranteed to be among those whose identi ers are in the set U  for any monotonic aggre  gation function  Therefore  those join results whose iden  ti ers are not contained in U can be safely pruned  It is shown in  6  that  if the rankings are independent and over the same set of N items  the cost for  nding the top K an   swer is O N d1 d K 1 d    which is sub linear in N  Hence  the cost of pruning is dominated by the initial sort operation  4  WEIGHT PREFERENCES We discuss handling user s preferences speci ed in the form of a strict partial order on the weights  A strict partial order on the weights is a binary preference rela  tion  P that is irre exive  i e    wi  P wi    asymmetric  i e    wi  P wj      wj  P wi    and transitive  i e     wi  P wj    wj  P wk      wi  P wk    For the ex  ample in Figure 2  a user interested in join results with highly rated restaurants  may express such preference using a partial order  wR  P wH   Note that for d   2  a par  tial order on the weights is e ectively a total order  while for d   2  a partial order can be a non total order  e g    w1  P w2   w1  P w3    Each preference  wi  P wj   implies a constraint  wi   wj   on the weights space  Let WP   fw 2  d1 j wi   wj   whenever  wi  P wj  g  Based on the properties of strict partial orders  WP is expressed in terms of linear constraints  and hence WP is a convex polyhedron embedded in  d1   We de ne a uniform probability density function over WP as follows  p w        if w 2 WP 0 otherwise  14  where   is a normalizing constant such that the integral of p w  over WP is equal to one  The techniques presented in Sections 3 1 and 3 2 remain applicable to this scenario  To illustrate  assume that d   2  Weight preferences give a total order on fw1  w2g  which implies that one of the two weights is greater than 0 5  since w1   w2   1   Hence  WP is given by either the upper half of  1  if w2   w1   or the lower half of  1  if w1   w2   Probabilities are thus given based on partitions of only one half of  1   For example  in Figure 5 a    3 2   0 if w1   w2  since in this case only the lower half of  1 is considered  5  SENSITIVITY MEASURES When a user provides an explicit weight vector  a corre  sponding ordering is immediately obtained  The user may be interested in receiving additional feedback indicating the largest perturbation in the input weights that does not a ect the ordering  Problem STB  as well as the likelihood of the ordering  Problem LIK   Such feedback can be important in interactive data analysis  and guiding scoring function tun  ing to capture user s preferences  We discuss our proposed solutions to these problems next  5 1 Stability of the Ordering wrt  the Weights Let w  be a weight vector  where O w       N  Let      K  be the polyhedron de ning the set of weights inducing    K  We show how to solve Problem STB  where we  nd the radius  K w    of the maximum volume hypersphere  K w    centered at w    and enclosed in      K   Computing  K w    requires determining the distances be  tween w  and each of the hyperplanes delimiting the poly  hedron      K   There are up to N  1 such hyperplanes  de ned as follows  wT s   1      wT s   2    wT s   2      wT s   3                  wT s   K1      wT s   K    wT s   K      wT s   K 1                  wT s   K      wT s   N     15  where   i  is the i th combination in    N  Since there are O N  hyperplanes  and computing the dis  tance between a point and a plan in a d dimensional space can be done in O d   the worst case time complexity for computing  K w    is O dN   The cost can be signi cantly reduced to be sublinear in N if the score vectors have been pre indexed  e g   removing dominated vectors or computing the tree of possible orderings  as illustrated in Section 3 2 1  Alternatively  one can measure stability of w  by aggregat  ing all the radii  k w    for k   1          K into a single value    K w      1 K max XK k 1  k w   D k   16  where i   max is the radius of the maximum volume hyper  sphere enclosed in  d1   and ii  D k  is a discount function adopted to emphasize changes that a ect the top elements in the ordering  e g   D k    1  log 2   k   5 2 Ordering Likelihood Given a weight vector w    where O w       N  let    N   h  1     2           N i  Let      K  be the polyhedron de ning the set of weights inducing    K  Then   K    N  is given by the ratio of the volume of      K  and the volume of  d1   We de ne      K  similar to Section 5 1  We compute the switching planes of    i     i 1   for i   1       K  1  and    K     j   for K   j   N  The number of these switch  ing planes is in O N   and      K  is the minimal convex polyhedron  created by the intersections of these switching planes  enclosing w    For d   2       K  is given by a partition of the w2 axis  This allows computing  K    N  in O N  by  nding the clos  est switching plane above w    and the closest switching plane below w    on the w2 axis       K  is the partition enclosed be  tween these two closest planes  For example  in Figure 5 a   for w     0 6  0 4   we have    4   h 2   3   4   1i  Then  if K   2  we need to compute the switching planes of   2   3     3   4   and   3   1   The closest top and bottom planes wrt  w     0 6  0 4  are the planes of   2   3  and   3   4   respec  tively  Hence       K  is given by the partition  1 6 3 5   which leads to  2    4    0 43  For d   3  the number of intersection points of O N  switching planes is in O N 2    The vertices of      K  are among these intersection points  Hence       K  can be found in O N 2   by inspecting all possible polygons to  nd the minimal convex polygon enclosing w    For ex  ample  in Figure 5 a   for w     0 5  0 4   we have    4   h 1   2   3   4i  Then  if K   2       K  is given by the poly  gon   0 21 0 3   0 6 0 4   0 14 0 86   whose area can be computed as shown in  13   giving  2    4    0 2254  In general  for d dimensions  the complexity of computing Problem LIK is O N 2 d2    which is the bound on the num          Table 1  CPU time to compute MPO on synthetics data sets  Zip an distribution d   2 d   3 d   4 N K   1 K   5 K   10 K   1 K   5 K   10 K   1 K   5 K   10 100 1 3 10 4 9 8 10 4 8 8 10 3 1 1 10 5 6 2 10 5 2 0 10 4 2 0 10 3 1 6 10 2 2 5 10 1 1000 1 6 10 4 1 6 10 3 9 2 10 3 1 6 10 5 1 6 10 4 1 1 10 3 1 7 10 2 3 2 10 2 2 0 10 1 10000 4 6 10 4 4 6 10 3 2 8 10 2 6 2 10 5 5 9 10 4 5 9 10 3 1 0 10 1 1 5 10 1 3 3 10 1 100000 3 3 10 3 2 9 10 2 1 5 10 1 6 4 10 4 1 7 10 3 2 1 10 2 7 1 10 1 1 2 10 0 1 6 10 0 Table 2  CPU time to compute MPO on synthetics data sets  exponential distribution d   2 d   3 d   4 N K   1 K   5 K   10 K   1 K   5 K   10 K   1 K   5 K   10 100 1 6 10 4 3 3 10 3 2 2 10 2 1 1 10 5 4 7 10 5 2 2 10 4 2 3 10 3 3 4 10 2 3 8 10 1 1000 2 6 10 4 4 9 10 3 3 3 10 2 1 1 10 5 2 2 10 4 1 3 10 3 1 8 10 2 6 1 10 2 4 4 10 1 10000 7 1 10 4 1 1 10 2 7 0 10 2 3 1 10 5 9 8 10 4 9 3 10 3 1 2 10 1 2 0 10 1 5 8 10 1 100000 3 9 10 3 6 8 10 2 4 2 10 1 6 2 10 4 2 9 10 3 2 2 10 2 7 1 10 1 1 0 10 0 1 6 10 0 ber of minimal convex polyhedra created by the intersections of O N  switching planes  However  for d   3  computing a polyhedron s volume is NP Hard  and hence we need to approximate  K    N  as discussed in Section 3 2 1  We conclude this section with one last result on the like  lihood of     ORA under Kendall tau distance and d   2  Let w   2 be the  rst intersection point  between a switch  ing plane and the w2 axis  above 0 5  When there is no such point  let w   2   1 0  Similarly  let w  2 be the  rst intersec  tion point  between a switching plane and the w2 axis  below 0 5  When there is no such point  let w  2   0  Then      ORA is represented by the  1 partition de ned in Theorem 5 1  Theorem 5 1  When there is no  i j   0 5      ORA un  der Kendall tau distance is given by the partition  w  2   w   2    Otherwise      ORA is given by either the partition  0 5  w   2    or the partition  w  2   0 5   Proof  When there is no  i j   0 5  then for all  i j   0 5  the switching plane of   i   j   is located above  or at  w   2   while for all  i j   0 5  the switching plane of   i   j   is located below  or at  w  2   Hence  for all   i   j    with  i j   0 5  the ordering induced by the partition  w  2   w   2   positions  i above  j   This means that  w  2   w   2   induces     ORA according to Theorem 3 5  Similarly  when there is at least one  i j   0 5  any one of the two partitions  0 5  w   2    or  w  2   0 5  induces an ordering that positions  i above  j i   i j   0 5  which means that either partition induces     ORA  For the example in Figure 5 a       ORA   h 2   3   1   4i and  4     ORA    0 17  which is the length of the partition  3 7 3 5   6  EXPERIMENTS In this section we experimentally evaluate the algorithms described in this paper and study the impact of various pa  rameters on execution  number of desired results  number of relations  number of join results  and data distribution  6 1 Methodology Data sets  First  we conduct our analysis on synthetic data sets  We generated d independent relations of tuples with scores sampled from a selected distribution among uni  form  Zip an  and exponential  such that their join produces N join results  with 2   d   4 and 10 2   N   10 5   While the number of join results can be large  our study involves a parameter K whose value is typically small  This leads 10    3 1 2 3 5 10 10    2 10    1 10 0 Top combinations   K rel  error s   100000 s   10000 s   1000 Figure 12  Relative approximation error  d   4  Data set d   2 d   3 d   4 New York 3871 41907 209599 San Francisco 9016 56200 591952 Boston 4712 9740 39030 Dallas 1424 1806 15196 Table 3  Number N of join results for real data sets to aggressive pruning of the join results  cf  Section 3 3   Hence  the subset of join results we need to process is usu  ally small  even though the number of all join results is large  Then  we consider real data sets with relations containing hotels  restaurants  theaters  and cars for rent in four di er  ent American cities  New York  San Francisco  Boston  and Dallas   Each data set is obtained by retrieving customer ratings  latitude and longitude of each such place by means of the YQL console 1   For each city  the relations are joined in such a way that only the join results in which the mutual distance between the objects is below a certain threshold are retained  Table 3 shows the number of join results N computed for each data set when d relations are joined  Methods and evaluation metrics  We test the dif  ferent algorithms described in this paper to compute MPO  For d   2  3 the result is exact  in the sense that the output of the algorithm is the correct ordering     MPO together with its probability p     MPO   For d   4 we resort to sampling to determine the possible orderings and their probabilities  As such  the result is an approximation of the correct MPO  Total CPU time  in seconds  is the metric we adopt for evaluating our results  The total CPU time also includes the time needed to prune dominated join results  as described in Section 3 3  For fairness  in every experiment we run our tests over ten di erent data sets and report the average  Relative approximation error is the metric we adopt for evaluating the accuracy of the solution produced by sampling  The relative approximation error is given by jp       MPO   p     MPO j p     MPO   where  p      MPO  is the ap  proximated value obtained by sampling  Number of children is the metric we adopt for characteriz  ing the structure of the tree constructed in the incremental approach  which gives an indication of the number of possi  ble orderings of length K  We do not run experiments to test ORA for the following reasons  For d   2  under Kendall tau distance  the solution amounts to sorting  so testing would uninterestingly evaluate the e ciency of a sorting algorithm  For d   2  ORA is NP hard  approximations of the Kendall tau distance are possible  and they are described in other works  4   Determining the largest perturbation that can be toler  ated without a ecting the ordering  as described in Section 5 1  is at most linear in the number of join results  Hence  the total CPU time needed is negligible when compared to the other problems addressed in this paper  Computing ordering likelihood has polynomial time com  1 Available at http   developer yaho</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pu2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10pu2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Probabilistic_and_uncertain_databases"/>
        <doc>Jigsaw  Ef   cient Optimization Over Uncertain Enterprise Data ### Oliver Kennedy     EPFL   Cornell University oliver kennedy ep    ch Suman Nath Microsoft Research sumann microsoft com ABSTRACT Probabilistic databases  in particular ones that allow users to externally de   ne models or probability distributions     so called VG Functions     are an ideal tool for constructing  simulating and analyzing hypothetical business scenarios  Enterprises often use such tools with parameterized models and need to explore a large parameter space in order to discover parameter values that optimize for a given goal  Parameter space is usually very large  making such exploration extremely expensive  We present Jigsaw  a probabilistic database based simulation framework that addresses this performance problem  In Jigsaw  users de   ne whatif style scenarios as parameterized probabilistic database queries and identify parameter values that achieve desired properties  Jigsaw uses a novel       ngerprinting    technique that e   ciently identi   es correlations between a query   s output distribution for di   erent parameter values  Using    ngerprints  Jigsaw is able to reuse work performed for di   erent parameter values  and obtain speedups of as much as 2 orders of magnitude for several real business scenarios  Categories and Subject Descriptors H 2 8  Database Applications   Database Applications    Scienti   c databases  Statistical databases General Terms Algorithms  Design  Performance Keywords Probabilistic database  Monte Carlo  Black box  Simulation ### 1  INTRODUCTION Enterprises often need to evaluate business scenarios to assess and manage    nancial  engineering  and operational    Work performed while interning at Microsoft Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  risks arising from uncertain data  Our experience working with a Microsoft Windows Azure cloud platform analytics team has revealed an increasing need for tools for developing timely plans for the expansion  deployment  and allocation of resources  When making these plans  which can involve dispensation of millions of dollars  accurate and e   cient simulation of many di   erent business scenarios is critical to establish the validity of speci   c decisions in a timely manner  Consider an analyst who wants to forecast the risk of running out of processing capacity in a cloud cluster  For that  she needs to combine various predictive models for CPU core demands and availability  These models are inherently uncertain due to imprecise predictions of future workload  possible downtime  delays in deployment  etc  Without effective tools  simulating and evaluating business scenarios based on uncertain models can be extremely challenging  Recently  a swath of probabilistic database  PDB  systems  3  4  5  12  13  16  18  have made probability distributions and models into    rst class citizens of the database world  PDB systems provide an ideal framework for analyzing business scenarios such as those described above  Some PDB systems  MCDB  12  and PIP  13  in particular  allow users to evaluate queries over user provided distributions  de   ned as stochastic black box sampling functions  also called variable generation  VG  functions  12    These systems use Monte Carlo sampling of the VG Functions to approximate the results of queries  Support for user provided distributions is an extremely useful feature for the analysts  who derive their baseline models using specialized  external tools such as R  Sparse  incomplete  or noisy data can transform even conceptually straightforward tasks  e g   extracting the rate and volatility of demand growth  into daunting challenges that necessitate the use of such external tools  A key challenge faced by PDB based simulation systems arises when models are parameterized and the system must explore a large parameter space to optimize for a given goal  In the previous example  a CPU core availability model might accept a set of candidate purchase dates and apply them according to a model for how long it takes to bring the hardware online  The analyst can then identify purchase dates that minimize the cloud   s cost of ownership  given a bound on the risk of overload  This is essentially a constrained optimization problem  albeit one where each iteration is an entire PDB query  The primary bottleneck in this context is the repeated  and potentially very costly  Monte Carlo estimation of query outputs for various parameter values  largely due to the expensive invocation of VG Functions   1  As the VG Functions are black boxes it is not correct to assume a relationship  e g   monotonic  continuous  etc   between a VG Function   s parameters and outputs  Consequently  the function must be evaluated for most  if not all  possible parameter values   2  The function may need to be evaluated over a range of steps  e g   if it describes time series data  like a daily CPU demand model   and output at each step may be dependent on prior steps  i e   the function describes a Markovian process  as discussed in Section 4   Therefore  with parameterization  even relatively simple scenarios taking tens of minutes  or even hours to evaluate  12  will now take hours or even days  Such a slow response is unacceptable in many practical situations where a business decision must be made quickly or if parameterized what if scenarios are to be evaluated interactively  In this paper  we present Jigsaw  a PDB based simulation framework that addresses the above performance problem  Jigsaw can e   ciently simulate a given business scenario by combining multiple parameterized stochastic black box functions  possibly over multiple time steps  Jigsaw is built around a simple PDB  which performs Monte Carlo simulation over entire databases  Using this PDB functionality  Jigsaw takes parameterized queries 1 and identi   es parameter values that optimize a given goal or achieve a desired property  Jigsaw   s performance improvement over existing related systems comes from the following two key observations  First  outputs of many enterprise related stochastic functions are strongly correlated under various parameter values  For example  in our earlier example  changing the purchase date of new hardware from May to July is unlikely to a   ect the cluster capacity in April or August  here  purchase week and the week being estimated     the    current    week are parameters   First  identifying such correlations can help avoid exploration of signi   cant parts of the parameter space  Second  when a simulation is Markovian  where the simulation consists of a series of steps  each depending on the simulation   s output for the prior step   outputs of successive steps often remain strongly correlated  This is particularly true for many processes of interest that are built around discontinuities  with discrete events occurring at random points in time  e g   the nondeterministic date when new hardware comes online   Identifying such Markovian dependencies enables the automated generation of simple non Markovian estimators  These estimators  valid for regions of the Markov chain  allow Jigsaw to skip the corresponding portions of the simulation  Jigsaw exploits the above two observations by using    ngerprints of stochastic functions  The    ngerprint of a stochastic black box function is a concise and easilycomputable data structure that summarizes its output distribution  Thus  a    ngerprint can be used to e   ciently determine a function   s correlation with another function  or its own instantiations under di   erent parameter values  After such correlation has been detected  Jigsaw can avoid expensive Monte Carlo estimation  and the associated VGFunction invocations  for a target point in the parameter space by using outputs for an already explored  correlated point  The speci   c    ngerprinting technique we use in this 1We assume a discrete    nite domain for each parameter  This is a reasonable expectation in our target application  Furthermore  with an in   nite parameter domain  continuity must be assumed for an optimization problem to be feasible  paper is based loosely on random testing  11   a well known technique in software engineering  the    ngerprint of a parameterized stochastic function is simply a sequence of its outputs under a    xed sequence of random inputs  i e   seed of its pseudorandom number generator   The use of a    xed set of random seeds ensures a deterministic relationship between correlated outputs of the stochastic functions  Note that correlation in the outputs of a stochastic function might often be obvious to a human onlooker  For example  excepting the days immediately after a hardware purchase  the day by day output of a simple cluster capacity model may be built out of the same distribution  However  forcing function authors to express such correlations through metadata is undesirable  as it negates the generality and clean abstraction o   ered by VG functions  Datadependent corner cases  discontinuities  and Markovian dependencies can make the metadata just as  or even more complex than the stochastic function itself  The stochastic nature of these black box functions only complicates matters further  Therefore  one important design goal for Jigsaw is to identify such correlation automatically  and    ngerprints are a step towards achieving that goal  In summary  we make the following contributions in this paper  First  we propose Jigsaw  an e   cient framework for running optimization queries over uncertain enterprise data  Section 2   It uses a combination of parameter exploration  Monte Carlo simulation  and Markovian process evaluation  Second  we propose an e   cient mechanism for computing    ngerprints of stochastic black box functions  Section 3   We demonstrate the use of    ngerprints in the following cases   1  We show how to use    ngerprints to e   ciently explore the large parameter space of an optimization query  based on a parameterized Monte Carlo simulation  Section 3   Fingerprints are used to identify similar parameter values across which VG Function invocations may be reused   2  We show how to use    ngerprints to accelerate the evaluation of some Markov processes  Section 4   By comparing the    ngerprint to that of a state independent estimator  we can potentially avoid computing many iterations of the process   3  We also describe how    ngerprints can be used to improve the e   ciency of an interactive  online scenario evaluation and parameter exploration tool called Fuzzy Prophet  which we have built on top of a PDB  Section 5   This tool uses basic probabilistic database techniques to allow users to instantiate business scenarios in the familiar environment of SQL  and analyze the e   ect of changing various scenario parameters with immediate feedback  Finally  we evaluate Jigsaw with several real world scenarios seen in an enterprise cloud management team  Section 6   The results show e   ectiveness of our various optimizations  Jigsaw is able to improve simulation performance by as much as 2 orders of magnitude  Note that Jigsaw has been developed in concert with an analytics team in Microsoft Windows Azure cloud platform  and hence the examples we use throughout the paper are in the context of managing a cloud infrastructure  However  we believe that the need for parameter optimization tools in PDB systems is a much more broad requirement  2  RUNNING SIMULATIONS In this section we    rst describe how Jigsaw looks like to a user via a few simple examples  We then brie   y describehow Jigsaw answers queries and point out speci   c challenges that we address in this paper  2 1 Probabilistic Databases Probabilistic database  PDB  systems allow users to pose queries over uncertain data  data speci   ed as a distribution over a range of possible values  rather than as one speci   c value  For example  OCR software might have di   culty distinguishing between a 9 and a 4  A PDB records both values and their corresponding probabilities  When the data is queried  the response is phrased as a distribution over possible results  although this distribution may be represented as an expectation  maximum likelihood  histogram  etc    Several PDB implementations can similarly represent and query continuous distributions  e g  a gaussian distribution representing a measurement and its statistical error   While a traditional DBMS stores a single instance of a database  a database in a PDB system represents a distribution over a  potentially in   nite  set of database instances typically referred to as possible worlds  Queries in a PDB are  conceptually  evaluated by evaluating the query in each possible world  The set of results forms an answer distribution for the query  Clearly  this approach is not feasible in general  so some PDBs compute approximate results using Monte Carlo methods  The MCDB  12  system  on which Jigsaw   s PDB implementation is based  instantiates a    nite set of databases by sampling randomly from the set of possible worlds  Queries are run on each sampled world in parallel  and the results are aggregated into a metric  e g  an expectation or standard deviation of the result  or binned into a histogram  Note that MCDB   s interaction with distributions being queried is limited to the generation of samples  This simple interface makes it possible for users to incorporate  nearly  any distribution into their queries  User de   ned probability distributions  e g  a user demand forecast  can be incorporated by constructing a stochastic black box function referred to as a VG Function  which generates samples drawn from the distribution  2 2 Example queries in Jigsaw Batch mode execution  Suppose that  in our context of cloud infrastructure  an analyst wishes to determine the optimal date and volume for several server purchase orders to keep the risk of running out of available CPU cores below a certain threshold  The later the purchases occur  the lower the hardware   s upkeep costs  but the greater the chance that cores will be unavailable when needed  The question of an ideal purchase date and volume is a simple constrained optimization problem  A Jigsaw user would specify this optimization problem in three stages   1  The user de   nes stochastic models forecasting CPU core availability and demand   2  The user speci   es inter model interactions to describe the scenario  and  3  Jigsaw solves the optimization problem by exploring the parameter space of purchase dates and volumes  For step  1   the user de   nes various stochastic models that Jigsaw uses as black boxes  These stochastic black boxes are essentially functions that produce samples 2 drawn 2 Canonical VG Functions in MCDB produce tables as output  For clarity  this paper uses a simpli   ed notion of stochastic black box functions that produce only single val     DEFINITION    DECLARE PARAMETER  current week AS RANGE 0 TO 52 STEP BY 1  DECLARE PARAMETER  purchase1 AS RANGE 0 TO 52 STEP BY 4  DECLARE PARAMETER  purchase2 AS RANGE 0 TO 52 STEP BY 4  DECLARE PARAMETER  feature release AS SET  12 36 44   SELECT DemandModel  current week   feature release  AS demand  CapacityModel  current week   purchase1   purchase2  AS capacity  CASE WHEN capacity   demand THEN 1 ELSE 0 END AS overload INTO results     BATCH MODE    OPTIMIZE SELECT  feature release   purchase1   purchase2 FROM results WHERE MAX EXPECT overload    0 01 GROUP BY feature release  purchase1  purchase2 FOR MAX  purchase1  MAX  purchase2 Figure 1  An example Jigsaw query  from the probability distribution that they intend to describe  This framework allows analysts to easily import externally de   ned models that describe a wide variety of processes and system characteristics  In our speci   c example  the user writes the following two functions  e g  based on a model derived in a statistical modeling application like R   DemandModel current week  feature release   CapacityModel current week  purchase1  purchase2   The DemandModel function produces a stochastic CPU core usage demand forecast for a given week in the future  taking into account expected future user arrival rates  individual user capacity requirements  and expected user reactions to planned special o   ers and system features  The CapacityModel function outputs a stochastic estimation of the number of CPU cores available on a given date in the future  given a set of future purchase dates   It also takes into account the current CPU core availability  future expected failure rates  and prediction  based on prior purchasing experiences of when new cores will come online after purchase  How the models are developed is orthogonal to Jigsaw   s execution engine  which simply treats the models as black boxes  This way  we establish a clean separation between expert knowledge and the task of simulating the underlying process  For steps  2  and  3   the user writes the SQL like query in Figure 1  The core of the scenario is a simple SQL SELECT query that produces an output result table     in this example  containing capacity  demand  and overload columns  Note that  as Jigsaw is built around a PDB system  this results table is speci   ed as a probability distribution over the space of possible results  Two aspects of the query require further discussion   a  The query contains several parameter variables  each pre   xed with a    Parameter variables  with their bounds and sets of permitted and initial values  are declared as part of the scenario using DECLARE PARAMETER statements and are equivalent to standard SQL variables from the user   s perspective   b  The optimization goal is expressed with an OPTIMIZE query  which iterates over the parameter space to    nd the latest purchase1 and purchase2 that keep the expected risk of overload  a condition de   ned as a week when capacity   demand  within a threshold  ues  We make this distinction explicit by using the term black box function  Naive extensions of Jigsaw   s    ngerprinting technique to VG functions are trivial  e g   extend the function with row and column id parameters  and optimized extensions are beyond the scope of this paper Figure 2  Jigsaw   s interactive interface  One possible implementation of the CapacityModel is of interest  This model is charaterized by a sequence of discrete events  e g   purchases or hardware failures   each a   ecting the cluster   s capacity  Each event is produced by a separate model  so the database engine itself can compute the cumulative e   ect of the events with a simple SQL SUM aggregate  Also consider the CapacityModel expectation viewed in a time series plot  Though each purchase has a stable long term impact on the cluster   s capacity  this plot is characterized by two distinct structures in the vicinity of each purchase date  We will return to this observation later  Interactive mode  Jigsaw can also be used in an interactive online mode  In this mode  the user modi   es various parameter values  e g   purchase date and volume  and quickly sees the outcome  e g   the risk of overload at a given date   As parameter values are modi   ed  the system continually updates a progressively re   ned estimate of the results table for those parameter values  This quickly gives a rough estimate of the    nal answer so that the user  not    nding the given parameter values interesting  can abandon the simulation in the middle and try a di   erent parameter value  This mode is particularly targeted at users who may not have an extensive statistics background  An analyst developed scenario can be used by an executive  e g  as part of a management dashboard tool  to quickly observe the expected outcome of speci   c    nancial decisions for various parameter values  The interactive mode  with the output shown in Figure 2  is expressed with the following execution query  parameter de   nition and SELECT portions of the query are same as in Figure 1      INTERACTIVE MODE    GRAPH OVER  current week EXPECT overload WITH bold red  EXPECT capacity WITH blue y2  EXPECT STDDEV demand WITH orange y2  The query above provides Jigsaw with a parameter to use as the graph   s X Axis  and speci   es how each column in the results table is to be graphed in the GUI  Figure 2   2 3 Jigsaw Simulation Process Figure 3 shows how a Jigsaw executes an optimization query in the batch mode  Each random table in the uncertain database is represented on disk by its schema  together with a set of black box functions that are used to generate realizations of uncertain attribute values  When a query is issued  the Parameter Enumerator module enumerates all feasible parameter values for the black box functions involved in the query  This brute force approach is necessary to guarantee that the optimization converges to the Figure 3  Processing optimization queries in Jigsaw global maximum for an arbitrary black box function  Note that Jigsaw   s    ngerprinting techniques remain applicable to more advanced techniques that use additional information about the black box  e g   gradient descent  if the black box is known to be continuous   For each parameter value  Jigsaw then invokes its PDB subsystem  shown inside the dotted box   The PDB subsystem  loosely modeled after MCDB  invokes the black box functions with the current set of parameter values to generate a set of n     1 independent and identically distributed  i i d   sampled instances  sometimes referred to as possible worlds  for parameter valuation Pa  we say that sample di is generated by instance  Pa  i   Recall that in a PDB  the output of a query is a probability distribution  Evaluating the query over each sampled possible world generates a set of i i d  samples of the results table   s distribution  These latter samples are then aggregated by the Estimator to compute one or more characteristics of interest  i e   mean  standard deviation  etc        for the output distribution  The process is repeated for all di   erent parameter values  Finally  the Selector component selects the parameter value  along with its output distribution  that satis   es the optimization goal  2 4 Jigsaw Challenges The most expensive aspect of Jigsaw   s simulation process is its interaction with the underlying PDB  This processing overhead is linear in the size of the parameter space and dominates all other processing tasks performed by Jigsaw  The primary goal of Jigsaw is to reduce the number of instances on which the PDB must be invoked  To achieve this  we exploit several observations about redundancy in computations  First  outputs of many enterprise related stochastic functions are strongly correlated under various parameter values  examples in Section 3   Identifying such correlations can help to avoid exploring large regions of the parameter space  Second  in many event based processes with markovian dependencies  i e  each step in the process depends on the output of the prior step   the markovian dependencies are relevant only in the steps near an event  A suitably crafted non markovian estimator function  examples in Section 4  may be used to reduce simulation required for the other steps  Finally  in interactive mode execution  aquick estimation of simulation results for a selected parameter value can often be given based on results from previously selected parameters  the estimation can then be gradually re   ned with more samples  To exploit the above observations  Jigsaw needs to address the following challenges      How can parameter values that produce the same  or similar  outputs be e   ciently identi   ed and exploited      How can correlated Markovian steps be e   ciently identi   ed and exploited      Can an accurate estimate be obtained for one parameter value in interactive mode by reusing results computed for other parameter values  In the rest of the paper  we discuss how Jigsaw addresses these challenges  3  FINGERPRINTS The key concept Jigsaw uses to reduce the number of Monte Carlo evaluations is    ngerprints  A    ngerprint of a stochastic black box function is a concise and easilycomputable data structure that summarizes its output distribution  Thus  a    ngerprint can be used to determine a function   s similarity with another function  or its own instantiations under di   erent parameter values  We will show a concrete example of    ngerprints in Section 3 1  but    rst we explain the general principle  The outputs of a deterministic function F evaluated on two di   erent values Pi and Pj   are deemed similar  denoted as F Pi     M F Pj    if there exists a closed form mapping function M that maps from F Pi  to F Pj    F Pi     M F Pj       F Pi    M F Pj    Consider a stochastic function F with output X   F Pi  and probability distribution f x   X Pi   F is similar at Pi and Pj if a closed form mapping function exists to map the domain of f x Pi  into that of f x Pj    F Pi     M F Pj          x   f x Pi    f M x  Pj   More generally  we can think of M as the central element of a family of mapping functions that map not only function values but also metrics  aggregates  and other derived values  E   cient mapping between members of this family can substantially reduce the sampling requirements of a computation  For example  consider a scenario where both expectations E F Pi   and E F Pj    are needed  and F Pi     M F Pj   can be e   ciently proven  An Mexpect derived from M such that E F Pj      Mexpect E F Pi     eliminates the need to explicitly compute E F Pj     Identifying the mapping function for an arbitrary pair of stochastic black box functions  F Pi   F Pj    is di   cult for two reasons   1  The functions are black boxes     interactions with the function are limited to sample generation   2  The functions are stochastic  In order to match two distributions  it is    rst necessary to approximate the distributions  i e   by sampling from both  negating the bene   ts of having established similarity   Rather than attempt to map the result distribution  Jigsaw employs a shortcut  We propose the abstract fingerprint operation  and corresponding mapping function Mf    which Algorithm 1 DemandModel current week  feature  Require  The current week being simulated  and a feature release date  Ensure  The demand for the week being simulated  1  demand   Normal       1     current week     2   0 1     current week   2  if current week   feature then 3  demand    Normal       0 2      current week     feature      2   0 2      current week     feature    e   ciently maps parameterized stochastic black box functions to concise  comparable data structures such that with high probability  F Pi     M F Pj       fingerprint F Pi     Mf  fingerprint F Pj    Fingerprints can be computed for individual stochastic black box functions  such as DemandModel in Figure 1  or combinations of such functions  Taken to one extreme  the entire Monte Carlo simulation shown inside the dashed box in Figure 3 can be treated as the stochastic function F  Thus  F Pi     M F Pj   implies that we can avoid expensive Monte Carlo simulations for parameter value Pj and estimate the output of Estimator Pj  accurately as Mest Estimator Pi    3 1 Computing Fingerprints Identifying similarities between the outputs of two functions is  in general  hard  15   Jigsaw uses a probabilistic approach based on the principle of random testing  11   a well known software testing technique  For random testing of a deterministic function F against a hypothesis function H  both functions are evaluated on m random inputs and the results are compared  The function F is declared satisfying the hypothesis H if the outputs of F and H match for all m random inputs  Random testing has two features in particular  that make it well suited to our needs   1  Random testing is simple and can be used while treating the functions as black boxes   2  Random testing has been shown to be robust for functions with a small number of conditional branches so that a small number of random inputs can exercise all code paths  We have found in our cloud infrastructure management context that almost all our stochastic functions are relatively simple and contain at most one or two conditional branches  3 We use the same principle to determine similarities of outputs of a stochastic black box function F under two valuations of the same parameters Pi and Pj   However  unlike random testing where the parameters are random and the function is deterministic  Jigsaw must deal with stochastic functions and    xed parameters  To make F deterministic  we extend F with a seed parameter    and ensure that all sources of randomness within F Pi      are replaced by invocations of a pseudorandom generator seeded by     In 3 The functions are kept simple in practice  modeling only one particular aspect of the system so that they can be trained and validated even with small  noisy data sets practice  these modi   cations are negligible  as randomness is typically obtained from system API calls  e g  rand     It is crucial for both invocations of F to use the same source of randomness to make their comparison meaningful  Consider two stochastic functions that output 0 and 1 with equal probability  When repeatedly evaluated with the same sequence of random seeds  they can be quickly declared to be equivalent with a very high probability  On the other hand  using di   erent seeds  equivalence testing is much more di   cult  Consider our example stochastic function in Algorithm 3 1  As a sum of two normal distributions  the function   s output is normally distributed for all inputs  Suppose the function is invoked twice as DemandModel 1 3  and DemandModel 2 4   Both invocations take the same code path  and their outputs will be drawn from linearly correlated distributions  In addition  by using a pseudorandom number generator seeded with the same value for each invocation  we ensure that there is not just a correlation  but a linear mapping from one    ngerprint to the other  In contrast  using di   erent random seeds would hide the one to one similarity in their outputs  A Concrete Fingerprint  The    ngerprint of a parameterized stochastic function F Pi   with respect to a vector of m seed values    k   is the vector of size m where the k   th entry of the vector is the output of F Pi  with   k as the random seed  More formally  fingerprint    k   F Pi        k   F Pi    k  0     k   m  For the remainder of the paper  we use this de   nition of    ngerprints and an implicit global seed set    k   randomly generated as part of the initialization process and held constant throughout  Note that using the same set of random seeds for di   erent parameter values does not a   ect the correctness of Jigsaw   s Monte Carlo simulations  Referring to Figure 3  since the seeds used by each Monte Carlo Generator are i i d  random  inputs to the Estimator Pi  are i i d  samples from query result distribution  Thus  the output of Estimator Pi  remains statistically correct  Using same set of seeds for di   erent parameter values introduces correlated error terms into the outputs of di   erent Estimators  but the Selector only compares  and never combines  the Estimator   s outputs  Mapping Functions  For    ngerprints  as de   ned above  we can de   ne    ngerprint mapping functions Mf that can be applied to each element of a    ngerprint  in order to identify its similarity with another    ngerprint   For example  consider two    ngerprints    1    0  1 2  2 3  1 3  1 5  and   2    0 1  1 3  2 4  1 4  1 6   The mapping functionM x    x 0 1 maps   1 to   2  In general  mapping functions should be   1  easy to parameterize   2  easy to validate   3  easy to compute   4  easily applied to simple aggregate properties  e g   expectation   Given two    ngerprints  Jigsaw can automatically compute a linear function  i e   M      x      x     that maps one    ngerprint to another  if such a mapping exists  Algorithm 2   Linear mapping functions ful   ll our desired characteristics precisely   1  The mapping function can be determined from two distinct values in a pair of    ngerprints   2  The remaining values in the    ngerprints can be used to validate the mapping   3  Linear functions are incredibly simple  and  4  can be easily applied to simple aggregate properties such as expectation and standard deviation  In general  the notion of similarity between two signatures Algorithm 2 F indLinearMapping   1    2  Require  Two    ngerprints   1 and   2 of size m Ensure  A linear function M x      x      such that M   1 i       2 i      i  and null if no such function exists 1            2 1        2 2      1 1        1 2   2           2 1          1 1  3  match     true 4  for i   3 to m do 5  if     1 i        6   2 i  then 6  match     false 7  return  M x      x       if match  null otherwise is application dependent  Therefore  Jigsaw allows users to provide their own classes of mapping functions  Using Fingerprints  With    ngerprints  Jigsaw executes Monte Carlo simulations for di   erent parameter values as follows  Let F denote the entire Monte Carlo simulation with a parameter value Pi  i e   the computation inside the dashed box in Figure 3   Thus  the    ngerprint of F Pi  is essentially the outputs of    rst m simulation rounds with parameter Pi  During execution  Jigsaw incrementally maintains a set of basis distributions  Each basis distribution is a tuple    i  oi   implying that Jigsaw has already computed the output metrics oi for some F Pi  with    ngerprint   i  For a new parameter value Pj   Jigsaw    rst computes    ngerprint   j of F Pj    as part of the    rst m rounds of simulation with parameter Pj    It then checks for a basis distribution with    ngerprint   k such that   j    M   k  If such a basis distribution exists  Jigsaw omits the subsequent rounds of simulation for Pj and returns Mest ok  instead  Retrieving Mapping Functions  When presented with an unknown distribution  Jigsaw compares each new    ngerprint against all the    ngerprints in the basis distribution  identifying a mapping to one of them if it exists  Algorithm 3 shows the process  Jigsaw    rst uses a suitable indexing scheme  described next  to prune the search space of candidate basis    ngerprints  For each pairing candidate  Jigsaw uses the FindMapping function to discover a possible mapping between the two    ngerprints  An instance of the FindMapping function  the FindLinearMapping function shown in Algorithm 2 searches for mappings of the form M x      x       If a mapping exists between two    ngerprints  Jigsaw uses the mapping to reuse work done for the existing basis distribution  If no mappable    ngerprint can be found  Jigsaw completes the simulation process and adds the results  i e   the    ngerprint and computed metric s   to the set of basis distributions  3 2 Indexing Fingerprints The existence of M can be computed quickly for any pair of    ngerprints  However  the expected number of times this test must be performed grows linearly with the number of basis distributions  Instead of naively scanning every basis distribution  Jigsaw builds an index over the basis    ngerprints  The goal of indexing is to quickly    nd a set of candidate basis    ngerprints that are similar to a given    ngerprint  i e   where a mapping exists   The set of    ngerprints returned by the index must contain all similar    ngerprints  In addition  it may contain few    ngerprints that are not similar to the givenAlgorithm 3 F indMatch F  Pa  Require  A stochastic black box function F  and a point in its parameter space Pa  Ensure  The pair  basis M   where basis is a basis distribution     ngerprint     output metrics o   and M is a mapping function such that       M fingerprint F Pa   1          F Pa    i  i      0  m   2  candidates     CandidateF ingerprint basis      3  for all basis     candidates do 4  M     F indMapping basis      5  if M 6  null then 6  return  basis M  7  return        Estimator F Pa     M x    x       ngerprint  these false positives are later discarded in Algorithm 3  Currently Jigsaw supports the following two indexing strategies that reduce the cost of matching linear mapped    ngerprints to a single hash table lookup with high probability  Normalization  The    rst indexing strategy is to translate the    ngerprints to their normal forms so that two similar    ngerprints have the same normal form  and hence can be retrieved by a hash lookup   Such normalization requires a class of mapping function that admits a normal form translation  For example  when using a linear mapping function  a    ngerprint   s normal form can be produced by taking the    rst two distinct sample values and identifying the linear translation that maps them to 0 and 1  or  any two prede   ned constants  respectively  If two    ngerprints have a linear mapping  then all  not just the    rst two  entries of their normal forms will be identical  Sorted SID  Normalization requires that the mapping function admits a normalized representation of a    ngerprint  In some cases  e g   a probabilistic mapping   no such normal form can be computed easily  In such cases  we assign each sample value in a    ngerprint an identi   er  e g   its index position in the    ngerprint   using the same identi   er ordering across all    ngerprints  We then sort the sample values in a    ngerprint  and take the resulting sequence of sample identi   ers  or  SIDs  as the hash key in the index  As long as the mapping function is monotonically increasing  the resultant ordering of SIDs will be consistent across all mappable distributions  Even if the mapping function is only monotonic  a similar e   ect can be achieved by comparing both the SID sequence and its inverse  4  MARKOVIAN JUMPS Jigsaw allows users to specify inter model dependencies  Consider two models where the    rst model predicts the release date of a particular feature of the cloud service  and the second model predicts demand  given that release date  Frequently  such dependencies are cyclical  the feature release date might be driven by demand  For example  su   ciently high demand might convince management to allocate additional development resources to the feature  As a consequence of this sort of cyclical dependency  the models and thus the simulation must be evaluated as a Markovian process  where a model is evaluated in discrete steps and its output for any given step is dependent on the prior step   s output  The discrete steps are usually small  e g   a day in the above example  so that outputs of other models a   ecting the model remain static within a step  Every step in the process must be simulated  even if the only output of interest is for one speci   c step  e g   user demand in two months   In the space of cloud logistics  models with this sort of cyclical dependency often have an interesting characteristic  the Markovian dependency is present only over certain steps  In the case of the feature release date  as long as the user demand remains strictly  or at least with high probability  below or above the threshold value  the feature release date is una   ected  For these periods  the demand and feature release date model can be treated as non Markovian  despite its cyclical dependency  Concretely  Markovian dependencies in this sort of model are characterized as  1  infrequent  and  2  often closely correlated  3  discontinuities in  4  an otherwise non Markovian process  Thus  given the state of the system at the beginning of one of these non Markovian regions  it is possible to create a non Markovian estimator function for the remainder of the region  These infrequent Markovian dependencies occur often in event based simulations  Forcing programmers to identify the ranges within which these dependencies occur is undesirable  Instead  Jigsaw can automatically identify nonMarkovian regions in these processes automatically by using    ngerprints  4 1 Fingerprinting Markov Processes Consider a model F that needs to be evaluated in a sequence  or a chain  of discrete steps  Assuming that Markovian dependencies are infrequent  outputs of F in many successive steps will not be a   ected by previous steps  To jump over such non Markovian steps and avoid expensive computation  Jigsaw uses a non Markovian estimator function E  discussed further in Section 4 2   which predicts the outputs of F at di   erent steps of the chain without considering the outputs  of F or other models  at previous steps  By comparing the    ngerprints of E and F  Jigsaw can e   ciently identify the regions over which E is a valid approximation  Recall that each    ngerprint of F is a set of its random outputs  Thus  the    ngerprint for any step in a Markov process can be used to generate the    ngerprint for the next step  Instead of evaluating the full set of n Monte Carlo simulation rounds of the Markov chain  we evaluate only a    ngerprint sized  m   n  set and compare it to the    ngerprint of an estimator function  If a mapping exists between the two  the estimator remains viable  To compute the value of a Markovian black box function at a particular step in the chain  Jigsaw does an exponentialskip length search of the chain until it    nds a point where the estimator fails to provide a mappable    ngerprint  From that point  it does a binary search to    nd the last point in the chain where the estimator provides a mappable    ngerprint  uses the estimator to rebuild the state of the Markov process  generates the next step  and repeats the process  This algorithm is shown explicitly in Algorithm 4  Consider the previous example of a cyclically dependent user demand and feature release date models  An example execution of the Markov Jump algorithm is illustrated in Figure 4  Jigsaw begins with an estimator for the Markov process that assumes the feature has not yet been released  the initial system state    4 a  It iterates over each step of the Markov process  computing only the    ngerprint and notAlgorithm 4 MarkovJump Fmkv  initial  target  Require  A function  Fmkv prev state    new state describing a Markov process and its estimator  respectively  An initial state for the functions  A target number of steps to return after  A statically de   ned    ngerprint size m  Ensure  The state of each instance of the Markov process after target steps  1  state      initial  initial            1     state 0       m  2  s     1  Fest     Fmkv   1  3  loop 4  for s 2   i     s do 5    i     Fmkv   i   1  6  if  s   target       Fest    M   target  then 7  return M Fest state   8  if Fest s  state 0       m      M   s then 9  s     s     2 10  else 11   valid M      MAXvalid    valid M  valid       s 2   s      Fest    M   valid   12  if valid     1 then state     Fmkv state   valid     1 13  else state     M Fest state   14  target     target     valid  s     1  15    1     state 0       m   Fest     Fmkv   1  the full set of instances being generated  At each step  the    ngerprint of the Markov function is compared to that of the estimator  The number of steps between comparisons grows exponentially until  4 b  the algorithm    nds a mismatch   4 c  At this point  the algorithm backtracks to the last matching value with a binary search and uses the estimator to regenerate the full state of the Markov process   4 d  The Markov process is used to step the full set of instances until the estimator function once again begins to produce matching    ngerprints  4 2 Generating Estimator Functions The user does not need to explicitly provide an estimator function  Simple cyclical dependencies between models allows us to extract an estimator function by    xing one model   s output to its value at a given step  Indeed  any Markov function that models an infrequently discontinuous process can be made into a viable estimator by reusing state in a similar way  A function Fmkv de   ning a Markov process with per step state Pi generates the next step   s state  Fmkv Pi  Q    Pi 1  We can de   ne a rudimentary estimator function Fest i by    xing Fmkv   s input state at one point in time  Fest i Q    Fmkv Pi  Q  Even this rudimentary estimator function can be quite powerful when combined with    ngerprints  any uniform changes in state are absorbed by the mapping function  For example  consider the Markov jump query illustrated in Figure 5  The special CHAIN parameter type is used to chain the output of one stage of the Markov computation to the following one     in this case chaining the output of ReleaseWeekModel to the subsequent DemandModel invocation  As before  ReleaseWeekModel has a single discontinuity at the point where DemandModel   s output exceeds a certain threshold  Each step in the Markov chain corresponds to Figure 4  An example execution of the Markov Jump algorithm  The algorithm starts by performing a    ngerprint sized sampling  a  from the markov chain  until the    ngerprint di   ers from a synthesized estimator  b   then backtracks and synthesizes a new estimator  c   This process repeats until it    nds an estimator valid for the remainder of the chain  d      DEFINITION    DECLARE PARAMETER  current week AS RANGE 0 TO 52 STEP BY 1  DECLARE PARAMETER  release week AS CHAIN release week FROM  current week    current week   1 INITIAL VALUE 52  SELECT ReleaseWeekModel demand  AS release week  demand FROM  SELECT DemandModel  current week   release week  AS demand  INTO results    BATCH MODE        Figure 5  A Jigsaw query with a Markovian dependency predictions for one speci   c week  The interesting output of this model is demand  An estimator from this value will be constructed by    xing release week  the chain parameter  at its initial value  Until the markov process enters the region of the chain  and after it exits  where the discontinuity is likely to occur  the demand model can be e   ectively approximated by this non Markovian estimator  5  INTERACTIVE WHAT IFS Jigsaw   s heuristic approach to sampling is ideally suited to the task of online what if exploration  Moreover  the sort of parameter exploration problems that Jigsaw addresses also bene   t from having a human in the loop   imprecise goal conditions that are di   cult to specify programmatically can often be reached easily by an expert human operator  A human operator indicates which regions of the parameter space are interesting  and Jigsaw provides progressively more accurate results for that region  Metadata supplementing the simulation query allows Jigsaw to interpret the query results and to produce and progressively re   ne a graphical representation of the query output for a given set of parameter values  Unlike its o   ine counterpart  the goal of online Jigsaw is to rapidly produce accurate metrics for a small set of points in the parameter space  Fingerprinting is used primarily toAlgorithm 5 SimplifiedEventLoop p  State  Require  One point of interest p  A lookup table State   containing  for all points  a mapping function M  the point   s    ngerprint     and the point   s basis distribution  1  loop 2       basis M      State p  3  next     p 4  task     T askHeuristic p  5  if task   re   nement then 6  candidate ids      id id 6    basis  7  else if task   validation then 8  candidate ids      id id     basis     id 6        9  else if task   exploration then 10  next     ExploreHeuristic p   Find a nearby point  11  if State next      6     then 12  candidate ids      id id 6    State next  basis  13  else 14  candidate ids      0  10  15  sample ids     P ickAtRandom 10  candidate ids  16  values     EvaluateBlackBox next  sample ids  17  State next         State next         values 18  if State next  basis    M State next     then 19   State next  basis  State next  M      F indMatch State next      20  else 21  State next  basis     State next  basis     M   1  values  improve the accuracy of Jigsaw   s initial guesses  a very small and quickly generated  e g   of size 10     ngerprint allows Jigsaw to identify a matching basis distribution and reuse metrics precomputed for it  Jigsaw provides the following three categories of processing tasks  Re   nement  Once the initial guess is generated  Jigsaw begins generating further samples for points  i e   parameter values  of interest  In addition to improving the accuracy of the displayed results  the new samples are used to improve the accuracy of the basis distribution   s precomputed metrics  Validation  Latency also places stringent requirements on the size of    ngerprints  Larger    ngerprints produce more accurate estimates  but take longer to produce  However  in an online setting  Jigsaw constructs the    ngerprint progressively  In addition to generating additional samples for the basis distribution  Jigsaw also reproduces samples for the points of interest that are already present in the basis distribution  The duplicate samples e   ectively extend the point   s    ngerprint by validating the existing mapping  if the new points do not match the values mapped from the basis distribution  Jigsaw    nds or creates a new basis distribution  Exploration  In addition to the above two processing tasks  Jigsaw heuristically selects points in the parameter space that are likely to be of interest to the user in the near future  e g   adjacent points in a discrete parameter space   For each point explored  Jigsaw either generates a    ngerprint  if none exists   or extends the point   s basis distribution with a small number of additional samples  For clarity  we have drawn a distinction between samples produced for    ngerprints and those produced for basis distributions  However  in most cases there is no di   erence between either process  For any invertible mapping function  samples are generated directly for the point of interest  and mapped back to the basis distribution by the inverse of the mapping function M   1   For example  for linear mappings M x      x       the inverse M   1  x    x           The core of online Jigsaw is a relatively simple pickevaluate update process   1  Pick the next set of  point  sampleID  pairs to generate samples for   2  Evaluate the query  and  3  Update the    ngerprint  basis  and mapping  This process is presented in Algorithm 5  6  EXPERIMENTS In this section we experimentally evaluate e   ectiveness of various optimizations used in Jigsaw  Implementations  The original prototype of Jigsaw is implemented as a C  PDB layer built on top of Microsoft SQL Server  The black box functions are implemented as stored procedure written in C   The C  layer interacts with the SQL Server query execution engine by simply invoking it on subqueries and post processing the results outside DBMS  However  this implementation is not well suited for performance evaluation of Jigsaw  as timing results are polluted by noise from interprocess communication and SQL interpretation and evaluation overheads  In order to achieve a more representative comparison  and to streamline the testing process  we have constructed a second prototype of Jigsaw query evaluation engine entirely in Ruby  without any commercial DBMS   Queries are implemented as black box functions in Ruby  and invoked by a driver process  This simple implementation is representative of how Jigsaw   s functionality can be implemented within a probabilistic database   s query evaluation engine  We compare these two prototypes in Section 6 1  Black Box Functions  Experiments use a variety of black boxes described in Figure 6  Though several synthetic blackboxes are used to identify speci   c performance characteristics  the Capacity  Demand  Overload  User Selection and Markov Step black boxes are permutations of actual Jigsaw use cases in real cloud infrastructure management scenarios  Speci   c numbers  i e   the mean and standard deviation of a normal distribution  have been replaced by ad hoc values  but the structure of these models remains intact  In all experiments  we explore the entire parameter space for particular black boxes  Experimental Setup  Experiments are performed on a 2 4 GHz Core2 Duo with 4 GB of RAM  Except where stated  experiments assume a need for exactly 1000 sample instances per point in the parameter space  and use a    ngerprint size of 10  All results shown are the average of 30 trials  6 1 Comparison of Two Prototypes Figure 7 shows a brief comparison of the relative performance of Jigsaw   s C    MS SQL implementation and the lightweight Ruby engine  As shown  for simple dataindependent queries  the Ruby implementation is able to achieve much better performance  as the dominant cost is that of invoking the black box  rather than the overheads of repeatedly invoking the query processor  The one case where the ruby implementation is not representative of the o   ine engine is black boxes that are heavily data dependent  as in the UserSelection simulation  As might be expected  aCapacity current date  purchase date 1  purchase date 2   The Capacity black box simulates a series of purchases  Each purchase increases the capacity of the server cluster after an exponentially distributed delay  Demand current date  feature release   The Demand black box simulates a simple linearly growing gaussian demand model  As of the feature release week  the growth rate is changed  Overload current date  purchase date 1  purchase date 2   A black box synthesized from Capacity and Demand  Demand   s feature release is ignored  and this black box returns 1 if Demand is greater than Capacity  and 0 otherwise  UserSelection current date   The UserSim black box simulates the per user requirements of each of a set of users  SynthBasis parameter point   A synthetic black box based on Demand  but with a deterministic number of basis distributions  MarkovStep current date  before or after   A simple Markovian process simulating the behavior of Demand with a Markovian dependency introduced between feature release and the prior date   s demand  MarkovBranch prior state   A synthetic black box where at each step  a state counter is incremented by one with a prede   ned probability  The states diverge at some speci   ed rate  Figure 6  Black boxes used to evaluate Jigsaw Model Online Speed O   ine Speed Demand 0 1964 s pc 0 00096 s pc Capacity 0 84525 s pc 0 0028 s pc Overload 5 4625 s pc 0 092825 s pc UserSelect 34 4 s pc 252 454 s pc Figure 7  User Interface Wrapper vs Core Engine Simulator Timing comparison  Values are in time per parameter combination   0  5  10  15  20  25  30 Usage Capacity Overload MarkovStep Computation Time  min  0 06 0 15 0 36 Full Evaluation Jigsaw Figure 8  Jigsaw vs fully exploring the parameter space  database engine   s ability to manage large datasets is superior to that of Ruby  The rest of the experiments in this section use less data dependent black boxes  and hence we use the Ruby implementation  However  relative performance gains demonstrated by the Ruby prototype are roughly similar to that in the C    MS SQL implementation  6 2 Baseline Performance We now analyze the standalone performance gain of    ngerprinting by comparing against a naive generate everything approach  Figure 8 shows timing results for several queries  each evaluated both with and without    ngerprinting  The extremely simplistic Demand model requires only one basis distribution for its entire    5000 point parameter space  and can be evaluated almost instantantaneously  Even relatively complex event based models like Capacity  which has a parameter space of    8000 points  and MarkovStep  evaluated over    2500 steps  require only a few basis distributions  Overload is an interesting case  despite being de   ned as a query over two two black boxes for which Jigsaw can provide a substantial performance boost  compare Overload  0 08  0 1  0 12  0 14  0 16  0 18  0 2  0 22  0 5 10 15 20 Computation Time  ms point  Structure Size Array Normalization Sorted SID Figure 9  Computation time versus the size of structures in the Capacity model with    8000 points and Demand and Capacity   s timing   the joint query is only computed in half the time  The reason for this is that the query produces a boolean result  the output of a comparison between two values  and information about the two original values is lost  E   ectively  Jigsaw is unable to reuse basis values by re mapping them  This strongly suggests that Jigsaw   s techniques can be further improved by incorporating them into a database engine with a symbolic execution strategy  e g   13    In such a system  database operations between random variables  i e   VGFunction generated values  mapped from the same basis dis</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10sn1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s10sn1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Social_networks_&amp;_community_data"/>
        <doc>Load Balanced Query Dissemination in Privacy Aware Online Communities ### Emiran Curtmola  Alin Deutsch UC San Diego  ecurtmola deutsch  cs ucsd edu ABSTRACT We propose a novel privacy preserving distributed infrastructure in which data resides only with the publishers owning it  The infras  tructure disseminates user queries to publishers  who answer them at their own discretion  The infrastructure enforces a publisher k  anonymity guarantee  which prevents leakage of information about which publishers are capable of answering a certain query  Given the virtual nature of the global data collection  we study the chal  lenging problem of efficiently locating publishers in the community that contain data items matching a specified query  We propose a distributed index structure  UQDT  that is organized as a union of Query Dissemination Trees  QDTs   and realized on an overlay  i e   logical  network infrastructure  Each QDT has data publishers as its leaf nodes  and overlay network nodes as its internal nodes  each internal node routes queries to publishers  based on a sum  mary of the data advertised by publishers in its subtrees  We exper  imentally evaluate design tradeoffs  and demonstrate that UQDT can maximize throughput by preventing any overlay network node from becoming a bottleneck  Categories and Subject Descriptors  H 2 4  Database Manage  ment   Systems   Distributed databases General Terms  Design  Performance ### 1  INTRODUCTION During the last decade  the web has enabled unparalleled access to the vast amount of electronic data that is continually being cre  ated  and search engine technology has made it feasible to issue queries and locate web sites that contain data of interest to a user  As the web evolves  two significant new trends are emerging  First  write access to the web is becoming increasingly democratic as it is easier for a large number of users to create and publish data on a wide variety of topics  this is evident from the proliferation of blogs  Wikis  e g   Wikipedia   user generated videos and photos  etc  Second  it is becoming easier to form web communities based on shared interests  this is evident in the considerably popularity of social networking sites like Facebook and MySpace  With the confluence of these two trends comes the natural desire to freely exchange data within the community     this includes making one   s Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specific permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     10 00  K K  Ramakrishnan  Divesh Srivastava AT T Labs     Research  kkrama divesh  research att com own data collection accessible to others within the community  and also being able to query  tag  and comment on the global collection that is the union of all local data of users within the community  Recent events have called attention to the pressing need to en  hance the infrastructure of online communities to enable freedom of speech without fear of retribution against the community users  People have come to learn that their online blogs along with the mainstream news websites can be easily censored  or worse  the true identity behind their online nicknames can be revealed  This information can be used to censor or discriminate certain individ  uals pertaining to various online activist groups or dissidents  To fully deliver on the promise of freely exchanging data  any commu  nity supporting infrastructure needs to enforce the key requirement of preserving the privacy of publishers  That is  there should be no easy way for any third party to infer the identity of publishers of documents on specific topics  This privacy preserving publishing requirement precludes some obvious approaches that reuse and build on existing centralized technologies  e g   search engines  hosted online communities  etc  While these models are designed to handle the large number of po  tential publishers and the dynamic nature of published data  en  abling a straightforward query access to the global data collec  tion  the downside is that publishers are disintermediated from con  sumers by the central site   i  The central site has control over the visibility of publishers to user queries  and can effectively censor publishers by choosing to not index them  and  ii  The central site has complete knowledge of all information created and advertised by publishers  Even under the unrealistic assumption that the cen  tral site is trusted by publishers  the site is vulnerable to third party censors1 and attackers  For this reason  we advocate a decentralized approach where there is no central authority  and the global data collection is vir  tual  More specifically  we make the following contributions  1  We propose a distributed privacy preserving publishing in  frastructure in which data resides only with its owner  The infras  tructure disseminates user queries to publishers  who answer them at their own discretion  Moreover  the way publishers advertise their data  in order to receive relevant queries  is designed to pre  vent any third party from pinpointing which publisher advertises what data  even when extensively colluding with or attacking com  munity members   2  Given the virtual nature of the global data collection  we ad  dress the challenging problem of efficiently disseminating queries to publishers that contain data items matching a specified query  We propose a distributed index structure  UQDT  that is organized as a union of query dissemination trees  QDTs   and realized on an 1See  for example  National Coalition Against Censorship  http   ncac org   and OpenNet Initiative  http   opennet net   overlay  i e   logical  network infrastructure  Each QDT has data publishers as its leaf nodes  and overlay network nodes as its in  ternal nodes  each QDT internal node maintains a summary of the data advertised by publishers in its subtrees  Unlike Distributed Hash Tables  DHTs   no QDT node has complete knowledge of all the publishers that publish an advertised data item  3  We define a notion of publisher k anonymity which guaran  tees that for every publisher p and published data item d  the infor  mation stored in the UQDT  as well as the communication required to maintain the UQDT  are insufficient to distinguish p from k     1 other potential publishers of item d  We show how to configure the UQDT to guarantee publisher k anonymity even when an arbitrary number of UQDT nodes are compromised by hacking  subpoena  collusion  or impersonation attacks  4  The adoption of the UQDT solution hinges on its perfor  mance  We present algorithms that use the UQDT for routing queries to publishers efficiently  following the parent child links from a QDT and making effective use of the advertised data summaries maintained by QDT internal nodes  While a single QDT suffices in principle to route queries  this results in congestion at the upper levels of the QDT  severely limiting the throughput of the overall index structure  and making it potentially vulnerable to Denial of Service attacks  We build on well known techniques for scalable dissemination trees and for    Russian Doll    search over sets  24   We show how UQDT can achieve load balancing and throughput maximization for a workload W by a judicious combination of  i  Overlaying multiple QDTs over the network  each with a dis  tinct root  and arranging for queries in W to be channeled in paral  lel through distinct QDTs  and  ii  Maintaining limited selectivity information about data items to help inform the routing strategy  To the best of our knowledge  there are no works that combine multi  ple trees for load balancing and hierarchical summaries for ad hoc query routing in distributed systems  5  We experimentally evaluate UQDT design tradeoffs through extensive simulations  using a real Wikipedia collection comprising about 1 1 million documents of total size 8 6GB  We demonstrate that UQDT can maximize throughput by preventing any overlay node from becoming a bottleneck  In addition  we show in  13  how UQDT empowers information publishers to join democratic communities and query their global collection in an ad hoc fash  ion using expressive queries  To this end  we explore various QDT topologies  e g   Scribe  6  generated multicast trees  as well as bal  anced structures   number of QDTs  and routing strategies  based on the selectivity information maintained   and show that  i  One can statically identify a near optimal number of QDTs for any spec  ified QDT topology  which maximizes throughput by preventing any overlay network node from becoming a bottleneck  and  ii  Main  taining selectivity information about a few popular data items  2     3   achieves considerable gains over random routing  and is al  most as good as a    fully informed    routing strategy  Paper Outline  We start with an overview of our proposed frame  work and the space of design tradeoffs in Section 2  Section 3 presents our operation choices  followed by the analysis of pub  lisher k anonymity in Section 4  Experimental setup and results are presented in Sections 5 and 6  We discuss related work in Sec  tion 7 and then conclude  2  OVERVIEW OF OUR FRAMEWORK Data and Query Model  For the purpose of information discov  ery and flexible querying  we abstract information as collections of data items  where each data item is described by a set of content descriptors  CDs   CDs are an abstraction of keywords  terms  or other atomic information units  30   For instance  in information retrieval applications  data items are text documents  and the CDs are the terms appearing in them  In relational databases  collections are tables  data items are tuples  and CDs are  attribute value  pairs  Further examples are given in Section 5  Given a data item D  we denote its set of CDs with cd D   We consider queries expressed as sets of CDs  and denote the set of CDs of query Q with cd Q   We say that data item D matches query Q if cd Q      cd D   Notice that the case of matching con  junctive keyword queries against text documents  the most com  mon Information Retrieval operation  corresponds to the particular case in which CDs are keywords  Given a data collection D  the result of Q on D  denoted Q D   is the set of data items in D that matchQ Q D    D   D DmatchesQ   Communities of Data Publishers and Consumers  We con  sider communities of autonomous publishers  who join the com  munity with their own locally stored data collection and make it available for querying  In return  they can query the global collec  tion consisting of the union of all local collections  Given our focus on providing democratic community access for autonomous publishers  we adopt a decentralized approach  In this setting there is no central control authority  the global collection is virtual  data resides only with the publishers owning it  The advan  tage is that publishers maintain complete control over who accesses their content  and how the content is  advertised  to the community  The challenge is efficient query evaluation while avoiding naive broadcast of queries to all publishers  We propose a distributed index structure that supports sending a query Q to all publishers relevant to Q while minimizing the number of irrelevant publishers reached by Q  We say that a publisher is relevant to Q if one of its local data items matches Q  Our indexing solution targets a service oriented logical network  in which we distinguish two types of nodes  There are data pub  lisher nodes  community members  that provide data services and connect to the network via direct links to nodes at its edge  The data are indexed inside the network  which consists of a set of inter  connected and reconfigurable router nodes  These are responsible for routing queries to the relevant publishers  In an internet scale distributed setting  it is natural that routers are controlled by a mul  titude of distinct network providers covering different autonomous administrative domains  Thus  no single provider controls more than a fraction of the entire network  and the resulting architecture is not centralized  While different queries might hit the same set of nodes  our goal is to balance the community search generated load at routers during query dissemination while preserving low space usage of index at a node and still preserving publisher k anonymity  Design Requirements  We consider the following key require  ments on the infrastructure design  First  published data should not be relinquished to anyone but to community members  and only by answering queries upon successful credential authentication of the query issuer  Note that harvest index query methods  e g   cen  tralized solutions  fail  Second  publishers should advertise just enough information in the community to be reached by user queries without disclosing their identity  Publishers advertise the contents of their local store by declaring a set of CDs appearing in their lo  cal collection  Note that not all existing CDs need to be declared  especially if they pertain to private data items  The advertised infor  mation plays the role of a distributed index that is described next  Publisher k anonymity  We propose a notion of privacy that protects community members by preventing an attacker from as  sociating them with the CDs they advertise  We define publisher k anonymity  detailed in Section 4   which guarantees that for ev  ery publisher p and published CD d  the information stored in the Publisher p CDs declared by p  cd p  P1 P2 P3 P4 P5 P6 P7 P8 Peking  Tibet  stocks  train  money Peking  yak tea  Hong Kong  train Peking  Tibet  yak tea  Hong Kong  money Peking  freedom  yak tea  stocks  money Peking  freedom  yak tea  stocks  money freedom  Tibet  stocks  money freedom  yak tea  stocks  money freedom  yak tea  stocks  money infrastructure  as well as the communication required for mainte  nance  are insufficient to distinguish p from k     1 other poten  tial publishers of d  The distributed index guarantees publisher k  anonymity even when the UQDT nodes are compromised by hack  ing  subpoena  collusion  or impersonation attacks  Query Dissemination Trees  We propose the organization of the internal nodes into a logical tree we call a Query Dissemination Tree  QDT   The internal QDT nodes are routers  the publishers are leaves  Regardless of which querier initiates a query Q  Q is sent to the root of the QDT  whence it propagates down the tree to the publishers  The intention is that  when Q reaches a node n with no publishers in its subtree that are relevant to Q  n prunes its subtree from the search  i e  it does not forward Q to its children  This pruning saves the network traffic and processing at n   s descendants  One immediate technical difficulty associated with this goal is how to instrument the index to efficiently determine that none of n   s descendant publishers are relevant to Q  Of course  it is infea  sible to maintain at every node n the collection of all data items in n   s subtree as being prohibitively wasteful in terms of space  It would also defeat the purpose of preserving privacy of publishers  it would require a publisher p to trust every router on the path lead  ing to p from the root  This is an unrealistic prerequisite  We present in two steps the way routers exploit the advertised CDs by publishers  In a first cut  we assume that it is feasible to store at every node n the set cd n  of all CDs declared by publish  ers located in n   s subtree  we revisit this assumption shortly   This assumption is supported by empirical evidence that  for real data sets  the overlap of CDs across data items in a collection is con  siderable  and the union of all CDs  with duplicate removal  is or  ders of magnitude smaller than the combined size of the collection  For instance  in Section 5 we describe a collection of 1 1 million Wikipedia articles of combined size 8 6 GB that has only 3 2 mil  lion distinct CDs  Note that when only cd n  is stored at a router n  n does not know which CD appears in which publisher  nor which sets of CDs appear together in a data item  This offers publishers an added degree of protection against compromised routers  Query Routing in Single QDT  In this setting  we consider the following simple query routing algorithm  Every query Q posed by a querier p is initially sent to the root of the QDT  in a message containing both Q and p   s address   When a router node n receives the message  it forwards it in parallel to each of its children in QDT if and only if cd Q      cd n   When a publisher node is reached  it sends back to p the result of Q against its local collection  Note that when cd Q        cd n  holds  it is guaranteed that n   s publisher descendants are irrelevant to Q  Therefore  the first cut routing algorithm never prunes relevant publishers  thus ensuring that the final result of Q over the global collection is computed in full  In contrast  when cd Q      cd n  holds  it is not necessarily the case that at least one publisher in n   s subtree is relevant to Q  This is because the CDs in cd Q  may not be co located in the same data item  or even at the same publisher  Therefore  the first cut algo  rithm may forward queries unnecessarily  generating non minimal traffic and processing load  This is a result of the unavoidable trade  off between censorship resistance and evaluation performance  EXAMPLE 2 1  Throughout the paper we use the following running example  Consider a network of 25 nodes that integrates news from 8 newspaper websites P1        P8  the remaining 17 nodes are routers   Figure 1 a  shows the CDs declared by each publisher  i e   simple keywords   Consider also a query workload consisting of the four queries shown in Figure 1 b   Without showing the ac  tual documents  assume that for every query Q there is at least one newspaper website that publishes a document matching Q  Assume for now that the routers and publishers are organized  a  Publishers    declared CDs   b  Query workload  Figure 1  Running Example Setup in the single QDT configuration QDT1  shown in Figure 3 a   The router nodes are identified by their preorder traversal rank  For simplicity  we assume that it is feasible for each node n to store all CDs declared by the publishers in its subtree  cd n   For exam  ple  node 2 stores all CDs published by P1 and P2  thus cd 2     Peking  Tibet  stocks  train  money  yak tea  Hong Kong   Query Q cd Q  Query Q cd Q  Q1 Q2 Peking  freedom Tibet Q3 Q4 train Hong Kong  money Lvl  Node Time Unit 12345678 1 node 1 Q1 Q2 Q3 Q4 2 node 2 node 8 node 13 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 3 node 3 node 9 node 14 node 16 Q2 Q3 Q1Q2 Q4 Q1 Q2 Q1 Q2 4 node 4 node 6 node 10 P4 P5 node 17 node 20 node 23 Q2 Q2 Q1Q2 Q1Q2 Q1 Q2 Q2 Q2 Q3 Q3 Q4 Q4 5 P1 P2 P 3 node 18 node 21 node 24 Q2 Q3 Q3 Q 2 Q2 Q 4 6 P6 P7 P8 Q2 Figure 2  Query Dissemination in Single QDT Configuration For simplicity  let us consider in this example that every node can process exactly one query per time unit  If all queries in the workload are issued simultaneously at time 0 and processed in the order Q1 to Q4  then Figure 2 shows their dissemination according to the first cut routing algorithm  For example  regardless of the issuing node  query Q3 is disseminated in QDT1 starting from the root node  node 1   which is congested and can only process Q3 at time unit 3  Because train is contained in cd 1   Q3 is forwarded to all of node 1   s children  in this case to 2  8 and 13  where the dissemination continues recursively  Since train does not appear in the CD sets of nodes 8 and 13  their subtrees are pruned   However  node 2   s CD set does match Q3 and the query is routed down to node 3 at time unit 5  then to nodes 4 and 6 at time unit 6  Both these nodes have a match and Q3 reaches the publisher nodes P1 and P2 at time unit 7  Each of the two publishers runs Q3 on its local collection and sends the result back to the issuing node      CD Set Summaries  We now revisit the assumption that all CDs in cd n  are stored with every router n  We address the case when cd n  is larger than can be comfortably stored at a router n with available memory of size M   To this end  we observe that we do not necessarily need to keep the exact set cd n   Instead  it suffices to store a summary smmM thereof at node n  We choose to represent smmM as a Counting Bloom Filter  5  18  of size M for its well  known properties  compactness and probabilistic set membership of CDs  i e   no false negatives  control over false positives rate   We obtain the final version of our routing algorithm by replacing in the above first cut every containment test with a call to a Bloom filter set membership test  Note that false positives do not affect the correctness of query evaluation  Throughput Maximization  We have so far confined our dis  cussion to the routing of a single query through the network  We next extend our solution to handle query workloads  sets of queries   We start by observing that the arrival of a query at node n triggers measurable computation effort pertaining both to the processing of the query and to its forwarding to n   s children  This limits the number of queries passing through n per time unit and can lead to congestion  Since queries pruned at upper levels in the tree never reach the lower levels  the fraction of any workload W reaching node n is a subset of the fraction reaching its ancestors  In particu  lar  the root becomes a bottleneck since it is reached by all of W   In contrast  edge routers at the leaves are reached by relatively small fractions of W and may not be heavily utilized  E X A M P L E 2 2  Revisiting Figure 2  observe that the num  ber of query messages reaching the nodes is significantly skewed among the tree levels  and ultimately among the nodes  decreasing from the root to leaves  Because all queries touch the top 2 levels  their nodes receive 4 messages each  while nodes on the lower lev  els receive 0  1  2 or 3 messages  Overall  it takes a total of 8 time units to disseminate all queries  of which the root alone introduces a delay of 4 time units  while nodes 21 and 24 remain idle      We propose to alleviate congestion at the upper levels of the QDT by spreading the load more uniformly across the nodes  Currently  there are two main solutions to achieve this  One class of algo  rithms replicate data  or indices of it  redundantly at the router nodes  Thus  each router can initiate to answer queries  Never  theless  this incurs increased updates cost as well additional space cost to store all replicas which is inappropriate with our initial set of goals  In contrast  we propose to partition the global data collection and interconnect the publishers for each partition block in a differ  ent overlay  We show next how this technique alleviates congestion while still preserving the space usage at routers  Therefore  our solution consists in overlaying multiple QDTs over the network  each with a distinct root  and arranging for var  ious fractions of W to be channeled in parallel through distinct QDTs  Since all QDTs are supported by the same underlying logi  cal network  a network node n participates in several QDTs  receiv  ing and forwarding queries via each of them  Balancing the load involves arranging for the distribution of levels associated with n to be  as close as possible to  uniform across the set of all QDTs  For example  the fact that n receives a high fraction of the queries flowing through QDT1 because it resides on an upper QDT1 level  is compensated by n being reached by only a small fraction of the queries flowing through QDT2  where it resides on a lower level  The goal of splitting the query workload into fractions that flow through distinct QDTs raises two fundamental technical obstacles  The first pertains to controlling memory consumption at the router nodes  If a node n participates in multiple QDTs  it must maintain separate summaries for each of its subtrees  It is important that the total space used by the union of all summaries associated with n should not exceed the space used by n   s summary in the single  QDT configuration  We satisfy this requirement by arranging for each of n   s summaries to pertain to disjoint CD sets  To this end  we partition the space of all possible CDs into a number of k dis  joint blocks P    Bi  1   i   k    We discuss shortly what consider  ations go into picking the value of k  and we describe in Section 3 how the partitioning is achieved in practice   We call each Bi a CD block  We assign to each CD block its own QDT  obtaining a family UQDT    QDTi  1   i   k   The second problem is the query semantics preservation  we need to ensure that  by being routed only on a single QDT  a query is guaranteed not to miss any relevant publishers  We achieve this soundness property by requiring each QDT to satisfy the following        QDTi contains as leaves all publishers whose local data collection has at least one CD in common with Bi  We defer to Section 3 the discussion on how the internal nodes of each QDTi are organized  Query Routing with Multiple QDTs  For every query Q  we pick the QDT to send it to as follows  The partition P induces a par  tition PQ    Qj 1   j   m on cd Q   such that for each Qj     PQ thereisBi    PwithQj  cd Q    Bi WecalleachsuchQj a query block and we say that the CD block Bi corresponds to Qj   Note that by definition each query block corresponds to precisely one CD block  which in turn corresponds by construction to pre  cisely one QDT  Given a query block Qj     PQ  we can therefore refer to    the    corresponding QDT  and denote it with qdt Qj    In general  Q has 1     m      cd Q   query blocks  with corre  sponding QDTs qdt Q1           qdt Qm   For routing Q  we only pick one of these QDTs  say qdt Qj    Regardless of how this pick is taken  we send to the root of this QDT a message containing three components   Qj   Q  p   where p is the address of the initiat  ing querier  qdt Qj   routes this message as described above in the single QDT case  with only three minor refinements      since every internal node n can participate in various QDTs  n stores one summary smmMT per QDT T       n uses Qj for routing in qdt Qj   i e  for lookup into the summary n smmMqdt Qj      and     leaf nodes use Q for evaluation against their local data  EXAMPLE 2 3  Example 2 2 shows how the congestion ap  pears inevitably in the upper levels of the dissemination tree  Here  we show how congestion can be alleviated by using multiple QDT overlays over the same nodes  We consider a configuration of 4 QDTs  each corresponding to a block in the CD space partition P  P is shown in Table 1  Block  CDs B1    Peking  freedom B2 Tibet  yak tea B3 Hong Kong  stocks B4 train  money Table 1  Blocks of the 4 Partition In general  internal nodes can be connected in any configuration at the network overlay layer  Figure 3 depicts 4 possible QDTs  one per CD space partition block  Table 2 shows the CD summaries maintained at every router  Since a router appears in multiple QDTs  it actually manages a set of summaries  For simplification purposes  we assume that each summary stores the exact set of CDs rather than its approximation  Figure 4 presents the routing diagram in the 4 partition over time  where queries Q1  Q4 are issued simultaneously at time 0  Query Q1 is a conjunctive query both of whose CDs fall in the first partition block B1  The only routing choice is hence the tree corresponding to B1  namely QDT1 shown in Figure 3 a   Since P     s blocks are disjoint  single conjunct queries also have only one Tree QDT1 QDT2 QDT3 Node  Data summary 4  6  3  2  10      Peking Node Time Unit 123456 node 1 node 2 node 8 node 13 node 3 node 9 node 14 node 16 node 4 node 6 node 10 P4 P5 node 17 node 20 node 23 P1 P2 P3 node 18 node 21 node 24 P6 P7 P8 Q1 Q2 Q4 Q1 Q4Q2 Q1 Q1 Q2 Q2 Q1 Q2 Q1 Q2 Q1 Q4 Q2 Q4 Q2 Q4 Q2 Q4 Q2 Q3 Q4 Q3 Q4 Q2 Q3 Q2 Q4 Q2 Q3 Q3 Q1 Q1 Q2 Q1 Q4 Q3 Q4 Q 4 Q2 Q2 Q2 Q2 Q3 Q 2 Q3 Q3 9  8  14  13  1 18  17  21  20  24  23  16 20  2  21      Tibet yak tea Tibet  yak tea Peking  freedom freedom 23  10  8  24  13  1 4  9  18  6  14  17  16  3 24 18 9 8 14 13 16 3       stocks 21  17 1  2 20  6  23  10  4 9  1  18  2  6  14  10  16     QDT4   money 17  4  8  21 3 13  24  23  20 Table 2  CD Summaries in the 4 QDT Configuration 13 2 8 13 9 14 16 3 9 14 16 4 6 10 17 4 6 10 P P 17 20 23 20 23 18 P P 21 24 1 45 45 Hong Kong Hong Kong  stocks train train  money P1 P2 P3 18 21 24 P1 P6 P7 P8  a  QDT1 for B1  4 20 P2 P3 2 8 13 Figure 4  Query Dissemination in 4 QDT Configuration multiple block queries  Proposition 1 uncovers an optimization op  portunity  the judicious QDT choice  out of several equally sound alternatives  towards throughput maximization  So we treat the spectrum of possible routing strategies as an optimization dimen  sion in its own right  The UQDT Design Space Layout  We remark that the number k of blocks in the partition P of the CD space defines a spectrum of possible configurations of the same network  thus adding a new di  mension to the optimization space  One extreme of this spectrum is the case k   1  which we have discussed above as the single QDT configuration  At the other extreme  we have the case in which each block of P is a singleton CD  We refer to it as the per CD configuration  We argue next that neither of the extremes results in optimal throughput  and that the value of k is an optimization di  mension we need to explore  Indeed  Example 2 1 and Example 2 3 show that the single QDT configuration is certainly not optimal  being outperformed by a 4 QDT configuration for the given query load  At the same time  constructing too many QDTs is counter  productive  since the increase in k decreases the size of the query blocks  thus resulting in less selective lookups in each node   s sum  mary  This translates into less pruning  i e  more query forwarding messages  the 4 QDT configuration in Example 2 3 generates 50 messages  as opposed to the 46 of the single QDT configuration in Example 2 1  In conclusion  as k increases  we observe two oppo  site effects  an increase in load balancing potential  but also in the overall load  number of messages  in the network  An independent consideration that precludes extremely high values of k is that the maintenance of any overlay network involves a small  but non zero control traffic overhead  6   Maintaining too many QDTs would amplify this overhead  In Section 3  we discuss the following optimization issues  all of which have significant impact on query throughput  How can a partition P of the infinite space of all possible CDs be chosen and represented finitely  this includes determining the value for k   How can P be used to efficiently determine PQ  How are the various QDTs corresponding to P organized for better throughput  How does the choice of QDT  the pick of j  impact throughput  3  OUR APPROACH In Section 2  we have provided an overview of our proposed so  lution for query dissemination  identifying the dimensions of the space of possible implementations  We delegate to Section 4 the P6 P7 P8 23 18 21 2124128  b  QDT2 for B2  61017 20 23 18 24 1 2 P P 8 45 45 13 3 13 3 9 P P 14 16 4 P1 P2 P3 9 14 16 P1 P2 P3 6 10 17 P6 P7P8 P6P7P8  c  QDT3 for B3   d  QDT4 for B4  Figure 3  Query Distribution Trees for the 4 Partition routing choice  For instance  Q2 and Q3 are routed using QDT2 in Figure 3 b   respectively QDT4 in Figure 3 d   Q3   s routing on QDT4 by CD train is highlighted in Figure 4  In contrast  query Q4 intersects CD blocks B3 and B4  which in  duce two query blocks  PQ4     Hong Kong    money    This offers two routing alternatives  either by using CD Hong Kong on QDT3  or by using money on QDT4  In the diagram  we assume that QDT3 was picked  When the subquery hits publishers P2 and P3 the full query Q4 is tested on the local store  only P3 has a match for both CDs of Q4   Comparing with Example 2 1  notice that the 4 QDT configu  ration outperforms the single QDT case  the former takes 6 time units to complete the dissemination  while the latter needs 8  The improved throughput is due to better load balance  contrast the behavior of routers 21 and 24  which remain completely idle in Figure 2 but shoulder part of the dissemination task in Figure 4  Finally  observe that the benefit of better node utilization out  weighs the drawback of using query blocks for pruning  instead of the entire  and more selective  set of query CDs  Indeed  the 4 QDT configuration wins despite its less aggressive pruning which leads to slightly more messages  50  as opposed to 46 for one QDT       It is easy to check that property       implies the soundness of our query evaluation algorithm  PROPOSITION 1  For every query Q  partition P  and pick of j  our query routing algorithm correctly computes Q   s answer  Obviously  for single block queries there is no choice and the QDT is uniquely determined  However  in the general case of discussion of how to configure and maintain the UQDT to ensure publisher anonymity  As a proof of concept for the viability of our approach  we developed an actual implementation  described in this section and evaluated experimentally in Section 6  QDT Topology  There are many possible topologies for organiz  ing the router nodes into a QDT  Although our solution is generic  we investigate two approaches  First  we take the pragmatic approach of    piggy backing    on top of a mature overlay tree building approach to disseminate messages to groups of nodes  also known as multicast groups   Since multi  cast overlay trees are constructed with a different goal than QDTs  it is not immediately clear that they are optimal for query dis  semination  though we show experimentally that we can    convert    them  achieving very good performance   However  one advantage of delegating the QDT construction to such off the shelf technol  ogy is that it is equipped to exploit information on the topology of the underlay network with minimal control overhead  Moreover  it maintains overlays dynamically  adapting to the change in underlay network conditions  One widely used representative of this class of tools is Scribe  6   and FreePastry  32  is one popular open source implementation  which we used  In addition  we consider home grown QDTs built for the ex  press purpose of balancing the forwarding effort among the routers  Since every router forwards a query to each of its children  the for  warding effort is linear in the node   s fanout  This suggests con  structing  nearly  balanced QDTs  with as little variation as possi  ble in the node fanouts  We need to construct such trees ourselves  since Scribe does not guarantee balanced trees  QDT Maintenance  When a publisher p joins the community  it declares a set cd p  of CDs it is willing to answer queries about  Recall from Section 2 that  to preserve soundness of query evalu  ation  we must satisfy property        To this end  we determine  as described shortly  all the CD blocks with non empty intersection with cd p   which in turn lets us identify all QDTs that p must join  The act of joining a given QDT is taken care of by Scribe which identifies the router node that will become the new publisher   s par  ent  Once the publisher is added to QDT T   the CD summaries of all its ancestors in T are updated by inserting cd p  into them  This insertion is implemented by simply obtaining once and for all the set of indices ind cd p    which is then passed bottom up from p to T    s root  so that every router on the way can increment its cor  responding Bloom filter counters  When p leaves a QDT T  the index set ind cd p   is also sent bottom up to p   s ancestors in T   each decrementing the corresponding counters  The case when an existing publisher p changes its list cd p  of declared CDs leads to the propagation of similar counter operations  Partitioning the CD Space  An important issue we need to ad  dress is how to represent the partition P of the CD space finitely  and how to efficiently determine which block a given CD belongs to  As described above  we need this test to quickly identify the QDTs a new publisher must join  Moreover  the same test is re  quired to compute the induced partition PQ of a query Q  in order to identify the QDT candidates for routing Q  We describe here our solution assuming that we have already established the number k of blocks in P  we discuss below how we determine k with an eye on load balancing   Given k  we implement P simply as a hash functionhP fromCDstotheset 1     k  wherehP distributes CDs uniformly over its range  Then each block Bi     P consists of allCDsmappedbyhP toi Bi    d disaCD hP d  i   Of course  each CD block is potentially infinite so we never really materialize it  Indeed  we don   t need to  all we need is to quickly determine  given a CD d  which CD block it belongs to  This oper  ation is implemented as a constant time invocation of hP  d   Load Balancing  The way we determine the number k of QDT trees  as well as their actual construction  are motivated by the goal of spreading the load evenly across routers  In the following  we de  note with Nr the number of router nodes in the service provider   s overlay network  and with Np the number of publisher nodes  Since in any QDT T   every router node is reached by a larger fraction of the query flow through T than its descendants in T   we need to en  sure that for every router n  the distribution of QDT levels n resides at is close to being uniform  We adopt a solution which is certainly not the only possible one  nor necessarily optimal  but it is easy to implement and  as proven experimentally in Section 6  it yields excellent performance  We start by constructing  using Scribe  a single QDT T1 whose internal nodes are the Nr routers and whose leaves are the Np publishers  Scribe tends to build trees of low height  in which the root has a significant fanout that dominates the fanouts of nodes in lower levels  The root and its children receive by far the highest fraction of queries flowing through the tree  and are hence in most need of relief through load balancing  Denoting with Nu the number of nodes on the top 2 upper lev  els in T1  Nu   1  number of router children of the root   we construct k      Nr     QDTs   Ti 1   i   k  Each Ti is an isomorphic Nu copy of T1  whose nodes are obtained by keeping the same Np leaves and only re shuffling the Nr internal nodes as follows  To completely specify Ti  we show how its Nr internal node positions are populated with the actual Nr routers  This can be formalized as a function ai from the set of Nr routers to the set  0     Nr    1  of positions in T1  We adopt the convention that the position of node n corresponds to n   s rank in the breadth first  left to right traversal of T1  position 0 is the root   Let    n      n     Nu  mod Nr be the right to left cyclic permutation with step Nu on  0          Nr     1   If a1 specifies the initial QDT T1  then for each 1   i     k  we populate Ti by cyclically permuting with step Nu thenodesofT1 atotalofi   1times ai     i   1    a1  EXAMPLE 3 1  In Example 2 1  there are Nr   17 routers  and the root of the initial tree QDT1 has three children  yielding Nu   4  We compute k      17      4 and construct the 4 trees 4 in Figure 3  Notice that the trees in Figure 3 b   3 c   3 d  are obtained by cyclically permuting to the left by 4 steps the tree in Figure 3 a  once  twice  respectively three times      It is easy to see that our methods of determining the number of QDTs  and of populating them  ensures the following fairness property  all routers appear precisely once in the top 2 levels of any QDT  Furthermore  the k level values associated to every router are distributed almost uniformly over all possible level values in T1  For instance  in Figure 3  router 1 appears on levels 1  4  4  3  Finally  note that building k   1 QDTs actually degrades the load balance  because the additional cyclic permutation causes a    wrap around    that returns some of the routers residing on the top two levels in T1 to the top two levels of Tk 1  subjecting these routers to unfair load  since we use the floor function to determine k  the wrap around is not necessarily complete   In general it fol  lows that  to maximize balance  we want to use a number of QDTs that is a multiple of     Nu      In Section 6  we validate this rule ex  Nr perimentally  also showing that choosing multiples higher than 1 is unnecessary  they do not improve load balance  while leading to higher control overhead  Routing Strategies  We next discuss how a node n that initiates a query Q picks the QDT to route Q on  First  n uses the hash func  tion hP described above to compute PQ    Qj 1   j   m  which in turn determines the set of candidate QDTs  qdt Qj  1   j   m  If m   1  n picks one of these candidates  We consider several alter  natives for implementing this pick  A simple solution is to choose 1   j   m at random  in the hope that randomness avoids sending many queries down the same QDT and alleviates congestion  We call this the random routing strategy  We also consider alternative strategies  all attempting to alleviate the effect we discussed in Section 2  as the number of QDTs in  creases  the selectivity of query blocks decreases  recall that  when routing Q through QDT qdt Qj    only the CDs in Qj are looked up in the summaries   This results in increased overall query for  warding and processing in the network  To compensate for this effect  the routing strategy should ideally use the most selective query block Qj for routing  as this results in the most aggressive pruning of qdt Qj     s subtrees during Q   s dissemination  Identifying the most selective block of a query is not trivial  as it requires determining the frequency of every CD in the global col  lection  and storing these statistics  or making them otherwise ac  cessible  at every publisher  We call the strategy assuming each publisher   s access to this information the fully informed routing strategy  Assuming independence between the CDs  the publisher initiating Q computes the selectivity of a query block Qj as the product of the individual frequencies of the CDs in Qj  Fully  informed routing is very expensive in terms of both space and traf  fic  Indeed  for large global collections  the number of CDs can be considerable  Moreover  space consumption is exacerbated by the fact that the frequency must be stored with every potential query initiator  A more serious problem is the traffic arising because the global collection is virtual  gathering and maintaining the appro  priate statistics requires constant communication between nodes  We therefore investigate a less ambitious strategy  instead of identifying the most selective query block for Q  its initiator p only tries to avoid using the least selective ones  It suffices to this end to maintain and store at each publisher a short list of the s least selective  most frequent  CDs in the global collection  with s a rel  atively small value ensuring small storage space and maintenance traffic consumption  Finding the overall top s most frequent CDs amounts to the distributed top s heavy hitters estimation  2  28   We implement a simple solution that exploits the already existing QDT overlays  employing them in a dual role as multicast  data dis  semination  trees  With every CD they advertise  publishers declare its frequency in their local collection  Each node n maintains a list n L of length at most s entries  each containing a CD and its fre  quency  For non root routers  the list gives the s most popular CDs across all their QDT subtrees  For QDT roots and publishers  the list holds most popular s CDs across the global collection  When  ever a node n updates its list  it propagates the new list bottom up along all QDTs n participates in  If n is a root  it propagates its list to the other k     1 roots  Whenever the root of a QDT T updates its list  it disseminates it top down to all publishers in T   When node n issues a query Q  it picks the QDT corresponding to Q   s most selective block according to the information in n L  Note that some query blocks may contain CDs not occurring in the n L list  These are treated as selective CDs  and blocks with the highest number of selective CDs are preferred  If multiple such query blocks exist  n breaks the tie by computing the selectivity of the conjunction of popular CDs in each block  using n L  If this still leaves more than one candidate query block  one is picked at random  We call this strategy partially informed routing  and ob  serve that it leads to a spectrum of strategies parameterized by the size of internal state reserved for the list of popular CDs  We use the term x informed routing in short for partially informed routing based on the list of the most popular x  of CDs  Notice that 100  informed routing becomes fully informed  and 0 informed routing degenerates to random routing  In Section 6  we show experimen  tally that  by keeping track of even very short lists  we observe performance very close to the fully informed strategy  and much better than the random strategy  EXAMPLE 3 2  We revisit Example 2 3  explaining why query Q4  which had two routing alternatives  was sent to QDT3  To en  able fully informed or partially informed routing  publishers main  tain frequencies of  some of  the CDs in the global collection  which in our case include money  published by 7 publishers   stocks and yak tea  published by 6 publishers   Notice that Hong Kong is de  clared by only 2 publishers and hence more selective than money  which is why it is preferred by the fully informed routing strat  egy  Since CD Hong Kong appears in block B3  the corresponding tree QDT3 is used  The same outcome is achieved for partially  informed routing  assuming for instance that publishers maintain only the 3 most popular CDs  the list includes CD money  signal  ing to Q4     s initiator to avoid routing by it      Finally  when no selectivity information is available  we fall back on heuristic routing  simply direct Q to the QDT corresponding to one of Q   s maximum cardinality blocks  breaking ties with random picks  This strategy is based on the heuristic that higher numbers of conjuncts tend to yield higher selectivity  4  PUBLISHER K ANONYMITY The challenge for the design of the UQDT maintenance proto  col is to simultaneously guarantee that  i  queries reach all rele  vant publishers   ii  network traffic is minimized and congestion avoided  and  iii  publishers are encouraged to register with the dissemination infrastructure  being guaranteed that the registration will not expose their connection to certain sensitive CDs  We have shown above how the UQDT infrastructure addresses requirements  i  and  ii   In this section  we focus on item  iii   The privacy guarantee  publisher k anonymity  Our approach here is to adapt the notion of k anonymity from relational table anonymization  33   In our context  we wish to guarantee that for every publisher p and every CD c  if p advertises c  then the rout  ing information stored in the UQDT and exchanged during UQDT maintenance does not allow p to be distinguished from at least k   1 other potential publishers of c  This involves ensuring that the set of publishers connected to the same edge router consists of at least k members  and that even edge routers cannot tell which among its k  publishers advertises any given CD  This latter requirement defends against the event when edge routers are compromised  by hacking  subpoena  or impersonation   As described shortly  this guarantee involves collaborative computation among the publish  ers of an edge router  We describe how this collaboration can be conducted without exposing a publisher even if  i  all other publish  ers in its group have been compromised and are colluding against it but the edge router is trusted  or  ii  the edge router and up to N     k publishers have been compromised  where N is the number of publishers in p   s group  First observe that  if every edge router e could be trusted to be  have as prescribed in Section 3  and to never be compromised  then the publishers would remain k anonymous if ancestors of the edge routers were compromised  Indeed  recall that e only stores and communicates to its parents in the UQDT the Bloom filter summary of cd e   i e  the union of all CD sets advertised by its publishers  The Bloom filter  which is the only exposed information  does not record which of the k  publishers advertises a particular CD  nor which sets of CDs occur together in some document  cd e  is there  fore insufficient to pinpoint who among e   s publishers advertises any given CD  By ensuring that e   s subtree contains sufficiently many publishers advertising CDs as a group  we enable each pub  lisher to remain anonymous by    hiding in the crowd    comprised of this group  As an added bonus  the CD summary implementation is hash based and does not distinguish among two distinct CDs with the same hash code  Publishers exploit this by declaring the hash codes of their advertised CDs rather than their actual value  An in  spection of the edge router   s summary will therefore fail to answer with certainty even the simple question whether a given CD is ad  vertised by some publisher  let alone by a given publisher  Recall from Section 3 that the price traded off for this added protection is that false positives to CD membership tests lead to queries being forwarded unnecessarily to the publishers  thus affecting perfor  mance  Our experiments show that this overhead is small and the false positives are negligible  Also note that  the further up an an  cestor a of an edge router e is  the more fuzziness a   s CD summary will contain  If a is compromised and its summary exposed  then each of e   s publishers is hidden not only in the crowd of e   s k  publishers  but in the larger crowd of all publishers in a   s subtree  Finally  note that the publishers not in a   s subtrees are not affected  But how do we defend against the case when the edge router e itself is compromised  Since the collection of documents stored at publishers is dynamic  every one of e   s publishers p  in some QDTi  needs to declare to e the set of CDs it wants to advertise  or stop advertising   A compromised e would record this information if p were to declare its CDs directly  To preserve p   s anonymity even against e  we designed the following protocol  Publishers p1       pN  with N     k  of the same group participat  ing in QDTi declare to their edge router e only batch updates of their advertised CDs  instead of sending up individual updates  To advertise a new set of CDs  publisher pj installs them in an initially empty Bloom filter  That is  it starts from a filter with all counters set to 0  hashes each CD  and increments the counters whose index is given by the hash codes  The resulting filter is publisher pj   s update Uj   The batch update S is the vector sum of all publisher updates  S   U1           UN   e receives S and sums it to its CD summary  since both are represented as Bloom filters  the operation reduces to vector sum   CD deletions are handled by subtracting S from e   s summary  It is easily shown that this protocol supports the correct maintenance of CD summaries  More  it ensures that e can  not figure out the individual updates  as it only receives their sum over all publishers  Note that e doesn   t even see the actual CDs  it obtains only their count  muddled by hash collisions   We can show that this protocol preserves k anonymity even if all routers are compromised  We must address one last issue  where can the batch update S be computed  Asking e to do so would defeat the purpose  as it would involve each publisher to send its update to e  Instead  S is computed collaboratively by the publishers  without involv  ing e  To find out the nodes connected to e  individual publish  ers can publish broadcast the router they are connected to  e g   by running one   s own Web service answering the    buddies    request using public key cryptography to rule out impersonation   To de  fend even against the case when publishers themselves are com  promised  we use the classical cryptographic technique of secure multi party computation  22   This allows a set of N publishers to compute the batch update without revealing the individual val  ues to each other or to outside observers of their communication traffic  This shields every publisher even against the case that all other publishers in its group are colluding against it  assuming that they are not also colluding with e  If e as well as some some pub  lishers are colluding  then e knows the updates of these publishers and can subtract them from the overall batch update  retrieving the batch update of the uncompromised publishers  If fewer than k of these remain  then anonymity decreases  One can defend against this case by arranging sufficiently large publisher group sizes N  so that compromising more than N     k of them is practically in  feasible  Another possible defense consists in publishers joining UQDT only together k     1 trusted    buddies     This does require trust  which however is bounded and does not need to extend to a vast unknown infrastructure  Secure multi party computation involves overhead  However  note that the publisher group only needs to compute as many sums as entries in the Bloom filter vector  This is a constant of the UQDT  independent of the size of the global document collection  Further  the computation is performed only on batch updates  so its over  head is manageable by adjusting the update frequency  Finally  recall k anonymity guarantee holds on a per CD basis  Since UQDT partitions the CD space among its member QDTs  there is no interaction between QDTs to derive compromising in  formation  If the guarantee holds for each QDT in isolation  it holds for entire UQDT  What we do not defend against  We emphasize that we are only concerned with putting publishers    minds at ease w r t  the safety of participating in the UQDT  We do not address here the orthogonal problem of how publishers decide whether to answer a query once it reaches them  recall that the query answer is sent directly to the query issuer   or whether to identify themselves in the answer  To guarantee that the query issuer is not an impersonator  and that the query answer cannot be observed by third parties  one can adopt existing techniques based on authentication credentials  encrypted channel communication  and anonymization proxies  discussed in related work   We do not aim to make the infrastructure impervi  ous to large scale censoring attacks  such as a governmental agency completely shutting down the Internet in a region  or a denial of  service  DOS  attack overloading the UQDT to decrease data avail  ability  However  note that the effect of DOS attacks is mitigated by our load balancing scheme  which maximizes throughput  5  EXPERIMENTAL SETUP The Initial Overlay Network  To analyze the effects of our im  plementation choices on query dissemination  we built a simulator of a 10 000 node overlay network consisting of Np   9  400 pub  lisher and Nr   600 router nodes  A Real Data Set  To obtain true to life community  we simulate a distributed community that shares an XML dump of Wikipedia  comprising about 1 1 million real Wikipedia documents which amount to 8 6 GB  14   We simulate that documents are each brought into the community by one of the 9 400 publishers  Due to lack of infor  mation on which publisher generated which document  we assign the documents to publishers in a uniform random manner  CD Definition  Since Wikipedia uses structural schema  i e   not ontological   the majority of the tags on the root to leaf XML paths are concerned with document organization  providing no se  mantic meaning  This motivates us to consider CDs defined as pairs  t  w   where w is a keyword that appears in context t  given by the last XML tag on the path from the root to w  We include this tag to support context aware queries that go beyond standard keyword search  Moreover  we focus only on the tags that carry meaning to users  e g      link        b        title        subtitle    and    category      The combination of such CDs yields a complex set of about 3 2 mil  lion distinct CDs accounting for 24  of the set of all distinct CDs obtained by considering all possible tags  We have tried other CD definitions  see  13   and obtained analogous results  The point is that the flexibility of CD definition is a key enabler for striking the right balance between query expressivity and space overhead  EXAMPLE 5 1  We setup the Bloom filter for each node   s sum  mary as follows  Fixing the false positive rate at 10   2  it follows from  18  that the optimum number of hash functions is 7 when the size of the Bloom filter at every router  assuming a single QDT configuration and counters of size 1 bit  is M   3 6 MB  which represents only 0 044  of the global collection size  For larger counter sizes  the false positive rate is even lower  For k QDTs  the global memory consumption per node stays the same  since the k Bloom filters at every node summarize disjoint sets of CDs  Each Bloom filter has size 3 6 k MB  and the same error rate of 10   2      Query Workload  We force the dissemination process to work under two extreme query types  We construct a family of 10 work  loads  WcF  1   c   10   each consisting of 5  000 c conjunct queries drawn at random from the space of queries with no match against the global collection  Similarly  we build the family of workloads  WcT 1   c   10  each comprising 5 000 c conjunct queries drawn at random from the space of queries with at least one match in the EXAMPLE 5 2  In Example 2 1  46 messages are used to dis  seminate 4 queries in 8 time units  while in Example 2 3  50 mes  sages disseminate the same query workload in 6 time units  Defin  ing throughput as the number of queries answered per time unit  the 4 QDT case has the higher throughput  The reason we don   t sim  ply use throughput as a metric is that it requires assumptions on the relative duration of processing and forwarding cost  in our running example  we take the simplifying assumption that forwarding cost takes constant time  independent of fanout   In Figure 2  the processing load for a node is the number of queries on its row  For example  the processing load for node 13 is 4  which is also the peak processing load  In Figure 4  the peak processing load is 3  experienced for instance by nodes 2 and 10  The forwarding load can be read by inspecting the transitions between columns and keeping track of parent child relationships in the various trees  In the single QDT case  Figure 2   root node 1 has the highest peak forwarding load  12  it forwards each of the 4 queries to its 3 children   In the 4 QDT configuration  Figure 4   the peak forwarding load is 6 messages  experienced by node 20  1 message for Q2  3 for Q3 and 2 for Q4   Notice that  compared to the single QDT  the 4 QDT configura  tion decreases both processing and forwarding peak load  which leads to improved throughput regardless of the concrete values of the per query processing and forwarding cost      The above considerations suggest comparing configurations by their degree of reduction of the peak processing and forwarding loads  Note  the ideal load balance is achieved when the peak    drops    to the average load  which is measured as the average over all router nodes  Note that our goal is not merely to achieve balance  as one can do so without improving throughput by simply raising the average load  Indeed  as discussed in Section 2  with increasing number k of QDTs both kinds of average load increase  though only slightly  as shown experimentally   This is because routing by smaller query blocks results in less pruning  which increases the overall number of messages  Thus  the lowest possible average load occurs in the single QDT configuration  and represents the ideal target for low  ering the peak load  Since we are interested in closing the gap between the peak load in a k QDT configuration and the ideal peak load  we report the ideal to actual load ratio metric  defined as the ratio between the peak load in the k QDT and the average load in the single QDT configuration  6  SIMULATION RESULTS In this section  we explore through extensive simulations the space of configurations defined by the three dimensions given by the topology of QDTs  number of QDTs  and routing strategies  We confirm empirically that the configuration choices we advo  cate achieve near optimal peak load reduction  and therefore near  optimal throughput  Warm up  Single QDT Configuration  In this experiment  we confirm that the number of messages reaching the various levels in a single QDT configuration is sufficiently skewed to justify our load balancing efforts  in particular that the routers on the first two levels of the tree bear the brunt of the load  For query workloads W2F and W2T   and the Scribe topology QDTS   we report in Table 3 for every level the total and average number of messages seen by its nodes  Notice that the average number of messages per node de  creases drastically below the upper two levels  Also notice that  un  surprisingly  workload W2T generates more overall messages  since its matching queries undergo less pruning than those in W2F   Effect of Number of QDTs  In this experiment  we validate our method for determining the number k of QDTs  recall Sec  collection  We also generate the 50  000 query workload W T       10 W T   The  W T  i workloads increase the overall forwarding c 1c c effort by forcing QDTs to send queries all the way to  some  leaves  Scribe QDTs  Recall from Section 3 that  even in multiple QDT configurations  the QDTs are isomorphic  We obtain a  unique up to isomorphism  QDTS topology using Scribe  We first convince ourselves of the faithfulness of the simulation  by generating a fam  ily of 20 Scribe tree topologies for the same node set  by varying the order in which the nodes join the network   We observe only non  essential variations across the family  thus boosting our confidence that picking any tree in this family is representative of Scribe   s be  havior  The particular Scribe tree we pick has 9  400 leaf nodes and 600 internal nodes  5 levels  average fanout of 16 7  and a maxi  mum fanout of 101  The fanout features a very skewed distribution  decreasing from root to leaves  this holds for all 20 Scribe trees we considered   The distribution of the number of nodes per tree level 1 to 5 is as follows  1 node  the root   40  of which 3 are publish  ers   1  189  6  163 and 2  607 nodes  We determine the number k of isomorphic copies as in Section 3  We have Nr   600 routers in total  among the 40 children of the root  37 are routers  We obtain Nu  1 37 38andhencek    Nr        600    15  Nu 38 Fanout balanced QDTs  We extend our simulation to QDT topologies not created by Scribe  We consider a topology QDTB that uses the same router and publisher nodes  but eliminates the skewed fanout distribution that is typical of Scribe trees  This is beneficial since a node   s fanout influences its forwarding cost  We first organize the 600 routers into a balanced skeleton tree with fanout 8  where levels 1  2  3  4  5 have  respectively  1  8  64  512 and the remaining 15 nodes  Next  we connect the 9  400 publish  ers to this skeleton tree  achieving for each node a fanout of 16 or 17  There are 75 non leaf routers in the skeleton tree  and each re  ceives 8 publishers  for a total fanout of 16  Among the leaf routers in the skeleton tree  400 receive 17 publishers and 125 receive 16 publishers  We determine the number k of fanout balanced QDTs in the usual manner  k       Nr           600       66  multi QDT overlay  defined as the number of queries answered per time unit  Throughput is a manifestation of two more fundamen  tal factors  namely the processing and forwarding effort at every router  For a given workload W   we define the processing load at node n  PLoad W  n   as the number of query messages reach  ing n across all QDTs it participates in  The forwarding load at n  FLoadW  n   is the number of query messages leaving n along all QDTs it participates in  Notice that none of the two measures is derivable from the other  since FLoadW  n  depends on n   s fanout distribution  over the QDTs it participates in  and on the amount of pruning at n  For both load flavors  we define the peak load  which is the maximum load over all nodes  Clearly  decreasing either or both kinds of peak load results in increased throughput  Nu 9 Metrics  Our goal is to improve the query throughput of the QDT level 1 2 3 4 5 Total   msg  per level 5 000 200 000 173 066 28 509 4 869 411 444 W2F Avg    msg  per node W2T Avg    msg  per node 5 000 5 000 535 83 74   Effect of Static Load Indicators  In the extended version  12   we introduce two load indicators to capture statically the balance degree of a k QDT configuration  and we confirm experimentally a good correlation with the dynamic query dissemination perfor  mance  These indicators measure for each node n its average tree level  and its average fanout  over all QDTs it participates in   Effect of Routing Strategy  We next compare the routing strate  gies defined in Section 3  For the partially informed strategy  we consider the case when publishers maintain the top s popular CDs for s   43k  74k and 124k  corresponding respectively to 1 37   2 33  and 3 89  of the total number of CDs in the global col  lection  We compare the strategies for workload W T and QDTS   reporting the ideal to actual peak load ratio in Figure 6  First  we note that random routing performs worst  closely fol  lowed by heuristic routing  Both strategies are significantly outper  formed by the  partially  or fully  informed ones for every k   1  with the exception of k   1 when all routing strategies coincide   The family of informed routing strategies follows a common trend  with increasing k     15  the gap between ideal and actual load shrinks drastically  reaches the sweet spot at k   15 and es  sentially saturates for k   15  with a slight increase at k   17 for processing load  due to discussed imbalance introduced by the wrap around   Interestingly  random and heuristic routing behave slightly dif  ferently  at k   5  the actual load gets closer to the ideal load than at k   15  This is caused by the following effect  The more QDTs  the more query blocks  which decreases the chance of a ran  dom pick hitting the most selective block  With increasing k  this effect starts generating non minimal traffic  eventually canceling the load balancing effect  This explains why the random strategy degrades with increasing k  The reason the degradation saturates is that the number of query blocks cannot increase indefinitely  it must saturate once all blocks become singletons   Heuristic routing suffers from essentially the same problem  the more blocks we split a query into  the smaller the variation in block cardinality  Recall that  for same cardinality query blocks  heuristic routing degener  ates to random  In contrast  for the informed routing family  the experiments show that this negative effect remains subtle  being canceled out by the judicious choice of selective query blocks  Finally  we get very close to the benefits of fully informed rout  ing in negligible space overhead  by maintaining the frequency for only a small fraction  3 89   of all CDs  These results strongly recommend partially informed routing over the other strategies  Effect of QDT Topology  We repeated all above experiments us  ing the fanout balanced QDT topology QDTB   observing the same trends as for the Scribe topology QDTS  We do not report the detailed results for lack of space  Instead  we summarize in Ta  ble 4 the comparison between the Scribe generated and the fanout  balanced topology  relative to the peak load reduction  For query workloadWT andthefully informedroutingstrategy weshowthe ideal to actual peak load ratio factor for the appropriate number of QDTs  15 for QDTS and 66 for QDTB      msg per level 5 000 200 000 636 507 513 464 193 575   1 548 546 5 000 5 000 146 5 2 Table 3  Messages per Level  k   1  QDTS   fully informed  T the number k of QDTS copies from 1 to 31  Figure 5 shows the tion 3   For workload W average and the peak load for both processing and forwarding  and fully informed routing  we increase 60 000 2 500 000 50 000 40 000 30 000 20 000 10 000 2 000 000 1 500 000 1 000 000 500 000 00 1 5 10 15 17 31 Number of QDTs k Avg processing load Peak processing load Avg forwarding load  Peak forwarding load Figure 5  Effect of Number of QDTs  W T   QDTS   fully  informed routing  Notice that with increasing k  the gap between the peak load and the average load decreases considerably  The highest load im  balance occurs for k   1 as shown in the big gap between the peak and the average values for both the processing and the forwarding load  As predicted by our analysis in Section 3  k   15 is indeed the    sweet spot    where the minimum gap is measured  Increasing k to 17 increases this gap  This is because the two additional cyclic permutations cause a    wrap around    of the routers from the top two levels of QDTS1 to the top two levels of QDTS16 and QDTS17 and thus introduce load imbalance  Also note that there is no point in looking at strict multiples of     Nr     beyond k   15  as they cost Nu more overlay maintenance overhead without bringing the peak load any closer to the average load  Finally  we observe that the negative effect of increasing overall number of messages with increasing k does occur  the average pro  cessing load is indeed the lowest for k   1 since routing is done using with all conjuncts  thus benefiting from maximum routing selectivity  However  the increase is very slow when compared to the decrease in peak load  The negative effect of average load in  crease is outweighed by that of peak load reduction  as shown by the closing gap between peak and average loads  We observe this behavior more accurately in terms of the ideal  to actual peak load ratio  which for increasing k approaches the ideal value 1  For example  the ideal to actual peak load ratio for the same values of k as in Figure 5 are  respectively  6 42  1 85  1 49  1 21  1 44 and 1 20  Figure 5 shows the same trend for the forwarding load  with the only difference that  while the gap of peak and average loads de  creases with growing k     15 and saturates once k exceeds 15  we remain far from the ideal reduction  for which the ideal to actual load ratio is 1   This is explained by the forwarding load   s correla  tion with the node fanouts and the fact that Scribe builds trees with highly skewed fanout distribution  Scribe  QDTS  k   15  QDTB  k   66 ideal to actual peak load ratio processing forwarding fanout balanced 1 21 9 3 1 18 2 3 Table 4  Effect of QDT Topology  W T   fully informed  Notice that both topologies come within reach of the ideal load reduction  when the ideal to actual load ratio is 1  for processing load  However  for forwarding load the Scribe topology misses the ideal by an order of magnitude  whereas the fanout balanced topology only by a factor of 2 3  The main reason not even the Processing Load  nr  of messages  Forwarding Load  nr  of messages  7 00 6 00 5 00 4 00 3 00 2 00 1 00 0 00 120 00 100 00 80 00 60 00 40 00 20 00 0 00 Ideal to actual peak load ratio 1 5 10 15 17 31 Number of QDTs k fully informed 3 89 informed 2 33 informed 1 37 informed heuristic random  a  Processing load  Ideal to actual peak load ratio 1 5 10 15 17 31 Number of QDTs k fully informed 3 89 informed 2 33 informed 1 37 informed heuristic random  b  Forwarding load  Query dissemination in P2P networks  Recently there has been a large body of work that focuses on finding only the peers with relevant data to a user   s query  These methods construct data summaries at nodes and use them as routing indices  e g   11  9   to disseminate the query in the network toward the relevant pub  lishers  These works are not focused on publisher anonymity  Replication based approaches  One way to increase data avail  ability and to balance the load  and therefore to improve the system throughput is to replicate all or parts of the data  or indices of it  redundantly at the router nodes  27  20   Disseminating queries to publishers in such a scenario is simple since each such router has global information  This however means that compromising a sin  gle router will violate the anonymity of all publishers  Partitioning based approaches  An alternative way to lever  age the distributed computational power is based on partitioning the data</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11dp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11dp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Data_provenance_workflow_and_cleaning"/>
        <doc>Interaction between Record Matching and Data Repairing ### Wenfei Fan 1 2 Jianzhong Li 2 Shuai Ma 3 Nan Tang 1 Wenyuan Yu 1 1University of Edinburgh 2Harbin Institute of Technology 3Beihang University  wenfei inf   ntang inf   wenyuan yu  ed ac uk lijzh hit edu cn mashuai act buaa edu cn Abstract Central to a data cleaning system are record matching and data repairing  Matching aims to identify tuples that refer to the same real world object  and repairing is to make a database consistent by    xing errors in the data by using constraints  These are treated as separate processes in current data cleaning systems  based on heuristic solutions  This paper studies a new problem  namely  the interaction between record matching and data repairing  We show that repairing can e   ectively help us identify matches  and vice versa  To capture the interaction  we propose a uniform framework that seamlessly uni   es repairing and matching operations  to clean a database based on integrity constraints  matching rules and master data  We give a full treatment of fundamental problems associated with data cleaning via matching and repairing  including the static analyses of constraints and rules taken together  and the complexity  termination and determinism analyses of data cleaning  We show that these problems are hard  ranging from NP  or coNPcomplete  to PSPACE complete  Nevertheless  we propose e   cient algorithms to clean data via both matching and repairing  The algorithms    nd deterministic  xes and reliable  xes based on con   dence and entropy analysis  respectively  which are more accurate than possible    xes generated by heuristics  We experimentally verify that our techniques signi   cantly improve the accuracy of record matching and data repairing taken as separate processes  using real life data  Categories and Subject Descriptors H 2  Database Management   General   integrity General Terms Theory  Algorithms  Experimentation Keywords conditional functional dependency  matching dependency  data cleaning ### 1  Introduction It has long been recognized that data residing in a database is often dirty  32   Dirty data in   icts a daunting Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  cost  it costs us businesses 600 billion dollars each year  16   With this comes the need for data cleaning systems  As an example  data cleaning tools deliver    an overall business value of more than 600 million GBP    each year at BT  31   In light of this  the market for data cleaning systems is growing at 17  annually  which substantially outpaces the 7  average of other IT segments  22   There are two central issues about data cleaning      Recording matching is to identify tuples that refer to the same real world entity  17 26       Data repairing is to    nd another database  a candidate repair  that is consistent and minimally di   ers from the original data  by    xing errors in the data  4 21   Most data cleaning systems in the market support record matching  and some also provide the functionality of data repairing  These systems treat matching and repairing as separate and independent processes  However  the two processes typically interact with each other  repairing helps us identify matches and vice versa  as illustrated below  Example 1 1  Consider two databases Dm and D from a UK bank  Dm maintains customer information collected when credit cards are issued  and is treated as clean master data  28   D consists of transaction records of credit cards  which may be dirty  The databases are speci   ed by schemas  card FN  LN  St  city AC  zip tel  dob  gd   tran FN  LN  St  city AC  post  phn  gd  item  when  where   Here a card tuple speci   es a UK credit card holder identi   ed by    rst name  FN   last name  LN   address  street  St   city  zip code   area code  AC   phone  tel   date of birth  dob  and gender  gd   A tran tuple is a record of a purchased item paid by a credit card at place where and time when  by a UK customer who is identi   ed by name  FN  LN   address  St  city  post code   AC  phone  phn  and gender  gd   Example instances of card and tran are shown in Figures 1 a  and 1 b   which are fractions of Dm and D  respectively  the cf rows in Fig  1 b  will be discussed later   Following  18  19   we use conditional functional dependencies  CFDs  18     1     4 to specify the consistency of tran data D  and a matching dependency  MD  19      as a rule for matching tuples across D and master card data Dm    1  tran  AC   131       city   Edi      2  tran  AC   020       city   Ldn      3  tran  city  phn       St AC  post      4  tran  FN   Bob       FN   Robert        tran LN  city  St  post    card LN  city  St  zip      tran FN      card FN      tran FN  phn    card FN tel   where  1  the CFD   1  resp    2  asserts that if the area code is 131  resp  020   the city must be Edi  resp  Ldn    2  CFD   3 is a traditional functional dependency  FD  asserting that city and phone number uniquely determine street  area codeFN LN St city AC zip tel dob gd s1  Mark Smith 10 Oak St Edi 131 EH8 9LE 3256778 10 10 1987 Male s2  Robert Brady 5 Wren St Ldn 020 WC1H 9SE 3887644 12 08 1975 Male  a  Master data Dm  An instance of schema card FN LN St city AC post phn gd item when where t1  M  Smith 10 Oak St Ldn 131 EH8 9LE 9999999 Male watch  350 GBP 11am 28 08 2010 UK cf  0 9   1 0   0 9   0 5   0 9   0 9   0 0   0 8   1 0   1 0   1 0  t2  Max Smith Po Box 25 Edi 131 EH8 9AB 3256778 Male DVD  800 INR 8pm 28 09 2010 India cf  0 7   1 0   0 5   0 9   0 7   0 6   0 8   0 8   1 0   1 0   1 0  t3  Bob Brady 5 Wren St Edi 020 WC1H 9SE 3887834 Male iPhone  599 GBP 6pm 06 11 2009 UK cf  0 6   1 0   0 9   0 2   0 9   0 8   0 9   0 8   1 0   1 0   1 0  t4  Robert Brady null Ldn 020 WC1E 7HX 3887644 Male necklace  2 100 USD 1pm 06 11 2009 USA cf  0 7   1 0   0 0   0 5   0 7   0 3   0 7   0 8   1 0   1 0   1 0   b  Database D  An instance of schema tran Figure 1  Example master data and database and postal code   3  the CFD   4 is a data standardization rule  if the    rst name is Bob  then it should be    normalized    as Robert  and  4  the MD    assures that for any tuple in D and any tuple in Dm  if they have the same last name and address  and moreover  if their    rst names are similar  then their phone and FN attributes can be identi   ed  Consider tuples t3 and t4 in D  The bank suspects that the two refer to the same person  If so  then these transaction records show that the same person made purchases in the UK and in the US at about the same time  taking into account the 5 hour time di   erence between the two countries   This indicates that a fraud has likely been committed  Observe that t3 and t4 are quite di   erent in their FN  city  St  post and Phn attributes  No rule allows us to identify the two directly  Nonetheless  they can indeed be matched by a sequence of interleaved matching and repairing operations   a  get a repair t     3 of t3 such that t     3 city  Ldn via the CFD   2  and t     3 FN  Robert by normalization with   4   b  match t     3 with s2 of Dm  to which    can be applied   c  as a result of the matching operation  get a repair t        3 of t3 by correcting t        3  phn  with the master data s2 tel    d     nd a repair t     4 of t4 via the FD   3  since t        3 and t4 agree on their city and phn attributes    3 can be applied  This allows us to enrich t4 St  and    x t4 post  by taking corresponding values from t        3   which have been con   rmed correct with the master data in step  c   At this point t        3 and t     4 agree on every attribute in connection with personal information  It is now evident enough that they indeed refer to the same person  hence a fraud  Observe that not only repairing helps matching  e g   from step  a  to  b    but matching also helps us repair the data  e g   step  d  is doable only after the matching in  b    2 This example tells us the following   1  When taken together  record matching and data repairing perform much better than being treated as separate processes   2  To make practical use of their interaction  matching and repairing operations should be interleaved  It does not help much to execute these processes consecutively one after another  There has been a host of work on record matching  e g    3  6  8  19  25  37   see  17  26  for surveys  as well as on data repairing  e g    4 7 10 20 21 29 39    However  the problem of interleaving record matching and data repairing to improve the accuracy has not been well addressed  Contributions  We investigate on cleaning data by uni  fying record matching and data repairing  and to provide a data cleaning solution that stresses accuracy   1  We investigate a new problem  stated as follows  Given a database D  master data Dm  and data quality rules consisting of CFDs    and matching rules     the data cleaning problem is to    nd a repair Dr of D such that  a  Dr is consistent  i e   satisfying the CFDs       b  no more tuples in Dr can be matched to master tuples in Dm by rules of     and  c  Dr minimally di   ers from the original data D  As opposed to record matching and data repairing  the data cleaning problem aims to    x errors in the data by unifying matching and repairing  and by leveraging master data  Here master data  a k a  reference data  is a single repository of high quality data that provides various applications with a synchronized  consistent view of its core business entities  28   It is being widely used in industry  supported by  e g   IBM  SAP  Microsoft and Oracle  To identify tuples from D and Dm  we use matching rules that are an extension of MDs  19  by supporting negative rules  e g   a male and female may not refer to the same person   3 37    2  We propose a uniform framework for data cleaning  We treat both CFDs and MDs as cleaning rules  which tell us how to    x errors  This yields a rule based logical framework  which allows us to seamlessly interleave repairing and matching operations  To assure the accuracy of    xes  we make use of  a  the con dence placed by the user in the accuracy of the data   b  entropy measuring the certainty of data  by the self information of the data itself  12  34   and  c  master data  28   We distinguish three classes of    xes   i  deterministic    xes for the unique solution to correct an error   ii  reliable    xes for those derived using entropy  and  iii  possible    xes for those generated by heuristics  The former two are more accurate than possible    xes   3  We investigate fundamental problems associated with data cleaning via both matching and repairing  We show the following   a  When CFDs and matching rules are taken together  the classical decision problems for dependencies  namely  the consistency and implication analyses  are NP complete and coNP complete  respectively  These problems have the same complexity as their counterparts for CFDs  18   i e   adding matching rules does not incur extra complexity   b  The data cleaning problem is NP complete  Worse still  it is approximation hard  i e   it is beyond reach in practice to    nd a polynomial time  PTIME  algorithm with a constant approximation ratio  35  unless P   NP   c  It is more challenging to decide whether a data cleaning process terminates and whether it yields deterministic    xes  these problems are both PSPACE complete   4  In light of the inherent complexity  we propose a threephase solution consisting of three algorithms   a  One algorithm identi   es deterministic  xes that are accurate  based on con   dence analysis and master data   b  When con    dence is low or unavailable  we provide another algorithm to compute reliable  xes by employing information entropy  inferring evidence from data itself to improve accuracy   c  To    x the remaining errors  we extend the heuristic based method  10  to    nd a consistent repair of the dirty data  These methods are complementary to each other  and can be used either alone or together   5  We experimentally evaluate the quality and scalability of our data cleaning methods with both matching and repairing  using real life datasets  DBLP and hospital data from US Dept  of Health   Human Services   We    nd that our methods substantially outperform matching and repairing taken as separate processes in the accuracy of    xes  up to 15  and 30   respectively  Moreover  deterministic    xes and reliable    xes are far more accurate than    xes generated by heuristic methods  Despite the high complexity of the cleaning problem  we also    nd that our algorithms scale reasonably well with the size of the data  We contend that a uni   ed process for repairing and matching is both important and feasible in practice  and that it should logically become part of data cleaning systems  We remark that master data is desirable in the process  However  in its absence our approach can be adapted by interleaving  a  record matching in a single data table with MDs  as described in  17   and  b  data repairing with CFDs  While deterministic    xes would have lower accuracy  reliable    xes and heuristic    xes would not degrade substantially  Organization  Section 2 reviews CFDs and extends MDs  Section 3 introduces the framework for data cleaning  Section 4 studies the fundamental problems for data cleaning  The two algorithms for data cleaning are provided in Sections 5 and 6   respectively  Section 7 reports our experimental    ndings  followed by open issues in Section 8  Related work  Record matching is also known as record linkage  entity resolution  merge purge and duplicate detection  e g    3 6 8 14 19 23 25 36 37   see  17 26  for surveys   Matching rules are studied in  19  25   positive  and  3  37   negative   Data repairing was    rst studied in  4  21   A variety of constraints have been used to specify the consistency of data in data repairing  such as FDs  38   FDs and inds  7   and CFDs  10  18   We employ CFDs  and extend MDs of  19  with negative rules  As remarked earlier  the problem of cleaning data by interleaving matching and repairing operations is not well addressed in previous work  The consistency and implication problems have been studied for CFDs  18  and MDs  19   We study these problems for MDs and CFDs put together  It is known that data repairing is NP complete  7  10   We show that data cleaning via repairing and matching is NP complete and approximationhard  We also study the termination and determinism analyses of data cleaning  which are not considered in  7 10   Several repairing algorithms have been proposed  7  10  20 21 29 39   Heuristic methods are developed in  7 10 21   based on FDs and inds  7   CFDs  18   and edit rules  21   The methods of  7 10  employ con   dence placed by users to guide a repairing process  Statistical inference is studied in  29  to derive missing values  To ensure the accuracy of repairs generated   29 39  require to consult users  In contrast to the previous work  we  a  unify repairing and matching   b  use con   dence just to derive deterministic    xes  and  c  leverage master data and entropy to improve the accuracy  Closer to our work is  20   also based on master data  It di   ers from our work in the following   i  While  20  aims to    x a single tuple via matching with editing rules  derived from MDs   we repair a database via both matching  MDs  and repairing  CFDs   a task far more challenging   ii  While  20  only relies on con   dence to warrant the accuracy  we use entropy analysis when the con   dence is either low or unavailable  There have also been e   orts to interleave merging and matching operations  14  23  36  37   Among these   1   23  clusters data rather than repair data  and it does not update data to    x errors  and  2   14 36 37  investigate record matching in the presence of error data  and advocate the need for data repairing to match records  The merge fusion operations adopted there are more restrictive than updates  value modi   cations  considered in this work and data repairing in general  Further  when no matches are found  no merge or fusion is conducted  whereas this work may still repair data with CFDs  There has also been a host of work on more general data cleaning  data transformation  which brings the data under a single common schema  30   ETL tools  see  5  for a survey  provide sophisticated data transformation methods  which can be employed to merge data sets and repair data based on reference data  These are essentially orthogonal  but complementary  to data repairing and this work  Information entropy measures the degree of uncertainty  12   the less the entropy is  the more certain the data is  It has proved e   ective in  e g   database design  schema matching  data anonymization and data clustering  34   We make a    rst e   ort to use it in data cleaning  we mark a    x reliable if its entropy is below a prede   ned threshold  2  Data Quality Rules Below we    rst review CFDs  18   which specify the consistency of data for data repairing  We then extend MDs  19  to match tuples across  a possibly dirty  database D and master data Dm  Both CFDs and MDs can be automatically discovered from data via pro   ling algorithms  e g    9 33    2 1 Conditional Functional Dependencies Following  18   we de   ne conditional functional dependen  cies CFDs on a relation schema R as follows  A CFD    de   ned on schema R is a pair R X     Y   tp   where  1  X     Y is a standard FD on R  referred to as the FD embedded in     and  2  tp is a pattern tuple with attributes in X and Y   where for each A in X     Y   tp A  is either a constant in the domain dom A  of attribute A  or an unnamed variable         that draws values from dom A   We separate the X and Y attributes in tp with            and refer to X and Y as the LHS and RHS of     respectively  Example 2 1  Recall the CFDs   1    3 and   4 given in Example 1  These can be formally expressed as follows    1  tran  AC       city   tp1    131     Edi      3  tran  city  phn       St AC  post   tp3                    4  tran  FN       FN   tp4    Bob     Robert   Note that FDs are a special case of CFDs in which pattern tuples consist of only wildcards  e g     3 given above  2 To give the formal semantics of CFDs  we use an operator     de   ned on constants and          v1     v2 if either v1   v2  or one of v1  v2 is          The operator     naturally extends to tuples  e g    131  Edi          Edi  but  020  Ldn            Edi   Consider an instance D of R  We say that D satis es theCFD     denoted by D        i    for all tuples t1  t2 in D  if t1 X    t2 X      tp X   then t1 Y     t2 Y       tp Y    Example 2 2  Recall the tran instance D of Fig  1 b  and the CFDs of Example 2 1  Observe that D        1 since tuple t1 AC    tp1  AC   but t1 city      tp1  city   i e   the single tuple t1 violates   1  Similarly  D        4  as t3 does not satisfy   4  Intuitively    4 says that no tuple t can have t FN    Bob  it has to be changed to Robert   In contrast  D      3  there exist no distinct tuples in D that agree on city and phn  2 We say that an instance D of R satis es a set    of CFDs  denoted by D        if D       for each            2 2 Positive and Negative Matching Dependencies Following  19 25   we de   ne matching dependencies  MDs  in terms of a set    of similarity predicates  e g   q grams  Jaro distance or edit distance  see e g    17  for a survey   We de   ne positive MDs and negative MDs across a data relation schema R and a master relation schema Rm  Positive MDs  A positive MD    on  R  Rm  is de   ned as      j    1 k   R Aj      j Rm Bj            i    1 h   R Ei    Rm Fi    where  1  for each j      1  k   Aj and Bj are attributes of R and Rm  respectively  with the same domain  similarly for Ei and Fi  i      1  h    and  2     j is a similarity predicate in    that is de   ned in the domain of R Aj   and Rm Bj    We refer to     j    1 k   R Aj      j Rm Bj    and     i    1 h   R Ei    Rm Fi   as the LHS  premise  and RHS of     respectively  Note that MDs were originally de   ned on two unreliable data sources  see  19  for a detailed discussion of their dynamic semantics   In contrast  we focus on matching tuples across a dirty source D and a clean master relation Dm  To cope with this  we re   ne the semantics of MDs as follows  For a tuple t     D and a tuple s     Dm  if for each j      1  k   t Aj   and s Bj   are similar  i e   t Aj      j s Bj    then t Ei  is changed to s Fi   the clean master data  for each i      1  h   We say that an instance D of R satis es the MD    w r t  master data Dm  denoted by  D  Dm         i    for all tuples t in D and all tuples s in Dm  if t Aj      j s Bj   for j      1  k   then t Ei    s Fi  for all i      1  h   Intuitively   D  Dm        if no more tuples from D can be matched  and hence updated  with master tuples in Dm  Example 2 3  Recall the MD    given in Example 1 1  Consider an instance D1 of tran consisting of a single tuple t     1  where t     1 city    Ldn and t     1 A    t1 A  for all the other attributes  for t1 given in Fig  1 b   Then  D1  Dm           since t     1 FN  phn      s1 FN tel  while  t     1 LN  city  St  post     s1 LN  city  St  Zip  and t     1 FN      s1 FN   This suggests that we correct t     1 FN  phn  using the master data s1 FN tel   2 Negative MDs  Along the same lines as  3  37   we de   ne a negative MD        as follows      j    1 k   R Aj       Rm Bj            i    1 h   R Ei      Rm Fi    It states that for any tuple t     D and any tuple s     Dm  if t Aj       s Bj    j      1  k    then t and s may not be identi   ed  Example 2 4  A negative MD de   ned on  tran  card  is         1   tran gd      card gd          i    1 7   tran Ai      card Bi    where  Ai  Bi  ranges over  FN  FN    LN  LN    St  St    AC  AC    city  city    post  zip  and  phn tel   It says that a male and a female may not refer to the same person  2 We say that an instance D of R satis es the negative MD        w r t  master data Dm  denoted by  D  Dm              if for all tuples t in D and all tuples s in Dm  if t Aj       s Bj   for all j      1  k   then there exists i      1  h  such that t Ei      s Fi   An instance D of R satis es a set    of  positive  negative  MDs w r t  master data Dm  denoted by  D  Dm         if  D  Dm        for all            Normalized CFDs and MDs  Given a CFD  resp  MD      we use LHS     and RHS     to denote the LHS and RHS of     respectively  It is called normalized if  RHS        1  i e   its right hand side consists of a single attribute  resp  attribute pair   As shown by  18 19   every CFD     resp  MD  can be expressed as an equivalent set S   of CFDs  resp  MDs   such that the cardinality of S   is bounded by the size of RHS      For instance  the CFDs   1    2 and   4 of Example 1 1 are normalized  While the CFD   3 is not normalized  it can be converted to an equivalent set of CFDs of the form   city  phn      Ai  tpi    where Ai ranges over St  AC and post  and tpi consists of wildcards only  similarly for the MD     We consider normalized CFDs  MDs  only in the sequel  3  A Uniform Framework for Data Cleaning We propose a rule based framework for data cleaning  It treats CFDs and MDs uniformly as cleaning rules  which tell us how to    x errors  and seamlessly interleaves matching and repairing operations  Section 3 1   Using cleaning rules we introduce a tri level data cleaning solution  which generates    xes with various levels of accuracy  depending on the information available about the data  Section 3 2   Consider a  possibly dirty  relation D of schema R  a master relation Dm of schema Rm  and a set                 where    is a set of CFDs on R  and    is a set of MDs on  R  Rm   3 1 A Rule based Logical Framework We    rst state the data cleaning problem  and then de   ne cleaning rules derived from CFDs and MDs  Data cleaning  The data cleaning problem  referred to as DCP  takes as input D  Dm and     it is to compute a repair Dr of D  i e   another database such that  a  Dr         b   Dr  Dm         and  c  cost Dr  D  is minimum  Intuitively   a  the repair Dr of D should be consistent   b  no more tuples in Dr can be matched to master data  and  c  Dr is accurate and is close to the original data D  Using the quality model of  10   we de   ne cost Dr  D  as      t   D     A   attr R  t A  cf     disA t A   t      A   max  t A     t      A    where  a  tuple t         Dr is the repair of tuple t     D   b  disA v  v       is the distance between values v  v         dom A   the smaller the distance is  the closer the two values are to each other   c   t A   denotes the size of t A   and  d  t A  cf is the con dence placed by the user in the accuracy of the attribute t A   see the cf rows in Fig  1 b    This quality metric says that the higher the con   dence of the attribute t A  is and the more distant v     is from v  the more costly the change is  Thus  the smaller cost Dr  D  is  the more accurate and closer to the original data Dr is  We use dis v  v       max  v    v        to measure the similarity of v and v     to ensure that longer strings with 1 character di   erence are closer than shorter strings with 1 character di   erence  As remarked in  10   con   dence can be derived via prove nance analysis  which can be reinforced by recent work on determining the reliability of data sources  e g    15    Cleaning rules  A variety of integrity constraints have been studied for data repairing  e g    7  10  18  38    As observed by  20   while there constraints help us determine whether data is dirty or not  i e   whether errors are present in the data  they do not tell us how to correct the errors  To make better practical use of constraints in data cleaning  we de   ne cleaning rules  which tell us what attributes should be updated and to what value they should be changed  From each MD in    and each CFD in     we derive a cleaning rule as follows  based on fuzzy logic  27    1  MDs  Consider an MD          j    1 k   R Aj      j Rm Bj         R E    Rm F    The cleaning rule derived from     denoted by       applies a master tuple s     Dm to a tuple t     D if t Aj      j s Bj   for each j      1  k   It updates t by letting  a  t E     s F  and  b  t A  cf    d  where d is the minimum t Aj   cf for all j      1  k  if    j is          That is       corrects t E  with clean master value s F   and infers the new con   dence of t E  following fuzzy logic  27    2  Constant CFDs  Consider a CFD   c   R X     A  tp1    where tp1  A  is a constant  The cleaning rule derived from   c applies to a tuple t     D if t X      tp1  X  but t A      tp1  A   It updates t by letting  a  t A     tp1  A   and  b  t A  cf   d  where d is the minimum t A       cf for all A         X  That is  the rule corrects t A  with the constant in the CFD   3  Variable CFDs  Consider a CFD   v    Y     B  tp2    where tp2  B  is a wildcard          The cleaning rule derived from   v is used to apply a tuple t2     D to another tuple t1     D  where t1 Y     t2 Y       tp2  Y   but t1 B      t2 B   It updates t1 by letting  a  t1 B     t2 B   and  b  t1 B  cf be the minimum t1 B       cf and t2 B       cf for all B         Y   While cleaning rules derived from MDs are similar to editing rules of  20   rules derived from  constant or variables  CFDs are not studied in  20   We use con   dence information and infer new con   dences based on fuzzy logic  27   Embedding negative MDs  Recall negative MDs from Section 2 2  The example below tells us that negative MDs can be converted to equivalent positive MDs  As a result  there is no need to treat them separately  Example 3 1  Consider the MD    in Example 1 1 and the negative MD        in Example 2 4  We de   ne        by incorporating the premise  gd  of        into the premise of              tran LN  city  St  post  gd    card LN  city  St  zip  gd      tran FN      card FN      tran FN  phn    card FN tel   Then no tuples with di   erent genders can be identi   ed as the same person  which is precisely what        is to enforce  In other words  the positive MD        is equivalent to the positive MD    and the negative MD          2 Indeed  it su   ces to consider only positive MDs  Proposition 3 1  Given a set      m of positive MDs and a set        m of negative MDs  there exists an algorithm that computes a set   m of positive MDs in O       m         m   time such that   m is equivalent to      m            m  2 A uniform framework  By treating both CFDs and MDs as cleaning rules  one can uniformly interleave matching and repairing operations  to facilitate their interactions  Example 3 2  As shown in Example 1 1  to clean tuples t3 and t4 of Fig  1 b   one needs to interleave matching and D  Master  data D Data  quality  rules  Dirty  Data D Deterministic    xes Con   dence based Reliable    xes Entropy based Possible    xes Heuristic based D   D m User con   dence entropy                       r Figure 2  Framework Overview repairing operations  These can be readily done by using cleaning rules derived from   2    4     and   3  Indeed  the cleaning process described in Example 1 1 is actually carried out by applying these rules  There is no need to distinguish between matching and repairing in the cleaning process  2 3 2 A Tri level Data Cleaning Solution Based on cleaning rules  we develop a data cleaning system UniClean  It takes as input a dirty relation D  a master relation Dm  a set of cleaning rules derived from     as well as thresholds             0  1  set by the users for con   dence and entropy  respectively  It generates a repair Dr of D with a small cost Dr  D   such that Dr       and  Dr  Dm         As opposed to previous repairing systems  7 10 20 21 29  39   UniClean generates    xes by unifying matching and repairing  via cleaning rules  Further  it stresses the accuracy by distinguishing these    xes with three levels of accuracy  Indeed  various    xes are found by three algorithms executed one after another  as shown in Fig  2 and illustrated below   1  Deterministic  xes based on con dences  The    rst algorithm identi   es erroneous attributes t A  to which there exists a unique    x  referred to as a deterministic  x  when some attributes of t are accurate  It    xes those errors based on con   dence  it uses a cleaning rule to update t A  only if certain attributes of t have con   dence above the threshold     It is evident that such    xes are accurate up to      2  Reliable  xes based on entropy  For attributes with low or unavailable con   dence  we correct them based on the relative certainty of the data  measured by entropy  Entropy has proved e   ective in data transmission  24  and compression  40   among other things  We use entropy to clean data  we apply a cleaning rule    to update an erroneous attribute t A  only if the entropy of    for certain attributes of t is below the threshold     Fixes generated via entropy are accurate to a certain degree  and are marked as reliable  xes   3  Possible  xes  Not all errors can be    xed in the    rst two phases  For the remaining errors  we adopt heuristic methods to generate    xes  referred to as possible  xes  To this end we extend the method of  10   by supporting cleaning rules derived from both CFDs and MDs  Fixes produced in the    rst two phases remain unchanged at this step  Notably  the other heuristic methods can also be  possibly  adapted  At the end of the process     xes are marked with three distinct signs  indicating deterministic  reliable and possible  respectively  We shall present methods based on con   dence and entropy in Sections 5 and 6  respectively  Due to space limitations  we opt to omit the algorithm of possible    xes  For interesting readers  please refer to  10  for more details 4  Fundamental Problems for Data Cleaning We now investigate fundamental problems associated with data cleaning  We    rst study the consistency and implication problems for CFDs and MDs taken together  from which cleaning rules are derived  We then establish the complexity bounds of the data cleaning problem as well as its termination and determinism analyses  These problems are not only of theoretical interest  but are also important to the development of data cleaning algorithms  The main conclusion of this section is that data cleaning via matching and repairing is inherently di   cult  all these problems are intractable  Consider a relation D of schema R  a master data Dm of schema Rm  and a set                 where    is a set of CFDs de   ned on R  and    is a set of MDs de   ned on  R  Rm   4 1 Reasoning about Data Quality Rules There are two classical problems for data quality rules  The consistency problem is to determine  given Dm and                 whether there exists a nonempty instance D of R such that D       and  D  Dm         Intuitively  this is to determine whether the rules in    are dirty themselves  The practical need for the consistency analysis is evident  it does not make sense to derive cleaning rules from    before    is assured consistent itself  We say that    implies another CFD  resp  MD      denoted by           if for any instance D of R  whenever D       and  D  Dm         then D        resp   D  Dm          The implication problem is to determine  given Dm     and another CFD  or MD      whether           Intuitively  the implication analysis helps us    nd and remove redundant rules from     i e   those that are a logical consequence of other rules in     to improve performance  These problems have been studied for CFDs and MDs separately  It is known that the consistency problem for MDs is trivial  any set of MDs is consistent  19   In contrast  there exist CFDs that are inconsistent  and the consistency analysis of CFDs is NP complete  18   It is also known that the implication problem for MDs and CFDs is in quadratic time  19  and coNP complete  18   respectively  We show that these problems for CFDs and MDs put together have the same complexity as their CFDs counterparts  That is  adding MDs to CFDs does not make our lives harder  Theorem 4 1  For CFDs and MDs put together  the consis  tency problem is NP complete  and the implication problem is coNP complete  when    is either a CFD or an MD   2 Proof  The upper bounds are veri   ed by establishing a small model property  The lower bounds follow from the intractability for their CFD counterparts  a special case  2 In the rest of the paper we consider only collections    of CFDs and MDs that are consistent  4 2 Analyzing the Data Cleaning Problem Recall the data cleaning problem  DCP  from Section 3  Complexity bounds  One wants to know how costly it is to compute a repair Dr  Below we show that it is intractable to decide whether there exists Dr with cost Dr  D  below a prede   ne bound  Worse still  it is infeasible in practice to    nd PTIME approximation algorithm with performance guarantee  Indeed  the problem is not even in apx  the class of problems that allow PTIME approximation algorithms with approximation ratio bounded by a constant  Theorem 4 2   a  The data cleaning problem  DCP  is NP  Symbols Semantics                A set    of CFDs and a set    of MDs Con   dence threshold  update threshold  and     1   2 entropy threshold  respectively   Selection operator in relational algebra   Projection operator in relational algebra The set  t   t     D  t Y       y  for each   y in       y   Y   Y    tp Y  D  w r t  CFD  Y     B  tp  Table 1  Summary of notations complete   b  Unless p   NP  for any constant     there exists no PTIME    approximation algorithm for DCP  2 Proof   a  The upper bound is veri   ed by giving an NP algorithm  The lower bound is by reduction from 3sat  35    b  This is veri   ed by reduction from 3sat  using gap techniques  35   Given any constant     we show that there exists an algorithm with approximation ratio    for DCP i    there is a PTIME algorithm for deciding 3sat  2 It is known that data repairing alone is NP complete  10   Theorem 4 2 tells us that when matching with MDs is incorporated  the problem is intractable and approximation hard  Termination and determinism analyses  There are two natural questions about rule based data cleaning methods such as the one proposed in Section 3   a  The termination problem is to determine whether a rule based process stops  That is  it reaches a  xpoint  such that no cleaning rules can be further applied   b  The determinism problem asks whether all terminating cleaning processes end up with the same repair  i e   all of them reach a unique    xpoint  The need for studying these problems is evident  A rulebased process is often non deterministic  multiple rules can be applied at the same time  We want to know whether the output of the process is independent of the order of the rules applied  Worse  it is known that even for repairing only  a rule based method may lead to an in nite process  10   Example 4 1  Consider the CFD   1   tran  AC       city   tp1    131     Edi   given in Example 2 1  and another CFD   5   tran  post       city   tp5    EH8 9AB     Ldn    Consider D1 consisting of a single tuple t2 given in Fig  1  Then a repairing process for D1 with   1 and   5 may fail to terminate  it changes t2 city  to Edi and Ldn back and forth  2 No matter how important  it is beyond reach in practice to    nd e   cient solutions to these two problems  Theorem 4 3  The termination and determinism problems are both PSPACE complete for rule based data cleaning  2 Proof  We verify the lower bound of these problems by reduction from the halting problem for linear bound automata  which is PSPACE complete  2   We show the upper bound by providing an algorithm for each of the two problems  which uses polynomial space in the size of input  2 5  Deterministic Fixes with Data Con   dence As shown in Fig  2  system UniClean    rst identi   es deterministic    xes based on con   dence analysis and master data  In this section we de   ne deterministic    xes  Section 5 1   and present an e   cient algorithm to    nd them  Section 5 2   In Table 1 we summarize some notations to be used in this Section and Section 6  5 1 Deterministic Fixes We de   ne deterministic    xes w r t  a con dence threshold    determined by domain experts  When    is high enough  e g   if it is close to 1  an attribute t A  is assured correctif t A  cf         We refer to such attributes as asserted attributes  Recall from Section 3 the de   nition of cleaning rules derived from MDs and CFDs  In the    rst phase of UniClean  we apply a cleaning rule    to tuples in a database D only when the attributes in the premise  i e   LHS  of    are all asserted  We say that a    x is deterministic w r t     and    if it is generated as follows  based on how    is derived   1  From an MD          j    1 k   R Aj      j Rm Bj         R E    Rm F    Suppose that    applies a tuple s     Dm to a tuple t     D  and generates a    x t E     s F   see Section 3 1   Then the    x is deterministic if t Aj   cf        for all j      1  k  and moreover  t E  cf       That is  t E  is changed to the master value s F  only if  a  all the premise attributes t Aj     s are asserted  and  b  t E  is not yet asserted   2  From a constant CFD   c   R X     A  tp1    Suppose that    applies to a tuple t     D and changes t A  to the constant tp1  A  in   c  Then the    x is deterministic if t Ai  cf        for all Ai     X and t A  cf        3  From a variable CFD   v    Y     B  tp   For each   y in  Y   Y    tp Y  D   we de   ne       y  to be the set  t   t     D  t Y       y   where   and   are the projection and selection operators  respectively  in relational algebra  1   That is  for all t1  t2 in       y   t1 Y     t2 Y       y     tp Y    Suppose that    applies a tuple t2 in       y  to another t1 in       y  for some   y  and changes t1 B  to t2 B   Then the    x is deterministic if  a  for all Bi     Y   t1 Bi  cf        and t2 Bi  cf          b  t2 B  cf         and moreover   c  t2 is the only tuple in       y  with t2 B  cf         hence t1 B  cf        That is  all the premise attributes of    are asserted  and t2 B  is the only value of B attribute in       y  that is assumed correct  while t1 B  is suspected erroneous  As observed by  20   when data quality rules and asserted attributes are assured correct  the    xes generated are unique  called    certain    in  20    While  20  only considers MDs  the observation remains intact for CFDs and MDs  Note that when an attribute t A  is updated by a deterministic    x  its con   dence t A  cf is upgraded to be the minimum of the con   dences of the premise attributes  see Section 3 1   As a result  t A  also becomes asserted  since all premise attributes have con   dence values above     In turn t A  can be used to generate deterministic    xes for other attributes in the cleaning process  In other words  the process for    nding deterministic    xes in a database D is recursive  Nevertheless  in the rest of the section we show that deterministic    xes can be found in PTIME  stated as follows  Theorem 5 1  Given master data Dm and a set    of CFDs and MDs  all deterministic  xes in a relation D can be found in O  D  Dm size      time  where size     is    s length  2 5 2 Con   dence based Data Cleaning We next introduce the algorithm  followed by the indexing structures and procedures that it employs  Algorithm  The algorithm  denoted by cRepair  is shown in Fig  3  It takes as input CFDs     MDs     master data Dm  dirty data D  and a con   dence threshold     It returns a partially cleaned repair D     with deterministic    xes marked  The algorithm    rst initializes variables and indexing structures  lines 1   6   It then recursively computes deterministic    xes  lines 7   15   by invoking procedures vCFDInfer  line 12   cCFDInfer  line 13   or MDInfer  line 14   for the rules derived from variable CFDs  constant CFDs  or MDs  respectively  These indexing structures and procedures will Algorithm cRepair Input  CFDs     MDs     master data Dm  dirty data D  and con   dence threshold    Output  A partial repair D    of D with deterministic    xes  1  D       D  H          for each variable CFD           2  for each t     D    do 3  Q t          P t          4  count t       0 for each                  5  for each attribute A     attr            do 6  if t A  cf       then update t  A   7  repeat 8  for each tuple t     D    do 9  while Q t  is not empty do 10       Q t  pop    11  case   of 12   1  variable CFD  D       vCFDInfer t         13   2  constant CFD  D       cCFDInfer t         14   3  MD  D       MDInfer t     Dm      15  until Q t       is empty for any t         D      16  return D      Figure 3  Algorithm cRepair be discussed immediately  It terminates when no more deterministic    xes can be found  line 15   Finally  a partially cleaned database D     is returned in which all deterministic    xes are identi   ed  line 16   Indexing structures  The algorithm uses the following indexing structures  to improve performance  Hash tables  We maintain a hash table for each variable CFD      R Y     B  tp   denoted as H    Given a   y      Y    tp Y   D  as the key  it returns a pair  list  val  as the value  i e   H   y     list  val   where  a  list consists of all the tuples t in       y  such that t Bi  cf        for each attribute Bi     Y   and  b  val is t B  if it is the only item in       y  with t B  cf         otherwise  val is nil  Notably  there exist no two t1  t2 in       y  such that t1 B      t2 B   t1 B  cf        and t2 B  cf         if the con   dence placed by the users is correct  Queues  We maintain for each tuple t a queue of rules that can be applied to t  denoted as Q t   More speci   cally  Q t  contains all rules            where t C  cf        for all attributes C in LHS      That is  the premise of    is asserted in t  Hash sets  For each tuple t     D  P t  stores the set of variable CFDs        Q t  such that H   t LHS       val   nil  i e   no B attribute in     t LHS       has a high con   dence  Counters  For each tuple t     D and each rule            count t      is a counter that maintains the number of current values of the attributes C     LHS     such that t C  cf         Procedures  We present the procedures used in cRepair  update  Given a new deterministic    x for t A   it propagates the change and    nds other deterministic    xes with t A    a  For each rule     if A     LHS      count t      is increased by 1 since one more attribute becomes asserted   b  If all attributes in LHS     are asserted     is inserted into the queue Q t    c  For a variable CFD            P t   if RHS          is A and H        t LHS            val   nil  then the newly asserted t A  makes it possible for those tuples in H        t LHS            list to have a deterministic    x  Thus        is removed from P t  and added to Q t   to be examined  vCFDInfer  Given a tuple t  a variable CFD    and the con     dence threshold     it    nds a deterministic    x for t by applying    if it exists  If the tuple t and the pattern tuple t p     match on their LHS     attributes  we have the follows   a  If t RHS      cf        and no B attribute value inH   t LHS       list is asserted  it takes t RHS      as the value of the B attributes in the set  The change is propagated by procedure update   b  If t RHS           and there is a B attribute value val in H   t LHS       list with a high con   dence  a deterministic    x is made by changing t RHS      to val  The change is propagated via update   c  If t RHS           but no asserted B attributes in H   t LHS       list  we cannot make a deterministic    x at the moment  Tuple t is added to H   t LHS       list and P t   for later checking  cCFDInfer and MDInfer  The    rst one takes as input a tuple t  a constant CFD    and the threshold     The second one takes as input t      master data Dm and an MD     They    nd deterministic    xes by applying the rules derived from     as described earlier  The changes made are propagated by invoking update t  RHS       We next show by example how algorithm cRepair works  Example 5 1  Consider the master data Dm and the relation D in Fig  1  Assume    consists of rules   1    2 and   3  derived from the CFDs   1    3 and the MD    of Example 1 1  respectively  Let the threshold    be 0 8  Using    and Dm  it    nds deterministic    xes for t1  t2     D w r t     as follows   1  After initialization  lines 1   6   we have   a  H  2         b  Q t1       1   Q t2       2    c  P t1    P t2         and  d  count t1    1    1  count t1    2    0  count t1    3    3  count t2    1    0  count t2    2    2  and count t2    3    2   2  After   2     Q t2  is checked  line 12   we have Q t2         P t2       2   and H  2  t2 city  phn       t2   nil    3  After   1     Q t1  is applied  line 13   Q t1       3   count t1    2    1 and count t1    3    4  This step    nds a deterministic    x t1 city     Edi  It upgrades t1 city  cf  0 8   4  When   3     Q t1  is used  line 14   it makes a    x t1 phn     s1 tel   and lets t1 phn  cf   0 8  Now we have Q t1       2  and count t1    2    2   5  When   2     Q t1  is used  line 14   it    nds a deterministic    x by letting t2 St    t1 St     10 Oak St  and t2 St  cf    0 8  Now we obtain Q t1        and P t2          6  Finally  the process terminates since Q t1    Q t2         Similarly  for tuples t3  t4     D  cRepair    nds a deterministic    x by letting t3 city     Ldn and t3 city  cf    0 8  2 Su   x trees for similarity checking of MDs  For cleaning rules derived from MDs  we need to conduct similar  ity checking  to which traditional indexing techniques are not directly applicable  To cope with this  we develop a technique upon su   x trees  13   The measure of similarity adopted is the length of the longest common substring of two strings  Generalized su   x trees are built for the blocking process with all the strings in the active domain  When querying the k most similar strings of v of length  v   we can extract the subtree T of su   x tree that only contains branches related to v that contains at most  v  2 nodes  and by traversing T to    nd the k most similar strings  In this way  we can identify k similar values from Dm in O k v  2   time  which reduces the search space from  Dm  to a constant number k of tuples  Our experimental study veri   es that the technique signi   cantly improves the performance  Complexity  Note that for each CFD in     each tuple t in D is examined at most twice  For each MD  each tuple t     D is checked at most  Dm  times  From these it follows that algorithm cRepair is in O  D  Dm size             time  With the optimization techniques above  the time complexity of cRepair is reduced to O  D size              6  Reliable Fixes with Information Entropy Deterministic    xes may not exist for some attributes  e g   when their con   dences are low or unreliable  To    nd accurate    xes for these attributes  UniClean looks for evidence from data itself  using entropy to measure the degree of certainty  Below we    rst de   ne entropy for data cleaning  Section 6 1   and then present an algorithm to    nd reliable    xes based on entropy  Section 6 2   Finally we present an indexing structure that underlines the algorithm  Section 6 3   6 1 Measuring Certainty with Entropy We start with an overview of the standard information entropy  and then de   ne entropy for resolving con   icts  Entropy  The entropy of a discrete random variable X with possible values  x1          xn  is de   ned as  12 34   H X        n i 1 pi     log 1 pi   where pi is the probability of xi for i      1  n   The entropy measures the degree of the certainty of the value of X   when H X   is su   ciently small  it is highly accurate that the value of X is the xj having the largest probability pj   The less H X   is  the more accurate the prediction is  Entropy for variable CFDs  We use entropy to resolve data con   icts  Consider a CFD      R Y     B  tp  de   ned on a relation D  where tp B  is a wildcard  Note that a deterministic    x may not exist when  e g   there are t1  t2 in       y   see Table 1  such that t1 B      t2 B  but both have high con   dence  Indeed  using the cleaning rule derived from     one may either let t1 B     t2 B  by applying t2 to t1  or let t2 B     t1 B  by applying t1 to t2  To    nd an accurate    x  we de   ne the entropy of    for Y     y  denoted by H    Y     y   as H    Y     y       k i 1  cntY B   y bi         y       logk        y   cntY B   y bi     where  a  k     B       y     the number of distinct B values in       y    b  for each i      1  k   bi      B       y     c  cntY B   y  bi  denotes the number of tuples t           y  with t B    bi  and  d         y   is the number of tuples in       y   Intuitively  we treat X     Y     y  as a random variable for the value of the B attribute in       y   with a set  B       y   of possible values  The probability for bi to be the value is pi   cntY B   y bi         y     Then when H    Y     y  is small enough  it is highly accurate to resolve the con   ict by letting t B    bj for each t           y   where bj is the one with the highest probability  or in other words  when cntY B   y  bj   is the maximum among all bi      B       y    In particular  H    Y     y    1 when cntY B   y  bi    cntBA   y  bj   for all distinct bi  bj      B       y    and if H    Y     y    0 for all   y      Y   Y    tp Y  D   then D        6 2 Entropy based Data Cleaning We    rst describe an algorithm based on entropy  followed by presenting its main procedures and auxiliary structures  Algorithm  The algorithm  referred to as eRepair  is shown in Fig  4  Given a set    of CFDs  a set    of MDs  a master relation Dm  dirty data D  and two thresholds   1 and   2 for update frequency and entropy  respectively  it    nds reliable    xes for D and returns a  partially cleaned  database D    Algorithm eRepair Input  CFDs     MDs     master data Dm  dirty data D  update threshold  1  entropy threshold  2  Output  A partial repair D    of D with reliable    xes  1  O    the order of            sorted via their dependency graph  2  D       D  3  repeat 4  for  i   1  i                  i    do 5       the i th rule in O  6  case   of 7   1  variable CFD  D       vCFDReslove D          1   2   8   2  constant CFD D       cCFDReslove D          1   9   3  MD  D       MDReslove D      Dm      1   10  until there are no changes in D    11  return D      Figure 4  Algorithm eRepair Figure 5  Example dependency graph in which reliable    xes are marked  The deterministic    xes found earlier by cRepair remain unchanged in the process  The algorithm    rst    nds an order O on the rules in            line 1   It then repeatedly applies the rules in the order O to resolve con   icts in D  lines 3   10   by invoking procedures vCFDReslove  line 7   cCFDReslove  line 8  or MDReslove  line 9   based on the types of the rules  lines 5 6   It terminates when either no more rules can be applied or all data values have been changed more than   1 times  i e   when there is no enough information to make reliable    xes  line 10   A partially cleaned database is returned with reliable    xes being marked  line 11   In a nutshell  algorithm eRepair    rst sorts cleaning rules derived from the CFDs and MDs  such that rules with relatively bigger impact are applied early  Following the order  it then applies the rules one by one  until no more reliable    xes can be found  Procedures  We next present those procedures  Sorting cleaning rules  To avoid unnecessary computation  we sort           based on its dependency graph G    V  E   Each rule of           is a node in V   and there is an edge from a rule   1 to another   2 if   2 can be applied after the application of   1  There exists an edge  u  v      E from node u to node v if RHS   u      LHS   v           Intuitively  edge  u  v  indicates that whether   v can be applied depends on the outcome of applying   u  Hence    u is applied before   v  For instance  the dependency graph of the CFDs and MDs given in Example 1 1 is shown in Fig  5  Based on G  we sort the rules as follows   1  Find strongly connected components  SCCs  in G  in linear time  11    2  By treating each SCC as a single node  we convert G into a DAG   3  Find a topological order on the nodes in the DAG  That is  a rule   1 is applied before another   2 if the application of   1 a   ects the application of   2   4  Finally  the nodes in each SCC are further sorted based on the ratio of its out degree to in degree  in a decreasing order  The higher the ratio is  the more e   ects it has on other nodes  Example 6 1  The dependency graph G in Fig  5 is an SCC  The ratios of out degree to in degree of the nodes   1    2    3    4 and    are 2 1   2 1   1 1   3 3 and 2 4   respectively  Hence the order O of these rules is   1     2     3     4       where those nodes with the same ratio are sorted randomly  2 vCFDReslove  It applies the cleaning rule derived from a variable CFD      R Y     B  tp   For each set       y  with   y in A B C E F H t1  a1 b1 c1 e1 f1 h1 t2  a1 b1 c1 e1 f2 h2 t3  a1 b1 c1 e1 f3 h3 t4  a1 b1 c1 e2 f1 h3 t5  a2 b2 c2 e1 f2 h4 t6  a2 b2 c2 e2 f1 h4 t7  a2 b2 c3 e3 f3 h5 t8  a2 b2 c4 e3 f3 h6 Figure 6  Example relation of schema R  Y   Y    tp Y  D   if H    Y     y  is smaller than the entropy threshold   2  it picks the value b     B       y   that has the maximum cntY B   y  b   Then for each tuple t           y   if t B  has been changed less than   1 times  i e   when t B  is not often changed by rules that may not converge on its value  t B  is changed to b  As remarked earlier  when the entropy H    Y     y  is small enough  it is highly accurate to resolve the con   icts in  B       y   by assigning b as their value  cCFDReslove  It applies the rule derived from a constant CFD      R X     A  tp1    For each tuple t     D  if  a  t X      tp1  X    b  t A      tp1  A   and  c  t A  has been changed less than   1 times  then t A  is changed to the constant tp1  A   MDReslove       It applies the rule derived from an MD      j    1 k   R Aj      j Rm Bj        R E    Rm F   For each tuple t     D  if there exists a master tuple s     Dm such that  a  t Aj      j s Bj   for j      1  k    b  t E      s F   and  c  t E  has been changed less than   1 times  then it assigns the master value s F  to t E   These procedures do not change those data values that are marked deterministic    xes by algorithm cRepair  We next show by example how algorithm cRepair works  Example 6 2  Consider an instance of schema R ABCEFH  shown in Fig  6  and a variable CFD      R ABC     E  tp1    where tp1 consists of wildcards only  i e      is an FD  Observe that  a  H    ABC    a1  b1  c1       0 8  b  H    ABC    a2  b2  c2   is 1  and  c  H    ABC    a2  b2  c3   and H    ABC    a2  b2  c4   are both 0  From these we can see the following   1  For     ABC    a2  b2  c3   and     ABC    a2  b2  c4    the entropy is 0  hence these sets of tuples do not violate     i e   there is no need to    x these tuples   2  The    x based on H    ABC    a1  b1  c1   is relatively accurate  but not those based on H    ABC    a2  b2  c2    Hence the algorithm will only change t4 E  to e1  and marks it as a reliable    x  In contrast  the data D of Fig  1 has too few tuples to infer sensible entropy  No reliable    xes are found for D  2 Complexity  The outer loop  lines 3   10  in algorithm eRepair runs in O   1 D   time  Each inner loop  lines 4   9  takes O  D        k D size      time using the optimization techniques of Section 5 1  where k is a constant  Thus  the algorithm takes O   1 D  2          1k D  2 size      time  6 3 Resolving Con   icts with a 2 in 1 Structure We can e   ciently identify tuples that match the LHS of constant CFDs by building an index on the LHS attributes in the database D  We can also e   ciently    nd tuples that match the LHS of MDs by leveraging the su   x tree structure developed in Section 5  However  for variable CFDs  two issues still remain   a  detecting violations and  b  computing entropy  These are rather costly and have to be recomputed when data is updated in the cleaning process  To do these we develop a 2 in 1 structure  which can be easily maintained  Let   V be the set of variables CFDs in     and attr   V   be the set of attributes appearing in   V   For each CFD   Figure 7  Example data structure for variable CFDs   R Y     B  tp  in   V   we build a structure consisting of a hash table and an AVL tree  11  T as follows  Hash table HTab  Recall       y     t   t     D  t Y       y  for y        Y   Y    tp Y  D  described earlier  For each       y   we insert an entry  key  val  into HTab  where key     y  and val is a pointer linking to a node u        l  r  o   where  a  u      H    Y     y    b  u l is the value count pair    y         y      c  u r is the set   b  cntY B   y  b     b      B       y     and  d  u o is the set of  partial  tuple IDs  t id   t           y    AVL tree T  For each   y      Y   Y    tp Y  D  with entropy H    Y     y    0  we create a node    v   HTab   y  in T  a pointer to the node u for       y  in HTab  For each node v in T  its left child vl        v    and its right child vr        v     Note that both the number HTab of entries in the hash table HTab and the number  T  of nodes in the AVL tree T are bounded by the number  D  of tuples in D  Example 6 3  Consider the relation in Fig  6 and the variable CFD    given in Example 6 2  The hash table HTab and the AVL tree T for    are shown in Fig  7  2 We next show how to use and maintain the structures   1  Lookup cost  For the CFD     it takes  a  O log  T   time to identify the set       y  of tuples with minimum entropy H    Y     y  in the AVL tree T  and  b  O 1  time to check whether two tuples in D satisfy    via the hash table HTab   2  Update cost  The initialization of both the hash table HTab and the AVL tree T can be done by scanning the database D once  and it takes O  D  log  D    V    time  After resolving some con   icts  the structures need to be maintained accordingly  Consider a set       y  of dirty tuples  When a reliable    x is found for       y  based on H    Y     y   we do the following   a  remove a node from tree T  which takes O log  T   time  where  T       D   and  b  update the hash tables and trees for all other CFDs  which takes O        y     V            y   log  D   time in total   3  Space cost  The structures take O  D size   V   space for all CFDs in   V in total  where size   V   is the size of   V   Putting these together  the structures are e   cient in both time and space  and are easy to be maintained  7  Experimental Study We next present an experimental study of our data cleaning techniques underlying UniClean  which unify matching and repairing operations  Using real life data  we evaluated  1  the e   ectiveness of our data cleaning algorithms   2  the accuracy of deterministic    xes and reliable    xes  and  3  the scalability of our algorithms with the size of data  Experimental Setting  We used two real life data sets   1  hosp data was taken from US Department of Health   Human Services 1   It has 100K records with 19 attributes  We designed 23 CFDs and 3 MDs for hosp  26 in total   2  dblp data was extracted from dblp Bibliography 2   It consists of 400K tuples  each with 12 attributes  We designed 7 CFDs and 3 MDs for dblp  10 in total   3  Master data for both datasets was carefully selected from the same data sources so that they were guaranteed to be correct and consistent w r t  the designed rules   4  Dirty datasets were produced by introducing noises to data from the two sources  controlled by four parameters   a   D   the data size   b  noi   the noise rate  which is the ratio of the number of erroneous attributes to the total number of attributes in D   c  dup   the duplicate rate  i e    the percentage of tuples in D that can    nd a match in the master data  and  d  asr   the asserted rate  For each attribute A  we randomly picked asr  of tuples t from the data and set t A  cf   1  while letting t      A  cf   0 for the other tuples t       The default value for asr  is 40   Algorithms  We implemented the following algorithms  all in Python   a  algorithms cRepair  eRepair and hRepair  an extension of algorithm in  10   in UniClean   b  the sorted neighborhood method of  25   denoted by SortN  for record matching based on MDs only  and  c  the heuristic repairing algorithm of  10   denoted by quaid  based on CFDs only  We use Uni to denote cleaning based on both CFDs and MDs  matching and repairing   and Uni CFD  to denote cleaning using CFDs  repairing  only  We used edit distance for similarity test  de   ned as the minimum number of single character insertions  deletions and substitutions needed to convert a value from v to v       Quality measuring  We adopted precision  recall and F  measure  which are commonly used in information retrieval  where F measure   2     precision    recall   precision   recall   For record matching   a  precision is the ratio of true matches  true positives  correctly found by an algorithm to all the duplicates found  and  b  recall is the ratio of true matches correctly found to all the matches between a dataset and master data  For data repairing   a  precision is the ratio of attributes correctly updated to the number of all the attributes updated  and  b  recall is the ratio of attributes corrected to the number of all erroneous attributes  All experiments were conducted on a Linux machine with a 3 0GHz Intel CPU and 4GB of Memory  Each experiment was run more than 5 times  and the average is reported here  Experimental Results  We conducted    ve sets of experiments   a  in the    rst two sets of experiments  we compared the e   ectiveness of our cleaning methods with both matching and repairing against its counterpart with only matching or only repairing   b  we evaluated the accuracy of deterministic    xes  reliable    xes and possible    xes in the third set of experiments   c  we evaluated the impact of the duplicate rate and asserted rate on the percentage of deterministic    xes found by our algorithm cRepair in the fourth set of experiments  and  d  the last set of experiments tested the scalability of Uni with both the size of dirty data and the size of master data  In all the experiments  we set the threshold for entropy and con   dence to be 0 8 and 1 0  respectively  1 http   www hospitalcompare hhs gov  2 http   www informatik uni trier de    ley db  0 2  0 3  0 4  0 5  0 6  0 7  0 8  0 9  1 2 4 6 8 10 F measu</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11dp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11dp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Data_provenance_workflow_and_cleaning"/>
        <doc>Tracing Data Errors with View Conditioned Causality ### Alexandra Meliou y Wolfgang Gatterbauer y Suman Nath x Dan Suciu y y University of Washington  Seattle  WA  USA  ameli gatter suciu  cs washington edu xMicrosoft Research  Redmond  WA  USA Suman Nath microsoft com ABSTRACT A surprising query result is often an indication of errors in the query or the underlying data  Recent work suggests us  ing causal reasoning to  nd explanations for the surprising result  In practice  however  one often has multiple queries and or multiple answers  some of which may be considered correct and others unexpected  In this paper  we focus on determining the causes of a set of unexpected results  pos  sibly conditioned on some prior knowledge of the correct  ness of another set of results  We call this problem View  Conditioned Causality  We adapt the de nitions of causa  lity and responsibility for the case of multiple answers views and provide a non trivial algorithm that reduces the problem of  nding causes and their responsibility to a satis ability problem that can be solved with existing tools  We evaluate both the accuracy and e ectiveness of our approach on a real dataset of user generated mobile device tracking data  and demonstrate that it can identify causes of error more e ec  tively than static Boolean in uence and alternative notions of causality  Categories and Subject Descriptors  E 0 Data General General Terms  Algorithms Keywords  causality  view conditioning  error tracing ### 1  INTRODUCTION Data transformations from a source to a target dataset are ubiquitous today and can be found in data integration  26   data exchange  25  3   and ETL tools  31   Users often detect errors in the target data  For example  a user may detect that an item in a target data instance is incorrect  the tuple should not be there  or some of its attribute values are erroneous  she would like to  nd out which of the many input tuples that contributed to the incorrect output is faulty  It is critical that the error be traced and corrected in the source data  because once an error is identi ed  one can prevent   This work was partially funded by NSF IIS 0911036  IIS  0915054  and IIS 0713576  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  Accelerometer      eriodicity   Cell Tower      ate of change   GPS     as signal   Light      ntensity   Is Driving   Is Indoor   h p r i M p   0 03  r    1  h  M   h  i   100  Figure 1  Two simple classi ers based on sensory input from a cell phone  The inputs to the classi ers are either Boolean variables  e g  h   or scalar values to which an appropriate threshold is applied  e g  i   Symbol M denotes a strict majority function  it from propagating to multiple items in the target data  This can be viewed as a form of  post factum  data cleaning  while in standard data cleaning one corrects errors before the data is transformed and integrated  in our setting the errors are detected only after the data has been transformed  In this paper  we show how to extend the concept of cau  sality  28  to trace errors in a source dataset that caused incorrect results in a target dataset  We showcase our moti  vation through a practical example  Example 1  Recommendation System   Consider a new generation smart phone  It has multiple sensors  e g  GPS  accelerometer  light  and cell tower signal   Based on these sensors  a set of classi ers can predict the owner s current activities  e g  walking  driving  working  being with family  or being in a business meeting   Using the knowledge of the user s current activities allows the system to serve the user with useful targeted recommendations  For example  if the user is away from their car around lunch time  the application will suggest restaurants within walking distance  This is an example of data transformation  the source data  input  are the sensors  the target data  output  are the ac  tivities  Inaccuracies in the sensor data are a common oc  currence  sensors have innate imprecisions  e g   the GPS may miscalculate the current location   or some sensors may become inhibited  e g   if the user places the cell phone in the glove compartment while driving  then the light sensor s reading is incorrect   As a consequence  an inferred activ  ity may be wrong  The application can often detect such errors from user feedback or based on the user s subsequent actions and reactions to the provided recommendations  Butthe main challenge is to actually identify the responsible sen  sor s   For example  in Fig  1 the  is driving  activity de  pends on 3 sensors  if  is driving  is wrong  which of the three sensors is erroneous  With this knowledge  the sys  tem could inhibit the reading from that sensor and therefore improve the other classi ers  The  rst step towards tracing errors in the source data is to compute the lineage of each item in the target data  Data lineage  or provenance  has been extensively studied recently  10  16  5   But as Example 1 shows  examining the lineage of an erroneous result is not su cient to identify the cause of the problem  all three sensor inputs are part of the lineage  and without further context  it is not possible to determine which speci c sensor is erroneous  In this work  we identify erroneous inputs by building upon a long history of work on causality theory  Albeit re  lated to lineage  causality is a more re ned notion  The lineage of an erroneous output re ects causal dependencies between the input and output data  and causal reasoning can use this information to return the possible causes ranked by their degree of responsibility  Causality has been studied extensively in philosophy  AI  and cognitive science  We  base our discussion on the established de nitions of actual causes and responsibility  rst developed by Halpern  Pearl and Chockler  11  17   and our previous work that adapted these de nitions to a database context  targeting query re  sult explanations  28   In error tracing  however  we face a richer context than in traditional causality theory  In addition to the erroneous output item  we also have access to several output items that we know are correct  In fact  there may even be more than one erroneous output item  Accounting for this richer context leads to much better error tracing  Example 2  Recommendation System   continued   A modern smart phone typically has 10 12 sensors  and could have up to 20 30 classi ers  predicting a variety of activities  Based on the user s response to recommendations  the sys  tem can typically derive that multiple predictions are correct or incorrect  Suppose  for example  that the system knows that 3 predictions are wrong and 2 are correct  Now the sys  tem has more information than just knowing that one clas  si er is wrong  The challenge is to  nd the most likely input sensor that could have caused the 3 faulty predictions  while at the same time allowing the 2 correct predictions  Thus  conditioned on the two correct outputs  we seek the most responsible sensor s  that explain the faulty classi ers  For example  assume the two simple classi ers of Fig  1  if both are incorrect  then intuitively  the GPS becomes the most likely culprit  if the  is driving  classi cation is wrong and  is indoor  is correct  then we know that the GPS input has to be correct  In this paper  we propose View Conditioned Causality  a novel concept that allows us to trace errors in transformed target data  such as query results or views  back to the input base data  Our main contributions are as follows   1  We propose a new de nition of view conditioned cau  sality and its associated notion of responsibility  In order to use causality for error tracing  we re ne existing notions of causality responsibility by conditioning on all knowledge of correct or incorrect results   2  Computing causality and responsibility is known to be hard in general  14   Previously  we have studied the complexity of causality and responsibility for conjunctive queries  28   Despite some tractable cases  computing the responsibility remains hard  in general  To date  no practi  cal algorithms exist to compute causes or responsibilities for all cases  In order to compute causality and responsibility in practice  we propose a novel algorithm for translating the view conditioned causality and responsibility problems into a SAT and a weighted MAX SAT problem respectively  for which very e ective tools exist  2  4   In the case of causa  lity  our algorithm produces a Boolean expression  SAT that is satis able i  a given input variable is a view conditioned cause  In the case of responsibility  our algorithm also pro  duces  in addition to the  hard  constraint  SAT  several  soft  clauses  the larger the number of soft clauses that cannot be satis ed  the lower the responsibility   3  The formula  SAT needs to be converted to CNF in order to be processed by general  Max SAT solvers  and this has the potential of an exponential blow up  Our third contribution describes an optimized conversion to CNF that results in exponentially smaller CNF expressions   4  Finally  we perform an experimental validation of our techniques on real data from an actual application analogous to Example 1  Our experiments verify the quality of our ap  proach in identifying erroneous inputs  and evaluate its per  formance  We show that view conditioned causality retains an average precision very close to 1  outperforming other techniques such as counterfactual causality and Boolean in   uence  by 60  and 75  respectively at certain instances  Our optimized algorithm for the CNF conversion is shown to produce SAT problems orders of magnitude smaller than the naive approach  while the SAT solvers were able to process even the larger generated problem instances in a few sec  onds  We further demonstrate interesting insights on how responsibility of features can function as a meaningful quan  ti er of classi er quality and robustness  classi ers that use features with low responsibility can be easily corrected by eliminating faulty inputs  whereas classi ers whose features are all of high responsibility  i e  they are all likely sources of failure   cannot achieve good correction rates  The structure of this paper follows its main contributions    We propose view conditioned causality and responsi  bility as a solution for tracing errors from views and transformations to source data  Sect  3     We demonstrate a non trivial algorithm that reduces the problems of computing causes and responsibility to the satis ability of Boolean expressions which can be solved with existing tools  Sect  4     We propose an optimized conversion algorithm of the resulting Boolean expressions into CNF  in order to be fed into a SAT or MaxSAT solver  Sect  5     We illustrate the e ectiveness of using our techniques for tracking errors in a real world classi er based rec  ommendation system  Sect  6   2  BACKGROUND AND RELATED WORK Causality is an active area of research  mostly in AI and philosophy  One of the basic notions in causality theory is counterfactual causality  29  27   An input variable is cou  nterfactual if a change in its value reverses the value of the output variable  Counterfactuals are a very intuitive notion  but very limited in their applicability as they don t supportdisjunctive causes  If Z occurs when X1 or X2 occurs  nei  ther of X1 or X2 are counterfactual causes for Z  Halpern and Pearl  17  extended counterfactuals by in  troducing actual causes which rely upon a graph structure called a causal network  and added the notion of permissive contingencies  The causal network is de ned by structural equations  which describe how variables relate to each other  for example Z   X1  X2 is a structural equation linking in  puts X1 and X2 to output Z  Contingencies are based on the notion of intervention  what becomes counterfactual if part of the input changes  For example  if X1 and X2 are both true  neither of them is counterfactual for Y   However  if the input changes in a certain way  for instance X2 is false  then X1 becomes counterfactual  One says that X1 is an actual cause of Z under the contingency X2   false  Such contingencies can be viewed as alternative possible worlds or variable assignments  Roughly  a variable X is an actual cause if there exists a contingency  a value assignment for the other variables  that makes X counterfactual  The notion of responsibility was  rst de ned in  11  as a measure of the degree of causality  the responsibility is in  verse proportional to the size of the smallest contingency set  This is a key notion for our setting  because responsibi  lity allows us to rank causes  Ranking is critical in complex queries with large lineages  and in the case of error tracing it serves as a comparison metric of the likelihood of the various error sources  However  determining causes and responsibi  lity was shown to be NP hard in general  14   Very recently  we extended the notions of causality and responsibility to database queries  28   Here the input vari  ables are tuples in the input database  the output variables are the answer tuples returned by the query  and the causal network is given by the lineage expression  An input tuple is a counterfactual cause for an output tuple if its removal from the input causes the output to disappear  An input tuple is an actual cause for an output tuple if there exists a set of tu  ples in the input  the contingency  such that  after removing them  the input tuple becomes a counterfactual cause  The complexity of causality and responsibility were analyzed as a function of the query  query complexity   These results are unrelated to our work  since we analyze directly the lineage expression and do not consider query complexity  Neither the general work on causality  nor its adaptation to data  base queries have studied the problem of tracing errors from output data to input data  Our work relates to prior work on data provenance  in the sense that our input is a provenance expression  There  ap  proaches are mainly classi ed into three categories  how   why   and where  provenance  6  10  13  16   Also  recent work has focused on the problem of explaining missing query results  i e  why a certain tuple does not appear as an an  swer  The work in this  eld is divided into data focused  22  20  19  where explanations are given as base tuples  and query focused  8  33  where explanations are based on query predicates  The ability to rank causes clearly distinguishes causality from the provenance and missing answers work  and makes it more appropriate for tracking errors  In inde  pendent work  Cheney  9  also discusses the analogy between provenance and causality  and describes de ning causal se  mantics for provenance graphs  An interesting  and related work is the view side e ects problem on tuple deletion and annotation propagation  7   This problem involves  nding a subset of tuples in the data  base whose deletion will eliminate a given tuple t from the view  with the minimum number of side e ects  other tu  ple eliminations  in the view  An analogous de nition of the problem exist for tuple insertion and updates  and all are shown to be hard  in general  Very recent work  12  studies the  side e ect free  version of this problem  where zero side e ects are enforced  and the aim is to minimize the subset of tuples selected from the database  Here  the same hardness results hold as in the general version of the problem  Note that these problems di er from computing responsibility  where we need to  nd the minimum set of input tuples that make a given tuple counterfactual  Also  the view side e ects problem is in PTIME for queries that do not contain joins together with projection or union  In contrast  we showed the hardness criterion for computing responsibilities to be entirely di erent  28   and even queries that only contain joins can be NP hard  3  VIEW CONDITIONING Formal setup  We use capital letters for variables  e g  X   small letters for value assignments  e g  x   and bold letters for sets of variables or assignments  e g  X or x   We denote Boolean values interchangeably with true and false or T and F  respectively  We consider a set of n input variables  also called parameters  X   fX1          Xng  and a set of observed output variables Z   fZ1          Zmg  Each input variable Xi takes values from a discrete or continuous domain  e g  reals  integers  Booleans  Each output variable Zj is Boolean  We denote     f 1           mg a transforma  tion from the input variables to the output variables  Each  j is called a lineage expression  and is a Boolean expression over the input variables  de ned as follows  Call a threshold predicate a predicate of the form Xi op t  where Xi is an input variable  op is one of           6       and t is a con  stant threshold value  Then  j is any Boolean expression over threshold predicates  Thus  the transformation   takes an input vector x of values  and computes the output vector z  where each zj    j  x   We write x j  z  to denote that z are the output values of the view transformations given input values x  In addition to the input and output vectors  we further assume to know the correct values for the out  put Z and denote this ground truth as z    fz 1          z mg  If zi    zi then variable Zi is correct  otherwise it is incorrect  For a simple illustration  consider a simple classi er that computes Z    X1   10     X2   3   Given two input values x1  x2 the classi er returns the output T or F  in an obvious way  Classi er systems are more complex types of transformations  For a more traditional transformation ex  ample  consider a select project join query  Each input tuple corresponds to a Boolean variable Xi  and each output tuple is a Boolean variable Zj   The formula  j is the standard lineage expression giving the dependency of Zj as a function of the input tuples  16   Note that our discussion will focus on classi er systems  but our results can be invariably used for other transfor  mation types like conjunctive queries as well  Even though data sizes are much larger in query scenarios than sensor based applications  the lineage of each output tuple is ac  tually bound by the query size  Moreover  we only need to keep as relevant variables those that participate in the lin  eage of erroneous outputs  thus greatly reducing the problem size  Finally  the SAT solver component is not prohibitive to this end  as modern SAT solvers can solve problems havingmillions of variables and clauses in less than 10 minutes of run time  cf   1    3 1 The Problem  Tracing Data Errors By comparing the output values z to the ground truth z   we can detect errors in the output  This means that there is an error in the input data X  but we don t know which one  and only know which output is erroneous  Thus  while we may learn the ground truth for the output values  we don t know the ground truth for the input variables  For example  in Example 1 it is not possible to know whether a sensor is inhibited by looking at its measurements  but it is quite possible to determine that a classi er was incorrect  by examining the user s actions  Our goal is to trace the output errors back to the input data  and this justi es the following problem  Error tracing problem  Given input values x for the input variables X  a set of transformations   comput  ing the values z of the output variables Z  and given a ground truth z  for the output variables  detect the sources of error in the input data  Clearly  if z   z  then there are no errors  We will assume that z  6 z  for the rest of the paper  In our problem setting  the ground truth includes values for all output variables Z  In practice that is often not the case  we know the ground truth for some  but not all output variables  In that case we simply restrict the output variables to those for which the ground truth is known  Thus  in the rest of this paper we will assume that the ground truth is known for all the output variables Z  3 2 Our Approach  View Conditioned Causality We de ne here counterfactual causes  and actual causes for a set of output values  which we call view conditioned causality  Then  we explain how this can be applied to trac  ing errors  As a  rst attempt we may try to say that an in  put Xi is a view conditioned counterfactual cause  or VCC cause  in short   if a change in the value xi  corrects  the output z to the ground truth output z   we say in this case that Xi is a VCC cause of zjz   and read that  z is condi  tioned on z    However  this is a tall order  The problem is that we are requiring that Xi  alone  can correct simultane  ously all erroneous output values  while keeping unchanged all correct output values  Instead  our de nition considers an entire set of input variables to be a counterfactual cause  Definition 3  VCC Cause   Consider input values x for the input variables X  a transformation   computing the output values z  and a ground truth z   A set Xc   X is a view conditioned counterfactual cause of zjz   if there exist values x i for each Xi 2 Xc  such that the following two conditions hold    x j  z and ffXi   xi j Xi 62 Xcg fXi    xi j Xi 2  XnXc gg j  z    Xc is minimal  i e  no subset of Xc is a VCC cause  Intuitively  a VCC cause is a minimal set of input variables for which there exists a changed assignment that results in output z   Example 4  VCC causes   Consider the views Z given in Fig  2b over the inputs X  where X1 and X3 take values X x X1 5 X2 T X3 2  a  Z   z Z1  X1   10    X2 T Z2  X3   0    X2 T Z3  X3   3  F  b  Figure 2   a   Value assignments for inputs X   b   The values of outputs Z are based on simple trans  formations  also called lineages    of the inputs X  from the integer domain  while X2 is Boolean  Based on the input values from Fig  2a  the observed output values are z   fz1  z2  z3g   fT  T  Fg  Assume that the ground truth is z    fF  T  Tg  where the erroneous results are underlined  Suppose  rst that we restrict the output variable to Z1  thus  ignore Z2  Z3   Both X1 and X2 are counterfactual causes of Z1  changing the value of X1  e g  X1   12   or the value of X2 from T to F  would make Z1 F  However  if we take into account all three output variables  then X2 is not a VCC cause of zjz   if X2 is switched to F  Z1 would be  cor  rected  but Z2 would also become F  which would contradict the ground truth z   In this example  there is a unique VCC cause of zjz   which is the set of inputs fX1  X3g  switching their values to 12 and 4  respectively  sets all views to the desired output z   Actual causes in view conditioned causality are de ned in a similar manner based on contingency sets  Definition 5  VC Cause   Consider input values x for the input variables X  a transformation   computing the output values z  and a ground truth z   A variable Xi 2 X is a view conditioned cause  VC cause  of zjz   if there ex  ists a set    X called the contingency of Xi  such that fXig    is a VCC cause of zjz   Note that every variable Xi in a VCC cause Xc of zjz  is an VC cause with contingency Xc n fXig  Also  every VCC cause is a VC cause with an empty contingency set  There  fore  in Example 4  X1 is a VC cause of zjz  with contingency X3  and X3 is a VC cause with contingency X1  Hence  even though in our example only sets of input variables are VCC causes  there are also individual input values which are VC causes  Finally  we de ne the notion of responsibility  which mea  sures the degree of causality  Definition 6  Responsibility   Let X be a set of in  puts to a set of views Z  and assume z and z  to be the actual and ground truth values of those views  respectively  If Xi   X is a VC cause of zjz   then its responsibility is de ned as  Xi   1 1   min jj where  is a contingency for Xi  The responsibility is a function of the minimal number of input variables that need to be modi ed together with Xi in order to achieve the output z  for the views  If Xi is cou  nterfactual  then its contingency set is empty  and therefore  Xi   1  If Xi is not a cause  by convention  Xi   0  Our approach to the error tracing problem  Given x X     z   we will compute all causes Xi  and rank them by their responsibility  Xi   This ranking is a good indicator for tracing the errors in the input data                  Preprocessing X1 X2 Xn Sensor data Transformation Z1 Zm   1       op t  1  1 Y  1  1 Y  1  op t n  1  n C1         m Y  m  2 Y  m  n op t  m  2 op t  m  n Cm Output variables P Input variables Figure 3  Structure of a classi er system  We end this section by comparing our de nition of cau  sality and responsibility to prior work  17  28   In the prior de nition  there is only one output variable  and this allows the de nition of a counterfactual cause to be restricted to a single input variable  As we have seen  in error tracing applications we need to consider multiple output variables  some correct and some incorrect   and here the simple def  inition of counterfactual cause no longer works  as we saw in Example 4   Our new Def  3 is an important extension to sets of counterfactual causes  On the other hand  this extension allows us to simplify  somewhat  the de nition of an actual cause  in that the contingency set  is simply part of a counterfactual cause  Example 7  Continuing our running example  we show it can be modeled in our formalism  Our application is a classi cation system with several sensor signals  and sev  eral classi ers determining user activities and status based on the sensor data  Sensor measurements can be in general vectors of values  like a time sequence of a signal  Before this data can be useful for the classi cation  it often under  goes some initial mathematical transformation to compute several useful metrics of the input signals  e g  frequency  signal strength etc   We are not interested in the raw sig  nal  but rather these computed metrics that serve as inputs to the classi ers  These represent our input x   fx1          xng  There are m classi ers fC1          Cmg  producing a set of out  puts Z   fZ1          Zmg  As an example  Z may represent a classi cation of the user s current activities  walking  driv  ing  in a meeting   An abstraction of the architecture of the system is shown in Fig  3  The classi er inputs x are continuous values  or Boolean values  Each classi er Cj uses a subset of the transformed values xj   x to compute a classi cation result Zj   Within each classi er Cj   each variable xi 2 xj is com  pared against one or more predetermined thresholds  result  ing in a Boolean variable Y  j  i   These thresholds are usually selected using machine learning techniques over some train  ing data  In general  Y  j  i    xi op ti   where op is one of f          g  and ti is a threshold value  The  nal output Zj of the classi er is determined by a Boolean function  j  Yj    4  REDUCTION TO SAT AND MAXSAT Computing the causality and responsibility are known to be NP hard  14   and the same holds for view conditioned  causality  Our approach for tracing errors to the data source is to reduce the causality responsibility problem to the SAT problem  and to partial weighted MaxSAT problem respec  tively  This is an intensively studied research area  an ex  tensive survey is in  4   and there exist several highly op  timized tools both for SAT and weighted MaxSAT  In this section we describe an algorithm for reducing the causa  lity responsibility problem to the SAT MaxSAT problem  In the next section we show how to convert the resulting Boolean expressions into CNF  so we can feed them directly into some of these tools  The algorithm consists of the following steps 1  Map from input variables with continuous domain to Boolean variables called partitioned variables  2  Construct the constraint expression   that captures the correlations between the partitioned variables  3  For each input Xi  construct a Boolean expression  SAT that is satis able i  Xi is a VC cause   SAT is a hard constraint  4  Construct  soft  constraints to account for the contin  gency set  the more soft constraints are violated while satisfying  SAT the larger the contingency set  4 1 Mapping to Boolean Variables Recall that while all the outputs Z are Boolean variables  some of the inputs X can be numerical values  and the Boolean expressions   are over threshold predicates over these inputs  e g    3    X1   2 5     X6   20   Since we need to map all inputs to Boolean variables  a  rst attempt is to create one Boolean variable for each threshold predicate  call it a threshold variable  An example threshold variable is Y  3  1    X1  2 5   where the subscript 1 refers to the input variable X1  and the superscript  3  refers to the classi er  3  But threshold variables are not independent  and this prevents us from using them in our translation  Example 8  Boolean mapping   Assume 3 classi ca  tions over 6 inputs   1     X1   10     X2   7        X5   10     X3   20       Y  1  1   Y  1  2      Y  1  5   Y  1  3    2     X1   4     X4   5       Y  2  1   Y  2  4    3    X1   2 5     X6   20    Y  3  1   Y  3  6 The three threshold variables Y  1  1   Y  2  1 and Y  3  1 for the input X1 are dependent in di erent ways  Y  1  1 cannot be true together with Y  2  1 or Y  3  1   whereas there exist values of X1 for which Y  2  1 and Y  3  1 can both be true  Therefore  we take a di erent approach  For a given vari  able Xi  let ft1          tTi g be the set of all thresholds that appear in threshold predicates associated with variable Xi across all transformations  We de ne a set of 2Ti 1 Boolean partition variables X i    fX i 1            X i 2Ti 1 g as follows  X i 1     Xi   t1  X i 2j     Xi   tj   1   j   Ti X i 2j 1     tj   Xi   tj 1  1   j   Ti X i 2Ti 1     Xi   tTi   Denote X   the set of all partition variables  X     S i X i    Then  we rewrite each threshold predicate as a disjunction       of these variables  and we rewrite each Boolean expression  j in terms of the partitioned variables  denoting      j the rewritten expression  when it is clear from the context  we drop the superscript    and use  j also for the translated expression  Note that the use of partition variables only linearly increases the number of variables with the number of thresholds per variable  with two new variables created for every threshold  Example 9  Example 8 continued   The partition vari  ables of X1 are given as  X 1 1     X1   2 5  X 1 5     4   X1   10  X 1 2     X1   2 5  X 1 6     X1   10  X 1 3     2 5   X1   4  X 1 7     X1   10  X 1 4     X1   4  The partition variables divide the domain of X1 into non  overlapping ranges  and therefore exactly one of them is true for any given value x1  The rewritten threshold predicates are  Y  1  1   X 1 7  Y  2  1   X 1 1    X 1 2    X 1 3  Y  3  1   X 1 1  Thus  each      j now depends on the new partition variables  Every value assignment x of X de nes a unique Boolean assignment  x of X     such that X i j  is true in  x i  the value Xi   xi satis es the partition predicate corresponding to the variable X i j    Then  one can check that  for every j  x j   j i   x j       j   If the input variable Xi is already a Boolean variable  it does not need to be re written with this process  but for consistency of notation we map it to the partition variables X i 1    Xi  and X i 2     Xi  4 2 The Partition Variable Constraint   The Boolean variables X i  are subject to the constraint that exactly one of these variables is true  This is captured by the following formula   i       j X i j       j l   X i j     X i l       i is a conjunction of 2 Ti   1  clauses  where Ti is the number of thresholds associated with variable Xi  Taking the conjunction over all input variables  we obtain the fol  lowing constraint expression          i  i  1  We have the following  Lemma 10  For any assignment      j    i  there exists values x such that  x      The proof is straightforward and omitted  The lemma says that any assignment satisfying the constraint   corre  sponds to some values x  Note that these values are not necessarily uniquely de ned  in Example 9 if   says that X1 3 is true  then we know that 2 5   X1   4  without knowing the actual value of x1  4 3 The Hard Constraint  SAT Given the observed output values z and the ground truth output values z   we de ne here two Boolean expressions  called VC expressions  which state that the outputs are z  or that the outputs are z  respectively             i zi T      i         i zi F       i    2         0     i z i T      i 1 A   0     i z i F       i 1 A  3  The superscript    is intended to remind us that these ex  pressions are over the partition variables X     we will drop the superscript from here on  unless it is needed to clarify the context  The expression   checks if the output is the  observed output  z  The expression   checks if the output   is the  ground truth  z   If at least one error is detected in the output  z  6 z   then   and   cannot be satis ed   simultaneously  Example 11  Example 9 continued   Assume the ob  served output is z   fz1  z2  z3g   fT  T  Fg and the ground truth is z    fF  T  Tg  Then  we drop superscripts           1    2     3        1    2    3 We give next a technical de nition  which measures the distance between two assignments  Definition 12  Constrained Distance   The distance between two assignments   j    and     j    of X   under constraint   is d           1 2 h         where h         is the hamming distance between the two Boolean vectors   X    and     X    If we have two input values x and x  that di er in exactly one variable Xi  then their corresponding assignments  x and  x  will di er in exactly two variables  hence  by our de nition  d  x   x     1  The crux of our translation is captured by the next de   nition and theorem  They map the notion of VC causes of zjz  into the a problem of  causes for             Definition 13  Consider the Boolean expressions          given by  Eq  1    Eq  2   and  Eq  3   and let   be an assignment of the partition variables X    Assume the fol  lowing      j     it satis es the constraints      j     it generates the  observed  output  A variable X i j  is a cause for          if there exist two assignments    and     such that       j    and     j     both satisfy the constraint        Xij    6     Xij       Xij   and d             1  they di er in only one input variable       6j     and     j         does not yet generate the ground truth  but     does generate the ground truth        We call the set    fX u v  j     X u v    6   X u v  g a con  tingency of X i j   and    is the contingent assignment  Theorem 14  VC cause equivalence   X i j   with   X i j     T  is a cause of          i  Xi is a VC cause of zjz   The proof is in the appendix  Thus  we reduce the prob  lem of computing VC causes of zjz  to the problem of com  puting causes for          We reduce the latter to the   satis ability problem  Given a variable X i j    we will construct a Boolean for  mula that is satis able if and only if X i j  is a cause of          We denote with          X i j     the Boolean expression that results from   after assigning   X i j  to its value under    whereas         X i j     denotes the expression that we get by assigning to it its negation  Similarly         X i     denotes the expression that results from assigning to   all variables   in X i  to their values under    If X i j  is a cause of          then there must exist an   assignment that makes the following formula true   SAT           X i               X i j              X i j      4  In the above formula    denotes the Boolean formula from   Eq  3  while   is the constraint expression  as de ned in Sect  4 2  The  rst term          X i       ensures the require  ment that    6j     when   X i  is set to its original val  ues the assignment does not satisfy    The second term            X i j       ensures that     j     when   Xi switches val  ues  since X i j  is forced to its negation  the assignment should satisfy    Finally  the third term enforces the con    straint over the partition variables  under the assumption that the value of Xi should change  We can state now our main result for the reduction from causality to SAT  Theorem 15  Xi is a VC cause of zjz  i   SAT given by  Eq  4  is satis able  Note that we cannot yet use a standard SAT solver to solve the satis ability of Eq  4  because these require the expres  sion to be translated into CNF  Translating Eq  4 into CNF is a non trivial task that can result in an exponential increase in size  and we address it in Sect  5  First we show how to extend this construction to compute the responsibility  4 4 Computing Responsibility We can now de ne responsibility  Definition 16  Fix the Boolean expressions          over variables X    The responsibility of X i j  is      1 1   min  d         where    is a contingent assignment of X i j   Like causality  we can also determine the responsibility of the input variables X over the Boolean partition variables X     Theorem 17  Responsibility Equivalence   A vari  able X i j   with   X i j     T is a cause for          and has responsibility   if and only if Xi is a VC cause of zjz  with responsibility    Computing the responsibility of a cause is a harder prob  lem  as it requires  nding the minimum size contingency set   Following Def  13  that corresponds to determining the assignment     that di ers the least from the original assign  ment    which satis es  SAT  We will solve the responsibility problem by creating an instance of a partial weighted MaxSAT problem  15   In partial weight MaxSAT each clause is given a weight  with a maximum weight identifying hard constraints  which are the clauses that are required to be satis ed  A solution to the problem  nds an assignment that satis es all the hard constraints  and maximizes the weight of the satis ed soft constraints  In our case   SAT  Eq  4  de nes hard constraints  all of its clauses should be satis ed for a variable to be a cause  In addition we ask the solver to  nd an assignment that is as close as possible to    We do this by adding the following formula           X i j    T X i j      X i j    F  X i j   5     represents in a Boolean expression the given assign  ment    Note that    is already written in CNF  and each clause consists of exactly one variable X i j    possibly negated  We assign to each clause a weight of 1  thus asking the solver to violate as few clauses as possible  The only assignment that makes    true is    For any other assignment      the number of clauses in    that are false is precisely d          Thus  a partial weighted MaxSAT solution will  nd the as  signment that satis es the hard constraints  SAT  and di ers the least from the assignment    We state this formally  Theorem 18  Consider the MaxSAT problem consisting of the hard constraint  SAT and the soft clauses in     and let t be the minimum number of clauses in    that are false under any assignment  Then  the responsibility of Xi for zjz  is     1  1   t 2    Note that t is divided by 2 in the responsibility compu  tation due to the constraint over the partition variables  a distance of 2 in an assignment in the partition space corre  sponds to a single variable change in the input space  5  CONVERSION INTO CNF In the previous section  we described how the problem of determining view conditioned causes and their responsibili  ties can be translated into a general satis ability problem  While satis ability is well known to be hard in general  there are highly optimized tools available that can solve satis a  bility for CNF expressions  18   Hence  we need to convert our general expression  SAT in Eq  4 into CNF before test  ing causality and responsibility for individual variables  The time complexity of CNF conversion is linear  30   but it can introduce exponential blow ups in the size of the expression  The problem comes from the necessity to negate  at sev  eral steps  CNF formulas  but then reformulate the result as CNF  The standard way to do push the negation into the literals by applying De Morgan s laws and distributiv  ity  There are more sophisticated conversion algorithms that can be employed  e g   30    but the optimization that we propose here is orthogonal to those  and can be used in con  junction with other conversion methods  In our problem  there are two critical points in the conversion  that is ex  pressing   as CNF  and that of expressing its negation                In this section we present an optimization that exploits the constraints of the Boolean partition variables in order to reduce the size of the converted CNF expression  Al  gorithm 1 provides a conversion that completely solves the problem for         is constructed in a way that ensures it is   already in CNF  and ameliorates the size increase for       The algorithm exploits the fact that a threshold predicate Y  j  i can either be written in the standard form given in Sect  4  or alternatively in what we call its dual form  For this section  we assume that the initial classi ers are given in CNF  but the proposed optimization can be bene cial even when that is not the case  Assume we have the following m   2 classi ers over n   3 inputs   1    X1   6     X2   8   2    X1   5     X3   7  Then the threshold predicate Y  1  1    X1   6  over the partitioned variables can be expressed with either of the fol  lowing two forms  Y  1  1   X 1 1    X 1 2    X 1 3  Y  1  1D   X   1 4    X   1 5  Here we use the optional subscript D to indicate the dual form and write X  i as short form for  Xi  When construct  ing  SAT  Algorithm 1 chooses at each step an equivalent expression that minimizes the total number of clauses in the resulting CNF  Line 2 to line 4 encode the conversion for    and add clauses for  j or   jD  depending on whether  zj is true or false  respectively  Here  again the subscript D in  dicates that   jD is calculated starting from the duals Y  j  iD instead of the standard form  and becomes only one clause  For example    1D    X 1 4    X 1 5    X 2 3    whereas starting from the standard form would result in   1    X   1 1    X   2 1       X   1 1    X   2 2       X   1 2    X   2 1       X   1 2    X   2 2       X   1 3    X   2 1       X   1 3    X   2 2    Denoting with f the number of ground truth values F    is   converted into O   mf n   clauses as follows         1D             fD    z   f clauses    f 1            m    z   O   mf n   clauses whereas the naive conversion would lead into O fk n   clauses  This di erence becomes even more pronounced for con  verting     where an exponential blow up is unavoidable    Here  Line 5 to line 11  rst construct the following formula           1            f    z      f conjuncts with n clauses     f 1D             mD    z      1 clause Converting this formula to CNF results in O n f   clauses  and this exponential size increase cannot be avoided  In contrast  the naive conversion would even lead to a double exponential number of clauses  Finally  line 12 to line 14 adds the constraint expression   by adding O mk 2   clauses  where k is the maximum number of partitions for any input variable  Algorithm  CNF conversion of a VC causality problem Input  Partitioned variables X     ground truth z  Boolean expressions  j and   jD for all j Output  General  SAT in CNF 1 Let  SAT be a conjunction of disjunctions  CNF  2   Add clauses for    3 8j 2  m  with  zj   T  add  j to  SAT 4 8j 2  m  with  zj   F  add   jD to  SAT 5   Add clauses for     6 Let   be a disjunction of literals 7 8j 2  m  with  zj   T  add   jD to   8 Let   be a disjunction of conjuncts 9 8j 2  m  with  zj   F  add  j to   10 Convert           from DNF into CNF with De Morgan 11 Add   to  SAT 12   Add clauses for   13 8i 2  n   add  W k2 ki  X i k    to  SAT 14 8i 2  n   j   l 2  ki   add  X   i j    X   i l    to  SAT Algorithm 1 converts the problem of  nding VC causes into solving a general CNF formula  The actual evaluation later replaces some clauses with actual values for variables  6  EXPERIMENTAL EVALUATION We validate our approach by using a real data set col  lected from a practical application that operates along the lines of our classi cation example  The application  called CARE  Context Aware Recommendation Engine   collects various sensor data from a user s smartphone  uses it to in  fer the current context  e g   whether the user is walking or driving   and provides various context aware services  For example  CARE allows a user to search for restaurants  If CARE thinks that the user is walking alone  it will show search results or ads of restaurants within the user s walk  ing distance from the current location  On the other hand  if it believes that the user is driving with their family  it will provide a list of family restaurants within driving distance  1 CARE relies on  i  a set of extractors equivalent of the pre processing layer of Fig  3  which extract useful features from sensor data  e g   user s speed from GPS data  and  ii  a set of classi ers that infer the user s current context based on the values of the features  These features are used as our input variables  and all the classi er outputs comprise our views  Assuming correctness of the classi er functions  er  rors in the classi cation can still occur due to various factors  such as innate unreliability of some of the sensory input  or even unpredictable user behavior  e g   the user is outdoors with the phone in their pocket  resulting in invalid light sen  sor data   CARE receives feedback on the correctness of the classi er outputs either directly from user input  or in  directly through the user response to the context based rec  ommendations  In cases of errors  it is important for CARE to determine which input is responsible for the errors  and thus ignore the corresponding classi ers in the recommen  dation process  For the experiments in this section  we collect data using CARE s data collection software  We use the software on Android based HTC Desire smartphones  When instructed  the software collects data from various phone sensors such as accelerometer  GPS  microphone  light sensor  and cell 1 In addition to the current inferred context  CARE also uses user preferences and historical data in making such recom  mendations              Accelerometer  Cell Tower  GPS  Light  Is Indoor   M   h  i   Ii   Audio  Periodicity  p HasSignal   h Rate of Change  r Avg  Intensity  i Speed  s Avg  Strength  a Zero crossing rate  z Spectral roll off  c Is Driving   M p   Pd  r     Rd  h  s     Sd  Is Walking   M p   Pw  Rs   r    Rw   h      s   Sw   Alone    A2     a    A1        a    A2       z   Z         a    A3       z   Z       c   C   Is Meeting   M   h  i   Im  a     Am  z     Zm  Figure 4  Structure of our classi er system based on sensory input from a mobile device  phone signal strength  During data collection  the software also asks the user to annotate the data with the true context  e g   walking or not  and validity of various sensor inputs  e g   a light sensor input is invalid if the phone is inside a bag   The data and annotations are then used to build one machine learning classi er  i e  a decision tree  for each context of interest  The resulting classi ers with  ve target contexts are shown in Fig  4  The dataset used in the experiments in this section was collected from three individuals  who carried our instru  mented smartphones over a period of three weeks and col  lected data during their daily activities  The dataset con  tains in total 801 di erent contexts  spanning a time range of over 149 hours in a three week period  Each of the contexts in the dataset has its start and end times  associated data from all sensors  its true label  the label given by classi ers  and the validity of each sensor input  Thus  for each case of misclassi cation  we use the sensor validity as the ground truth for causality against which we validate our approach  Implementation  Our satis ability reduction algorithm was implemented in Java  2 For every instance of the input data  our algorithm constructs the appropriate data  les in standard DIMACS CNF and WCNF format  21   We use the minisat  32  and MiniMaxSat  18  solvers  to solve the SAT and partial weighted MaxSAT instances  respectively  6 1 Effectiveness In our experiments we want to validate the quality of infor  mation of the view conditioned responsibility ranking over possible sources of error  We ran our algorithms over the test data collected through the CARE system  and each test case took only a couple of seconds to complete  We  rst evaluate the e ectiveness of our approach compared to other schemes using average precision as the metric of comparison  23   Av  erage precision  AP  quanti es the accuracy of a ranking in identifying points of relevance  Given the ground truth on the validity of the sensory input  an average precision of 1 means that the faulty sensors were indeed ranked  rst in the ranking of most likely errors  We compare view conditioned causality against three other techniques that can be used to produce error rankings   i  Boolean variable in uence   ii  view conditioned counter  factuals  and  iii  causality without conditioning  Boolean in uence produces a static ranking of the input parameters 2 In practice  the algorithm would be running on the phone itself  If the implementation is heavy weight  it can run on the Cloud and the phone can communicate with it when needed  for each classi er  The in uence of a variable Xi in formula   is de ned as the probability that   remains undetermined when values are assigned to all variables except Xi  24   For example  in the simple formula     A    B   C   the in u  ence of A is 0 75  whereas the in uence of B is 0 25  Note that in uence quanti es the importance of a variable within a function over all possible assignments  On the other hand  causality based schemes quantify this importance based on the actual assignment  Intuitively  we expect schemes that take into account the current state of the world to be more accurate in error identi cation  Figure 5a shows the results of this  rst set of experiments  The horizontal axis shows the number of faults in the input parameters  starting from a single parameter failing  and going up to 7  recall from Fig  4 that the classi ers operate over 8 parameters in total   Causality based schemes are depicted with solid lines  Also depicted are random aver  age precision and the worst case average precision for each fault range  3 Note that view conditioned causes are always a better predictor for errors than view conditioned counter  factuals or causes without conditioning  And this di erence gets more pronounced with the number of faults  Also note that that the precision of counterfactual causes follows that of random ranking from a point on  It is normal because as the number of faults increases  we stop having any counter  factual causes at all  The dip observed across all schemes at the instances with 2 faults is due to the lack of granularity in the ground truth of sensor validity  Our ground truth is restricted to the level of the sensors  e g  GPS   rather than the individual parameters  e g  h   So when the GPS is considered faulty  the features h and s are both deemed incorrect  However  with this few errors  only one or two classi ers are wrong  and they are likely to only include h  not s   The result is that s is not really a cause  it is not part of the classi ers that failed   but because it is always considered to fail with h in the ground truth  it is re ected as loss in precision  In Figure 5b we studied the next step  how well can we correct errors in the classi ers  once the likely input errors are identi ed  The graph displays the portion of errors in the particular classi er that can be corrected if the top ranked view conditioned cause is omitted from the calculation of the classi er result  This involves a modi cation in the classi er function  For example  the driving classi cation is a strict majority function between 4 predicates  if feature p is omit  ted  then it becomes a strict majority among the other three predicates  An interesting observation in our experiments is that certain classi ers can achieve better correction ratios than others  For example  omitting features r or s corrects more than 85  of the errors of the driving classi cation  whereas omitting parameter s corrects the walking classi er by only 25   This behavior may seem random or surprising at  rst  but in fact it highlights a very important insight that causality brings into error correction  This can be un  derstood from Fig  5c  which focuses on three of the classi   ers  walking  driving  and meeting  to explain the reasons for this di erence in corrections  Note that the driving clas  si cation  as seen in Fig  4  uses 4 features  p  r  s and h   two of which  r and s  are counterfactual      1 in Fig  5c   whereas the other two have zero responsibility  This is very 3 Random AP is the expected AP when a list of k relevant and n total items is randomly sorted  Worst case AP is when those k are sorted last in the list of n total items    0 2 1 2 3 4 5 6 7 0 0 2 0 4 0 6 0 8 1 Number of Faults Mean Average Precision View Conditioned causality Causality  no conditioning  Countef actual causality Boolean In   uence Worst case Average Precision Random Average Precision  a  Mean average precision  AP  0 Walking Driving Alone Indoor Meeting 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 Correction ratio Classi   ers p r h s z a c i  b  Ratio of Corrections 0 Walking Driving Meeting 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 Feature Responsibility Classi   ers p r h s z a c i  c  Responsibilities per parameter Figure 5   a   Mean average precision across di erent approaches  View conditioned causality outperforms the static and simpler causality schemes   b   Portion of errors that get corrected when the parameter with the highest responsibility is omitted from the computation   c   Comparison among the responsibility rankings for di erent classi ers   b   c   The ratio of correction actually depends on whether there exist variables reliable for the classi er  compare walking and driving   important  as it means that the parameters h and p are very unlikely to cause errors in the driving classi cation  and are thus reliable predictors for it  Thus  when omitting r or s from the classi er function  the result becomes much more reliable  On the other hand  the walking classi er does not have reliable features  as all of them have fairly high respon  sibility across our test cases  When all features of a classi er have high responsibility  it means that all of them are likely causes of error  Without a single reliable predictor  it is less likely that we can achieve corrections by simply removing a parameter and not changing the classi er function other  wise  The meeting classi er stands on a middle ground  it has one reliable predicate  based on parameter i   but still contains 3 unreliable ones resulting in less gain compared to the driving classi cation  Hence  using responsibility to determine the reliability of features is very important in this application  as it can quantify the quality of a classi er  and implies possible ways to improve it  For example  in the driving classi cation  the vast di erence between the reli  able and unreliable features is an indication that parameters r and s should be disregarded in this classi er altogether  whereas the absence of reliable predicates for walking likely means that the learned models need to be improved  6 2 Scalability In our second set of experiments we study how our ap  proach performs for bigger problem sizes  We generate syn  thetic data with a similar structure to the one in CARE  We randomly generate classi er functions as a conjunction over ranges of the di erent parameters  and randomly assign clas  si cation failures with a probability of about 10   following the failure rate that we observed in the real data  In the context of an application like the CARE system we do not expect to have more than 15 20 parameters features and 20  30 classi ers  Yet in this evaluation  we generate instances from 5 to 70 parameters  and from 5 to 60 classi ers  Figure 6a and Fig  6d show the execution time of our CNF conversion algorithm 1  which takes as input the classi ca  tion formulas  variable assignments and observed errors  and produces instances of SAT and partial weighted MaxSAT  Note that t</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11ir1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11ir1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Information_retrieval"/>
        <doc>Context sensitive Ranking for Document Retrieval ###  Liang Jeff Chen UC San Diego La Jolla  CA  US jeffchen cs ucsd edu Yannis Papakonstantinou UC San Diego yannis cs ucsd edu ABSTRACT We study the problem of context sensitive ranking for document retrieval  where a context is de   ned as a sub collection of documents  and is speci   ed by queries provided by domain interested users  The motivation of context sensitive search is that the ranking of the same keyword query generally depends on the context  The reason is that the underlying keyword statistics differ signi   cantly from one context to another  The query evaluation challenge is the computation of keyword statistics at runtime  which involves expensive online aggregations  We appropriately leverage and extend materialized view research in order to deliver algorithms and data structures that evaluate context sensitive queries ef   ciently  Specifically  a number of views are selected and materialized  each corresponding to one or more large contexts  Materialized views are used at query time to compute statistics which are used to compute ranking scores  Experimental results show that the contextsensitive ranking generally improves the ranking quality  while our materialized view based technique improves the query ef   ciency  Categories and Subject Descriptors H 3 3  Information Storage and Retrieval   Information Search and Retrieval   Retrieval models  H 2 4  Database Management   Systems   Query processing General Terms Algorithms  Performance Keywords Context sensitive ranking  materialized views  view selection ### 1  INTRODUCTION While research in information retrieval  IR  has generated many effective ranking models for general purpose search  existing ranking models may not deliver satisfactory rankings for domain experts  In this paper  we propose a new query model that allows  Work supported by NSF IIS 0713672 and NSF DC 0910820 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  expert users to specify contexts  The ranking of the query result is computed based on keyword statistics collected from the specialized contexts  This is motivated by the observation that keyword statistics usually vary dramatically from one domain to another and therefore the ranking of the query result will vary accordingly  For example  while    leukemia    is rare over the Web  it is a fairly common term in biomedical science  as captured in the PubMed 1 database of 18 million articles   and is extremely common among articles of PubMed that are annotated to be cancer related  Conventional ranking models will consider    leukemia    as a discriminative term  which is not true from the perspective of a cancer researcher or doctor  who typically narrows his her interest in the cancer related articles  Given that all ranking models and functions use keyword statistics to compute ranking scores  specialized keyword statistics naturally lead to specialized rankings for users interested in narrow domains  1 1 A Motivating Example PubMed contains 18 million biomedical citations  All the citations include title  abstract  and authors    information  Citations are often linked to full text articles  Additionally  every citation is annotated with one or more MeSH  Medical Subject Headings  terms from a controlled vocabulary  which speci   es a variety of concepts in biomedical science  e g      anatomy        diseases        diagnosis     MeSH terms in the vocabulary are organized in a hierarchy  as shown in Figure 1  A MeSH term may appear in several places in the hierarchy tree  MeSH digestive system neoplasms liver pancreas        anatomy disease        Figure 1  MeSH terms and the hierarchy The vocabulary and the hierarchy of MeSH terms represent an ontology of biomedical science  Each MeSH term represents a biomedical concept and indexes a list of related citations  A combination of MeSH terms represents a context that spans the corresponding concepts  For example     neoplasms    and    digestive system    represent two concepts under    diseases    and    anatomy    respectively  The combination of the two terms identify a set of 1 http   www ncbi nlm nih gov pubmed citations  which form a search context for researchers and doctors concentrating on gastrointestinal  GI  cancer  A researcher or a doctor can specify such a context by utilizing tools that visualize the MeSH term ontology and enable the user to navigate in the ontology and select terms of interest for the context  For example  the tool of Figure 2 mimics the widely used ontology navigator provided by PubMed 2 and extends it with the ability to select terms during the navigation  In Figure 2  we see two snapshots of the ontology navigator  at the point where the user has just selected the context terms  Note that the use of such tools for specifying the context removes the risk of mistyping the context trems  which would otherwise be an important risk since in context based search only documents that contain the context terms will be retrieved  Figure 2  Choosing context terms using hierarchical ontology navigation in PubMed Keyword distributions and statistics often vary dramatically from one specialized context to another  For example  research on cancer and research on digestive system have very different terminology  Using keyword statistics from a specialized context in ranking functions will deliver a specialized ranking order for the documents in that context  For example  the classical TF IDF model uses document frequency as term weights to boost the ranking of documents that contain query terms that are rare in the collection  The rationale is that rare query terms are more discriminative  and therefore are more important in identifying relevant documents than frequent query terms  In the above example  a query term that is frequent for the citations on    neoplasms    may be rare for the citations on    digestive system     The ranking order of two documents may be reversed when users are interested in different contexts  Consider the query  pancreas  leukemia   and two citations C1     Complications following pancreas transplant    and C2     Organ failure in patients with acute leukemia     both annotated with the MeSH term    digestive system     Assume we rank the two citations    titles by tf   idf  Since both citations match precisely one single query term  the ranking order of the two citations is only determined by idf  Without a context speci   cation  the frequency of leukemia is higher than pancreas in PubMed  Hence  C1 is ranked higher than C2  However  if the query is issued by a GI doctor or researcher  whose focus is on digestive system  the frequency of leukemia is much less than pancreas in the corresponding context  and therefore C2 should be ranked higher than C1  Intuitively  pancreas transplant is a common topic among GI researchers  Leukemia in the query is more discriminative in the context  Given that C2 is annotated with    digestive system     it is very likely that the organs mentioned in the C2   s title refer to digestive organs  which include pancreas  2 http   www nlm nih gov mesh MBrowser html 1 2 Contribution We propose a new query model that extends conventional keyword queries and allows domain interested users to specify search contexts  A search context is de   ned as a sub collection of documents that users are interested in  The goal of context sensitive ranking is to use keyword statistics based on user de   ned contexts to rank documents in the contexts  The query processing of context sensitive queries presents novel performance challenges that are not met in the processing of conventional keyword queries  In conventional keyword query evaluation  the context is always    xed  it is the entire document collection  So the statistics are precomputed at indexing time  For context sensitive ranking  however  contexts are speci   ed by users at query time and can be arbitrary subsets of the document collection  Therefore  the collection speci   c statistics  such as document frequency  also have to be computed at query time  A straightforward solution to compute keyword statistics is using standard text search techniques to materialize contexts at query time and gathering required statistics accordingly  Unfortunately  this solution is not always cheap  The challenges are two  First  a common approach to materialize contexts is to intersect inverted lists of keywords  e g   MeSH terms in PubMed  While intersection operations are ef   cient for most keyword combinations  intersecting very long inverted lists is still expensive  9   Second  computing keyword statistics not only requires intersections  but also aggregations  As we will show later  some statistics in conventional ranking models demand aggregations of the documents in the contexts  which can be very expensive when the contexts are large  In this paper  we propose a materialized view technique to overcome the above challenges  We reduce the problem of computing keyword statistics to evaluating aggregation queries  and use materialized views to improve query performance  Given that there is a huge number of possible context speci   cations  the technical challenge is how to choose a reasonable number of views to materialize  We present two algorithms for view selection  The goal is to guarantee good system performance for worst case queries  Key contributions of the paper include  1  We propose a novel query model that allows expert users to specify search contexts  The ranking model uses keyword statistics collected from the speci   ed contexts to rank documents in the contexts  2  We reduce the problem of computing keyword statistics to evaluating aggregation queries  and leverage materialized views to improve query ef   ciency  Two algorithms are proposed to select a number of views to materialize  3  We perform thorough experiments on the PubMed data set  Results show that context sensitive ranking improves the ranking quality remarkably  compared with the conventional ranking models  The materialized view technique improves the ef   ciency of worst case queries signi   cantly  The overall performance of the system is guaranteed  The paper is organized as follows  Section 2 de   nes the query model and the ranking model for context sensitive ranking  Query evaluation is discussed in Section 3  Section 4 reduces the problem of computing keyword statistics to evaluating aggregation queries  and presents a view based technique to compute statistics  Two view selection algorithms are presented in Section 5 to choose a reasonable number of views to materialize  Experiments and results are elaborated in Section 6  Related work is discussed in Section 7  Section 8 is the conclusion 2  DATA MODEL   QUERY SEMANTICS In this section  we formally de   ne the query model  Section 2 1  and the ranking model  Section 2 2  for context sensitive ranking  2 1 Query Model A document  denoted by d  is modeled as a tuple of    elds  each consisting of a bag of words  A    eld may refer to abstract  category  e g   MeSH annotations in PubMed   keywords or full content  Let D denote a collection of documents  A query for context sensitive search  Qc   QkjP  consists of two parts  a context speci   cation  P  that de   nes a set of documents as the search context  and a set of keywords  Qk  as the conventional keyword query  The unranked result of Qc is a set of documents in the search context that contain all the keywords  De   nition 1  A predicate    eld of D contains literals  each of which is a single keyword  called context predicate  De   nition 2  A context speci   cation P   p1   p2         pc is a conjunction of context predicates that specify a sub collection DP   D such that 8d 2 DP   d satis   es P  In other words  DP    P  D     p1 p2    pc  D   Consider a conventional n keyword query Qk   w1           wn in context P  The unranked result of Qc   QkjP is a set of documents in Dp that contain all the keywords  Qc D    Qk DP      P  D     w1  D             wn  D  where  wi  D    fdjd 2 D   d contains wig  i   1          n  Without loss of generality  we assume the predicate    eld in context predicates is    xed  and simply use a conjunction of keywords m1   m2           mc to denote the context speci   cation  For example  the query Qc   w1   w2jm1   m2 over PubMed de   nes a set of documents DP that contain MeSH terms m1 and m2 as the context  The unranked result of Qc is a set of documents in DP that contain w1 and w2  2 2 Ranking Model Next we de   ne the ranking of the result of Qc  We start by presenting a generic representation of ranking functions for conventional keyword queries  We then evolve it to the context sensitive ranking function  In the following  we use Qt to denote conventional keyword queries  Various ranking functions were developed in the literature to rank documents with respect to keyword queries  In general  they combine keyword statistics to a single score to evaluate the relevance between a query and a document  The statistics used in the ranking models can be classi   ed into three categories  queryspeci   c  document speci   c and collection speci   c    A query speci   c statistic  denoted by Sq Qt   is a statistic computed from the input query Qt  e g   query length    A document speci   c statistic  denoted by Sd d   is a statistic computed from document d  e g   term count of wi in d  Every document has its unique document speci   c statistics    A collection speci   c statistic  denoted by Sc D   is a statistic computed from the collection D  Conceptually  a collectionspeci   c statistic is calculated by aggregating parameters of individual documents in the collection to a single value  For example  term count of wi in the collection is calculated by summing up the number of occurrences of wi in every document in the collection  Table 1 summarizes atomic statistics used in a variety of ranking models  including vector space models  TF IDF   language models and probabilistic relevance models  Note that some compound statistics used in the models can be computed by combinations of atomic statistics  For example  average document length  avgdl  is calculated by collection length divided by collection cardinality  avgdl   len D  jDj   Let Sq Qt  be a set of query speci   c statistics for Qt  Sd d  be a set of document speci   c statistics for d  and Sc D  be a set of collection speci   c statistics for D  Given a query Qt and a document d 2 D  a conventional ranking function f    takes as arguments statistics from Sq Qt   Sd d   Sc D   and computes a score of d with respect to Qt  score Qt  d    f Sq Qt   Sd d   Sc D    1  In context sensitive ranking  a context speci   cation P de   nes a set of documents DP   D of interest  Accordingly  the statistics used in the ranking function should be based on DP   rather than D  Speci   cally  given a context sensitive query Q   QkjP and a document d 2 DP   the ranking score is computed as  score QkjP  d    f Sq Qk   Sd d   Sc DP     2  The ranking model for context sensitive ranking uses the same computation function f as the conventional IR model  but different input statistics  In particular  Qk in Qc is a n keyword conventional keyword query  and Sq Qk  in Formula 2 is equivalent to Sq Qt  in Formula 1  Sd d  is the same in both formulas  The only difference is Sc  Sc DP   in Formula 2 collects statistics from DP   whereas Sc D  in Formula 1 collects statistics from D  EXAMPLE 2 1  TF IDF weighting is a well known ranking model  Among its variants  the pivoted normalization formula  30  is considered to be one of the best performing vector space models and is widely used in many text search systems  Its mathematical representation is shown in Formula 3  where s is a constant and is usually set to 0 2  The other variables    meanings can be found in Table 1  score Qt  d    X w2Qt 1   ln 1   ln tf w  d     1  s    s   len d  avgdl   tq w  Qt    ln jDj   1 df w  D   3  where avgdl   len D  jDj   The statistics used in the pivoted normalization formula are classi   ed as follows    tq w  Qt  is a query speci   c statistic    tf w  d   len d  are document speci   c statistics    df w  D   jDj  len D  are collection speci   c statistics  The context sensitive version of the pivoted normalization formula for Qc   QkjP replaces every Sc D  with Sc DP    i e   8d 2 DP   score QkjP  d    X w2Qk 1   ln 1   ln tf w  d     1  s    s   len d  avgdlP   tq w  Qk    ln jDP j   1 df w  DP    4  where avgdlP   len DP   jDP j    Table 1  Statistics used in ranking functions Scope Statistics Notation term count in the collection  for keyword w  tc w  D  collection length len D  collection speci   c collection cardinality jDj document count  for keyword w  df w  D  unique term count in the collection utc D  term count in document tf w  d  document speci   c document length len d  unique term count in document utc d  term count in query  for w  tq w  Qt  query speci   c query length len Qt  unique term count in query utc Qt  3  QUERY EVALUATION We discuss query evaluation of context sensitive queries in this section  We    rst describe a straightforward evaluation  Section 3 1   and then analyze its performance bottlenecks  Section 3 2   which will be tackled in later sections  3 1 Straightforward Evaluation Query evaluation of a context sensitive query evaluates the unranked result set  and computes keyword statistics to further compute ranking scores  Unlike conventional keyword query evaluation where all statistics are precomputed at indexing time  collectionspeci   c statistics Sc DP   for context sensitive ranking must be computed at query time  because contexts are speci   ed by queries and can be arbitrary subsets of the document collection  A straightforward evaluation of a context sensitive query is to materialize the context collection and compute required statistics accordingly  Thereafter the query evaluation is the same as conventional keyword queries  Let Lw    w D  be the inverted list of w  Consider the query Qc   w1   w2jm1  m2 and the TF IDF ranking function in Formula 4  By query semantics  the unranked result of Qc is evaluated as Lw1   Lw2   Lm1   Lm2   In other words  intersecting the inverted lists of the four keywords returns the complete result set  To compute collection speci   c statistics  the query plan must satisfy the following constraints  1  Document count for wi  df wi  DP    is the number of documents in the context that contain wi  which is evaluated as j wi  D     P  D j  Therefore  the query plan must include Lw1   Lm1   Lm2 and Lw2   Lm1   Lm2   where the    rst expression computes document count for w1  and the second expression computes document count for w2  2  Collection cardinality jDP j is evaluated asj m1  D   m2  D j  Hence  the query plan must include Lm1   Lm2   3  Collection length len DP   requires a SUM aggregation on the lengths of the documents in the context  i e    sum  m1  D     m2  D   where   denotes an aggregation operator  Putting the above constraints together  the execution plan of Qc is shown in Figure 3  where    means    intersection with aggregation     At the bottom level  Lm1 and Lm2 are intersected to return documents in the context  Two aggregations  denoted by  count and  sum  are performed upon Lm1   Lm2 to compute collection cardinality and collection length  The result of Lm1   Lm2 is further intersected with Lw1 and Lw2 respectively to obtain document count for w1 and w2  The    nal result is computed by the highest intersection operator  Lm1 Lm2       sum                Lw1 Lw2   count       count   count Figure 3  The execution plan of Qc   w1   w2jm1   m2 3 2 Performance Analysis We introduce a simple cost model to quantify the cost of the straightforward evaluation  The purpose of the model is not to estimate the cost as accurate as possible  but to analytically demonstrate the bottlenecks of the straightforward evaluation  3 2 1 Cost Models for Inverted List Intersection and Aggregation The core operation of the query plan in Figure 3 is the intersection of inverted lists  In standard text search systems  a simple representation of an entry in an inverted list is a pair of document ID and term count  i e   hdocid  tfi  Inverted lists are ordered by document ID so that two lists can be merged ef   ciently  A simple cost model for the merge join is jLij   jLj j  where Li and Lj are two inverted lists  In addition to the standard merge join  inverted lists are partitioned into segments and skip pointers are maintained to jump between consecutive segments  24   When two inverted lists are scanned  if the current document ID of the    rst inverted list does not fall in the segment of the second inverted list  the whole segment of the second inverted list can be skipped  Let M0 be the number of entries in one segment  N o i be the number of segments in Li whose ranges overlap with some segment s  in Lj   and N o j be the number of segments in Lj whose ranges overlap with some segment s  in Li  Then the cost of the intersection with skip pointers is M0   N o i  N o j    Since N o i   jLij M0 and N 0 j   jLj j M0   we have M0    N o i   N o j     jLij   jLj j  Therefore  the cost model of the intersection is  cost Li   Lj     M0    N o i   N o j   An aggregation over a list requires a full scan of the elements in the list  Hence  the cost model of the aggregation is  cost   P     j   mi2P Lmi j3 2 2 Analysis Intersecting inverted lists is generally considered to be ef   cient  The skip pointer optimization improves the ef   ciency signi   cantly when the join cardinality is small  as many segments can be skipped  In particular  when jLij is orders of magnitude smaller than jLj j  Li   s entries span at most jLij segments of Lj   i e   each entry in Li falls in a separate segment of Lj   In such a case  the cost for the intersection of Li and Lj is jLij   jLij   M0  which can be much cheaper than jLij   jLj j  However  intersecting very long inverted lists is not cheap  9   In particular  when the join cardinality is not small  the intersection cannot take advantages of skip pointers and all segments must be Pscanned  The cost of the context materialization is bounded by mi2P jLmi j  While inverted list intersections in conventional keyword query evaluation can start from the most selective keyword  the evaluation of context sensitive ranking must fully materialize the context  Intuitively  the context size tends to be fairly large  because the purpose of the context speci   cation is to de   ne a general search scope  rather than to    lter out speci   c information as the keywords in conventional queries  In standard text search systems  when the keywords in the query are not selective and the result size  i e   the join cardinality  is very large  top K processing techniques have been developed to reorder inverted lists so that only a small fraction of the lists are processed to generate top K results  This strategy  however  is not applicable for context sensitive ranking before all collection speci   c statistics are computed  The performance of the query will still be bounded by the complexity of the context materialization  In addition to the cost of intersections  the cost of aggregations is proportional to the context size which is less than P mi2P jLmi j  PROPOSITION 3 1  The cost of a context sensitive query Qc   QkjP is bounded by O  P mi2P jLmi j  in the worst case  The above analysis shows that context sensitive ranking can be fairly expensive when the context is not selective  The performance of Qc   QkjP can be orders of magnitudes slower than the conventional query Qt   Qk   P  by query semantics  the unranked result of Qt   Qk   P is the same as the unranked result of Qc   This makes context sensitive ranking unacceptable in these scenarios  as it sacri   ces ef   ciency too much  4  COMPUTING STATISTICS USING MATERIALIZED VIEWS While context sensitive ranking leverages customized statistics to provide specialized rankings  current text search systems cannot ef   ciently evaluate queries specifying very large contexts  The technical challenge is to maintain query ef   ciency as close as possible to conventional keyword queries  In the section  we reduce the problem to evaluating aggregation queries  and leverage materialized views to achieve this goal  Computing collection speci   c statistics essentially involves online aggregations  Similar problems were encountered in OLAP  12   an approach to quickly analyze multi dimensional data  A large body of OLAP queries involve expensive aggregations which must be answered in a short time  The most important mechanism in OLAP that allows such performance is the use of the data cube  a materialized view that aggregates data along some dimensions  Aggregation queries then can be answered from the materialized views which are typically much smaller than the raw table  We incorporate a similar idea for the context sensitive ranking problem  4 1 Formalization A document collection D is modeled as a wide sparse table T  as shown in Table 2  In addition to document ID  columns are classi   ed into two categories  keyword columns  e g   m1  m2   each corresponding to a keyword mi that can be used in context speci     cations  and parameter columns  e g   len d  and tf d  wi    each corresponding to a parameter on which a collection speci   c statistic aggregates  Every row corresponds to a document di  An entry in row di and column mj is 1 if di contains mj   otherwise  the entry is 0  Table 2  A relational representation of the document collection docid len d  tf d  w1        m1 m2       mn d1 156 15       1 0       0 d2 98 7       0 1       1                                                 Given the wide sparse table T  computing a collection speci   c statistic Sc DP   of context P   mj1  mj2       mjc is equivalent to evaluating an aggregation query  Sc DP    SELECT Stats Aggre para d   FROM T WHERE mj1   1 AND       AND mjc   1 where Stats Aggre is the aggregation function for Sc and para d  is the document parameter upon which Sc aggregates  Let K   fmi1   mi2           mik g be a subset of fm1          mng  VK is a materialized view that groups by K and aggregates the documents    parameters of every group  VK  SELECT mi1   mi2           mik   Stats Aggre para d   AS ContxPara FROM T GROUP BY K We refer to mi1   mi2         mik as keyword columns  and ContxPara as parameter columns in the materialized view  Given the view de   nition  if P   K  the aggregation query that computes Sc DP   can be rewritten as follows  Sc DP    SELECT Stats Aggre ContxPara  FROM VK WHERE mj1   1 AND       AND mjc   1 The GROUP BY clause in the view de   nition essentially partitions the document collection  Every tuple in the view is an aggregation on one partition  The evaluation of the rewritten query aggregates partial aggregation results and avoids scanning the raw table  EXAMPLE 4 1  Consider a two keyword context speci   cation P   m1  m3  Collection length len DP   and collection cardinality jDP j can be translated to a SUM and a COUNT aggregation on the wide sparse table respectively  len DP    SELECT SUM len d   FROM T WHERE m1   1 AND m3   1 jDP j  SELECT COUNT    FROM T WHERE m1   1 AND m3   1Let K   fm1  m2  m3g  The view VK  SELECT m1  m2  m3  SUM len d   AS ContxtLen  COUNT    AS ContxCount FROM T GROUP BY m1  m2  m3 partitions D into 2 3 partitions  The tuple V  m1   0  m2   1  m3   1  aggregates the statistics of the documents that contain m2 and m3  but do not contain m1  Similarly  the tuple V  m1   0  m2   0  m3   0  aggregates the statistics of the documents that do not contain m1  m2 or m3  Having the view VK  collection length and collection cardinality for P   m1   m3 can be computed as follows  len DP     VK m1   1  m2   0  m3   1  ContxLen   VK m1   1  m2   1  m3   1  ContxLen jDP j   VK m1   1  m2   0  m3   1  ContxCount   VK m1   1  m2   1  m3   1  ContxCount 4 2 View Usability A view is usable for a query if it can be used to compute complete or partial results of the query  THEOREM 4 1  View VK is usable for computing the collectionspeci   c statistic Sc DP   for context P if 1  VK includes a parameter column that aggregates the documents    parameters of Sc  2  P   K  VK groups by K and projects out all the other keyword columns  For a context speci   cation that contains mj 2  K  the aggregation query of a collection speci   c statistic requires mj in the WHERE clause  Therefore  VK cannot be used to answer the aggregation query if P   K  4 3 Complexity If a materialized view is usable  collection speci   c statistics can be computed by aggregating the materialized view  whose complexity is only determined by the view size  regardless of the context size  In other words  by choosing appropriate view sizes  query performance of context sensitive ranking can be guaranteed  THEOREM 4 2  If view VK is usable for Sc DP   in context P  the complexity of computing Sc DP   is O  V iewSize VK      which is bounded by O  2 jKj     Without additional indexes built on the view  computing collection speci   c statistics using a view requires a full scan of the view  Theoretically  the number of tuples in the view is exponential to the number of keywords columns  However  the actual number of non empty tuples can be much smaller  Consider two keywords m1  m2 that always appear in the same documents  The tuples VK m1   1  m2   0  and VK m1   0  m2   1  are always empty  Similarly  if m1 and m2 never appear in the same document  the tuple VK m1   1  m2   1  is always empty  While computing accurate view size needs a full scan of the entire document collection  a simple approach to estimate the view size is sampling  a small number of documents are sampled and mapped to VK  The number of non empty tuples after the mapping is estimated as the view size  In the following  we use V iewSize    to denote a function that returns the size of a given view  either by sampling or by scanning  5  VIEW SELECTION Materialized views improve query performance  in particular the computation of collection speci   c statistics  signi   cantly  Ideally  if we can materialize views that cover all possible context speci     cations  query performance of context sensitive ranking is guaranteed  However  this would cost exponential disk storage  which is not feasible for any system  The challenge is how to choose a small number of views to materialize to guarantee the system   s overall performance  Cost analysis in Section 3 2 shows that the straightforward approach relying on standard text search systems can still achieve acceptable performance for small contexts  Query performance would be orders of magnitudes slower when user speci   ed contexts are very large  Hence  context speci   cations  i e   keyword combinations  corresponding to large contexts should be covered by at least one view  so that performance of worst case queries is bounded  Queries whose context speci   cations are not covered by any views are evaluated by the straightforward approach  In addition to the context size  view size is the second parameter that needs to be constrained  The cost of computing collectionspeci   c statistics using views is proportional to the view size  Therefore  the sizes of materialized views should be as small as possible  Given a context speci   cation P  let ContextSize P  be the size of context P  We formalize the view selection problem as follows  PROBLEM STATEMENT 5 1  Given a threshold of context size TC and a threshold of view size TV      nd a set of views V   fVK1   VK2        g such that 1  8VKi 2 V  V iewSize VKi     TV   2  For every possible context speci   cation P  if ContextSize P    TC  then 9VKj 2 V such that P   Kj   Finding keyword combinations that specify large contexts is equivalent to mining association rules of keywords such that their supports  in terms of the number of documents that contain the keywords  are greater than TC  Based on this reduction  we propose two view selection algorithms in the following sections  5 1 Data Mining based Selection A number of algorithms for mining association rules have been proposed in the data mining literature  e g   Apriori  2   FP growth  13   Eclat  36   Given a set of items and a set of transactions  the algorithms scan the transaction set one or more times and return combinations of items whose occurrences in transactions  called support  are greater than a pre speci   ed threshold  called minimum support   In our problem setting  an item is mapped to a keyword  and a transaction is mapped to a document  Association rule mining algorithms return a set of keyword combinations  whose supports are greater than TC  Given a set of high support keyword combinations  a naive approach for view selection is to create one view for each combination  However  this would result in a very large number of views  While aggregations on individual views are ef   cient enough  matching a view for the given query at query time would be prohibitively expensive  Therefore  under the data mining setting  we reformulate the view selection problem as follows  PROBLEM STATEMENT 5 2  Given a set of high support keyword combinations P   fP1  P2       g     nd the minimal number of views V   fVK1   VK2        g such that  1  8VKi 2 V  V iewSize VKi     TV   2  8P 2 P  9VKj 2 V such that P   Kj   THEOREM 5 1  Given a set of high support keyword combinations  the view selection problem is NP hard  Algorithm 1 presents a greedy algorithm that takes as an input a set of keyword combinations generated by association rule mining algorithms  and returns a set of views to materialize  Two heuristics are used for algorithm design  First  for two keyword combinations P1  P2  if P1   P2  a view covering P2 is usable for P1  In other words  we only need to consider P2 for the view selection purpose  Second  in order to reduce the total number of views  the overlap of the keyword combinations that are covered by a view is expected to be maximized  The algorithm    rst removes keyword combinations that are subsets of other combinations  Line 1 in Algorithm 1   according to the    rst heuristic  For each newly created view VKi   the algorithm iteratively scans uncovered keyword combinations and adds the one that has the maximal overlap with Ki  Line 6 9 in Algorithm 1   until the size of VKi reaches TV   Algorithm 1  Data mining based View Selection input   A set of keyword combinations P    P1  P2         generated by association rule mining algorithms output  A set of views V   fVK1   VK2        g 1 Scan P and remove Pi such that 9Pj 2 P  Pi   Pj   2 i   0  3 while P is not empty do Create a new view VKi 4   Ki      5 Remove Pj with the largest size from P  and add it to VKi   i e   Ki   Pj   while V iewSize VKi 6     TV do 7 Remove Pm from P such that  1  jKi   Pmj is maximized  and  2  V iewSize VKi Pm    TV   8 Ki   Ki   Pm  9 end 10 V   V   fVKi g   11 i   i   1  12 end 13 return K An implicit assumption of Algorithm 1 is that for any input keyword combination P  V iewSize VP     TV   This assumption can be guaranteed by setting an upper bound on the number of keywords when applying association rule mining algorithms  The upper bound on jPj is reasonable in practice  Statistics from standard text search systems have shown that most user queries have no more than 5 keywords  3   The number of keywords in context speci   cations is expected to be even smaller  5 2 Graph Decomposition based Selection Many existing algorithms for mining association rules achieve good ef   ciency  But mining association rules is still an expensive operation  In particular  to discover a combination of size k  Pk   fm1  m2          mkg   k1 combinations must be visited  i e   P1   fm1g  P2   fm1  m2g          Pk1   fm1  m2          mk1g  and their supports must be computed accurately  even though we are only interested in Pk for the view selection purpose  The selection algorithm in Section 5 1 essentially presents a bottom up approach to select views  keyword combinations whose G m1 m2 m3  a  The original graph G1 G2 m1 m2 m1 m2 m3  b  Subgraphs after decomposition Figure 4  The    rst graph decomposition scheme supports are greater than TC are generated    rst  Then a set of views are selected to cover all of them  In this section  we present a top down approach to select views  The idea is based on decomposing the keyword set to smaller subsets  until each keyword subset is small enough to be covered by one view whose size is less than TV   The key of this approach is that the decomposition process does not violate the principle of view selection  keyword combinations with high supports should be covered by at least one view  Under this principle  the algorithm skips many combinations and only computes accurate supports when necessary  5 2 1 Graph decomposition Schemes De   nition 3  A Keyword Association Graph  KAG  is a graph of keywords  where vertex mi represents a keyword  and the weight of the edge emimj represents the number of documents mi and mj co occur  Edges with zero weight do not appear in the graph  A KAG constructs pair wise relationships between keywords  and implicitly captures k ary  k   3  keyword relationships  m1  m2          mk co occur in the same document only if m1  m2          mk form a clique in the KAG  Initially  edges whose weights are less than TC can be removed from the graph  because cliques containing such edges do not have high supports and therefore are not considered for view selection  A connected component is a subgraph of KAG in which any two vertexes are connected to each other  As the    rst step  the KAG is decomposed to a set of connected components  We only need to consider views covering individual components  Without loss of generality  we assume the KAG is fully connected  and has only one connected component  For a view that covers a subgraph  the view size is determined by the number of vertexes in the subgraph  Initially  the KAG has one component  which contains all vertexes  It is too large to be covered by one view  We need to decompose the KAG into subgraphs so that views covering individual subgraphs are smaller than TV   A cut divides the KAG G    V  E  into two parts  as shown in Figure 4a  Since the graph is fully connected  some edges    endpoints are in different parts  In Figure 4a  m1  m2  m3 form a clique and some of its edges cross the two parts  The goal of the decomposition is to completely separate the graph  The question is  how to deal with the crossing edges  The principle of the decomposition is that if the support of a clique  i e   a keyword combination  is greater than TC  the clique must be kept holistically in one subgraph after the decomposition  so that at least one view will cover it  In Figure 4a  if the support of fm1  m2  m3g is greater than TC  after the decomposition  at least one subgraph needs to contain the clique  To this end  m1  m2 and the edge between them are replicated in G2 after the decomposition  as shown in Figure 4b  Notice that m1  m2 and the edge em1m2 are kept in G1 as well  The reason is that other vertexes in G1 may form cliques with them  Removing m1  m2 and the edge     G m1 m2 m3  a  The original graph G1 G2 m1 m2 m1 m2 m3  b  Subgraphs after decomposition Figure 5  The second graph decomposition scheme em1m2 from G1 may lose keyword combinations that should be covered by views  If the support of fm1  m2  m3g is less than TC  the corresponding clique is decomposable  because we do not need any view to cover it  This is the second decomposition scheme  as shown in Figures 5  Compared with the    rst decomposition scheme  the edge em1m2 is not replicated in G2  Hence  G2 in Figure 5b is sparser than G2 in Figure 4b  A formal representation of the decomposition schemes is described as follows  De   nition 4  A vertex separator is a set of vertexes whose removal separates a graph into two distinct connected components  Let S0 be a vertex separator whose removal separates the vertexes in the KAG G    V  E  into S1 and S2  i e   V   S1   S2   S0  Given S0  G    V  E  is decomposed into G1    V1  E1   G2    V2  E2  such that    V1   S1   S0  V2   S2   S0    8mi 2 S1  mj 2 S1  if emimj 2 E  emimj 2 E1    8mi 2 S2  mj 2 S2  if emimj 2 E  emimj 2 E2    8m0 2 S0  if 9mi 2 S1  em0mi 2 E  then em0mi 2 E1  if 9mj 2 S2  em0mj 2 E  then em0mj 2 E2    8mi 2 S0  mj 2 S0  if emimj 2 E  emimj 2 E1    8mi 2 S0  mj 2 S0  if  1  there exists a clique containing mi  mj and vertex es  in S2  and  2  the support of the clique is greater than TC  emimj is replicated in E2  In the example in Figure 4 and 5  S0   fm1  m2g  Theoretically  whether to replicate the edge em1m2 in G2 or not depends on whether the support of the clique containing em1m2 is greater than TC  Since the support of the clique cannot be derived from the graph  we still need to compute support  which is similar to mining association rules  However  recall that as long as the view selection principle is satis   ed  either decomposition scheme is correct  If the support of the clique is unknown  we may implicitly assume that the support is greater than TC  and all the edges in the clique are replicated in G2  In other words  using the    rst decomposition scheme always leads to a correct decomposition  The above analysis indicates that computing support is not always necessary for the view selection purpose  especially when the subgraphs are large and sparse  The    rst decomposition scheme becomes less effective when the graphs are smaller and denser  and eventually is invalid for the subgraphs that are cliques  5 2 2 Graph Decomposition Algorithm Having the decomposition schemes  the remaining question is how to choose the vertex separator S0 so that the graph can be decomposed ef   ciently  Two factors are considered     rst  S1 and S2 should be about the same size  so that the sizes of all subgraphs decreases fast as the decomposition proceeds  Second  the number of vertexes in S0 should be minimized  Since vertexes in S0 are replicated in G1 and G2  and the view size is directly related to the number of vertexes in a subgraph  we want to minimize the number of replicated vertexes  The optimization function for the graph decomposition is de   ned as follows  min jS0j minfjS1j  jS2jg   jS0j  5  The numerator minimizes the number of vertexes to be replicated  The denominator ensures that neither of the subgraphs is too small  Given the optimization function in Formula 5  the graph decomposition problem is NP hard  6   A number of approximation algorithms have been developed  Most recently  paper  11  exhibits an O  p log n  approximation algorithm for    nding balanced vertex separators in general graph  with approximation ratio of O  p log opt  where opt is the size of an optimal separator  The pseudo code of the algorithm that decomposes the KAG is shown in Algorithm 2  Algorithm 2  Graph decomposition input   A KAG G    V  E  output  A vertex separator  S1  S2  S0  1 Let V   fv1  v2          vng   2 foreach 1   i   n do 3 Create the augmented graph by adding a source s and a sink t to G  4 Connect s to vj   1   j   i  and connect t to vk  i   k   n  Find the minimum capacity s  t separator S i 5 0   Let S i 1    V   fs  tg   S i 0  S i 2   V   S i 1   S i 6 0    7 end return  S i 1  S i 2  S i 0  such that jS i 0 j jEi 12 j is minimal  where jE i 12j is 8 the number of edges euv  u 2 S i 1   S i 0  v 2 S i 2   S i 0  5 3 Hybrid Approach The data mining based selection and the decomposition based selection have strengths in different directions  The data miningbased approach is strict  and only covers keyword combinations that must be covered  Therefore  it is space ef   cient  However  it has to enumerate a very large number of keyword combinations  The decomposition based selection  on the other hand  usually covers more keyword combinations than required  While it has high ef   ciency when the graph is large and sparse  its capability is limited when the graph is small and dense  In implementation  we uses a hybrid approach to select views  Initially  the graph decomposition algorithm quickly decomposes the KAG into subgraphs  most of which can be covered by individual views  The data mining based approach is used thereafter to further decompose the remaining subgraphs  each of which is a clique and is still too large to be covered by one view  6  EXPERIMENTS We use the PubMed data set to evaluate the effectiveness of context sensitive ranking and the ef   ciency of the materialized view technique  PubMed maintains 18 million citations  each annotated with one or more MeSH terms  We use combinations of MeSH                   terms to specify contexts and conventional keywords to search the citations    titles and abstracts  To deal with MeSH term inheritance  if a citation is annotated with the term t  all the ancestors of t in the hierarchy are attached to the citation  The average number of MeSH terms in a citation after the inheritance is 44  In the experiments  we use the Lucene library 3 as the standard text search system  Lucene is a general purpose text search system and re   ects the state of the art of keyword query evaluation  We only use Lucene for performance evaluation  but not for ranking  The reason is that Lucene   s ranking module provides limited interfaces for customized ranking  which is not suitable for our contextsensitive ranking model  The algorithms and the framework are implemented under Java 6  All the experiments are performed on an Intel i7 860 PC  with 8G memory  6 1 Ranking Quality We evaluate the effectiveness of context sensitive ranking using the TREC Genomics benchmark of 2007  16   which consists of 162 048 full text documents  a small fraction of the PubMed data set  The TREC Genomics also contains 34 topics in the form of biological questions  which were collected from bench biologists and represent actual information needs in biomedical research  For each query  relevant documents were tagged manually by biologists based on pooled results from team submissions as the gold standard  Given the TREC Genomics questions  conventional keyword queries are constructed by extracting one or more noun keywords from the questions  For example  for the question    What symptoms are caused by human parvovirus infection     a possible keyword query is Qk   symptoms   human   parvovirus   infection  Then we rely on PubMed   s Automatic Term Mapping  ATM  to construct appropriate contexts  Given a set of keywords  PubMed   s ATM maps them to one or more MeSH terms  For the previous example  ATM maps the keywords to two MeSH terms  Humans and Parvovirus  Then P   Humans   Parvovirus speci   es the context that studies Humans and Parvovirus  For the constructed context sensitive queries  we exclude those queries whose result sets are too small  less than 20   or the corresponding relevant document sets in the gold standard are too small  less than 5   since ranking thereof is not so important  Altogether 30 queries qualify for the experiment  The main concern of ranking quality in practice is the number of relevant results in top few returned results  which are most likely to be examined by users  To study this aspect  we measure the rank precision among top ranked results  i e   the number of relevant results in top K results  For the TREC Genomics benchmark  the relevance of a document to the query is based on whether the TREC Genomics gold standard includes the document  In the experiments  K is set to 20  as statistics from PubMed has shown that most users do not go beyond looking top 20  22   In additional to the precision  the reciprocal rank  33  is another popular measure for evaluating top ranked results  The reciprocal rank is the inverse of the position of the    rst relevant document in the ranked results  The higher the reciprocal rank of the query  the better the ranking is  In particular  if the    rst result is relevant  the reciprocal rank is 1 1   1  In the experiments  we use the TF IDF model as shown in Formula 4  While more sophisticated ranking functions are in use nowadays  TF IDF still remains at the core and provides a clean way to measure the effect of context sensitivity  3 http   lucene apache org  Given a context sensitive query Qc   QkjP  we compare the context sensitive ranking and the conventional ranking  The conventional ranking of Qc is equivalent to the ranking of the conventional query Qt   Qk  P  where P is treated as a boolean    lter in Qt and does not contribute to ranking scores  The measures of the precision and the reciprocal rank are shown in Figures 6  where the x axis denotes the query ID  In Figure 6a and 6b  the y axis denotes the number of relevant results in top 20 results  In Figure 6c and 6d  the y axis denotes the reciprocal rank  whose maximum value is 1  Figure 6a and 6b show that context sensitive ranking delivers better ranking in 21 out of 30 queries  with occasional large improvements over conventional ranking  e g   Q8 and Q9   while in the few occasions conventional ranking is superior  Q15  Q16  Q30  and the gap is not large  Statistically  the mean precisions of conventional ranking and context sensitive ranking over 30 queries are 7 9 and 10 2 respectively  the mean reciprocal ranks over 30 queries are 0 62 and 0 78 respectively  It is worth pointing out that some queries shown in Figure 6 do not bene   t from context sensitive ranking  Our observation is that ranking effectiveness depends on how well a context speci   cation    ts the original TREC query  In the experiments  the contexts are mechanically generated by PubMed   s ATM mapping  We expect that context sensitive ranking can deliver more improvements over conventional rankings for real life queries  as their contexts are constructed by domain expects  6 2 View Selection To select views for materialization  we set TC to 1  of the PubMed data set  PubMed has 18 million citations  so the absolute value of TC is 180  000  In other words  only contexts whose sizes are greater than 180  000 are covered by views  Query performance under this setting will be shown in Section 6 3  The maximum view size TV is set to 2 12   4096 tuples  Note that this is the number of non empty tuples  The actual number of keyword columns in a view can be much higher than 12  Ef   ciency of View Selection  We    rst apply two mining algorithms  Apriori  2   FP growth  13   on the complete PubMed data set  Unfortunately  both algorithms fail  Speci   cally  by setting the minimal support to 1  of the number of the documents  the implementation of FP growth runs out of memory when building the FPtree  which invalidates the algorithm  The Apriori algorithm can swap intermediate results to disk  but requires multiples scans of the data set  Even if we limit the maximal size of keyword combinations to 8  it would take weeks to generate all valid combinations  In general  the algorithms for mining association rules have dif     culties for the PubMed   s scale and our threshold  Although increasing the threshold can improve the ef   ciency of the mining process  as we will see in Section 6 3  the 1  threshold guarantees that all queries can be evaluated in a reasonable amount of time  We then test the hybrid approach  the graph decomposition algorithm is    rst applied  684 MeSH terms whose frequencies are greater than TC are selected to form the initial KAG  It takes 24 hours to decompose the original graph to subgraphs  each of which is either  1  small enough to be covered by one view or  2  large and very dense  i e   a clique   When a subgraph is a clique and is still too large to be covered by one view  the data mining based approach is used for further decomposition  Since individual cliques are much smaller than the original graph  the data mining based approach can achieve good ef   ciency  Altogether  the hybrid approach takes 40 hours  and selects 3 523 views  In our problem setting  context speci   cations are comprised of0 5 10 15 20 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15   of relevant results Query ID Conventional Context sensitive  a  Precision 0 5 10 15 20 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30   of relevant results Query ID Conventional Context sensitive  b  Precision 0 0 2 0 4 0 6 0 8 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Reciprocal rank Query ID Conventional Context sensitive  c  Reciprocal rank 0 0 2 0 4 0 6 0 8 1 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Reciprocal rank Query ID Conventional Context sensitive  d  Reciprocal rank Figure 6  Ranking quality of top 20 results MeSH terms  which are from a well controlled vocabulary and are fairly stable  Given that the threshold of the context size  TC  is set to a    xed percentage of the size of the document set   the number of views to materialize is stable  and does not change much as the the document set scales  The size of the document set  jDj  only has limited impact on the algorithms    complexities  Speci   cally  since the graphdecomposition based algorithm is based on the KAG  which is comprised of MeSH terms  its complexity is independent of jDj  The data mining based algorithm is based on mining association rules  which need to scan the document set one or more times  Hence  the complexity of the mining process is proportional to jDj  Overall  the complexity of the view selection increases linearly with jDj  Storage usage  For a materialized view VK  while keyword columns  i e   K  determines the number of tuples in VK  the storage of VK is also dependent on parameter columns  e g   len D   tf d  wi   which are speci   ed by a speci   c ranking function  In the experiments  we use the TF IDF formula which demands document count df wi  DP   of every query term which can be any keyword in the document set  Storing df wi  DP   for all the keywords in the document set would result in tens of thousands of parameter columns in VK  In our system  VK only stores the df wi  DP   column if jLwi j   TC  In other words  document counts of keywords with low frequencies are computed at query time  Consider the query Qc   w1   w2jm1   m2 and the materialized view VK  K   fm1  m2  m3g  Assume jLw2 j   TC  Then document count of w2  which is evaluated as jLw2   Lm1   Lm2 j  cannot be computed from VK  However  since jLw2 j   TC  the support of fw2  m1  m2g must be less than TC  and Lw2   Lm1   Lm2 can be evaluated ef   ciently at query time  Notice that the evaluation of Lw2   Lm1   Lm2 can start from the most selective keyword and leverage the optimization of skip pointers  The intersection Lm1   Lm2 is not enforced in the query plan  because collection cardinality jLm1  Lm2 j and other statistics can be evaluated from VK directly  There are 910 keywords in the document set whose frequencies are greater than TC  Therefore  every materialized view contains 912 parameter columns  the other two columns are context length and context cardinality   Given that the maximal number of the tuples in a materialized view is 4096  the maximal storage of a single view is 14 3 MB  The total storage of the materialized views is 12 77 GB  For comparison  the original data set of PubMed takes 70 GB  and the Lucene index takes 5 72 GB  The average storage of a single view is 3 71 MB  which means that most views have fewer tuples than 4096  The cost of using a materialized view to compute statistics is very small  6 3 Query Performance Next we evaluate the performance of context sensitive queries  The complete PubMed data set is used in the experiments  The straightforward evaluation  which was described in Section 3 1  is implemented as follows  for each collection speci   c statistic  a conventional keyword query that materializes the corresponding document set is constructed and sent to Lucene  After Lucene returns the document set  an aggregation is performed upon it  Consider the example query Qc   w1  w2jm1  m2 in Figure 3  Four collection speci   c statistics are required for the TF IDF function  document count for w1  w2  collection cardinality and collection length  Hence  three conventional queries are evaluated by Lucene  Q 1 t   m1   m2  Q 2 t   w1   m1   m2 and Q 3 t   w2 m1 m2  upon which the required statistics can be computed  Basically  we simulate the execution plan of a context sensitive query in Lucene by issuing multiple conventional keyword queries  With the materialized view technique  before sending keyword queries to Lucene  collection speci   c statistics are matched over the views    rst  If a view is usable for a collection speci   c statistic  no Lucene eva</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11ir2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s11ir2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#Information_retrieval"/>
        <doc>Score Consistent Algebraic Optimization of Full Text Search Queries with GRAF ###T Nathan Bales UC San Diego La Jolla  CA nbales cs ucsd edu Alin Deutsch UC San Diego La Jolla  CA deutsch cs ucsd edu Vasilis Vassalos A U E B  Athens  Greece vassalos aueb gr ABSTRACT We address two open problems involving algebraic execution of full text search queries  First  we show how to correctly apply traditional database rewrite optimizations to full text algebra plans with integrated scoring  and explain why existing techniques fail  Second  we show how our techniques are applied in a generic scoring framework that supports a wide class of scoring algorithms  including algorithms seen in the literature and user de   ned scoring  Categories and Subject Descriptors H 3 3  Information Search and Retrieval   Search process General Terms Design  Performance ### 1  INTRODUCTION Though not commonly used for web search  search systems that reason about individual word positions  henceforth  full text search  are commonly used in contexts where expert users require more focused queries and precise results  9   Full text search systems 3  27  14  22  19  power search for enterprise  government  academia  scienti   c applications such as Westlaw 32   PubMed 30   and the U S  Library of Congress  The choice of a powerful but more complex language is appropriate for sophisticated expert users and for search systems with GUI generated queries  Expressive power  and thus improved precision  entails complex evaluation plans and higher evaluation cost  To make full text search feasible despite high evaluation cost  plans must be optimized like database queries  Expert users who demand the power of full text search  also expect to be able to use their preferred ranking function  21   or tailor a ranking function  6  to their speci   c need  It follows that developers of full text search systems need to support a wide variety of ranking functions  even plug in ranking functions  to appeal to the widest possible audience  This fact is supported anecdotally  Terrier  22  ships with  as of September 2010  two different weight aggregation techniques  15 different term weighting functions  and support for user de   ned term weighting functions  Many state of the art full text systems do not support multiple ranking algorithms  27  14   and those that do  like Terrier  support narrow classes thereof  Rigid plan generation  which hard codes Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  assumptions about the speci   c ranking algorithm  limits generic ranking support in these systems  such systems only support ranking algorithms that    t their assumptions  Ranking techniques such as proximity metrics  16  25  do not    t typical evaluation plans  To support a wider class of ranking algorithms  IR systems must either generate highly generic plans  sacri   cing performance   implement multiple rigid generators  one per supported ranking algorithm  sacri   cing maintainability and extensibility   or deploy a    exible generator that takes a ranking algorithm as a parameter and adjusts plans to    t it  This paper studies    exible plan generation  Our approach is to add generic ranking to database style algebraic strategies for full text evaluation recently proposed in the database community  2  7   Ranking is de   ned against a    canonical    plan that  for each document d     rst computes the set S of query    matches    and then scores d based on S  From the canonical plan and a selected ranking algorithm  our optimizer    nds a plan which avoids large intermediate results by interleaving matching and scoring  If  for instance  the Lucene ranking algorithm  27  is selected  then our optimizer    nds a plan of comparable performance to the one yielded by Lucene   s rigid plan generator  If a context sensitive ranking algorithm is selected  then our optimizer    nds an optimized plan consistent with that algorithm  An optimal plan for contextsensitive ranking will look very different from an optimal plan for Lucene   s ranking  even for the same query  Prior works in full text algebras  2  7  provide a useful foundation for extension with generic ranking  but these works focus on the boolean retrieval problem  identifying  but not ranking documents that match the query  and  as we show in Section 2  optimizers designed for boolean retrieval cannot be straightforwardly re purposed for consistent ranked retrieval  Simply put  scoring discussed in prior work depends on intermediate results that optimizers may change  Different scoring schemes are sensitive to different changes in the intermediate results  An optimizer needs to carefully avoid optimizations that lead to score changes  while making full use of those that preserve scores  thus the optimizer should choose which optimizations to apply based on the selected scoring scheme  Building a generic optimizer that correctly applies optimizations for plug in ranking algorithms  is the primary technical challenge we address  We call an optimizer score consistent if all produced execution plans of a particular query yield the same score for the same combination of document and ranking algorithm  Given the effort usually exerted by application owners into ranking results  and the competitive advantage ranking algorithms often entail  making sure scores do not change due to optimization is not only a technical challenge but also a serious business issue  Ranking algorithms are frequently tuned for speci   c applications to guarantee some level of precision  recall  and or    fairness    that 769must be maintained  Moreover  a score consistent optimizer and execution engine can be updated and improved without potentially adverse effects on the  carefully tuned  ranking of the results and without requiring changes to ranking implementation  Desiderata  The goal of this work is to provide a system that   1  supports expressive full text search and    exible generic scoring   2  has an optimizer that exploits both known and novel optimizations techniques without introducing inconsistent scoring   3  despite overhead from generic scoring  performs competitively with systems using a    xed scoring algorithm  and  4  isolates scoring interface from optimizer such that scoring scheme developers need not understand the optimizer or respond to optimizer changes  Contributions  This paper presents GRAFT  Generic Ranking Algebra for Full Text   a full text algebra with integrated generic scoring framework  We show how GRAFT supports known and novel optimizations by providing the optimizer just enough information about a selected scoring implementation     including userde   ned implementations     to select only the compatible optimizations  Reaching this result  we make the following contributions  1  We study the interplay between scoring and match computation and show that blind application of traditional database optimizations to match computation can result in different scores  Such blind application of optimizations is known to arise naturally in systems in which the optimizer is developed initially for boolean retrieval  and scoring is integrated afterward  see Section 2   2  We present the    rst formal model for generic ranking of full text queries  The model comprises a small set of operators  a scoring algebra SA   We validate the expressive power of our model by capturing ranking algorithms and techniques found in the literature  7  13  16  20  22  25  28  29  34   3  We propose an integrated algebraic model for the interplay between score computation and match computation  This model allows us to reason about the correctness of traditional optimizations in ranked full text query systems  4  We show that our integrated model accommodates novel optimizations involving both scoring and match algebra operators  5  We identify a small set of SA operator properties relevant to applicability of optimizations  We show how to automatically con   gure the optimizer  based on these properties  to exploit the available optimization potential without compromising score consistency  6  Finally  we show experimentally that optimization allows a system that supports generic scoring to compete with a system that does not  and that our novel optimizations are powerful  Paper organization  Section 2 motivates our work with examples that illustrate how we solve the score consistency problem found in existing full text algebras  Section 3 describes a relational model for full text based on sequences of match tuples called match tables  Section 4 de   nes our novel model for generic scoring and outlines the GRAFT framework  Section 5 discusses optimizations  both classical database optimizations  and optimizations that cross the boundary between matching and scoring  With each optimization  we list the scoring scheme properties that must hold for the optimization to be score consistent  Section 6 discusses the complexity of evaluating relational full text queries  Section 7 is a study of scoring schemes observed in the literature 7  13  16  20  22  25  28  29  34   Section 8 reports on the implementation and experimental evaluation of GRAFT  We conclude in Section 9  2  MOTIVATION We motivate our work with an example that both illustrates why state of the art full text algebra solutions are unsatisfactory for generic ranking  and gives the intuition behind our solution  This section also introduces vocabulary used in the following sections  TOKEN DOC  INDOC  DOCS OFFSETS    emulator    dw 1 2768  64     free    dw 1 332335  3     foss    dw 1 2044  179     software    dw 4 71735  4  32  180  189     windows    dw 4 43949  27  42  144  187  Figure 1  A fragment of a normalized term position index for a document dw  The fragment includes all positions of keywords used in later examples  OFFSETS is a list of positions of the token in dw   INDOC is the number of occurrences of the keyword in dw  and  DOCS is the the count of documents in the collection that contain the keyword  A    free     d  f   d  sf 1 1J1 1J2 A    emulator     d  e  A    software     d  s  Plan 1  An algebraic plan which evaluates the matches to Q1  A    free     d  f   d  sf 1 1J1 1J2 A    emulator     d  e  A    software     d  s  Plan 2  An algebraic plan which evaluates the matches to Q1  with a selection pushed through join J2  Expressive power to reason about term positions is the fundamental difference between full text search and classical keyword search  The document model for full text search is a sequence of words  whereas for pure keyword search it is a bag of words  Figure 1 shows part of a full text index for document dw 1   This index contains positional information  it records the position  offset  of each word occurrence in the document  The expressive power of full text search is illustrated in Query Q1  a simple query over some collection of documents that includes dw     nd documents with    emulator        free    and    software    s t     free    appears immediately before    software     Q1  Keyword search systems that do not index the full text  word positions  cannot answer this query without scanning the document text because    appears immediately before    involves term positions  which the bag of words document model does not maintain  Document dw is an answer to Q1 because it has one match to Q1 using tokens at offsets 64  emulator   3  free   and 4  software   Informally  matches are tuples consisting of a document id and token offsets that satisfy the query  such as hdw  64  3  4i  If not for the    appear immediately before    clause  Q1 would have four matches  one of the four different positions for    software     hdw  64  3  4i  hdw  64  3  32i  hdw  64  3  180i  and hdw  64  3  189i  Three of these tuples are not matches because the value of the fourth    eld minus the value of third    eld is greater than one  indicating that    free    does not appear immediately before    software     Plan 1 and Plan 2 both compute the answer to Q1 under Boolean semantics  These plans compute    match tuples    for each document  but project only the document ids  a set of documents is the answer to a boolean search query   A    emulator     d  e  is a relation consisting of every pair of document id  d  and offset  e  such that    emulator    appears in d at position e  A    emulator     d  e  abstracts a term position index scan over all positions of    emulator    in the document collection  The joins  J1 and J2 for reference  are natural inner joins  and the other operators are standard relational algebra 15   Plan 1 processes selection after the joins in a manner consistent with automatic translations from a calculus  Plan 2 selects before join J2  and is derived from Plan 1 using a textbook selection pushing rewrite that preserves Boolean semantics  State of the Art  Plans 1 and 2 illustrate why state of the art fulltext algebras cannot simultaneously facilitate generic scoring and 1 dw is a real document  speci   cally  the abstract portion of the Wikipedia article Wine  software  as of March 10  2009  770  score consistent optimizations  The state of the art full text algebra  7  extends each match tuple with a score  and extends each algebra operator with a function to manipulate the scores  As plan evaluation constructs and combines match tuples  it simultaneously computes and aggregates match scores using the scoring functions  Join  for instance  is extended with the function SJ  Score Join   SJ mL  mR  computes the score of an output tuple m  such that the arguments mL and mR are tuples that join to form m  This framework supports generic ranking  A new ranking algorithm is    plugged in    by providing new scoring function implementations  An example implementation  given in  7   for SJ mL  mR  is mL s jMRj   mR s jMLj where mL is a tuple from the left input  mL s is the score of tuple mL  jMRj is the number of tuples from the right input that join with mL  Intuitively  the value mL s is distributed equally among the output tuples mL contributes to  so that the join operator does not create or eliminate score value  Returning to the example  assume mL is hdw  64  si  s is the score of the tuple   In Plan 1  mL joins  join J2  with 4 tuples and the value mL s is distributed evenly between them  Three of the resulting tuples are eliminated in the selection  thus only one quarter of mL s   s score value contributes to the    nal score of dw  In Plan 2  the three tuples have already been eliminated by the selection before they join mL  The value of mL s is distributed to only one tuple  and the whole value of mL s contributes to the    nal score of dw  Thus  while the selection pushing optimization does not affect which matches are computed by the plan  it affects document scores  To avoid score inconsistency the selection pushing optimization must be disabled when using this scoring function  Score inconsistency in this example is not the fault of a poorly chosen scoring function  it is a result of encapsulating score computation into operators that compute matches  Textbook selection pushing was designed for Relational Algebra and preserves the semantics of computing tuples  matches   but it is unaware of scoring  which it does not preserve  Proposed Solution  We model scoring using similar scoring functions as the framework in  7   but without encapsulating them in the relational algebra operators  Instead  scoring functions are standalone aggregate functions that interact with the other relational algebra operators in the standard way  We de   ne the correctness of scoring based on the principle of score isolation  A plan is score isolated when all matches are computed by a matching subplan which contains no scoring  This subplan yields a match table containing the matches in a de   ned order  e g  lexicographic   Scoring semantics are de   ned as aggregation of the match table  thus  conceptually  isolating scoring from match computation  Plan 3 is a score isolated plan for Q1  showing the matching subplan in a box  The match table is well de   ned at the semantic  optimization independent level  thus enabling an optimization independent de   nition of the intended scoring semantics  namely score isolation  This in turn is key to de   ne score consistency meaningfully  by identifying the score to be preserved  Following the matching subplan  matches for the same document are grouped and aggregated into a document score  Two score aggregation operators are used in Plan 3  SJ and SG  Score Group   SJ still computes the score of joined matches  but not immediately when joined  Instead  SJ is explicitly called outside the matching subplan  SG is an aggregate function  like SUM in SQL  that computes the score of grouped matches  For speci   cs on algebra notation see Section 3 2 and for scoring semantics see Section 4  Match tables can be large  see Section 6   Optimizing scoreisolated query plans entails interleaving matching and scoring in a way that avoids materializing the entire match table while computing the same answers and scores  A crucial part of this work seeks  d s SJ e SJ f s    djs SG s   sf 1 1J1 A    free     d  f    lexicographic order  A    emulator     d  e  1J2 A    software     d  s  Plan 3  A score isolated evaluation plan for Q1 displaying the matching subplan within a box  A    free     d  f   d s SJ f s   d s SJ e s   djs SG s   sf 1 1J1 1J2 A    emulator     d  e  A    software     d  s  Plan 4  A plan derived from Plan 3 by rewrite optimization   Only for some scoring schemes   machinery to optimize such plans in a score preserving way using a mix of classical relational optimizations as well as novel optimizations that cross the boundary between matching and scoring  De   nition 1  Score Consistency   An optimizer isscore consistent if  given a score isolated input plan  it produces only plans that compute the same answers and scores as the input plan  To interleave matching and scoring without violating score consistency  an optimizer must know which optimizations are tolerated by the selected scoring function implementation  A na  ve approach requires each function to specify  in meta data  which rewrites it tolerates  For instance  to reach optimized Plan 4 from score isolated Plan 3  it must be true that SG tolerates unsorted matches  since the sort operator is removed   and that SJ tolerates being pushed through joins  But this na  ve approach requires the scoring function designer to know and care about the inner workings of the optimizer  and limits the applicability of the scoring function implementation to future optimizations  A better approach has the scoring designer specify a small set of fundamental properties about her implementation   e g  which functions are commutative  fully associative  idempotent  monotonic  etc   and allow the optimizer to infer which optimizations will preserve score consistency given a selected scoring function  In Section 5 we describe our solution along these lines  3  RELATIONAL MODEL FOR FULL TEXT The relational model for full text evaluation has recently been explored within the database community  7  2   Full text queries in the relational model are    rst order formulae over term positions  In this section we de   ne MCalc  a full text calculus used to specify a set of matches  and MA  a full text algebra used to compute those matches  Ranked retrieval needs this set of matches because scoring functions measure the connection between document and query  matches establish that connection  3 1 Matching Calculus The Matching Calculus  MCalc  speci   es the set of matches to a full text query in a collection of documents  in the style of the Domain Relational Calculus  The basic primitive in MCalc is the HAS predicate  HAS d  p  k  is true when the word k appears in document d at position p  and is false otherwise  All HAS predicates have one document variable argument  one position variable argument  and one keyword  variable or literal  argument  Full text predicates de   ne relationships between the positions of keywords  For example  DISTANCE p1  p2  n is true when the distance from position p1 to p2 is exactly n  and PROXIMITY p1  p2  n  is true when the distance from position p1 to p2 is at most n  More generally  MCalc supports full text predicates of the form PRED  p  c   which specify constraints on position variables in the vector p   Optional constants  c   parameterize the constraints  MCalc is general enough to support generic positional predicates  5   771  MCalc queries take the form fhd  p ij   p  is satis   edg  where   is a    rst order logic formula over the primitives HAS  EMPTY  described below  and full text predicates  The free variables in   are exactly the document variable d and the position variables in p   Example 1  MCalc Query   fhd p0  p1  p2ij HAS d  p0     emulator       HAS d  p1     free       HAS d  p2     software       DISTANCE p1  p2  1 g  Q2  Query Q2 is a MCalc query that    nds all matches to Q1  The HAS predicates restricts position variables p0  p1  and p2 to positions of keywords    emulator        free     and    software    respectively  The DISTANCE predicate speci   es the distance between values of p1     free     and p2     software     must be exactly 1  expressing the requirement that    free    occur immediately before    software       MCalc also has a built in EMPTY predicate that is necessary to support optional keywords  and to ensure safe disjunctive queries  EMPTY p  is true when p binds to the    empty position    symbol    An empty position value does not imply the keyword associated with the variable is absent  just that its presence  or lack thereof  is inconsequential to a particular match  Example 2  MCalc Query with Disjunction   fhd  p0 p1  p2  p3  p4ij    0     1     HAS d  p0     windows       HAS d  p1     emulator       WINDOW p0  p1  50 g   0   EMPTY p2    EMPTY p3    HAS d  p4     foss       1   HAS d  p2     free       HAS d  p3     software       EMPTY p4    DISTANCE p2  p3  1   Q3  Q3 asks for matches where the keywords    windows    and    emulator    appear within a window of 50 words  and are accompanied by either the keyword    foss     or by the phrase    free software       The formal meaning of    match    is given with respect to MCalc  De   nition 2  Match   The tuple hd  p i is a match of query   in document d iff hd  p i is a satisfying assignment for   that maps the free position variables in   to either word positions in d or to    MCalc vs  State of the Art  State of the art full text calculi  such as FTC 7   were not designed to specify a set of matches  only documents   and sometimes under specify matches  Matches to FTC queries may contain positions of keywords not mentioned in the query 5   such matches are not useful for scoring  MCalc adds a safe range 1  requirement  similar to SQL  which restricts matches to only those useful for scoring by binding under speci   ed position variables to   via the EMPTY predicate  Otherwise MCalc has equivalent expressive power to FTC  which was shown in  7  to subsume other full text speci   cations  including predicate based languages  8  4   and text regional algebras 12   See  5  for details on safety  expressive power  and MCalc   s relationship to FTC  3 2 Matching Algebra We express evaluation plans for MCalc queries in the Matching Algebra  MA   MA has enough expressive power to express all safe MCalc queries  as Relational Algebra has enough expressive power to express safe Relational Calculus queries  Queries may be translated between MCalc and MA using traditional translation methods developed for the Relational Calculus and Relational Algebra  1   MA operates on  and outputs match tables  Match tables are lists of matches  which are tuples of form hd  p0       pni where d ranges over documents  and pi ranges over term positions and    Match tables are lists  rather than sets  of tuples  table rows and columns are both sequenced  and tables may contain duplicate rows  We use    matches        tuples    and    rows    interchangeably  The use of tables is consistent with practical relational algebra and SQL implementations  The match table for query Q3 on document dw  from Section 2  is shown in Figure 2  The Matching Algebra consists of familiar Relational Algebra operators  natural join 1  outer bag union    11   selection    generalized projection    anti join    group    and sort     A formal speci   cation and description of each operator is available 5   Due to space constraints  we focus this discussion on the novel Atomic Match Factory  A  and clarify our notationfor   and    Atomic Match Factory  A  is the terminal operator in MA  corresponds logically to the MCalc HAS predicate  and abstracts a scan of the term position index  The expression A d  p  k  produces a match table with two columns  speci   cally  a document id named d and a term position named p  The match table contains one tuple hd  pi for each d and p satisfying HAS d  p  k      one for each occurrence of keyword k in the index  To simplify presentation we assume that a system has a single library of documents indexed  and that all queries are applied to the entire library  Notation  The subscript of   contains a list of attribute names and assignments with the form a f b   The attribute names are the usual    elds that are preserved through a projection  while the assignment means attribute a in the output tuple gets the value of function f applied to attribute b in the input tuple  The subscript of   has the form AjB  where A is the list of group by attributes  and B a list of assignments of the form a f b   In this case  f is an aggregate function  e g  SUM   and    eld a in the output tuple is the result of using f to aggregate the values of b in the grouped input tuples  We use the following shorthand for binary aggregation operators  a   b    a    g0 b g1 b       gn b  where g0       gn are the grouped tuples  4  SCORING Scoring functions measure evidence that establishes the connection between a document and a query  For GRAFT  we consider a class of scoring algorithms  called match scoring algorithms  that measure speci   cally the evidence contained in the list of matches to the query against the document  We choose match scoring algorithms for several reasons  Matchscoring captures many real world algorithms because the list of matches is a superset of the evidence they require  The match table  which is already computed for boolean evaluation  contains the list of matches  Finally  we can model match scoring algorithms using a simple algebra of scoring operators that can be integrated with the Match Algebra  uncovering optimization opportunities  We validate the expressiveness of our algebra  and of matchscoring  in Section 7 by demonstrating how several scoring algorithms seen in the literature  7  13  16  20  25  22  28  29  34  are implemented as scoring schemes in our algebra  A scoring scheme is an implementation of the operators of our scoring algebra  Section 4 1 introduces the Scoring Algebra  SA  and explains the underlying intuition  Section 4 2 discusses subtleties of score aggregation which must be considered when choosing a scoring algebra plan  and Section 4 3 explains how SA and MA work together to evaluate ranked full text queries  Due to space constraints  we relegate to  5  the formalization of the intuitive presentation in Section 4 1 and Section 4 2   5  de   nes the score of a match table inductively  as an aggregation of scores of match subtables  and summarizes the rules that govern the allowed choices of subtables  4 1 Scoring Algebra The Scoring Algebra  SA  is an algebra of composable abstract scoring operators  whose implementations  called scoring schemes  express match scoring functions  A canonical SA plan takes as input a match table  the result of computing a MA query  and outputs 772 a list of scored documents  An optimized plan interleaves SA and MA operators  as discussed in Section 4 3  to avoid materializing full match tables because  as we show in Section 6  their size can be quite large  A score for document d is computed from the query matches to d using the following three step process  Step 1  Initialization  The term position p in each table cell is replaced with an initial score value   p     is the initializer function in SA  and typically implements a term weighting function such as TF IDF 18   BM25 18   KL Divergence 18   etc  Step 2  Aggregation  Initial scores are aggregated into a single  but not    nal  aggregate score value  Modeling this step using a single aggregation operator would be too crude  because the match table has two dimensions with different semantics  Term positions within the same row have a different relationship than positions in the same column  Furthermore  term positions in the same row that match conjunctive sub expression    foo       bar    have a different relationship than positions matching disjunctive sub expression    foo       bar     details in Section 4 2 1   We developed three binary aggregation operators to re   ect the relationships between positions  We call term positions within the same match table column alternates of each other  Within the same row  we say that positions in columns corresponding to a conjunctive  disjunctive  subexpression of the query are conjuncted  disjuncted   The relationship among positions induces a relationship among scores  If positions p  p 0 are alternates conjuncted disjuncted  we say that so are the scores   p    p 0    Conjuncted  disjuncted  and alternate scores are combined using  respectively  the conjunctive combinator 7  the disjunctive combinator 6  and the alternate combinator    Step 3  Finalization  The    nalization function   computes a      nal     oating point  score for a document from the aggregate score produced by Step 2  The aggregate score is a structure  called an internal score  composed of one or more values that are aggregated independently  Internal scores are necessary so that score aggregation  which is de   ned using binary operators  can express complicated aggregation functions that do not normally compose when expressed as binary operators  For instance  the mean of fa       y  zg cannot be computed given the mean of fa       yg and value z  It can be computed given z and both the sum and count of fa       yg as the pair  s  c   Adding z to s and incrementing c yields a new pair  s 0   c 0    The internal score type for mean is the pair hsum  counti and    s 0   c 0      s 0 c 0 yields the    nal score  The   function also performs other post processing including normalization and incorporation of match unrelated score components such as document age  PageRank 23   etc  Summary  SA comprises six operators        7  6      The initialization function   scores individual match table cells  Three binary score aggregation operators 7  6  and   aggregate the scores of the individual cells into a single  internal  aggregate score  Finally  the    nalization function   post processes the internal score  Example 3  MEANSUM Scoring Scheme Implementation   We present a scoring scheme called MEANSUM  chosen because it helps illustrate our points effectively in a single example  More scoring schemes re   ecting the real world scoring algorithms in  7  13  16  20  22  25  28  29  34  can be seen in Section 7    d  c  p    if p is   then   return h0 0  1i else   let tf idf    p  InDoc d length   d collectionSize p  Docs return htf idf  1i   s1  s2    return hs1 sum   s2 sum  s1 count   s2 counti 7 s1  s2    return hs1 sum   s2 sum  s1 counti 6 s1  s2    return hs1 sum   s2 sum  s1 counti   d  s    let mean    s sum s count return 1  1 ln mean e  MEANSUM de   nes the score of a document as the average score of all its alternate matches  and the score of a match as the total score of the individual positions in the match  Term positions in MEANSUM are scored by t   df 18   The initializer function   produces a score for a single match table cell populated by a term position or    The function takes three arguments  a document d  a match table column c  and a term position p  Position p must appear in document d and match the query variable corresponding to column c  Each of these arguments is not merely an id  but a collection of relevant statistics  e g  the document argument d includes the document length as d length   When the position argument is    the implementation returns the pair of zero and one  Otherwise  p contains a word position summary  and the implementation returns a pair of a t   df value and the value one  MEANSUM uses pairs as its internal score type  The members of the pair are the two components of a mean computation  the sum of t   df scores  and the count of aggregated matches  rows   The alternate combinator combines alternate scores s1 and s2  Each input score is a sum of t   dfs  and a count of matches  The sums are added  as are the counts  alternate match sets must be disjoint by de   nition   MEANSUM does not differentiate between conjuncted and disjuncted scores  thus 7 and 6 have the same implementation  The arguments s1 and s2 are conjuncted or disjuncted scores  The intuition behind the implementation is similar to that of    only that in this case s1 and s2 by de   nition refer to the same set of matches  so they have the same counts  which are preserved by 7 and 6  The    nalizer function   computes the    nal score value as a    oating point number    takes two arguments  d is the document  and s is the internal score computed by aggregating the entire match table  The sum member of the pair score is divided by the count member to compute the mean  and the    nal score is computed by normalizing the mean to the range  0 1     4 2 Match Table Aggregation SA operators aggregate match  sub tables into single aggregate scores  This aggregation is formally de   ned from the top down  but practically implemented using bottom up plans  For now  we con   ne our discussion to the top down de   nition  Match table scoring is de   ned inductively  Two column wise or row wise subtables are chosen and scored recursively  The scores of the subtables are combined using 7  6  or   to reach a score for the full table  Row wise subtables partition the match table rows  Column wise subtables partition the match table columns  and thus the query variables  which provide the column names   The query variables are partitioned into conjuncted or disjuncted variable sets  see Section 4 2 1 for more   Match tables with more than one column and or more than one row allow multiple partitions into subtables  The choice of subtables cannot be arbitrary  rather it must consider the selected scoring scheme  As a simple example  consider a match table with two rows a and b  There are two possible row wise subtable pairs   fag  fbg  and  fbg  fag  which differ merely in order  Given these two choices  the score of the match table is either score a   score b  or score b    score a   Clearly  if   is commutative  then either pair results in the same score  otherwise the score will depend on which pair is chosen  and a score consistent optimizer must choose so as not to perturb the desired order among matches  A generic score consistent optimizer must therefore know if the   implementation of the selected scoring scheme is commutative  While the commutativity of   provides a simple example  the following two sub sections detail two far more subtle issues that guide the choice among potential subtables  773 p0 p1 p2 p3 p4 27 64     179 27 64 3 4   42 64     179 42 64 3 4   Figure 2  The match table for Q3 over dw  Each cell contains either the empty position symbol    or a term position  Only term offset is shown for each term position  the other statistics  See Figure 1  are hidden for brevity  4 2 1 Choosing Column Wise Subtables The structure of the query expression de   nes relationships between match table columns and between column wise subtables  Two columns q and r are conjuncted if q   r is a subexpression of the query and disjuncted if q   r is a subexpression of the query  Similarly  two column wise subtables with variable  column  sets Q and R are conjuncted if   Q      R  is a subexpression of the query  where   Q  is a subexpression over the free variable set Q  Since the columns in a full match table have a one to one mapping to the free variables in a valid query expression  valid conjuncted or disjuncted subtables are identi   ed using the query   s syntax tree  A scoring plan    p  is a syntactic transformation of a query    p  which provides information needed to determine columnwise subtables  the structure of conjunctions and disjunctions between free position variables  The transformation procedure is as follows  erase all non HAS predicates  erase HAS predicates with quanti   ed position variables  erase all negations  erase dangling local connectives  replace each remaining HAS predicate with its position variable argument  and    nally  replace the remaining   and   with 7 and 6 respectively  The safety condition on MCalc 5  guarantees this procedure yields a meaningful scoring plan  Example 4  Scoring Plan   The scoring plan for Q3 is obtained by following these steps  First remove the non HAS predicates from     HAS d  p0     windows       HAS d  p1     emulator           HAS d  p2     free       HAS d  p3     software         HAS d  p4     foss       Then replace HAS d  px  k  with px  and   and   with 7 and 6 to arrive at the scoring plan     p0 7 p1  7   p2 7 p3  6 p4    Generic scoring support constrains the selection of column wise subtables  In the absence of scoring  MCalc queries obey FO logic equivalences  Query q   r is equivalent to r   q because   is commutative  These two FO equivalent queries result in the scoring plans q 7 r and r 7 q  that are equivalent only when 7 is commutative for the selected scoring scheme  Similar issues occur with associativity  commutativity  and monotonicity of both 7 and 6  Since query equivalence is based on properties of FO logic and scoring plan equivalence is not  a question arises  from which of the many potential syntax trees for   is   derived  The matching plan is obtained from a syntax tree derived using standard FO logic equivalences  The scoring plan is obtained from a syntax tree derived using the properties  see Section 5 1  of the selected scoring scheme  The decoupling of the Matching and Scoring Algebras thus allows an optimizer to reorder joins  using the    exibility of FO logic equivalence  even when a rigid scoring scheme is selected that does not allow reordering of the score aggregation operators  Example 5  Score Computation   In this example we walk through the process of computing a score using MEANSUM  Score computation starts from the match table  Figure 2 shows the match table used in this example  which contains matches in document dw  introduced in Section 2  for Q3  We score the match table by aggregating the scores for its subtables  For the example  we choose column wise subtables until only single column subtables remain  We    rst compute the scoring plan for Q3       p0 7p1 7  p2 7p3 6p4   Each individual column is a column wise subtable that must be    rst scored by aggregating the initial scores of its rows  We show how column p4 is scored  Column p4 has four alternate position values   179     179      We split p4 into two  then four row wise subtables     179           179          Each new subtable is a single  uninitialized cell  we continue the example by showing how these cells are initialized  The    rst and third single cell subtables both contain term position 179    uses statistics from 179   s index record  Figure 1  Section 2   hOffset 179   Docs 2044   InDoc 1       i    also requires two document parameters  which are dw length 207 and dw collectionSize 4 638 535  Finally    dw  p4 h179       i    h 1 207   4638535 2044   1i   h10 96  1i  The second and fourth single cell subtables each have the empty term position    Based on the   implementation for MEANSUM    dw  p4       h0 0  1i  We next aggregate the initial scores of the alternate positions in column p4 using the alternate combinator   h10 96  1i   h0  1i     h10 96  1i   h0  1i    h10 96  2i   h10 96  2i   h21 92  4i The scores of the other columns are computed similarly  bringing us back to the computation for the whole match table which is completed with the conjunctive and disjunctive combinators   h8 156  4i7h32 38  4i 7  h0 134  4i7h2 498  4i 6h21 92  4i    h40 536  4i 7  h2 632  4i 6 h21 92  4i    h40 536  4i 7 h24 552  4i   h65 086  4i Finally  the    nalizer function   computes the    nal  normalized document score from the aggregated score    d h65 086  4i    1  1 ln  65 086 4  e    0 660    4 2 2 Scoring Directionality Scoring directionality has to do with whether row wise subtables or column wise subtables are selected    rst  Some scoring schemes are sensitive to this choice  There are two simple scoring patterns  row    rst  and column    rst  Row    rst scoring involves    rst computing the score of each row  and combining those scores  For row     rst scoring  row wise subtables are always chosen when a match table has more than one row  Column    rst scoring involves    rst computing the score of each column  and combining those scores  For column    rst scoring  column wise subtables are always chosen when a match table has more than one column  In hybrid scoring row wise and column wise score aggregation may be interleaved  Many scoring schemes are directional  they will compute different scores under row    rst aggregation than under column    rst aggregation  When a scoring scheme is directional  one direction  row    rst or column    rst  is    correct        in that it is the intention of the scoring scheme designer     and hybrid scoring is incorrect  Example 6  Scoring Directionality   Directionality is easy to illustrate with a simple example  Consider the match table Mbool of conjunctive query a b  where scores are either T  true  or F  false   We de   ne 7       and         If we choose row    rst aggregation  then the score computation is shown as sr  Similarly  sc shows column    rst aggregation  Mbool    a b T F F T sr     T   F     F   T    F   F   F sc     T   F     F   T    T   T   T Scores computed by row    rst and column    rst aggregations are different even in this overly simplistic scenario    Score evaluation must respect the appropriate pattern for directional scoring schemes or computed scores will be wrong  774 Diagonal scoring schemes compute the same score using a row     rst  column    rst  or hybrid scoring pattern  and are highly desirable because they provide the optimizer more    exibility  Some optimizations discussed in Section 5 only work for diagonal schemes  Several scoring schemes we study in Section 7 are diagonal  De   nition 3  Diagonal Scoring   A scheme is diagonal iff the following properties hold   w 7 x     y 7 z     w   y  7  x   z   w 6 x     y 6 z     w   y  6  x   z  4 3 Integrating Matching   Scoring Algebras We now introduce GRAFT  Generic Ranking Algebra for Full Text   which integrates MA and SA  In GRAFT  the operators of the scoring algebra are hosted by   and   operators  which in scoreisolated plans are performed outside the matching subplan   Hosted aggregations are a familiar concept  The SQL query    SELECT a b as c  SUM d  FROM foo GROUP BY a b    translates to a relational algebra expression where the SUM aggregate is hosted by a group by operator and the addition  a b  is hosted by a generalized projection  In GRAFT    is hosted by the group operator    while 6  7    and   are hosted by projection    Plans produced by automatic MCalc to GRAFT translation are score isolated  introduced in Section 2   Score isolated plans consist of a matching subplan with no scoring operations that computes a match table  and a scoring portion  everything above the matching subplan  which computes the match table score  The optimizer starts with a canonical score isolated plan  rewriting it iteratively  There are two canonical score isolated plans for any MCalc query which compute scores in a row    rst  column    rst  manner  Which one is used depends on the directionality of the selected scoring scheme as discussed in Section 4 2 2  Both plans share the same matching subplan  Canonical matching subplans use a right deep join tree and join order follows the order of keywords in the query  Selections follow joins  and a sort follows selections  Plan 7 shows the matching subplan of the canonical score isolated plan for Q3  The scoring portion of the row    rst canonical plan    rst scores each match table cell and row by evaluating   and the scoring plan   in the context of a    It then aggregates the row scores using   in the context of    Finally  it computes the    nal scoring using   in the context of    Plan 6 shows the scoring portion of the row    rst canonical score isolated plan for Q3  The scoring portion of the column    rst canonical plan    rst scores each match table cell by evaluating   in the context of    It then uses   in the context of   to compute column scores  It then evaluates the scoring plan   to combine column scores  and computes the    nal scoring using   in the context of    Plan 5 shows the scoring portion of the column    rst canonical plan for Q3  5  OPTIMIZATION Optimizing GRAFT queries involves interleaving matching and scoring to avoid materializing the entire match table  while computing the same answers and scores as the canonical score isolated plan  The performance of GRAFT queries stems from the size of the match table     a crucial intermediate result in score isolated plans  We base our semantics on the match table for two reasons  it allows us to use an unrestricted  fully expressive set of full text predicates  and we consider expressive ranking algorithms that use all of the matches as evidence for scoring  In the worst case  the match table is the cross product of the position list for each query keyword  Since the number of positions of each keyword scales linearly with the size of the data  the size of this cross product is O WQ   where W is the size of the library in words  and Q is the size of the query  Eager materialization of the match table  as in a score isolated query plan  is a costly step  potentially incurring exponential complexity  Match tables are  however  only conceptual  they need not always be materialized  and certainly not eagerly  In Section 5 1 we describe some design time speci   ed properties of scoring scheme implementations that are relevant to the applicability of optimizations  In Section 5 2 we list useful relational optimizations  both classical and novel  and discuss how each is related to properties from Section 5 1  5 1 Optimization Relevant Properties With respect to a selected scoring scheme  some optimizations are valid  preserve score consistency  or invalid  do not preserve score consistency   The optimizer must be able to discriminate and only apply valid optimizations  To do this  the optimizer needs to know some properties of each scoring scheme implementation  These properties are declared by the scoring scheme developer  We keep the set of properties simple  small  and high level to reduce burden on the developer and improve maintainability across updates  The scoring scheme developer can specify the properties without understanding the workings of the optimizer  and changes in the optimizer will not break previously speci   ed scoring schemes  The set of speci   ed properties includes basic mathematical properties of aggregation operators  associativity  commutativity  monotonicity  and idempotency of 7  6  and    If the scoring scheme is not diagonal  see Section 4 2 2   the developer must specify whether she intends the scoring to be row    rst or column    rst  Finally  we de   ne here three additional properties that are useful for determining the validity of interesting optimizations  A   operator multiplies if a sequence of equal scores can be aggregated in one operation  Speci   cally  if there exists and operator    implementable in constant time  such that  s1   s2         sk   s0   k where 8i  j   si   sj A scoring scheme is constant when all matches for a document have the same score  and the alternate combinator   is idempotent  The constant property implies that    nding one match for a document is enough to score the document  Speci   cally  8m1  m2    matches m1  doc    matches m2  doc      score m1    score m2    score m1    score m2   A scoring scheme is positional if term positions factor into scores  Speci   cally  a scoring scheme implementing   is positional iff  9p0  p1  d  k   HAS d  p0  k    HAS d  p1  k      p0   6   p1  5 2 Study of Optimizations Under Scoring We    rst relate some classical relational optimizations to the relevant properties an optimizer must consult to check validity of each optimization for a selected scoring scheme  Then we do the same for both existing and novel full text speci   c optimizations  5 2 1 Classical Optimizations Sort Elimination  Canonical GRAFT plans have a single sort operator which guarantees a well de   ned order to matches in the match table  This order is necessary for scoring schemes where   is non commutative  When   commutes  the order is irrelevant and the sort operator may be removed  If   does not commute  sorting can sometimes be eliminated using classical techniques  24   Selection Pushing   Join Reordering  Selection pushing and join reordering  are both textbook relational algebra optimizations  Since score aggregation in GRAFT is decoupled from join and selection operators  these optimizations are not prohibited by any scoring schemes  They must still be applied carefully because  for instance  some join orders are more amenable than others to optimizations that push score aggregation  described below   775 d s  v7w7  x7y 6z    matches Q3  dw   d s   s   djv   v  w   w  x   x  y   y  z   z   d v   v  w   w  x   x  y   y  z   z  Plan 5  Scoring portion of the column    rst canonical score isolated plan for Q3   d s  v7w7  x7y 6z    matches Q3  dw   d s   s   djs   s   d v   v  w   w  x   x  y   y  z   z  Plan 6  Scoring portion of the row    rst canonical score isolated plan for Q3  A    emulator     d  w   d v w x y z ASCENDING  WINDOW v w 50  DISTANCE x y 1  A    windows     d  v  matches Q3  dw  1 1   A    software     d  y  1 A    foss     d  z  A    free     d  x  Plan 7  Canonical matching subplan for Q3 Eager Aggregation  One way to avoid full materialization of match tables is to eagerly aggregate the matches in intermediate results by pushing group bys down the plan  See Yan and Larson  33  for details on eager aggregation  Yan and Larson also studied eager aggregation in the context of generic aggregate functions  33   We can directly map their aggregate function classi   cations to our properties and assert that eager aggregation is applicable when   is fully associative  Additionally  and unique to our application  when the selected scoring scheme requires row    rst scoring    operators hosting   may not be pushed down through   operators hosting 7 or 6  as doing so would violate the row    rst requirement   Eager Counting  Eager counting  33  is a technique that groups n identical tuples into a single tuple with a count value n  When two eagerly counted tuples join  their counts are multiplied  Counted tuples are expanded for aggregation  but otherwise eagerly counted tuples act as regular tuples in the system  The expansion step prior to aggregation can be avoided for score aggregation if the   operator multiplies  see Section 5 1   Zig Zag Joins  The zig zag join 15  is a special case of sortmerge join useful in fully streaming plans subplans when each join attribute is indexed  The zig zag join consumes its inputs by exploiting order to signal the index scan over one join attribute to skip directly to the value of the other join attribute  Zig zag join signals the index scan operator even if it is several levels down the operator tree  thus bypassing large swaths of intermediate results from earlier joins  Zig zag is a powerful join technique for GRAFT since every plan leaf is an ordered index scan  In the context of relational full text  zig zag joins perform the same function as the skip pointers 18  commonly used in IR systems to accelerate inverted list intersection  Rank Joins  Top k optimizations speed up query execution by    rst exploring the documents that show the highest potential for a high score  and avoiding further exploration of lower scoring documents once the top K are established  The relational rank join 17  is a state of the art algorithm for ef   ciently joining two rank order tuple streams  producing a rank order tuple stream  Since MA uses a standard relational join  the rank join may  under some circumstances  be used to implement a match join  The GRAFT rank join hosts the 7 operator which it uses to combine the scores of joining tuples  Therefore  a rank join may only be used where 7 may be pushed into the matching subplan  Additionally  rank join only works with monotonically increasing ranking functions  17   so 7 must be monotonically increasing for rank join to apply  A similar rank union operator is also possible  Rank union hosts the 6 operator  requiring it to be monotonically increasing  5 2 2 Full Text Speci   c Optimizations Forward Scan Joins  Botev et al   7  identi   ed a set of common full text predicates  PPREDS  that can be executed in a single forward pass over the index and can be used as join predicates to zig zag joins  They developed an ef   cient evaluation plan for a full text language restricted to PPRED  The language restriction enables the forward scan join technique  a stateless zig zag join that advances both its inputs in a forward only manner  We discuss the complexity results for the PPRED algorithm in Section 6  The forward scan join may be used as a physical join operator in GRAFT queries  but only for very speci   c scoring schemes  Speci   cally  the scoring scheme must be constant  see Section 5 1  since the forward scan join may miss some matches  5   5 2 3 Novel Full Text Optimizations Alternate Elimination  For constant scoring schemes  alternate aggregation is unnecessary since the score of any match is the document score  In this case  group by operators used to aggregate scores may be replaced by an alternate elimination operator  A   AjB P     A when all aggregation functions in B are   The implementation of alternate elimination differs from group by in two crucial ways   1  it emits a new result match as soon as a new group is seen instead of waiting to see all group members  and  2  it signals its child operators to skip any further tuples in the group  Alternate elimination is similar to relational algebra   s duplicate elimination  except that matches  unlike tuples  are by definition never duplicates  Because matches behave like duplicates under a constant scoring scheme we can treat them as such  and in doing so we exploit the scoring scheme property  Pre Counting  Positions are not always used by a query  For example  consider a keyword k that is involved in no full text predicates  Unless the positions are needed by the scoring scheme  they may be eliminated by the rewrite A d  p  k     d A d  p  k    The projection creates duplicates  which may be eagerly counted by the rewrite  d A d  p  k      djc COUNT     d A d  p  k     In this case  the Match Factory  projection  and eager counting groupby can be replaced by a much more ef   cient Pre Counting Atomic Match Factory CA   djc COUNT     d A d  p  k      CA d  p  k  The novelty and ef   ciency of pre counting is at the physical level  instead of the term position index  CA scans a much smaller termdocument index  a logical subset of the term position index   Precounting yields signi   cant performance gains over eager counting  in Section 8 we report a query with twenty fold runtime speedup  Since it forgets positions  pre counting is valid only for nonpositional scoring schemes  5 2 4 Discussion of Optimizations Table 1 summarizes speci   cally which scoring scheme properties are required by each optimization discussed in this section  One positive feature of Table 1 is that there are no restrictions on classical optimizations  join reordering  selection pushing  zig zag joins  and eager counting   This fact is a direct consequence of our aggregation scoring model that decouples scoring from match computation as much as possible  As shown in Section 2  this is in contrast to state of the art systems that encapsulate score computation in join and selection operators  they must either give up these optimizations or score consistent generic ranking  776OPTIMIZATION OPERATOR REQ  DIRECTION REQ    elim    commutes 1 reordering   pushing zig zag 1 forward scan 1 constant alt  elim  constant eager agg    fully associative not row    rst eager count pre count non positional rank join 7 monotonic increasing diagonal rank union 6 monotonic increasing diagonal Table 1  Each optimization listed can be applied when the selected scoring scheme satis   es the operator and direction requirements listed in the same row  Entries in this table are limited to properties that determine the optimization   s correctness  An optimizer may consult other properties to assist optimization heuristics  e g  eager count is not generally not helpful for positional scoring schemes    d score   s   djx   x  y   y   djv   v  w   w   d x   x  y   y   d v   v  w   w   d z   z   djs   r  A    windows     d  w  A    emulator     d  v   d r  v7w7  x7y 6z   1 1DISTANCE x y 1    1WINDOW v w 50  CA    foss     d  z  A    free     d  x  A    software     d  y  Plan 8  An optimized evaluation plan for Q3 for a diagonal  non positional scoring scheme  Optimizations involving grouping and aggregation  show quite a few restrictions in Table 1  This is not a surprise  as   is inseparable from the group by operator  Four optimizations are described  in the middle row block of Table 1   each with different scoring scheme restrictions as well as different applicability within queries  Given their different applicability  these optimizations should be viewed as complementary  Example 7  Optimized Plan   Plan 8 is a plan for Q3 showing various optimizations  The PreCounting Atomic Match Factory has been chosen for the keyword    foss    since it is not involved in any predicates  Selections are pushed down into join predicates and joins are reordered  Group by operators are pushed down beneath joins  Finally  the sort operator has been eliminated    In general  GRAFT   s extensible design allows incorporating as optimizations various techniques from IR systems with rigid plan generation and or restricted query language  GRAFT broadens the applicability of these techniques to queries and scoring schemes such systems do not support  by identifying the query subplans that are score consistent with these techniques  6  COMPLEXITY The evaluation complexity of full text languages has been extensively studied  7   MCalc and MA have the same evaluation complexity as the similarly expressive FTC 7   LOGSPACE complete for the data and PSPACE complete for the expression size  This result should not be a surprise  First order relational calculus evaluation has the same data and expression complexities 31   Despite this complexity  optimizers have made SQL and relational algebra practical even for very large datasets  The relational framework of GRAFT opens similar optimization opportunities for MCalc  Lower complexity evaluation plans for restricted full text languages have also been studied  but the restrictions placed on scoring generality for these plans have not been studied  Here we consider two such plans from  7  focusing speci   cally on the scoring scheme restrictions implied by the plans  Languages in the class BOOL  no full text predicates   have an evaluation plan in O D   Q 2   7  where D is the number of documents in the library and Q the number of keywords in the query  This algorithm scans a term document index instead of a termposition index  Because term positions are not scanned  positional scoring schemes  16  25  cannot be used with this algorithm  The BOOL algorithm is simulated in GRAFT using the pre counting optimization with the same restriction  Languages in the class PPRED  discussed in Section 5 2 2  have an evaluation strategy that is O W   Q 2   7  where W is the number of words in the collection  This algorithm is guaranteed to    nd a match in a document if one exists 7   but will not necessarily    nd all matches 5   Because matches are missed  this algorithm is compatible only with scoring schemes that are constant and thus do not require all matches  The PPRED algorithm is simulated in GRAFT using forward scan joins with the same restriction  Clever evaluation techniques are crucial to low complexity plans for restricted languages  and for optimizing plans for expressive languages  but they must be applied carefully when using generic scoring  With some scoring schemes  the restricted language plans and similar optimizations become invalid  To illustrate how careful one must be  we point out that the paper which describes the ef   cient plans for BOOL and PPRED  7  also describes a scoring scheme  which we call Join Normalized in Section 7  that is not score consistent for either plan  7  STUDY OF REAL SCORING SCHEMES To validate the expressiveness of our Scoring Algebra  we considered scoring algorithms from the literature  7  13  16  20  22  25  27  28  29  34   From these  we identi   ed seven appropriate scoring schemes that capture all the scoring algorithms  We implemented all seven schemes in our prototype and analyzed the set of scoring relevant properties of the implementations  As shown in Table 2  even in our sample of scoring schemes there is signi   cant variance in these properties  We combined our list of optimization requirements  Table 1  with the scoring scheme property analysis  Table 2  and obtained a list of rewrite optimizations that each scoring scheme allows  Table 3   The schemes we implemented are as follows  AnySum    d  a  p    return Bm25 d  p  7 sL  sR    return sL   sR 6 sL  sR    return sL   sR   sL  sR    return sL   s    return s AnySum is a scoring scheme typical of keyword search systems that    nd a single match per document  and do not differentiate between different positions of a term  Thus all positions  including    for a keyword have the same term weight  and consequently all matches to a document have the same score  AnySum de   nes the score of a document as the same as the score of its matches  the number of matches in a document does not factor into score  The score of a match is the sum of the BM25 18   similar to t   df  measures of the term positions that together form the match  The scoring schemes we studied from Terrier 22   DFR models   Timber 34   as implemented for INEX  are instances of AnySum  Terrier also uses a similar scoring scheme for language model scoring where the score of a match is the product  vs sum  of the term position scores  777SumBest  Extends AnySum  overrides    d  a  p    if p is     return 0 0 else return Bm25 d  p    sL  sR    return max  sL  sR  SumBest is column    rst  initializes the score of non   positions to BM25 18  and the score of   to 0  It de   nes a column score as the maximum score in that column  and the document score as the sum of the column scores  Lucene  Lucene  27  is a respected open source keyword search engine with limited support for full text predicates  This consists of the PROXIMITY and the phrase predicate  and  in contrast to GRAFT  allows no    plug in    full text predicates  The scoring scheme used by Lucene goes slightly beyond the scoring model we present here  Speci   cally  for the proximity predicate  imperfect matches are allowed  whose scores re   ect the divergence from the proximity parameter  Because we feel that no evaluation of our GRAFT prototype is complete without a comparison to Lucene  and because we didn   t want to rule out proximity predicates in this comparison  we have implemented in our prototype an extension to capture this special matching behavior  We omit presentation of this extension because it is an ad hoc solution to a more general problem  fuzzy matching  Fuzzy matching for MCalc queries is beyond the scope of this paper  and an interesting follow up topic  Excluding the special handling of proximity predicates  the Lucene scoring scheme coincides with SumBest  Join Normalized Weighting    d  a  p    if p is     return h0 0  d occurrences a i let scr    T f Idf d  p  let size    p countInDoc return hscr  sizei 7 sL  sR    let scr    sL scr sR size   sR scr sL size let size    sL size   sR size return hscr  sizei 6 sL  sR    let size     sL size   sR size    sL size   sR size let src    8       sL scr 2 if sR scr   0 0 sR scr 2 if sL scr   0 0 sL scr 2 sR size   sR scr 2 sL size else return hscr  sizei   sL  sR    return hsL scr   sR scr  sR sizei   s    return s scr Join Normalized weighting implements the scoring from  7  as discussed in Section 2  and a similar scoring scheme from  20   When implemented in the GRAFT framework  the Join Normalized scoring scheme does not have access to the size of intermediate results  because our scoring model does not include an explicit API to this statistic   To overcome this  the scoring scheme maintains the desired statistic in the size    eld of the internal score structure  As detailed in Section 2 the intermediate result size changes under optimizations  To obtain a well de   ned score  we compute the size intermediate results would have in a canonical  score isolated plan  i e  the intermediate results are subtables of the match table   Event Model    d  a  p    if p is     return 0 0 else return Bm25 d  p  7 sL  sR    return sL   sR 6 sL  sR    return sL   sR   sL   sR    sL  sR    return sL   sR   sL   sR    s    return s The probabilistic event model found in  13  and  29  treats the initial term weights as probabilistic events  The score of a match is the conjunction and or disjunction of the term weights according to the scoring plan  using the standard inclusion exclusion principle under the independence assumption  Finally  a document score is a disjunction of the scores to all matches  Any Sum Join Mean Event BestSum Sum Best Lucene Normal Sum Model  MinDist directional col row row positional X2 X   associates left   commutes X X X X X X X   monotonic inc X X X X X   idempotent X X X X   multiplies X X X X X X X   constant X 7 associates 7 commutes X X X X X X X 7 monotonic inc X X X X X X 6 associates 6 commutes X X X X X X X 6 monotonic inc X X X X X X X Table 2  Optimization relevant properties of scoring schemes that we implemented in our prototype for our study  2 Lucene is positional only for queries with phrase or proximity predicates  Any Sum Join Mean Event BestSum Sum Best Lucene Normal Sum Model  MinDist   elim  X X X X X X X 1 reordering X X X X X X X   pushing X X X X X X X zig zag 1 X X X X X X X forward scan 1 X alt  elim  X eager agg  X X X X X eager count X X X X X X X pre count X X X X X X rank join X X X X rank union X X X X Table 3  By combining Table 1 and Table 2 we derive the set of optimizations that may be consistently applied for each scoring scheme  BestSum MinDist    d  a  p    if p is     return h0 0 1     i let scr    Bm25 d  p  let pos     p offset  let dist    MinDist pos  return hscr  dist  posi 7 sL  sR    let scr    sL scr   sR scr let pos    sL pos sR pos let dist    MinDist pos  return hscr  dist  posi 6 sL  sR    return sL 7 sR   sL  sR    let scr    max sL scr  sR scr  let dist    min sL dist  sR dist  return hscr  disti   s    return s scr   log 1   es dist   BestSum MinDist uses the MinDist proximity measure from  25   MinDist gives a high score to matches where two matching terms are very close  and a low score when no two matching terms are very close  MinDist over full text is interpreted as applying to individual matches  since the proximity of keywords that do not occur in the same match is irrelevant  BestSum MinDist computes the score of an individual match as the sum of the BM25 score of each term position in the match  multiplied by the MinDist metric  The score of a document is the score of its highest scoring match  MinDist concerns term position so BestSum MinDist is positional  All the proximity measures found in  25  and the scoring schemes from  28  16  are implemented similarly to BestSum MinDist  8  EXPERIMENTAL RESULTS We report on experiments showing both that  a  our novel optimizations effectively improve query performance beyond classical optimizations and  b  despite additional overhead from generic scoring  GRAFT performance frequently exceeds state of the art full text search systems that do not implement generic scoring  778   Our contributions regarding classical optimizations  join order  selection pushing  eager aggregation  eager count  zig zag and rankjoins  are in building a framework to  correctly  exploit them  not their development  We do not validate their potential here  Data  As a test dataset  we used a snapshot of the English Wikipedia from September 2010  26   We indexed the text from all articles  talk pages  disambiguation pages  and    gure detail documents  The index constitutes 2 4 billion words  12 million unique terms  distributed over 5 2 million documents  Queries  We give results with respect to eight queries formulated over Wikipedia  listed below in a shorthand syntax that is more concise than MCalc  Position variables are implicit  Keywords are conjuncted unless separated by a vertical bar  Quotes imply a PHRASE predicate  Other predicates are preceded by keyword arguments in parenthesis and followed by constant arguments in brackets  Q8 is the translation of MCalc query Q3 to this shorthand  Q4  san francisco fault line Q5  dinosaur species list  image   picture   drawing   illustration  Q6     orange county convention center    orlando Q7     san francisco       fault line    Q8   windows emulator WINDOW 50   foss      free software     Q9   free wireless internet PROXIMITY 10  service Q10  arizona      shing   hunting   rules   regulations  WINDOW 20  Q11     rick warren     obama inauguration PROXIMITY 4   controversy invocation PROXIMITY 15  Q4 and Q5 are simple boolean keyword queries  used as yardsticks to measure the overhead introduced by support for full text predicates  Q6 and Q7 have phrase predicates which  from the full text perspective  are syntactic sugar over a series of DISTANCE predicates  Q8 through Q11 have predicates typically only found in full text search systems  Besides Q8 our queries are non arti   cial   Q8 was constructed for the examples in earlier sections   Q4 and Q7 were chosen at random from a friend   s web search history  with permission   The rest are loosely based on topics from the TREC 2009 Web Track  10   Web track queries are simple keyword search  not full text search  so we rephrased topic descriptions into full text queries  Measurement Methodology  Each measurement was repeated nine times in succession  and we report the average of the    ve median times  This methodology was chosen to minimize the chance that a garbage collection or JIT event would occur during one measurement and not during another  All systems tested cache index entries in RAM  Measurements are all taken on a warm cache  no measured times include disk access  Platform  Experiments ran on a Phenom II 940 CPU  using the IcedTea6 1 8 1 JVM restricted to 4GB of RAM on Linux 2 6  Our GRAFT implementation is single threaded  Plans and Optimizer  Starting with a canonical plan     rst the selection pushing rewrite is applied iteratively until the plan converges  Then either the eager aggregation or eager counting rewrite  is applied similarly  Eager counting is used when the scoring scheme is constant  in this case eager counting always performs better  or if the scoring scheme does not support eager aggregation  Plans used in experiments are listed in  5  to ensure repeatability  We expect a cost based optimizer to outperform the heuristic optimization we used  Cost based optimization is beyond the scope of this work  Alternate Elimination  To measure the bene   t of alternate elimination  we started with plans optimized as described above  We used the AnySum scoring scheme  and replaced each group by opQ4 Q5 Q6 Q7 Q8 Q9 Q10 Q11 0  20  40  60  80  100      execution time reduction Alt Elim Pre Count Combined Figure 3  Execution time reduction provided by Alternate Elimination optimization  Pre Counting optimization  and a combination of both over the classical eager count optimization  erator </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores"/>
        <doc>Ef   cient In Memory Indexing with Generalized Pre   x Trees  ###  Matthias Boehm  Benjamin Schlegel  Peter Benjamin Volk  Ulrike Fischer  Dirk Habich  Wolfgang Lehner TU Dresden  Database Technology Group  Dresden  Germany Abstract  Ef   cient data structures for in memory indexing gain in importance due to  1  the exponentially increasing amount of data   2  the growing main memory capacity  and  3  the gap between main memory and CPU speed  In consequence  there are high performance demands for in memory data structures  Such index structures are used   with minor changes   as primary or secondary indices in almost every DBMS  Typically  tree based or hash based structures are used  while structures based on pre   x trees  tries  are neglected in this context  For tree based and hash based structures  the major disadvantages are inherently caused by the need for reorganization and key comparisons  In contrast  the major disadvantage of trie based structures in terms of high memory consumption  created and accessed nodes  could be improved  In this paper  we argue for reconsidering pre   x trees as in memory index structures and we present the generalized trie  which is a pre   x tree with variable pre   x length for indexing arbitrary data types of    xed or variable length  The variable pre   x length enables the adjustment of the trie height and its memory consumption  Further  we introduce concepts for reducing the number of created and accessed trie levels  This trie is order preserving and has deterministic trie paths for keys  and hence  it does not require any dynamic reorganization or key comparisons  Finally  the generalized trie yields improvements compared to existing in memory index structures  especially for skewed data  In conclusion  the generalized trie is applicable as general purpose in memory index structure in many different OLTP or hybrid  OLTP and OLAP  data management systems that require balanced read write performance  1  ###  Introduction Index structures are core components of typical data management systems  While this area has been studied for a long time  many aspects need to be reconsidered in the context of modern hardware architectures  Due to the increasing main memory capacity and the growing gap between CPU speed and main memory latency  MBK00   especially  inmemory indexing gains in importance  The speci   c characteristics of in memory indexing compared to disk based indexing are that  1  pointer intensive index structures with small node sizes can be preferred instead of page based structures due to smaller block granularities of main memory  and that  2  the number of required key transformations and comparisons as well as ef   cient main memory management and cache consciousness are crucial in   uencing factors on the overall performance  For update intensive in memory indexing in the context of online transaction processing  OLTP   typically  tree based or hash based techniques are used  while tries are usually neglected  All of those structures have their speci   c drawbacks  Tree based structures require re organization  tree balancing by rotation or node splitting merging  and many key comparisons compared to hash based or trie based techniques  Hash based techniques heavily rely on assumptions about the data distribution of keys and they require reorganization  rehashing  as well  Tries are typically only designed for string operations  they often require dynamic reorganization  pre   x splits   and they can cause higher memory consumption compared to tree  or hash based structures  The disadvantages of tree based and hashbased techniques are caused inherently by their structure  In contrast  the disadvantages of tries can be addressed appropriately  This optimization potential enables us to generalize existing trie structures in order to make them applicable for ef   cient in memory indexing  While tree based and hash based structures still have their application areas  we argue that a generalization of existing trie structures for in memory indexing can  in particular with regard to a balanced read write performance and for skewed data  achieve performance improvements compared to existing index structures  This trie generalization focuses on the goals of  1  trie indexing for arbitrary data types   2  order preserving key storage   3  deterministic trie paths  no need for dynamic reorganization  except leaf splits   and  4  ef   cient memory organization  Our primary contribution is the reconsideration and adaptation of pre   x trees for ef   cient in memory indexing  Furthermore  we make the following more concrete contributions  which also re   ect the structure of this paper    First of all  in Section 2  we survey array based  tree based  hash based and triebased index structures and discuss their main drawbacks    Then  in Section 3  we describe the generalization of pre   x trees  This includes the formal foundation and a physical realization of the generalized trie    Subsequently  in Section 4  we introduce the optimization techniques bypass jumper array and trie expansion  Furthermore  we provide insights into important read and write algorithms as well as additional memory optimization concepts    We present selected results of our experimental evaluation in Section 5    Finally  we conclude the paper and mention future work in Section 6  2 Analysis of Existing Solutions In order to give an overview of related index structures  we brie   y survey the main categories of in memory index structures but refer for more details to a comparison of basic in memory structures  LC86  and a detailed time analysis  which also includes the number of required key comparisons  LDW09   As the notation  we use the set of records R  where N denotes the number of records  with N   jRj  Each record ri 2 R exhibits the structure ri    ki    i   where ki denotes the key and  i denotes the associated information  payload   Furthermore  all records of R exhibit the same data type  where k with k   jki j denotes the length of all keys in terms of the number of bits  If data types of variable length are used  the key length denotes the maximum key length  For example  a VARCHAR 256  results in k   2 048 Essentially  index structures are distinguished into the categories   1  sorted arrays   2  trees   3  hash based structures  and  4  pre   x trees  tries   In the following  we survey existing work according to these categories  Sorted Arrays  The simplest index structure is an array of records  Unsorted arrays cause linear time complexity of O N   Hence  often sorted arrays are used in combination with binary search  Knu97   It is known that the worst case time complexity of binary search is O log N   We have recently presented a k ary search algorithm  SGL09  that uses SIMD instructions  yielding a signi   cant improvement over binary search  However  the worstcase complexity is still O log N   Although sorted arrays are advantageous for several application scenarios  they fall short on update intensive workloads due to the need for maintaining the sorted order of records  moving records   In case of partitions of many duplicates  the shuf   ing technique  IKM07   that moves as few as possible records of a partition  can minimize the costs for order maintenance  Another approach is to use sorted arrays with gaps according to a used    ll factor such that only few tuples might be moved but the space requirements are increased  While a linked list would allow for more ef   cient updates  it is not applicable for binary search  Tree Based Structures  Typically  tree based structures are used for ef   cient in memory indexing  These structures are distinguished into unbalanced and balanced trees  The binary tree  Knu97  is an unbalanced tree where a node has at most two children  This structure can degenerate  causing more nodes to be accessed than for a balanced tree  In contrast  especially for in memory indexing  balanced tree structures are used  There is a wide variety of existing structures such as B trees  BM72   B    trees  Jan95   red blacktrees  GS78   AVL trees  Knu97  and T trees  LC86   With the aim of tree balancing  rules for node splitting merging or tree rotation are used  Current research focuses on cacheconscious tree structures for T Trees  LSLC07  and B Trees  CGMV02  RR99  RR00  and on exploiting modern hardware like SIMD instructions  ZR02  or architecture aware tree indexing on CPUs and GPUs  KCS   10   All of those balanced tree structures exhibit a logarithmic time complexity of O log N  in terms of accessed nodes for all operations  Additionally  they require a total number of O log N  key comparisons  This is especially important when indexing arbitrary data types such as VARCHAR  Hash Based Structures  In contrast to tree based structures  hash based structures rely on a hash function to determine the slot of a key within the hash table  an array   Depending on the used hash function  this approach makes assumptions about the data distribution of keys and can be order preserving  For chained bucket hashing  Knu97   no reorganization is required because the size of the hash table is    xed  However  in the worst case  it degenerates to a linked list and thus  the worst case search time complexity is O N   In contrast to this  there are several techniques that rely on dynamic reorganization such as extendible hashing  FNPS79   linear hashing  Lit80  Lar88   and modi   ed linear hashing  LC86   Current research focuses on ef   ciently probing multiple hash buckets using SIMD instructions  Ros07   Due to reorganization  those structures exhibit a search time complexity of O 1   However  additional overhead for dynamic hash table extension and re hashing  worst case  O N   is required and thus it can be slower than tree based structures  In conclusion  hash based structures are advantageous for uniformly distributed keys rather than for skewed data Trie Based Structures  The basic concept of pre   x trees  tries   Fre60    also called digital search trees  Knu97    is to use parts of a key ki  with key length k  to determine the path within the trie  Pre   x trees are mainly used for string indexing  where each node holds an array of references according to the used character alphabet  HZW02  and therefore they were neglected for in memory indexing of arbitrary data types  LC86   With that concept  the worst case time complexity is O k  for all operations because  in general  the number of accessed nodes is independent of the number of records N  independent of the trie    ll factor   However  the maximum number of indexable records is N0   2 k   Compared to tree based structures  more nodes are accessed because k   log N  where k   log N only if N0   N  However  only partial key comparisons are required per node  where in the worst case  a single full key comparison is required in total for any operations  The most recognized trie structure is the radix tree  Knu97   where radix tree  patricia trie  Mor68  and crit bit tree  critical bit  are used as synonyms  Essentially  these structures maintain a tree of string pre   xes  Each node represents the string position  where the strings of the left and the right sub trie differ  Thus  a node has at least two children and each edge can encode multiple characters  Mor68   In contrast to traditional tries  using one character per trie level   this yields a signi   cant string compression  Typically  this structure is only used for string indexing and ef   cient string operations  This data type restriction was addressed with the extension Kart  key alteration radix tree   where bit positions  rather than single character positions  are used for difference encoding  This allows for indexing arbitrary data types but due to the encoding of different pre   x lengths within a trie  there is still the need for reorganization  arbitrary pre   x splits   In conclusion  the advantage of trie structures is the good time complexity of O k  with only few  partial  key comparisons  which is especially useful when indexing strings  Existing trie based structures are therefore mainly designed for those string operations rather than for indexing arbitrary data types  Hybrid Structures  Furthermore  there are several hybrid index structures such as pre     x hash trees  CE70   trie and hash   HAT tries  AS07   trie and hash   ternary search trees  BS97   trie and binary tree   pre   x B trees  BU77   trie and B tree   partial keys  BMR01   trie and B tree T tree   J    trees  LDW09   trie and B tree T tree   CS pre   xtrees  BHF09   trie and CSB tree   and Burst tries  HZW02   trie and arbitrary structure for containers   Interestingly  all of these here mentioned hybrid structures use to some extend trie based concepts  3 Generalized Pre   x Trees Due to the disadvantages of existing structures  we outline our so called generalized trie as a new in memory data structure  It is a generalization of pre   x trees  tries  for indexing arbitrary data types of    xed and variable length in the form of byte sequences  The novel characteristic compared to existing trie based structures is an assembly of known and some new techniques  In detail  we use  1  a pre   x size of variable length   2  a bypass structure for leading zeros   3  dynamic  pre   x based trie expansion  and  4  optimizations for pointer intensive in memory indexing  In this section  we focus on the formal foundation of this trie generalization  the core operational concepts  and still existing problems Level 16 0 1 Level 15 0 1 Level 15 0 1 Level 14 0 1 Level 1 0 1 Level 1 0 1 Level 14 0 1 Level 1 0 1 Level 14 0 1 Level 1 0 1  a  k 0   1  h   16  Level 4 0 1 2 15 Level 3 0 1 2 15 Level 2 0 1 2 15 Level 1 0 1 2 15 Level 1 0 1 2 15  b  k 0   4  h   4  Level 1 0 1 2 3 4 5 6 7 8 9 65535  c  k 0   16  h   1  Figure 1  Example Generalized Trie with k   16 3 1 Formal Foundation As the foundation of this work  we de   ne the generalized trie as follows  De   nition 1 Generalized Trie  The generalized trie is a pre   x tree with variable pre   x length of k 0 bits  We de   ne that  1  k 0   2 i   where i 2 Z     and  2  k 0 must be a divisor of the maximum key length k  The generalized trie is then de   ned as follows    Given an arbitrary data type of length k and a pre   x length k 0   the trie exhibits a    xed height h   k k 0     Each node of the trie includes an array of s   2 k 0 pointers  node size   The generalized trie is a non clustered index with h levels  The root node describes level h  while nodes of level 1 are leaf nodes and point to the data items  Thus  leaf nodes are identi   ed by the trie structure  The trie path for a given key ki at level l is determined with the  h  l  th pre   x of ki   The single nodes of the trie are only created if necessary  As a result  we have a deterministic pre   x tree that indexes distinct keys of arbitrary data types without the need for  1  multiple key comparisons or  2  reorganization  The pre   x length k 0 can be used to adjust the required space of single nodes and the number of accessed nodes for an operation  The following example shows the resulting spectrum  Example 1 Con   guration of Generalized Tries  Assume the data type SHORT 2  with a key length of k   16  On the one side  we can set k 0   1  where each node contains two pointers  bit set  bit not set   This would result in a structure similar to a binary tree with a trie height h   16  On the other side  we could set k 0   16  which results in a height h   1 node containing an array of 65 536 pointers to keys  In the latter case  the complete key determines the array index  Figure 1 illustrates this con   guration possibility using k 0   1  Figure 1 a   binary pre   x tree   k 0   4  Figure 1 b   well balanced hierarchical con   guration   and k 0   k   16  Figure 1 c   a sorted array with gaps   Clearly  when con   guring k 0   this is a trade off between the number of accessed nodes and space consumption  An increasing k 0 will cause decreased node utilization and thus  in  creased memory consumption  while the steps from the root to the leafs are reduced for all operations  Furthermore  the generalized trie has the following two important properties    Deterministic Property  Using a pre   x length of k 0   a single node contains s   2 k 0 pointers  Each key has then only one path within the trie that is independent of any other keys  Due to these deterministic trie paths  only a single key comparison and no dynamic reorganization are required for any operations    Worst Case Time Complexity  Based on the given key type of length k  the generalized trie has a time complexity of O h  and hence  has constant complexity in terms of the    ll factor of the trie  the number of tuples N   Due to the dynamic con   guration of k 0   we access at most h   k k 0 different nodes  where h however might be higher than log N   and compare at most O 1  keys for any operations  In contrast to the original trie  Fre60   we use a variable pre   x length  not single characters  and have a deterministic trie of    xed height  The    xed height allows the determination of leaves  If the root level is given by h   k k 0   leaves are only present at level 1  For data types of variable length such as VARCHAR  the trie height is set to the maximum key length rounded up to a factor of k 0   In order to ensure that leaves are only present at level 1  keys with length k   h   k 0 are logically padded with zeros at the end because padding at the beginning would lead to loosing the property of being order preserving  3 2 Physical Data Structure and Existing Problems Based on the formal de   nition of the generalized trie  we now explain the IXByte that is a physical data structure realizing such a trie generalization  We describe the main data structure and its most important operational concepts  The IXByte is designed as a hierarchical data structure in order to leverage the deterministic property of the generalized trie  A single index  L0Item  is created with the data type length as a parameter  In order to support also duplicates of keys  we use a further hierarchical structure of key partitions ki  L1Item  and payloads  i  L2Items   Then  the single keys are indexed as a generalized trie  while the payloads of all records associated with the same key are maintained as a list of L2Items within this L1Item partition  Furthermore  the node data structure used within the IXByte is de   ned as an array of s   2 k 0 void  pointers  We use generic void  pointers in order to refer to both inner nodes and L1Items  leaves   where the    xed maximum trie height h determines the leaves  The IXByte works on order preserving byte sequences  big endian  and therefore arbitrary data types can be supported as long as their keys can be transformed accordingly  Example 2 IXByte Operations  Assume an index of data type SHORT 2  with k   16 and a pre   x length of k 0   4  We insert the record ri    107  value3   where the resulting key parts are illustrated at the left side of Figure 2 and by emphasized pointers  We start at the root node  level 4  and use the value of bits 0 3 as array index in order to determine the child pointer  We repeat this with bits 4 7 on level 3  and with bits 8 11 on level 2  Finally  on level 1  we use the value of bits 12 15 as our array index  We know that thisLevel 1 Level 4 1 Example  Short k 16  k    4  h 4 INSERT key 107  payload    value3      key   107 0000 0000 0110 1011 path 0 0 6 11 0 2 15 Level 3 0 1 2 15 Level 3 0 1 2 15 Level 2 0 1 2 15 Level 1 0 1 2 15 0 1 2 15 Key 107 Key 96 P    value2    P    value3    Key 2 11 P    value4    P    value3    P    value2    6 Key 65409 Key 15 P    value7    P    value2    P    value8    Level 2 0 1 2 15 Level 1 0 1 2 15 8 Level 2 0 1 2 15 Level 1 0 1 2 11 15 Key 61451 P    value9    L2Items L1Items L0Item key partition payloads   of duplicates  Figure 2  Example IXByte for k   16  k 0   4  h   4 pointer must reference an L1Item  key partition   If no partition exists  we create a new one  otherwise  we just insert the payload into the partition  The deterministic property  where the trie path depends only on the key itself  implies that this structure is update friendly because no reorganizations  pre   x splits  are required  At the same time it also allows ef   cient lookups because no key comparisons are required  However  two typical problems arise when using such a trie structure  Problem 1 Trie Height  A problem of this general trie solution is the height of the trie  number of levels   For example  a VARCHAR 256  index with k 0   4 would result in a trie height h   512  In this scenario  keys with a variable key length k smaller than h   k 0 are logically padded with zeros at the end in order to ensure an order preserving deterministic structure  The advantage of tries   in the sense of constant time for all operations    might be a disadvantage because the constant number of nodes to be accessed is really high  512   This problem has two facets  namely the number of accessed trie nodes  operational  and the number of created nodes  memory consumption   Problem 2 Memory Consumption  Due to the    xed trie height  an inserted key can cause the full expansion  creation of trie nodes  of all levels of the trie if no pre   x can be reused  In the worst case  an insert causes the creation of h  1 nodes  Furthermore  each node requires a space of 2 k 0   size ptr  byte  For example  on a 64bit architecture  where a pointer ptr requires 8 byte  and with a pre   x length of k 0   4  the node size is 128 byte  In the following  we tackle these problems  The trie height is reduced by the techniquestrie expansion and bypass jumper array  There  trie expansion implicitly leads to decreased memory consumption as well  In addition  we also apply an explicit memory reduction by pointer reduction and memory alignment  structure compression   4 Selected Optimization Techniques and Algorithms In this section  we present selected optimization techniques that address the problems of the potentially large trie height and memory consumption  We also discuss selected algorithms  4 1 Bypass Jumper Array Approaches that reduce the large height of the trie  e g   h   512 for k   2048 and k 0   4  are required  while preserving the properties of the generalized trie  The core concept of the technique bypass jumper array is to bypass trie nodes for leading zeros of a key  In order to enable this  we  1  preallocate all direct 0 pointers  nodes where all parents are only referenced by 0 pointers  of the complete trie  and we  2  create a socalled jumper array of size h  where each cell points to one of those preallocated nodes  Finally  we  3  use the jumper array to bypass higher trie levels if possible  We show the concept of this bypass jumper array using our running example  Example 3 Bypass Jumper Array  Recall Example 2  For this index of height h   4  Figure 3   we  1  preallocate all four direct 0 pointers   2  create the jumper array c of size four  where cell cj corresponds to the trie level l with j   l  Then  we  3  use the jumper array to jump directly to level l   djki j k 0 e that is determined by counting leading zeros  clz   For example  when inserting the key ki   107  we can directly jump to the preallocated node at level 2  With regard to the applicability  we distinguish data types of    xed and variable length  First  for data types with variable length jki j  e g   VARCHAR   we change the index layout  Keys with a length smaller than h  k 0 are now logically padded with zeros at the beginning and thus  we loose the property of being order preserving  However  if this is acceptable the bene   t is signi   cant  We assume that the key length jki j is uniformly distributed in the interval  1  k   As a result  the number of accessed trie nodes is reduced from h to h 2 in expectation  This causes signi   cant performance improvements due to fewer accessed nodes  logical partitioning into h subtries but with pre   x sharing   However  the time complexity is still O h   Second  this technique also works for    xed length data types such as INT  by counting leading zeros and without any index layout modi   cation  Unfortunately  for uniformly distributed keys  the impact is lower because the probability of having a key length jki j   k depends on k 0 with P djki j k 0 e   k k 0     1 2 k 0   For example  if k 0   4  only 1 16 of all keys would bene   t at all from this technique  The probability of a speci   c length is given by P  djki j k 0 e   k k 0  x     1 2 k 0   x   where x denotes the number of nodes that can be bypassed  However  numerical values are typically non uniformly distributed and rather small  e g   key sequences  and hence  they could also bene   t from this technique  In the description of algorithms  we will refer to this technique as bypass top levels  4 2 Trie Expansion Another reason for the huge trie height is that the index has a    xed height according to its data type and thus  each record is stored on level 1  This is also a reason for the huge memory consumption  because a single record can cause the creation of h  1 new nodes  Based on this problem  we investigated the dynamic trie expansion  The core concept is to defer the access to and creation of trie nodes during insert operations until it is required  Level 1 Level 4 1 bypass jumper array 0 2 15 Level 3 0 1 2 15 Level 3 0 1 2 15 Level 2 0 1 2 15 Level 1 0 1 2 15 0 1 2 15 Key 107 Key 96 P    value2    P    value3    Key 2 11 P    value4    P    value3    P    value2    6 Key 65409 Key 15 Key 61451 P    value9    P    value7    P    value2      not expanded P    value8    Example  Short k 16  k    4  h 4 INSERT key 107  payload    value3      key   107 0000 0000 0110 1011 path 0 0 6 11 1 2 3 clz   k      2 Figure 3  Example IXByte for k   16  k 0   4  h   4 with bypass jumper array and trie expansion with respect to the deterministic property  In other words  a tuple can now be referenced on any level rather than only at the leaf nodes  It is possible to reference a record instead of an inner node if and only if there is only one record that exhibits a particular pre   x  that is given by the position in the trie   A similar concept has already been used within the original algorithm  Fre60  for string indexing  For data types of variable length  e g   VARCHAR   the logical padding with zeros at the end  or beginning  respectively  ensures the deterministic property even in case of trie expansion  We use the following example to illustrate the idea of trie expansion  Example 4 Trie Expansion  Recall our running example of a SHORT 2  index with h   4  Figure 3 includes an example of the trie expansion  Consider the keys k1   61 451 and k2   65 409  If we insert k2   65 409  we can refer to it on level 4 instead of on level 1 because so far  no other key with a    rst pre   x of 11112 exists  Further  if we insert k1   61 451  we need to expand the third level for this sub trie because k1 and k2 both share the same pre   x 11112  However  we do not need to expand any further because the second pre   x of both keys  00002 and 11112  differs  This still ensures the deterministic property because we only expand the deterministic trie path of keys if it is required  In contrast to the bypass jumper array  this technique does not only bypass existing nodes but in   uences the number of created nodes  While the dynamic trie expansion signi   cantly reduces the trie height and therefore also the memory consumption for sparsely populated subtries  it requires changes of the trie node data structure  A node is now de   ned as an array of s   2 k 0 void  pointers and an array of ds 8e bytes to signal the expansion of a sub trie  The i th bit of this byte array determines if the i th pointer references an inner node  otherwise  the pointer directly references a record or is NULL  This is required due to the generic pointers  However  the evaluation of this    ag is a simple bit mask operation  In the algorithmic description  we refer to this as isExpanded  While so far  we have required 2 k 0   size ptr  byte for a node  now a single node has a size of size node    2 k 0   size ptr    d2 k 0  8e   2 k 0   size ptr    d2 k 03 e  As an example  for k 0   4 without trie expansion  the node size was 128 byte  enabling trie expansion adds two more bytes to the node  The downside of this technique are costs for splitting and merging of nodes  but still no inner pre   x splits  as well as an unaligned data structure with the drawback that the node size is no longer a factor or divisor of the cache line size  However  due to the signi   cant reduction of created nodes  the total memory consumption is signi   cantly reduced  which improves the performance for all operations  4 3 Memory Optimization The solution presented so far still has the drawbacks of  1  creating many small data structures  many malloc calls    2  a fairly high memory consumption  and  3  an unaligned data structure  expanded    ags   Hence  we apply several memory optimization techniques  First  we use memory preallocation in order to address the many small data structures  Each instance of an index includes a byte array of con   gurable size that is created using virtual memory upon index creation  We hold a pointer to this array  mem ptr  and the allocation position pos as well as we use lists of free objects for reusing memory  We bene   t because the operating system only maps physical pages of this array when they are accessed  Second  we reduce the memory consumption of trie nodes with the concept of reduced pointers  which store only the offset within the preallocated memory instead of the pointer to a certain memory position itself  The real pointer is then computed by the reference to the full array  mem ptr  plus the given offset  reduced pointer   which reduces the required size of a pointer  for example  for a preallocated memory of less than 2 GB   from 8 byte to 4 byte on 64bit architectures  Third  cache consciousness has signi   cant in   uence on the overall performance  We align the trie node size to factors of the cache line size  Based on the concept of reduced pointers  the idea is to use the    rst bit of such a pointer as internal    ag to indicate whether or not this pointer is an expanded node  Following this  we do not need any additional    ags per node  As a result  the    nal structure of our trie node is de   ned as an array of s   2 k 0 uint reduced pointers resulting in a node size of size node    2 k 0   size rptr    2 k 0   4  For example  using k 0   4  we get a node size of 64 byte  which is equal to the cache line size of modern processors  4 4 Algorithms So far  we have mainly discussed structural properties of the IXByte  now  we focus on the operational aspects  The main operations are get key   point query   getMin    getNext key1   scan   insert key payload   and delete key payload   Updates are represented by a delete insert pair  It is worth mentioning that keys of arbitrary data types are converted into byte arrays such that all operations are only implemented once according to this byte representation  All algorithms to search and modify generalized tries include the optimization approaches bypass jumper array and trie expansion  We use the operations get and insert as illustrative example algorithms  Algorithm 1 shows the get algorithm  A single index instance ix includes the root trie node and the level of this root node  given by h   k k 0    First  the bypass jumper array is used to jump directly to the trie node of level djki j k 0 e if required  line 2   Here  level and node are passed by reference and set accordingly  Then  the get algorithm mainly comprises a while loop  lines 3 10   For each iteration  we go one level down the trie Algorithm 1 get  non recursive  Require  index ix  key ki  key length jkij 1  node   ix trie  level   ix level 2  bypass top levels node level jkij ix  3  while  level   level  1    0 do 4  pos   computePosition level ki jkij  5  if   isExpanded node pos  then 6  if node ptr pos  key   ki then 7  return node ptr pos  8  else 9  return NULL    null if no key or different key 10  node   node ptr pos     go one level down Algorithm 2 insert  non recursive  Require  index ix  key ki  key length jkij 1  node   ix trie  level   ix level 2  bypass top levels node level jkij ix  3  while  level   level  1    0 do 4  pos   computePosition level ki jkij  5  if   isExpanded node pos  then 6  if node ptr pos   6 NULL then 7  entry   node ptr pos  8  if entry key   ki then 9  return entry 10  while true do 11  tmp  createNode      node pre   x splitting 12  node ptr pos1    tmp 13  setExpanded node pos  14  node   tmp  level   level  1 15  pos1   computePosition level entry ki jkij  16  pos2   computePosition level ki jkij  17  if pos1  6 pos2 then 18  node ptr pos1    entry 19  return node ptr pos2   createL1 ki jkij  20  else 21  pos   pos1 22  else 23  return node ptr pos   createL1 ki jkij  24  node   node ptr pos     go one level down where the loop terminates if level   0  For each iteration  we    rst compute the position within the pointer array  computePosition  line 4   It is determined by the  h  l  th pre   x of key ki   If the pointer speci   ed by pos is not expanded  see trie expansion   there must be a reference to a key  or the pointer is NULL  Hence  by checking for the right key  line 6   we could simply return this key partition or NULL  Otherwise  the pointer is expanded   there must be a reference to another trie node     Similar to the get operation  Algorithm 2 uses the same core concept for the insert operation  After we have jumped to the lowest possible trie node in case it is applicable  line 2   the algorithm comprises a main while loop  lines 3 24   The structure of this loop is similar to the get operation  However  we maintain the dynamic trie expansion of sub tries  If a pointer is not expanded and not NULL  we have reached a record  In case the keys are equivalent  we simply return the existing entry  Otherwise  we use an inner while loop  lines 10 21  to expand the sub trie as long as it is required  leaf node splitting   Basically  we can stop splitting such nodes if we reach a level where the pre   xes of both keys are different  lines 17 19   If the current pointer is already expanded  we follow this pointer and go one level down  line 24   Note that this splitting does not require any dynamic reorganization because we just go down the trie as long as it is required  Both algorithms use the computePosition multiple times in order to compute the current pre   x at each trie level  If many levels of the trie are expanded  it is advantageous to pre compute all positions in advance in one run over the key  The other algorithms work similar to the presented ones  For getMin  getNext  and delete  we additionally maintain a stack of parent nodes and current positions on each level in order to realize non recursive algorithms  The getNext searches for the current key  and starting from this position  it returns the minimum key that is greater than this  Further  the delete searches a speci   c record and then deletes the given  key value  pair and recursively collapse trie nodes if required  The index can also store NULL keys  which is required if used as a secondary index  Therefore  an additional single key partition  L1Item  is referenced by the index instance  Furthermore  all algorithms include related NULL checks at the beginning  4 5 Discussion of Related Work The closest work compared with our approach is the Pre   x Hash Tree  CE70   which was de   ned by Coffmann and Eve from a theoretical perspective  They use a hash function hj to map the key ki into a hash code bi of    xed length j  Thereafter  the pre   x hash tree uses pre   x based tree nodes that contain s   2 k 0 child references  In the case of the simplest hash function of bi   hk ki    ki   this is comparable to the generalized trie  In contrast to this  we presented the generalized trie that does not rely on any hash function  i e   it does not require related indirections for over   ow lists of different keys and it can index keys of variable length  Furthermore  we explained the IXByte as a physical realization of the generalized trie including optimizations for the large trie height and memory consumption as well as the description of ef   cient algorithms using this data structure  5 Experimental Evaluation In this section  we present selected experimental results concerning the performance  scalability  memory consumption  and the comparison with existing index structures  We used synthetically generated data  a real data set as well as the MIT main memory indexingbenchmark  Rei09   In general  the evaluation shows the following results    Data Types  The IXByte achieves high performance for arbitrary data types  where an increasing key length causes only moderate overhead    Skew Awareness  It turns out that the IXByte is particularly appropriate for skewed data  sequence  real  because many keys share equal pre   xes  while uniform data represents the worst case    Pre   x Length  A variable pre   x length  speci   c to our IXByte  of k 0   4 turned out to be most ef   cient and robust due to  1  a node size equal to the cacheline size  64 byte  and  2  a good trade off between trie height and node utilization    Comparison  The IXByte shows improvements compared with a B    tree and a T tree on different data sets  Most importantly  it exhibits linear scaling on skewed data  Even on uniform data  worst case  it is comparable to a hash map  As a result  the IXByte can be used as a general purpose data structure because  especially for skewed in memory data  it is more ef   cient than existing tree based or hashbased solutions  The complete C source code  including all experiments  is available at wwwdb inf tu dresden de dexter  Our core data structure IXByte is part of an overall index server  which is able to maintain an arbitrary number of indices  of different data types  in a multi threaded fashion  Essentially  it implements the API  de   ned by the MIT main memory benchmark  Rei09  including memory management  synchronization and transaction logging  transient UNDO log   All reported results were measured from outside the index server   except the comparison with B    trees  5 1 Experimental Setting As our test environment  we used a machine with a quad core Intel Core i7 920 processor  2 67GHz  and hyper threading  two threads per core   It uses Fedora Core 14  64bit  as operating system and 6GB of RAM are available  Furthermore  we used the GCC compiler with the following    ags   O3  fPIC  lpthread  combine  While the proposed IXByte is able to index arbitrary data types  here  we used only the types SHORT  4 byte   INT  8 byte  and VARCHAR 128   128 byte   In order to evaluate the in   uence of skew and other data properties we used the following data sets    Key Sequence  best case synthetic data   We generated a key sequence  highest skew  of N records with values  1  N  in sorted order  For the data type VARCHAR  we convert this value into a string representation    Uniform Data Distribution  worst case synthetic data   In opposite to the key sequence  we additionally generated N records with uniformly distributed keys with values  1  2 k   in unsorted order  For VARCHAR 128   we    rst determined a random length in the interval  1  128   For each position  we then picked a random character out of the alphabet of 52 printable characters    Real Data  Aside from the synthetically generated data  we also used the DBLP data set  Ley09  as a real data set  In more detail  we indexed all 2 311 462 distinct key attributes of this data set as a VARCHAR 128  index instance We evaluated the different operations insert  get  getNext  scan   and delete  Further  we varied the pre   x length with k 0 2 f1  2  4  8g  In addition  we compared our generalized pre   x trie against other index structures  We used an unoptimized B    tree  Avi09  and optimized it  memory optimizations  similar to our index structure in order to ensure a fair comparison  Furthermore  we used the MIT benchmark as well as the optimized hash map and T tree implementations from the winner  Clement Genzmer  and another    nalist  Cagri Balkesen  of the SIGMOD Programming Contest 2009  5 2 Synthetically Generated Data The    rst set of experiments uses the synthetically generated data sets sequence and uniform  We    xed a pre   x length of k 0   4 as well as enabled trie expansion and the described memory optimizations  Then  we measured the execution time for the mentioned operations  Figure 4 shows the results of this set of experiments  Experiment S 1  The    rst experiment uses the sequence data set  The goal was to evaluate the performance with an increasing number of records N  For each database size  this comprises N inserts  N point queries  get   a scan of N elements  getNext  as well as N deletes  where we used all keys of the generated sequence  We repeated this experiment for the three different data types  Figure 4     rst row  shows the measured results  First  SHORT 4  shows the best performance due to the lowest trie height  h   8   Note that for sequences  the complete trie height is expanded  We further observe that INT 8   h   16  shows only a minor overhead  while for VARCHAR  h   256   this overhead is higher  Those differences are caused by the trie height and additional memory requirements for the larger keys  From an operation type perspective  delete always has the worst performance  followed by insert  which is reasoned by memory management in addition to the search operations  Further  getNext is typically slower than get  because for a getNext  a similar point query is used in order to    nd the old key  and from there  it    nds the next key  which was required by scans with concurrent updates  Interestingly  getNext is better than get for VARCHAR indices  which was reasoned by cache displacement on get due to the huge height for the VARCHAR indices  Furthermore  get  insert  and delete require additional overhead for key generation and copying of keys into the local memory of our index server  The technique bypass jumper array led to an improvement  not included in the    gures  between 9  and 17  for the different operations  However  and most importantly  all operations show  1  a linear scaling according to the number of tuples  constant time for a single operation  due to the    xed number of accessed nodes and  2  a moderate overhead according to the key size  Experiment S 2  For a second experiment  we repeated Experiment S 1 with the uniform data set but exactly the same con   guration  Figure 4  second row  illustrates the measured results of this experiment  We observe similar characteristics   with some exceptions    compared to the sequence data set  In general  the index is notably slower for uniform key distributions  This has different reasons  Recall that uniform data is the worst case regarding space requirements  while this is the best case with respect to the average number of accessed nodes per operation  The costs for allocating loading more memory are higher than the bene   t reached by the lower number of trie nodes  Due to uniformly generatedSHORT 4  INT 8  VARCHAR 128  Sequence Uniform Figure 4  Basic Performance Results keys  each single operation  except the scan operation  accesses different nodes  As a result of uniform access and the higher memory consumption  many nodes are emitted faster from the cache  Another difference of uniform keys to key sequences is that the execution time increases slightly super linear with an increasing number of tuples  logarithmic time for a single operation   This effect is caused by the logarithmically increasing number of accessed nodes with an increasing number of tuples  The second difference concerns the scan performance  where the getNext is faster than get for all data types  Essentially  this is a caching effect because the scan is the only operation that accesses the records in sequential order  Hence  the higher trie nodes are cached across multiple operations  The technique bypass jumper array lead to an improvement  not shown in the    gures  between 4  and 7  for the different operations  which is lower than in experiment S 1 because the trie does not use the full height for uniform data and thus fewer nodes are bypassed  As a result  our index shows good performance and an almost linear scaling for sequences  best case  and uniform data  worst case   However  it is best suited for skewed data  5 3 Real DBLP Data The sequence and uniform data sets are extremes in terms of skew and thus give upper and lower bounds for the number of accessed nodes as well as required space  We now additionally use the real world DBLP data set that lies within this spectrum  Experiment R 1  As a preprocessing step  we extracted all distinct publication keys  which are concatenations of the type of publication  the conference journal name  and the short bibtex key  e g   conf sigmod LarsonLZZ07   We used a VARCHAR 128  index and truncated all keys that exceeded this key length  only a few   The number of distinct items was 2 311 462  Then  we inserted this data set  in sorted order  and evaluated a  Time VARCHAR  sorted   b  Space VARCHAR  deterministic   c  Time VARCHAR  unsorted  Figure 5  Performance on Real Data Set  DBLP  execution time and space requirements  Figures 5 a  and 5 b  show the results  We observe an almost linear scalability with increasing data size  similar to our other experiments  However  in contrast to sequences and uniform keys  there are two main differences  First  the getNext operation shows the worst performance compared to all other operations  while in the other experiments  it was usually faster than insert and delete  because many pre   xes are shared such that insert and delete become more ef   cient  Similar to this  get is also slower than in other cases compared to the insert delete functions  This effect is caused by long keys  where we need to access many nodes for each point query  Note that due to transactional requirements each getNext also includes such a point query  which reasoned that getNext was the slowest operation  Furthermore  we see three main parts  Up to 700 000 records  there is a linear scaling  Then  from 700 000 to 1 600 000 records  we see a slower increase  From there to the end  we observe a scaling similar to the    rst part  This is caused by the huge number of conferences that all share the same pre   x  e g   conf sigmod    which results in good compression  It is also worth to note that the performance is strongly correlated to the required memory  Experiment R 2  As a comparison  we repeated the experiment with unsorted keys  see Figure 5 c    Due to the deterministic property  the total memory consumption was equivalent to the previous experiment  In contrast to sorted keys  we observe that the performance was much lower due to cache displacement because created nodes are distributed over the memory  The getNext operation now performs best because it is the only one with a sequential access pattern with regard to the trie nodes  Most important  we still observe a linear scaling due to the good compression  As a result  the trie works also well for real world data  We conclude that the higher the skew  the higher the performance because we require less memory  5 4 Comparison with Existing Structures For comparison with existing structures  we used a general purpose B    tree  We optimized the implementation with memory preallocation in order to achieve a fair comparison  For the IXByte  we used a pre   x length of k 0   4  while we con   gured the B    Tree as a tree of order four  pointers per node  Knu97   because experiments with different con   gurations showed that this leads to highest performance for this experimental setting Experiment C 1  Due to Sequence Uniform Figure 6  SHORT 4  Comparison IXByte and B    Tree the data type restriction of the used B    tree  we used SHORT 4  only  We varied the number of tuples N and measured the execution time for insert and get as well as the memory consumption  The delete operation of the B    tree was similar to the insert and a scan operation was not supported  We    rst used the sequence data set  In contrast to our other experiments  we measured the time for the operations of the core index structures rather than from outside the index server because the B    tree does not implement the MIT main memory benchmark API  Figure 6  left column  shows the results  We yield an execution time improvement of up to factor 5 due to key comparisons and node splitting merging within the B    tree  Interestingly  the IXByte also requires thre</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores"/>
        <doc>Dictionary based Order preserving String Compression for Main Memory Column Stores  ###  Carsten Binnig ETH Zurich binnigc inf ethz ch Stefan Hildenbrand ETH Zurich stefanhi inf ethz ch Franz F  rber SAP AG franz faerber sap com ABSTRACT Column oriented database systems  19  23  perform better than traditional row oriented database systems on analytical workloads such as those found in decision support and business intelligence applications  Moreover  recent work  1  24  has shown that lightweight compression schemes signi   cantly improve the query processing performance of these systems  One such a lightweight compression scheme is to use a dictionary in order to replace long  variable length  values of a certain domain with shorter     xedlength  integer codes  In order to further improve expensive query operations such as sorting and searching  column stores often use order preserving compression schemes  In contrast to the existing work  in this paper we argue that orderpreserving dictionary compression does not only pay off for attributes with a small    xed domain size but also for long string attributes with a large domain size which might change over time  Consequently  we introduce new data structures that ef   ciently support an order preserving dictionary compression for  variablelength  string attributes with a large domain size that is likely to change over time  The main idea is that we model a dictionary as a table that speci   es a mapping from string values to arbitrary integer codes  and vice versa  and we introduce a novel indexing approach that provides ef   cient access paths to such a dictionary while compressing the index data  Our experiments show that our data structures are as fast as  or in some cases even faster than  other stateof the art data structures for dictionaries while being less memory intensive  Categories and Subject Descriptors H 2 7  Database Management   Database Administration   Data dictionary directory  E 4  Data   Coding and Information Theory   Data compaction and compression  H 3 1  Information Storage and Retrieval   Content Analysis and Indexing   Dictionaries  Indexing methods General Terms Algorithms  Design  Performance Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  1  ###  INTRODUCTION Column oriented database systems  such as Monet DB  23  and C Store  19   perform better than traditional row oriented database systems on analytical workloads such as those found in decision support and business intelligence applications  Recent work  1  24  has shown that lightweight compression schemes for columnoriented database systems  called column stores further on  enable query processing on top of compressed data and thus lead to signi   cant improvements of the query processing performance  Dictionary encoding is such a light weight compression scheme that replaces long  variable length  values of a certain domain with shorter     xed length  integer codes  1   In order to compress the data of a certain column that is loaded into a data warehouse using such a compression scheme  existing column stores usually create an array of distinct values  i e   the dictionary  and then store each attribute value of that column as an index into that array  Dictionaries are usually used in column stores if the size of the corresponding domain is small  Bit packing is then used on top of dictionaries to further compress the data  12   This compression scheme calculates the minimal number of bits that are necessary to represent the maximal index into the dictionary  Bit packing makes sense if the size of the domain is stable  or known a priori   However  in many practical data warehousing scenarios the domain size is not stable  As an example  think of a cube inside a data warehouse of a big supermarket chain which holds the sales of all products per category  e g   whole milk  low fat milk  fat free milk    While the total number of categories is not too large  it is likely that the categories will change over time  i e   new products are added to the selection of the supermarket   In order to deal with situations where the domain size is not known a priori  existing column stores usually analyze the    rst bulk of data that is loaded in order to    nd out the current domain size of a certain attribute  e g   the total number of product categories  and then derive the minimal number of bits  for bit packing   However  if subsequent bulks of data contain new values that were not loaded previously  existing column stores usually have to decode all the previously loaded data  e g   the data stored inside the sales cube  and then encode that data again together with the new bulk using more bits to represent the new domain size  This situation becomes even worse if different attributes  that are not known a priori  share the same global dictionary to enable join processing or union operations directly on top of the encoded data  In addition  column stores often use order preserving compression schemes to further improve expensive query operations such as sorting and searching because these operations can then be executed directly on the encoded data  However  order preserving compression schemes either generate variable length codes  e g   283                Whole Milk   Quart 32100 Whole Milk   Gallon 32000 value code Whole Milk   Half Gallon 32050                 Whole Milk     Quart 32100 Whole Milk     Gallon 32000 value code String dictionary  order preserving  Product data  non encoded  Product column  encoded          Whole Milk   Quart Whole Milk   Gallon p name 499 1 rid         1 32000 32100 p name 499 rid Whole Milk   Half Gallon     Whole Milk   Gallon p name     999 500 rid 999 32050     499 32100         32000 p name 500 rid     first bulk     second bulk  a  Data Loading  2 bulks  Query  original   Query  rewritten   Select SUM o total   p name From Sales  Products Where p name  Whole Milk   Group by p name Select SUM o total   p name From Sales  Products Where p name     32000 And p name     32100 Group by p name  b  Query Compilation Whole Milk   Half Gallon 32050                 Whole Milk   Quart 32100 Whole Milk   Gallon 32000 value code String Dictionary  order preserving   Query Result  non encoded   Query Result  encoded   Whole Milk   Quart Whole Milk   Half Gallon Whole Milk   Gallon p name 32050 32000 32100 p name  c  Query Execution Figure 1  Dictionary based order preserving string compression  2   that are known to be more expensive for query processing in column stores than    xed length codes  12   or they generate    xedlength codes  e g   by using indexes in a sorted array  that are more dif   cult to extend when new values should be encoded in an orderpreserving way  In contrast to the existing work  in this paper we argue that orderpreserving dictionary compression does not only pay off for attributes with small domain sizes but also for long string attributes with a large domain size  For example  the sales cube mentioned before could contain product names of type VARCHAR 100   If we encode one million different product names using a dictionary that generates    xed length integer codes  e g   32 bit   we would get a very good average compression rate for that column  However  using a sorted array and indexes into that array as    xed length integer codes is too expensive for large dictionaries where the domain size is not known a priori  There are different reasons for this  First  using a sorted array and binary search as the only access path for encoding data is not ef   cient for large string dictionaries  Second  if the index into the sorted array is used as integer code  each time a new bulk of string data is loaded it is likely that the complete dictionary has to be rebuilt to generate orderpreserving codes and all attributes that use that dictionary have to be re encoded  For the same reasons  strict bit packing on top of an order preserving dictionary compression scheme does not make sense either  Motivated by these considerations  this paper introduces data structures that ef   ciently support an order preserving dictionarycompression of  variable length  string attributes where the domain size is not known a priori  e g   when the dictionary is shared by different attributes   Furthermore  the integer codes that are generated have a    xed length to leverage ef   cient query processing techniques in column stores and we do not use bit packing on top of these integer codes to ef   ciently be able to support updates  Consequently  in this paper we model the dictionary as a table that speci   es a mapping from string values to arbitrary integer codes and vice versa  Our goal is also to provide ef   cient access paths  i e   index structures  to such a dictionary  More precisely  we identify index structures for a string dictionary that ef   ciently support the following tasks      Data loading  As discussed before  data is usually loaded bulk wise into a data warehouse  This means that the dictionary must ef   ciently support the encoding of bulks of string values using integer codes  The encoding logically consists of two operations  the    rst operation is a bulk lookup of the integer codes for the string values that are already a part of the dictionary and the second operation is the bulk insertion of the new string values as well as the generation of orderpreserving integer codes for those new values  As an example  in Figure 1  a  we see how two bulks of product data  i e   the column p name  are loaded into the sales cube      Query Compilation  In order to execute analytical queries directly on top of encoded data  it is necessary to rewrite the query predicates  If an order preserving encoding scheme is used  this step is trivial  The string constants of equalityand range predicates only have to be replaced by the corresponding integer codes  Moreover  pre   x predicates  e g   p name    Whole Milk      can be mapped to range predicates 1   Consequently  a string dictionary should enable ef     cient lookups to rewrite string constants as well as string pre   xes  As an example  in Figure 1  b   we see how the predicate of a query is rewritten      Query Execution  During query execution  the    nal query result  and sometimes intermediate query results  must be decoded using the dictionary  which can be seen as a semijoin of the encoded result with the dictionary   As most column stores use vectorized query operations  or sometimes even materialize intermediate query results   12  24   a stringdictionary should also support the ef   cient decoding of the query results for bulks  i e   bulk lookups of string values for a given list of integer codes   As an example  in Figure 1  c   we see how the dictionary is used to decode the column p name of the encoded query result  While all the tasks above are time critical in today   s data warehouses  query processing is the most time critical one  Consequently  the data structures that we present in this paper should be fast for encoding but the main optimization goal is the performance of decoding integer codes during query execution  In that respect  in this paper we identify ef   cient  cache conscious  indexes that support the encoding and decoding of string values using a dictionary  While there has already been a lot of work to optimize index structures for data warehouses on modern hardware platforms  i e   multi core systems with different cache levels   much of this work concentrated on cache conscious indexes for numerical data  e g   the CSS Tree  16  and the CSB    Tree  17    However  there has been almost no work on indexes that enable  cache  ef   cient bulk lookups and insertions of string values  Consequently  we focus on indexes for encoding string data  1 In this paper we do not support wildcards at arbitrary positions  284In addition  the amount of main memory as well as the size of the CPU caches of today   s hardware are constantly growing  Consequently  data warehouses start to hold all the critical data in main memory  e g   the data that is used for online analytical reporting   In that respect  in this paper we address the question of how to compress the dictionary in order to keep as much data as possible in the different levels of the memory hierarchy  Again  while there has been a lot of work on compressing indexes for numerical data  10   almost no work exists for string data  5   Thus  the contributions of this paper are   1  In Section 2  we introduce a new approach for indexing a dictionary of string values  called shared leaves  that leverages an order preserving encoding scheme ef   ciently  In the shared leaves approach  indexes on different attributes  that can be clustered the same way  can share the same leaves in order to reduce the memory consumption while still providing ef   cient access paths   2  In Section 3  we introduce a concrete leaf structure for the shared leaves approach that can be used by the indexes of a dictionary for ef   ciently encoding and decoding string values while the leaf structure itself is compressed  We also discuss the most important operations on this leaf structure  i e   lookup and update  and analyze their costs   3  As another contribution  in Section 4  we present two new cache conscious string indexes that can be used on top of our leaf structure to ef   ciently support the encoding of string data in the dictionary  For the decoding of integer codes we argue why the CSS Tree  16  is optimal in our case   4  Finally  in Section 5  our experiments evaluate the new leaf structure and the new cache conscious indexes under different types of workloads and show a detailed analysis of their performance and their memory behavior  As one result  the experiments show that in terms of performance our leaf structure is as ef   cient as other read optimized indexes while using less memory  due to compression   2  OVERVIEW In this section  we    rst discuss the operations that an orderpreserving string dictionary must support  Afterwards  we present a new idea for indexing such a dictionary  called shared leaves  which is not bound to particular index structures and we show how the operations above can be implemented using this approach  Finally  we discuss requirements and design decisions for index structures that can be ef   ciently used for a dictionary together with the shared leaves approach  2 1 Dictionary Operations As mentioned in Section 1  in this paper we model a string dictionary as a table T with two attributes  T    value  code   Thus  table T de   nes a mapping of variable length string values  de   ned by the attribute value  to    xed length integer codes  de   ned by the attribute code  and vice versa  In order to support the data loading as well as the query processing task inside a column store  the interface of the dictionary should support the following two bulk operations for encoding and decoding string values      encode  values     codes  This bulk operation is used during data loading in order to encode data of a string column  i e   the values  with corresponding integer codes  i e   the codes   This operation involves  1  the lookup of codes for those strings that are already in the dictionary and  2  the encode index decode index Traditional approach  direct   leaves   value  code  leaves   code  value  encode index decode index Traditional approach  indirect   leaves   value  rid  leaves  code  rid  Dictionary  rid  value  code  Shared leaves  encode index leaves  code   value  decode index Figure 2  Traditional approaches vs  Shared leaves insertion of new string values as well as the generation of order preserving codes for those new values      decode  codes     values  This bulk operation is used during query processing in order to decode  intermediate  query results  i e   a bulk of integer codes  using the corresponding string values  i e   the values   Moreover  the interface of the dictionary should also support the following two operations in order to enable the rewrite of the query predicates  for query processing       lookup   value  type      code  This operation is used during query compilation in order to rewrite a string constant  i e   the value  in an equality predicate  e g   p name      Whole Milk   Gallon     or in a range predicate  e g   p name        Whole Milk   Gallon     with the corresponding integer code  i e   code   The parameter type speci   es whether the dictionary should execute an exact match lookup  as it is necessary for string constants in equalitypredicates  or return the integer code for the next smaller  or larger  string value  as it is necessary for string constants in range predicates   An example will be given in the following subsection      lookup  pre   x      mincode  maxcode   This operation is used during query compilation to rewrite the prefix of a pre   x predicate  e g   p name      Whole Milk      with the corresponding integer ranges  i e   the mincode and the maxcode   Again  an example will be given in the following subsection  In order to use table T to ef   ciently support all these operations  we propose to build indexes for both attributes of table T  value and code  because encoding and decoding usually access only a subset of the dictionary  Moreover  indexing the dictionary is especially important if the dictionary is shared between different attributes  because then it is more likely that only a subset of the dictionary is touched  if the domains of the individual attributes are not completely overlapping   Consequently  we believe that in many cases a sequential scan of the complete dictionary does not pay off  The choice of whether to use a sequential scan or an index to access the dictionary  however has to be done as a part of the cost based query optimization  because it strongly depends on the particular workload   However  this discussion is out of the scope of this paper  2 2 Shared leaves Indexing Traditional approaches for indexing can be classi   ed into two general categories  direct and indirect indexes  Using these approaches for indexing the two attributes  value and code  of table T would result in the following situations  see Figure 2   285 1  In the direct indexing approach  two indexes for encoding and decoding are created that hold the data of table T directly in their leaves  In this case  the table T itself does not need to be explicitly kept in main memory since the data of T is stored in the indexes   2  In the indirect indexing approach  two indexes for encoding and decoding are created that hold only references to the data inside table T  i e   a row identi   er rid   In this case  the table T itself needs to be explicitly kept in main memory  While direct indexing  1  has the disadvantage of holding the data of table T redundantly in the two indexes which is not optimal if the indexes should be main memory resident  indirect indexing  2   which is standard in main memory databases  9  7   requires one level of indirection more than the direct indexing approach  i e   pointers into the table T   Thus   2  results in higher cache miss rates on modern CPUs  Another alternative to index the dictionary data is to extend a standard index  e g   a B    Tree  in order to support two key attributes instead of one  i e   in our case for value and code    However  in that case both access paths of the index need to read the two key attributes during lookup which increases the cache miss rates  especially when decoding the integer codes   The new idea of this paper is that the two indexes for encoding and decoding share the same leaves  see shared leaves approach in Figure 2  where both indexes directly hold the data of table T in their leaves but avoid the redundancy of the direct indexing approach  Thus  the shared leaves also avoid the additional indirection level of the indirect indexing approach  As the string dictionary uses an order preserving encoding scheme  the string values and the integer codes in table T follow the same sort order  i e   we can have clustered indexes on both columns and thus can share the leaves between two direct indexes   Consequently  as the attribute values value and code of table T can both be kept in sort order inside the leaves  the leaves can provide ef   cient access paths for both lookup directions  i e   for the encoding and decoding  using a standard search method for sorted data  e g   binary search or interpolation search   Moreover  using the shared leaves for indexing the dictionary means that T does not have to be kept explicitly in main memory because the leaves hold all the data of table T  as for direct indexes   In the following  we discuss how the shared leaves approach can support the bulk operations mentioned at the beginning of this section to support the data loading and query processing inside a column store  Figure 3 shows an example of how the shared leaves approach can be used to ef   ciently support the bulk operations for encoding and decoding string values  In order to encode a list of string values  e g   the list shown at the top of Figure 3   the encode index is used to propagate these values to the corresponding leaves  Once the leaves are reached  a standard search algorithm can be used inside a leaf to lookup the integer code for each single string value  The decoding operation of a list of integer codes works similar  The only difference is that the decode index is used to propagate the integer codes down to the corresponding leaves  e g   the list of integer codes shown on the bottom of Figure 3   If some integer codes for string values are not found by the lookup operation on the encode index  e g   the string values    aac    and    aad    in our example   these string values must be inserted into the dictionary  i e   the shared leaves  and new integer codes must be generated for those values  see the right side of Figure 3   The new codes for these string values have to be added to the result  i e   the list of codes  that are returned by the encoding operation  aaz 40 aaf 30 aae 20 aab 10 value code zzm 970 zzb 960 value code     encode  index  e decode  index  d  zzb  aad       aac    960   1        1   30  960       20   aaf  zzb       aae  aae 20 aad 16 aac 13 aab 10 value code zzm 970 zzb 960 value code     encode index  e    decode  index  d     aac  aad  Bulk Lookup  Bulk Insert   Load  shared leaves  l shared leaves  l    Figure 3  Operations on indexes with shared leaves Moreover  the encoding and decoding indexes must be updated if necessary  In this paper  we do not focus on how to generate new orderpreserving integer codes for new string values that are inserted into the dictionary  In order to generate the codes  we simply partition the code range where new string values are inserted in into equidistant intervals  e g   in our example the two strings    aac    and    aab    are inserted into the code range between 10 and 20   The limits of these intervals represent the new codes  e g   13 for    aac    and 17 for    aad    in our example   In case that the range is smaller than the number of new string values that have to be inserted in the dictionary  re encoding of some string values as well as updating the data  i e   the columns of a table  that use these string values becomes necessary  Analyzing more sophisticated orderpreserving encoding schemes that are optimal  i e   that require minimal re encoding  under certain workloads as well as strategies for re encoding the dictionary data are an avenue of our future research work  In the following  we discuss how the shared leaves approach can support the two lookup operations mentioned at the beginning of this section that support the predicate rewrite a column store  The lookup operation which is necessary to rewrite the equalityand range predicates is similar to the bulk lookup explained before  the encoding index propagates the string constant to the corresponding leaf and then a standard search algorithm can be used on the leaf to return the corresponding integer code  For example  in order to rewrite the predicate value        zzc    using the encode index in Figure 3  left side   the encode index propagates the string value    zzc    to the rightmost leaf and this leaf is used to lookup the next integer code for that string value that is equal or greater than the given value  i e   the integer code 970 for the string value    zzm      The rewritten predicate thus would be code     970  In order to support the other lookup operation that is necessary to rewrite a pre   x predicate  the encoding index needs to propagate the string pre   x to those leaves which contain the minimum and the maximum string value that matches this pre   x  For example  in order to rewrite the predicate value      aa     using the encode index in Figure 3  left side   the encode index has to propagate the pre   x to the    rst leaf which contains the minimum and the maximum string value that matches this pre   x  Afterwards  those leaves are used to map the strings that represent the boundaries for the given pre   x to the corresponding codes  e g   in our example we retrieve the codes for    aab    and    aaz    and rewrite the predicate as 10     code     40   2862 3 Requirements and Design Decisions The main requirement is that the dictionary should be fast for encoding  i e   the bulk lookup insert of integer codes for a list of string values  but the optimization goal is the performance for decoding  i e   the bulk lookup of string values for a given list of integer codes   Thus  the data structures of the dictionary  i e   leaves and indexes  should also be optimized for encoding decoding bulks instead of single values  Moreover  the data structures should be optimized for modern CPUs  i e   they should be cache conscious and the operations should be easy to parallelize   In the following we discuss further requirements and design decision for the leaf structure and the indexes of the dictionary  Leaf structure  The most important requirement for the leaf structure is that it must be able to hold the string values as well as the integer codes in sort order to enable ef   cient lookup operations  e g   binary search  for both encoding and decoding  while the leaf structure should be optimized for decoding   As the dictionary must be memory resident  the memory footprint of the dictionary should be small  i e   it might make sense to apply a lightweight compression scheme such as incremental encoding to the leaf data   Moreover  the leaf should support the encoding of variable length string values  While the lookup operations on a leaf are trivial for    xed length string values that are not compressed  the lookup operations get more complex if the leaves should also support variable length string values and compression  When encoding a bulk of string values  new string values might be inserted into the dictionary which involves updating the sharedleaves  Consequently  the leaf should also enable ef   cient bulk loads and bulk updates  Finally  note that the leaf structure can be totally different from the data structure that is used for the index nodes on top  A concrete leaf structure that satis   es these requirements is discussed in detail in the next section  Encode Decode index structure  Same as the leaf structure  the indexes for encoding and decoding should keep their keys in sort order to enable ef   cient lookup operations over the sorted leaves  Another requirement is that the encode index must also support the propagation not only of string constants but also of string pre   xes to the corresponding leaves in order to support the predicate rewrite task  Moreover  the indexes should also be memory resident and thus have a small memory footprint  When bulk encoding a list of string values using the encoding index  in addition to the lookup of the integer codes for string values that are already a part of the dictionary  it might be necessary to insert new string values into the dictionary  i e   update the leaves as well as the both indexes for encoding and decoding  and generate new order preserving codes for those values  We propose to combine these two bulk operations  lookup and insert  into one operation  In order to support this  we see different strategies   1  All Bulked  First  propagate the string values that need to be encoded to the corresponding leaves using the encode index and lookup the codes for those strings that are already in the leaves  Afterwards  insert the new values that were not found by the lookup into the leaves and if appropriate reorganize the updated leaf level  e g   create a leaf level where all leaves are    lled up to the maximal leaf size   Afterwards generate integer codes for the new string values and bulk load a new encode and a new decode index from the updated leaf level  in a bottom up way    2  Hybrid  First  propagate the string values that need to be encoded to the corresponding leaves using the encoding index and update the encoding index directly  i e   do updates inplace during propagation   Then  lookup the codes for those strings that are already in the leaves  Afterwards  insert the new values that were not found by the lookup into the leaves and generate integer codes for all new string values  Finally  bulk load a new decode index from the updated leaf level  bottom up    3  All In Place  First  propagate the string values that need to be encoded to the corresponding leaves using the encoding index and update the encoding index directly  i e   do updates in place during propagation   Then  lookup the codes for those strings that are already in the leaves  Afterwards  insert the new values that were not found by the lookup into the leaves and generate integer codes for all new string values  Propagate each update on the leaf level that causes an update of the decode index  e g   a split of a leaf  directly to the decode index and apply the update  In the    rst two strategies above  the decode index is bulk loaded from the updated leaf level  which means that it should provide a better search performance for decoding which is our main optimization goal  Consequently  in this paper we focus on the    rst two strategies  In order to guarantee consistency of the data dictionary  for simplicity we decide to lock the complete indexes as well as the leaves during data loading  i e   the encoding of string values  because this usually happens only at prede   ned points in time in data warehousing  i e   once a day   Thus  no concurrent updates and reads are possible during data loading  However  during query processing  i e   for decoding query results   we allow concurrency because these are read only operations  For persisting the dictionary  currently we only write the updates leaves sequentially to disk as a part of data loading  More sophisticated persistence strategies are a part of our future work  3  LEAF STRUCTURE In this section  we present a leaf structure that can be used in the shared leaves approach  see Section 2  for ef   ciently encoding and decoding variable length string values on a particular platform  The general idea of this leaf structure is to keep as much string values as well as the corresponding    xed length integer codes sorted and compressed together in one chunk of memory in order to increase the cache locality during data loading and lookup operations 2   3 1 Memory Layout Figure 4 shows an example of the memory layout of one concrete instance of a leaf structure that represents the dictionary shown in Figure 1  c   The leaf structure compresses the string values using incremental encoding  21  while each n th string  e g   each 16 th string value  is not compressed to enable an ef   cient lookup of strings without having to decompress the complete leaf data  In the example  value 16    Whole Milk   Gallon    is not compressed and value 17    Whole Milk   Half Gallon    is compressed using incremental encoding  i e   the length of the common pre   x compared to the previous value  e g   11  is stored together with the suf   x that is different  e g      Half Gallon     In order to enable an ef   cient lookup using this leaf structure  an offset vector is stored at the end of the leaf that holds references  i e   offsets  and the integer codes of all uncompressed strings of 2 In this paper we assume that string values are encoded in ASCII using one byte per character  2871016 1   2   8 3    0    4    0     0 0 0 W H E A T B R         1008     3    2    0    0    0          168 T  0 3    2    1    0    0     160 2   0   5   0 11 Q U A R 152 G A L L O N  0 3 144 N  0 11 H A L F 136 L K   G A L L O 128 W H O L E M I offset 16 code 0 offset 0 code 16 value 16  uncompressed  value 17  prefix compressed  code 17 offset vector leaf data Figure 4  Leaf for variable length string values a leaf also in a sorted way  For example  the offset 128 and the code 32000 are stored in the offset vector for value 16  see Figure 4   The integer codes of the compressed string values are stored together with the compressed string values in the data section and not in the offset vector  e g   the code 32050 for value 17   The offset vector is stored in a reverse way to enable ef   cient data loading while having no negative effect on the lookup operations  In order to adapt the leaf structure to a certain platform  e g   to the cache sizes of a CPU  different parameters are available      Leaf size l  De   nes the memory size in bytes that is initially allocated for a leaf  In our example  we used l   1024      Offset size o  De   nes the number of bytes that are used as offset inside the leaf  This parameter has in   uence on the maximal leaf size  If we use o   2 bytes  as in the example in Figure 4  then the maximal leaf size is 2 16 bytes  64kB       Code size c  De   nes the number of bytes that are used to represent an integer codeword  If we use c   4  we can generate 2 32 different codewords      Pre   x size p  De   nes the number of bytes that are used to encode the length of the common pre   x for incremental encoding  This parameter is determined by the maximal string length  For example  if we use strings with a maximal length of 255  then we can use p   1 because using one byte allows us to encode a common pre   x length from 0 to 255      Decode interval d  De   nes the interval that is used to store uncompressed strings  16 in our example   This parameter has in   uence on the size of the offset vector  i e   the smaller this interval is  the more space the offset vector will use   We do not allow the de   nition of these parameters on the granularity of bits because most CPUs have a better performance on data that is byte aligned  or even word  or double word aligned on modern CPUs   We decide to use byte aligned data  and not wordor double word aligned data  because the other variants might lead to a dramatic increase in the memory consumption of the leaves  As an example  assume that we want to tune the leaf structure for a platform using one CPU  with one core  having an L2 cache of 3MB and an L1 cache of 32kB  Moreover  assume that the leaf should only store strings with an average length of 50 characters and a maximum length of 255 characters  which means that p   1 can be used   In that case it would make sense to use a leaf size not bigger than the L2 cache size  say max  2MB  Consequently  an offset size o   3 is suf   cient to address all values inside such a leaf  The code size depends only on the overall number of strings that should be stored in a dictionary  We assume that c   4 is an appropriate choice for this example  De   ning the value for the decode interval d is more dif   cult  in general  we want to make sure that the offset vector once loaded remains in the L1 cache  e g   having a max  size of 32kB  such that an ef   cient binary search is possible for a bulk of strings  With the given settings of the example above  we assume that a leaf will be able to store approx  42000 strings  which results from the max  leaf size of 2MB and the average string length of 50   Moreover  each uncompressed string utilizes 7 bytes of the offset vector  for the offset and the code   Consequently  the offset vector can store max  32k 7     4681 entries  i e   offsets and codes  for uncompressed strings which means that we could store each d     42000 4681     9th string uncompressed in the example  3 2 Leaf Operations The most important operations on the leaf structure are the lookup operations for encoding string values with their integer codes and decoding the integer codes with their string values  Moreover  the leaf also supports updates  i e   inserting new strings   In the following paragraphs  we examine these operations in detail  Bulk Lookup  In this paragraph  we    rst explain how the lookup works for a single value and then discuss some optimizations for bulks  The leaf supports the following lookup operations  one to lookup the code for a given string value  i e   value v     code c  and another one that supports the lookup vice versa  i e   code c     value v   In order to search the code c for a given value v  the procedure is as follows  1  Use the offset vector to execute a binary search over the uncompressed strings of a leaf in order to    nd an uncompressed string value v     that satis   es v         v and no other uncompressed value v   exists with v       v   v      2  If v       v return the corresponding code c for v that is stored in the offset vector  3  Otherwise  sequentially search value v from value v     on until v is found or the next uncompressed value appears  In the    rst case return the code c  in the second case indicate that the value v was not found  Note that for sequentially searching over the incrementally encoded string values no decompression of the leaf data is necessary  Algorithm 1 shows a search function that enables the sequential search over the compressed leaf data  The parameters of this function are  the leaf data  i e   leaf   the offset where to start and end the sequential search  i e   start and end   as well as the value that we search for  i e   v   The return value is the corresponding code c if the value is found  otherwise the algorithm returns    1  The basic idea of the algorithm is that it keeps track of the common pre   x length  i e   prefix len  of the current string  at the offset start  and the search string v  If this common pre   x length is the same as the length of the search string then the correct value is found and the code can be returned  The variables p and c are constants that represent the pre   x size and the code size to increment the offset value start  The lookup operation to    nd a string value v for a given code c works similar as the lookup operation mentioned before using the offset vector  The differences are that the    rst step  i e   the search over the offset vector  can be executed without jumping into the leaf data section because the codes of the uncompressed strings are stored together with the offset vector  In contrast to the other lookup operation  for the search over the offset vector we theoretically expect to get only one L1 cache miss using a simpli   ed cache 288Algorithm 1 Sequential search of string v on compressed leaf function SEQUENTIALSEARCH leaf  start  end  v  v         leaf start      read string v     at offset start start     start   size v           increment offest by string size pref ix len     pref ix len v  v           calculate common pre   x len while start     end and pref ix len    v  do curr pref ix len     leaf start      get curr  pre   x len start     start   p     increment offest by pre   x size p   1 v         leaf start  start     start   size v       if curr pref ix len    pref ix len then continue     pre   x of curr  value v     too short long else if compare v       v    0 then return  1     curr  value v     comes after search value v end if pref ix len     pref ix len   pref ix len v  v       start     start   c     increment offest by code size c   4 end while if pref ix len    v  then return leaf start     c      string v found  return code else return  1     string v not found end if end function model  if the offset vector    ts into the L1 cache   Another difference of this lookup operation is that during the sequential search the string values have to be incrementally decompressed  When executing both lookup operations  we expect to theoretically get one L2 cache miss in a simpli   ed cache model if we assume that loading the complete leaf causes one miss and the leaf    ts into the L2 cache 3   The average costs for these two lookup operations are as follows  where n is the number of strings codes stored in a leaf and d is the decode interval   O log n d     O d  In order to optimize the lookup operation for bulks  i e   a list of string values or a list of integer codes   we can sort the lookup probe  By doing this  we can avoid some search overhead by minimizing the search space after each search of a single lookup probe  i e   we do not have to look at that part of the offset vector leaf data that we already analyzed   Bulk Update  In this paragraph  we explain how to insert new strings into the leaf structure  As we assume that data is loaded in bulks  we explain the initial bulk load and the bulk insert of strings into an existing leaf  In order to initially bulk load a leaf with a list of string values  we    rst have to sort the string values  Afterwards  the leaf data can be written sequentially from the beginning of the leaf while the offset vector is written reversely from the end  If the string values do not utilize the complete memory allocated for a leaf  because we do not analyze the compression rate before bulk loading  then the offset vector can be moved to the end of the compressed data section and the unused memory can be released  In order to insert a list of new string values into an existing leaf  we again have to sort these string values    rst  Afterwards  we can do a sort merge of these new string values and the existing leaf in order to create a new leaf  The sort merge is cheaper if we can reuse as much of the compressed data of the existing leaf as possible and thus do not have to decode and compress the leaf data again  3 A more    ne grained cache model would respect cache lines and cache associativity  However  as we focus on bulk lookups our simpli   ed model is suf   cient to estimate the costs of cache misses  Ideally  the new string values start after the last value of the existing leaf  In that case  we only have to compress the new string values without decoding the leaf  If the list of string values and the existing leaf data do not    t into one leaf anymore  the data has to be split  However  as the split strategy depends on the index structure that we build on top of the leaves  we discuss this in the next section  4  CACHE CONSCIOUS INDEXES In this section  we present new cache conscious index structures that can be used on top of the leaf structure presented in the previous section  These indexes support one of the    rst two update strategies  All Bulked and Hybrid  discussed in Section 2  For the encoding index  we present a new cache sensitive version of the patricia trie  called CS Array Trie  that supports the Hybrid update strategy and a cache sensitive version of the Pre   x B Tree  5   called CS Pre   x Tree  that supports the All Bulked update strategy  As decoding index  we reuse the CSS Tree  16  which is known to be optimized for read only workloads  We create a CSS Tree over the leaves of the dictionary using the minimal integer codes of each leaf as keys of the index  i e   the CSS Tree is only used to propagate the integer values that are to be decoded to the corresponding leaves   As the CSS Tree can be bulk loaded ef   ciently in a bottom up way using the leaves of the dictionary  it satis   es the requirements for both update strategies  Hybrid and All Bulked   4 1 CS Array Trie As a    rst cache conscious index structure that can be used as an encode index to propagate string lookup probes and updates to the corresponding leaf in a shared leaf approach  we present the CS Array Trie  Compared to existing trie implementations the CSArray Trie uses read optimized cache conscious data structures for the index nodes and does not decompose the strings completely  4 1 1 General Idea Many existing trie implementations are using nodes that hold an array of pointers to the nodes of the next level with the size of the alphabet  e g   128 for ASCII   While such an implementation allows ef   cient updates and lookups on each node  it is not memoryef   cient because the array trie allocates space for a pointers per node  where a is the size of the alphabet  while a pointer uses 8 bytes on a 64 bit system   Other trie implementations avoid the memory overhead and use a sorted linked list  13  to hold only the characters of the indexed strings together with a pointer to the next level of the trie  While this implementation still offers ef   cient node updates  the lookup must execute a search over the characters stored in the linked list  However  linked lists are known to be not very cache ef   cient on modern CPUs because of pointer chasing  20   Moreover  most existing trie implementations decompose the indexed strings completely  i e   each letter of a string results in a node of the trie   Compared to the implementations mentioned above  a node of the CS Array Trie uses an array instead of a linked list to store the characters of the indexed string values  Compared to a linked list an array is not ef   cient when sequentially inserting single values into a trie  However  when bulk inserting new values into a CS ArrayTrie  we need grow the array of a node only once for each bulk  In order to lookup a string value of a CS Array Trie  the search over the characters of a node  i e   the array  is more ef   cient than the sequential search on a linked list because the array supports binary search and all characters are stored clustered in memory  The second key idea of the CS Array Trie is that it does not decompose the string values completely  The CS Array Trie stores 289a set of strings that have the same pre   x together using the leaf structure that we discussed in the previous section  A leaf of the CS Array Trie stores the complete strings and not only their suf     xes  without the common pre   x  in order to enable ef   cient decoding of integer codes using the same leaves  as described in our shared leaves approach   Moreover  storing the complete strings in a leaf  i e   repeating the same pre   x  is still space ef   cient because we use incremental encoding to compress the strings  Finally  for those trie nodes that only hold a pointer to one child node  we use the path compression used in patricia tries  15   Figure 5  left side  shows an example of a CS Array Trie that indexes nine different strings  If we follow the path    aa    in the trie  we reach a leaf that holds only strings that start with the pre   x    aa     The leaves are shown as uncompressed tables for readability  The physical memory layout is as discussed in Section 3  4 1 2 Index Operations In order to encode a bulk of string values during data loading using that trie  we implement the Hybrid update strategy for the CS Array Trie which means that new string values are inserted into the trie when the strings that should be encoded are propagated through the encoding index  i e  the trie   In order to leverage the fact of having bulk inserts  we propagate the string values  in preorder  to the leaves using variable buffers at each node of the trie to increase the cache locality during lookup as described in  22   Moreover  when using buffers at each node we can grow the array of characters stored inside a node only once per bulk  The work in  22  showed that this effectively reduces data cache misses for treebased indexes and results in a better overall lookup performance  Figure 5  right side  shows an example for encoding a bulk of six strings using the existing encode index  i e   the trie shown on the left side   First  all strings that need to be encoded are propagated from the root node of the trie to the    rst level of nodes creating three buffers   1    2   and  3    In order to keep the order of strings in the lookup probe for creating the encoded result  at the bottom of Figure 5   a sequence number is added for each value in the buffers  as suggested in  22    The strings that are propagated from the root to the    rst level of buffers are analyzed and the missing character    m    for the string    mzb    is added to the root node 4   Afterwards  the buffer  1  is propagated to the next level creating two new buffers  4  and  5  and buffer 1  is returned to a buffer pool  to avoid expensive memory allocation for buffer pages   Next  buffers  4  and  5  are processed  which means that values for the codes for existing string values are looked up and new strings are inserted into the existing leaves with a placeholder as their integer code  e g      1   While buffer  4  contains two new strings    aax    and    aay    that are inserted to the leftmost leaf  buffer  5  contains only one new string    amc    that is inserted the next leaf  Note  that the new values in buffers  4  and  5  are not yet deleted and kept to lookup the integer codes for those string values  that are not yet generated   A question that arises here is  whether the new strings    t into the existing leaf  i e   the new leaf size is expected to be less than the maximal leaf size  or whether the leaf must be split up into several leaves  In order to estimate the expected leaf size  we add the size  uncompressed  of all new strings in a buffer page as well as the size of their new codes  without eliminating duplicates  to the current leaf size  If the bulk is heavily skewed and contains many duplicates it is likely that we decide to split a leaf even if it is not necessary  When all string values are propagated to their corresponding 4 All updates on the trie and the leaves are shown in light gray in Figure 5  right side   leaves  i e   the new strings are inserted into the leaves   new integer codes are generated for the new string values  This is done by analyzing the number of strings that are inserted in between two existing string values  For example  in order to generate codes for the three new string values that are inserted into the    rst two leaves between    aam    and    amd    in Figure 5  right side   we have to generate three new codes that must    t into the range between 40 and 50  Using our equi distance approach  discussed in Section 2   we generate 43  45  and 48 as new codes  Therefore  the trie must allow us to sequentially analyze all leaves in sort order  i e   each leaf has a pointer to the next leaf   Finally  after generating the integer codes for the new string values of the trie  we use the buffer pages that we kept on the leaf level  e g   the new strings in the buffers  2    4   and  5  in the example  to lookup the integer codes for the new string values and use the sequence number to put the integer code at the right position in the answer  see bottom of Figure 5   Another task that can ef   ciently be supported using the CS ArrayTrie  is the predicate rewrite  for equality  and range predicates the constants are simply propagated through the trie without buffering  For pre   x predicates  the pre   x is used to    nd the minimal and maximal string value that matches this pre   x  which is also trivial when using a trie   4 1 3 Cost Analysis For propagating a bulk of string values from the root of a CSArray Trie to the leaves  we theoretically expect to get one L2 data cache miss for each node in our simpli   ed cache model  In addition  for leaf that has to be processed during the lookup  we expect to get another L2 data cache miss using our simpli   ed cache model  Moreover  the input and output buffers of a node should be designed to    t into the L2 cache together with one node  to allow ef   cient copying from input to output buffers   In that case  we expect to get one cache miss for each buffer page that needs to be loaded  Moreover  generating the new integer codes will cause one L2 data cache miss for each leaf of the trie  Finally  executing the lookup of the new integer codes  will also cause one cache miss for each buffer page that has to be loaded plus the L2 cache misses for executing the lookup operations on the leaves  as discussed in the section before   The costs for encoding a bulk that contains no new string values  i e   a pure lookup  are composed of the costs for propagating the strings through the trie plus the costs for the lookup of the codes for all strings using the leaves  If we assume that all strings in the lookup probe have the same length m and are distributed uniformly  the height of the trie is loga s l  in the best case and equal to the length of the string m in the worst case  where s is the total number of strings  l is the number of strings that    t in one leaf  and a the size of the alphabet   The lookup costs on each node are O log a    Thus  the average costs for propagation are  O s       loga s l    m  2      log a   4 1 4 Parallelization The propagation of string values from the root of the trie to the leaves  including the update of the nodes  can be easily parallelized for different sub tries because sub tries share no data  Moreover  the generation of new integer codes can be done in parallel as well  without locking any data structures   For this we need to    nd out which leaves hold contiguous new string values  i e   sometimes a contiguous list of new string values might span more than one leaf as shown in Figure 5 on the right side for the    rst two leaves   Finally  the lookup operation of the new string values can also be 290aam 40 aaf 30 aae 20 aab 10 value code zzm 90 zzb 80 value code o o a z o m o o a z o o a m amo 70 amk 60 amd 50 value code zzm 90 zzb 80 value code amc 48 amo 70 amk 60 amd 50 value code o o a m String values   zzb  aax  amc  amo  aay  mzb   aax 1    amc 2    amo 3   aay 4  Integer codes   80  43  48  70  45  75   aax 1    zzb 0    aay 4   amc 2    amo 3  mzb 75 value code aay 45 aax 43 aam 40 aaf 30 aae 20 aab 10 value code  mzb 5   1   4   5   2   3  Encode index Shared leaves Figure 5  Encode data using the CS Array Trie parallelized without locking any data structures  In Figure 5  right side   the new values in the buffer pages  2    4   and  5  can be processed by individual threads for example  We can see that each thread writes the resulting integer codes to different index positions of the result  shown at the bottom  using the sequence number of the buffer pages  Thus  no locks on the result vector are necessary  4 2 CS Pre   x Tree As a second cache conscious index structure that can be used as an encode index to propagate string lookup probes and updates to the corresponding leaf in a shared leaf approach  we present the CSPre   x Tree  This index structure combines ideas from the Pre   xB Tree  5  and the CSS Tree  16   4 2 1 General Idea Same as the Pre   x B Tree  a node of a CS Pre   x Tree contains the shortest pre   xes that enable the propagation of string values to the corresponding child nodes  However  instead of storing a pointer to each child  the CS Pre   x Tree uses a contiguous block of memory for all nodes and offsets to navigate through this block  as the CSS Tree   This effectively reduces memory consumption and avoids negative effects on the performance due to pointer chasing  In order to further reduce the memory footprint of the CS Pre   xTree  we only store the offset to the    rst child node explicitly  Since the nodes have a    xed size s  we can calculate the offset to a child node using offset arithmetics  i e   the i th child of a node can be found at offset o   offset first child     i     s    In order to enable fast search over the variable length keys of a node  e g   binary search   we store the offsets to the keys  i e   the string pre   xes  in an offset vector at the beginning of each node  Moreover  the node size has to be    xed in order to use offset arithmetics for computing the index to the child nodes  Thus  the number of children of a node is variable because we store variablelength keys inside a node  4 2 2 Index Operations The CS Pre   x Tree  as the CSS Tree  can only be bulk loaded in a bottom up way which means that it is only suitable for the Allbulked update strategy discussed in Section 2  Using the All bulked update strategy for the encoding a list of string values means that the new string values  that are not yet a part of the dictionary  must    rst be inserted into the leaves  in sort order  and then the index can be created  Thus  if the    rst bulk of string values should be encoded using the All bulked update strategy  the complete leaf level has to be built using these string values  Therefore  we reuse the idea in  18  and create a trie  more precisely a CS Array Trie  to partition the string values into buckets that can be sorted ef   ciently using multikey quicksort  6   The sorted string values can then be used to create leaves that are    lled up to the maximum leaf size  From these leaves  a new encode index  i e   a CS Pre   x Tree  is bulk loaded in a bottom up way  Figure 6 shows an example of a CS Pre   x Tree  In the following  we describe the bulk load procedure of a CS Pre   x Tree from a given leaf level  1  In order to bulk load the CS Pre   x Tree  we process the leaf level in sort order  We start with the    rst two leaves and calculate the shortest pre   x to distinguish the largest value of the    rst leaf and the smallest value of the second leaf  In our example it is suf   cient to store the pre   x    am    in order to distinguish the    rst two leaves  This pre   x is stored in a node of the CS Pre   x Tree  Note  a node does not store a pointer to each child since we can derive an offset into the leaf level  i e   the sorted list of leaves   Since we do not know the size of the offset vector in advance  we write the offset vector from left to right and store the keys from right to left  The example assumes a    xed node size of 32 bytes and therefore we store offset 29 in the offset vector and write the pre   x at the corresponding position  2  Next  we calculate the shortest pre   x to distinguish the largest value of the second leaf and the smallest value of the third leaf and so on until all leaves are processed  If a node is full  we start a new node and store the index to the    rst leaf that will be a child of this new node as an anchor  In our example the    rst node covers the    rst four leaves and therefore the index of the    rst child in the second node is 4  Note that the nodes are stored continuously in memory  3  As long as more than one node is created for a certain level of the CS Pre   x Tree  we add another level on top with nodes 291aaf 30 aae 20 aab 10 value code amq 80 bbk 100 bbd 90 value code amo 70 amk 60 amd 50 value code  bc  amq  am  Keys  reversed   29  25  22  Offset vector 0 3 Idx of 1st child No of keys bcx 130 bca 110 bct 120 value code bdl 160 bdf 150 bcz 140 value code dfe 200 value code dfb 190 cdd 180 cda 170 value code  dfe  cd  Keys  29  25  Offset vector 4 2 Idx No  bcz  Keys  reversed   29  Offset vector 0 1 Offset to 1st child No of keys Offset to root   2         Encode index Shared leaves Figure 6  Example of a CS Pre   x Tree that store pre   xes that distinguish their child nodes  In our example we have one node on top of the lowest level of the CS Pre   x Tree  This node is the root of the tree and we store the offset to this node in the contiguous memory block  2 in our example   Since the tree is built bottom up  the nodes have to be stored in that sequence in memory  For subsequent bulks  we use the existing CS Pre   x Tree to propagate the string values that are to be encoded to the corresponding leaves  In our current implementation we buffer the string values only on the leaf level  and not within the CS Pre   x Tree   Afterwards  we do a sort merge of the existing leaf with the new string values stored in the buffers  If the new string values in the buffers and the values of the existing leaf do not    t into one leaf  we simply create another leaf  This very simple sort merge strategy can be improved in different ways  for example  one could try to create equally sized leaves to avoid degenerated leaves that contain only a small number of strings  Moreover  after encoding several bulks it might make sense to reorganize the complete leaf level by    lling all leaves to their maximum  Once all updates are processed we bulk load a new CS Pre   x Tree from the merged leaf level  For example  in order to propagate a bulk of strings to the leaves using the CS Pre   x Tree  we proceed as follows  assume we are looking for value    amk    in our example in Figure 6  We start at the root node and see that    amk    is smaller than    bcz     thus we proceed at the    rst child of this node at offset 0  We    nd that    amk    is between    am    and    amq    and thus proceed at the second child of this node at index 1  calculated from the index of the    rst child   The CS Pre   x Tree stores the information that nodes below a certain offset point to leaves instead of internal nodes  To rewrite query predicates using the CS Pre   x Tree  we do a simple lookup with the string constants if it is an equality predicate or a range predicate  If it is a pre   x predicate  the pre   x is used to    nd the minimal string value that matches the pre   x  The lookup for the pre   x will end up at leaf that contains this value even if the value itself is not in the dictionary  From that leaf on  we execute a sequential search for the maximum string value that matches the pre   x  We could save some effort compared to a sequential search  if we also use the index to    nd the leaf that holds the maximum value directly  similar to a skip list   One problem of using a contiguous block of memory and offsets is that the memory has to be allocated in advance  We calculate the maximum amount of memory that all nodes of the CS Pre   x Tree need by introducing an arti   cial limit on the maximum length of the keys in the tree  We then can calculate the minimum number of keys that    t into one node and thus can estimate the maximal number of nodes that we need to store the data  One possibility to overcome this problem is to leverage the idea of a CSB Tree  17  to use a mix of pointers and offset arithmetics  e g   one pointer per node  to identify the correct child and thus allow multiple blocks of memory instead of one single block  4 2 3 Cost Analysis We suggest to set the size of a node of a CS Pre   x Tree at most to the size of the L2 cache  Thus for propagating a bulk of string values from the root of a CS Pre   x Tree to the leaves  we theoretically expect to get one L2 data cache miss  in our simpli   ed cache model  for each node that is traversed during the lookup for each string value and another L2 data cache miss when the value is written to the corresponding buffer of the leaf  which should also be designed to    t into the cache   Moreover  the generation of new codes and the encoding itself will each cause one L2 data cache miss for each leaf in the leaf level  if the leaf and the output buffer    t in the L2 cache together   The costs for encoding a bulk that contains no new string values  i e   a pure lookup  is composed of the costs for propagating the strings through the tree plus the costs of the lookup in the leaf  If we assume that all strings in the lookup probe have the same length and are distributed uniformly  the height of the tree is logk s l   where s is the total number of strings  l is the number of strings that    t in one leaf  and k the number of keys that    t into one node   The lookup costs on each node are O log k    Thus  the average costs for propagation are  O s     logk s l      log k   Compared to the CS Array Trie  it is more expensive to build a CS Pre   x Tree because the data has to be sorted    rst and then the CS Pre   x Tree is loaded bottom up as opposed to the CS ArrayTrie that is loaded top down and implicitly sorts the data during that process  We show in our experiments that the CS Pre   x Tree performs slightly better than the CS Array Trie for a pure lookup workloads  i e   encoding a bulk of strings that does not contain new values  since on average the tree is expected to be less high than the trie and the leaves are organized more compact  4 2 4 Parallelization Our current implementation supports multiple threads at the leaf level  Once the bulk is completely buffered at the leaves  the lookup can be executed in parallel as described for the CS Array Trie  Since we want to support variable length keys we cannot parallelize the bottom up bulk loading of the tree  292Currently  the bulk lookup on the index is single threaded since we are not buffering the requests within the tree and thus have no point to easily distribute the workload to multiple threads  One way to parallelize the lookup would be to partition the bulk before accessing the index and process these partitions using multiple threads  Since the buffers at the leaf level would then be shared by several threads  either locking would be needed or the partitioning has to be done according to the sort order  which is expensive  5  PERFORMANCE EXPERIMENTS This section shows the results of our performance experiments with the prototype of our string dictionary  i e   the leaf structure discussed in Section 3 and the cache conscious indexes discussed in Section 4   We executed three experiments  the    rst experiment  Section 5 1  shows the ef   ciency of our leaf structure and compares it to other read optimized indexes 5   the second experiment  Section 5 2  examines the two new cache conscious indexes using different workloads  and    nally the last experiment  Section 5 3  shows the overall performance and scalability of our approach  We implemented the data structures in C   and optimized them for a 64 bit Suse Linux Server  kernel 2 6 18  with two Intel Xeon 5450 CPUs  each having four cores  and 16 GB of main memory  Each CPU has two L2 caches of 6MB  where two cores share one L2 cache  and one L1 cache for each core with 32kB for data as well as 32kB for instructions  In order to generate meaningful workloads  we implemented a string data generator that allows us to tune different parameters like the number of strings  string length  alphabet size  the distribution  and others  We did not use the TPC H data generator dbgen  for example  because most string attributes either follow a certain pattern  e g   the customer name is composed of the pre   x    Customer    and a unique number  or the domain size of such attributes is too low  e g   the name of a country   Thus the data generated by dbgen does not allow us to generate workloads that let us analyze our data structures with workloads that have certain interesting properties  We will show the properties of the workloads that we generated for each experiment individually  5 1 Ef   ciency of Leaf Structure In this experiment  we analyze the ef   ciency of the leaf structure discussed in Section 3  The general idea of this experiment is to show the performance and memory consumption of the leaf operations for two different workloads  We used the parameters in the following table to con   gure the leaf structure     rst part  and to generate our workloads  second part   Parameter Value Leaf size l 64kB   16MB Offset size o 4 bytes Code size c 8 bytes Pre   x size p 1 byte Decode interval d 16 String length  1  25   2  100 String number  1      450000   2      150000 Alphabet size 128 Distribution Distinct  unsorted  The two different workloads   1  and  2   were designed that each of these workloads    ts into a leaf with a size of 16 MB while 5We used PAPI to measure the performance counters  http   icl cs utk edu papi   each workload uses a different    xed string length and thus represents a different number of strings  We only used distinct unsorted workloads  i e   no skew  because these workloads represent the worst case for all lookup operations  i e   each string value integer code of the leaf is encoded decoded once for each workload   We used each of these workloads to load a set of leaves that hold the workload in a sorted way while for each workload we used different leaf sizes varying from 64kB to 16MB  resulting in a different set of leaves for each combination of workload and leaf size   The    rst goal of this experiment is to show the costs  of the bulk loading and bulk lookup operations  caused by the different workloads  of approximately the same size in memory  using leaves with different sizes without the overhead of an encoding and decoding index on top  by simulating the pure lookup on the leaves   We measured the time as well as the L2 cache misses that resulted from executing the bulk loading of the leaves and executing the lookup operations for encoding as well as decoding  In order to load the leaves  we    rst sorted the workloads and then bulk loaded each leaf up to its maximal size  see Section 3   Afterwards  we generated the integer codes for these leaves  As shown in the table before  we use 8 bytes for the integer code in this experiment to show the memory consumption expected for encoding attributes with a large domain size  In order to measure the pure lookup performance of the leaf structures  we assigned each string value of the workloads mentioned above to the corresponding leaf using a buffer and then we looked up the code for each string in the individual buffers  i e   we created a corresponding encoded workload for each leaf   Finally  we used the encoded workload to execute the lookup operation  in the same way as described before  on the leaves to decode the integer codes again  A second goal of this experiment is to show a comparison of the 16MB leaf structure  which can be used for encoding as well as for decoding  and two cache conscious read optimized index structures using the workloads  1  and  2   for encoding the string values we compare the leaf structure to the compact chain hash table  4  and for decoding integer codes we compare the leaf structure to the CSS tree  16   The main results of this experiment are that  1  the leaf structure is optimal when having a medium size      512kB  and  2  the performance of our leaf structure is comparable to the read optimized index structures mentioned above while using less memory      Figure 7  a  shows the time and memory that is needed to bulk load the leaf structure  of 16MB size  compared to bulk loading the compact chain hash table and the CSS tree  As a result  we can see that bulk loading the leaf structure is faster for both workloads  i e   string length 25 and 100  and uses less memory compared to the compact chain hash table and the CSS tree      Figure 7  b  shows that the time and the L2 data cache misses  L2CM  for encoding the two workloads of this experiment  using the bulk loaded leaves  are increasing for large leaf sizes  Moreover  in terms of performance we can see that the 16MB leaf structure is comparable to the compact chain hash map  Map  while offering sorted access  i e   the leaf structure is a little slower       Finally  in Figure 7  c  we can see that our leaf structure is optimized for decoding  While decoding 450k strings of length 100 using the smallest leaf size takes about 50ms  it takes 200ms to encode them  Moreover  the L2 cache misses for encoding are almost twice as high as for decoding these 293  0   20   40   60   80   100   120   140   160 Leaf Hash Map CSSTree  0  2  4  6  8  10  12  14  16  18  20  22  24 Time  ms  Memory  MB  Time25 Time100 Memory25 Memory100  a  Bulk Loading  100  150  200  250  300  350  400  64 128 256 512 1024 2048 4096 8192 16384  0  500000  1e 06  1 5e 06  2e 06  2 5e 06  3e 06  3 5e 06  4e 06  4 5e 06 Time  ms  L2 cache misses Leaf size  log   kB  Time Map25 Time Map100 Time Leaf25 Time Leaf100 L2CM Map25 L2CM Map100 L2CM Leaf25 L2CM Leaf100  b  Bulk Lookup  Encoding   0  20  40  60  80  100  120  64 128 256 512 1024 2048 4096 8192 16384  0  250000  500000  750000  1e 06  1 25e 06  1 5e 06 Time  ms  L2 cache misses Leaf size  log   kB  Time Tree25 Time Tree100 Time Leaf25 Time Leaf100 L2CM Tree25 L2CM Tree100 L2CM Leaf25 L2CM Leaf100  c  Bulk Lookup  Decoding  Figure 7  Performance and memory overhead of the leaf structure strings  Finally  compared to the CSS Tree  Tree  our 16MB leaf structure is again only a little slower when used for decoding  5 2 Ef   ciency of Encoding Indexes This experime</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores"/>
        <doc>Self organizing Tuple Reconstruction in Column stores  ###  Stratos Idreos CWI Amsterdam The Netherlands idreos cwi nl Martin L  Kersten CWI Amsterdam The Netherlands mk cwi nl Stefan Manegold CWI Amsterdam The Netherlands manegold cwi nl ABSTRACT Column stores gained popularity as a promising physical de  sign alternative  Each attribute of a relation is physically stored as a separate column allowing queries to load only the required attributes  The overhead incurred is on the  y tuple reconstruction for multi attribute queries  Each tu  ple reconstruction is a join of two columns based on tuple IDs  making it a signi cant cost component  The ultimate physical design is to have multiple presorted copies of each base table such that tuples are already appropriately orga  nized in multiple di erent orders across the various columns  This requires the ability to predict the workload  idle time to prepare  and infrequent updates  In this paper  we propose a novel design  partial side  ways cracking  that minimizes the tuple reconstruction cost in a self organizing way  It achieves performance similar to using presorted data  but without requiring the heavy initial presorting step itself  Instead  it handles dynamic  unpredictable workloads with no idle time and frequent up  dates  Auxiliary dynamic data structures  called cracker maps  provide a direct mapping between pairs of attributes used together in queries for tuple reconstruction  A map is continuously physically reorganized as an integral part of query evaluation  providing faster and reduced data access for future queries  To enable  exible and self organizing be  havior in storage limited environments  maps are material  ized only partially as demanded by the workload  Each map is a collection of separate chunks that are individually reor  ganized  dropped or recreated as needed  We implemented partial sideways cracking in an open source column store  A detailed experimental analysis demonstrates that it brings signi cant performance bene ts for multi attribute queries  Categories and Subject Descriptors  H 2  DATABASE MANAGEMENT   Physical Design   Systems General Terms  Algorithms  Performance  Design Keywords  Database Cracking  Self organization Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  1  ###  INTRODUCTION A prime feature of column stores is to provide improved performance over row stores in the case that workloads re  quire only a few attributes of wide tables at a time  Each relation R is physically stored as a set of columns  one col  umn for each attribute of R  This way  a query needs to load only the required attributes from each relevant relation  This happens at the expense of requiring explicit  partial  tuple reconstruction in case multiple attributes are required  Each tuple reconstruction is a join between two columns based on tuple IDs positions and becomes a signi cant cost component in column stores especially for multi attribute queries  2  6  10   Whenever possible  position based join  matching and sequential data access are exploited  For each relation Ri in a query plan q  a column store needs to per  form at least Ni  1 tuple reconstruction operations for Ri within q  given that Ni attributes of Ri participate in q  Column stores perform tuple reconstruction in two ways  2   With early tuple reconstruction  the required attributes are glued together as early as possible  i e   while the columns are loaded  leveraging N ary processing to evaluate the query  On the other hand  late tuple reconstruction exploits the column store architecture to its maximum  During query processing   reconstruction  merely refers to getting the at  tribute values of qualifying tuples from their base columns as late as possible  i e   only once an attribute is required in the query plan  This approach allows the query engine to exploit CPU  and cache optimized vector like operator implementa  tions throughout the whole query evaluation  N ary tuples are formed only once the  nal result is delivered  Like most modern column stores  12  4  15   we focus on late reconstruction  Comparing early and late reconstruc  tion  the educative analysis in  2  observes that the latter incurs the overhead of reconstructing a column more than once  in case it occurs more than once in a query  Further  more  exploiting sequential access patterns during recon  struction is not always possible since many operators  joins  group by  order by etc   are not tuple order preserving  The ultimate access pattern is to have multiple copies for each relation R  such that each copy is presorted on an other attribute in R  All tuple reconstructions of R attributes initiated by a restriction on an attribute A can be performed using the copy that is sorted on A  This way  the tuple reconstruction does not only exploit sequential access  but also bene ts from focused access to only a small consecutive area in the base column  as de ned by the restriction on A  rather than scattered access to the whole column  However  such a direction requires the ability to predict the workload and the luxury of idle time to prepare the physical design  In addition  up to date there is no e cient way to maintain multiple sorted copies under updates in a column store  thus it requires read only or infrequently updated environments  In this paper  we propose a self organizing direction that achieves performance similar to using presorted data  but comes without the hefty initial price tag of presorting it  self  Instead  it handles dynamic unpredictable workloads with frequent updates and with no need for idle time  Our approach exploits database cracking  7  8  9  that sets a promising direction towards continuous self organization of data storage based on selections in incoming queries  We introduce a novel design  partial sideways cracking  that provides a self organizing behavior for both selections and tuple reconstructions  It gracefully handles any kind of complex multi attribute query  It uses auxiliary self  organizing data structures to materialize mappings between pairs of attributes used together in queries for tuple recon  struction  Based on the workload  these cracker maps are continuously kept aligned by being physically reorganized  while processing queries  allowing the DBMS to handle tuple reconstruction using cache friendly access patterns  To enhance performance and adaptability  in particular in environments with storage restrictions  cracker maps are im  plemented as dynamic collections of separate chunks  This enables  exible storage management by adaptively main  taining only those chunks of a map that are required to process incoming queries  Chunks adapt individually to the query workload  Each chunk of a map is separately reorga  nized  dropped if extra storage space is needed  or recreated  entirely or in parts  if necessary  We implemented partial sideways cracking on top of an open source column oriented DBMS  MonetDB 1  15   The paper presents an extensive experimental analysis using both synthetic workloads and the TPC H benchmark  It clearly shows that partial sideways cracking brings a self organizing behavior and signi cant bene ts even in the presence of ran  dom workloads  storage restrictions and updates  The remainder of this paper is organized as follows  Sec  tion 2 provides the necessary background  Then  to enhance readability and fully cover the research space  we present partial sideways cracking in two steps  Focusing on the tuple reconstruction problem and neglecting storage restrictions at  rst  Section 3 introduces the basic sideways cracking technique using fully materialized maps  accompanied with an extensive experimental analysis  Then  Section 4 extends the basic approach with  exible storage management using partial maps  Detailed experiments demonstrate the signif  icant bene ts over the initial full materialization approach  Then  Section 5 presents the bene ts of sideways cracking with the TPC H benchmark  Related work is discussed in Section 6  and Section 7 concludes the paper  2  BACKGROUND This section brie y presents the experimentation plat  form  MonetDB  v 5 4   and the basics of database cracking  2 1 A Column oriented DBMS MonetDB is a full  edged column store using late tuple reconstruction  Every relational table is represented as a col  1 Partial sideways cracking is part of the latest release of MonetDB  available via http   monetdb cwi nl  lection of Binary Association Tables  BATs   Each BAT is a set of two columns  For a relation R of k attributes  there exist k BATs  each BAT storing the respective attribute as  key attr  pairs  The system generated key identi es the relational tuple that attribute value attr belongs to  i e   all attribute values of a single tuple are assigned the same key  Key values form a dense ascending sequence representing the position of an attribute value in the column  Thus  for base BATs  the key column typically is a virtual non materialized column  For each relational tuple t of R  all attributes of t are stored in the same position in their respective column representations  The position is determined by the inser  tion order of the tuples  This tuple order alignment across all base columns allows the column oriented system to per  form tuple reconstructions e ciently in the presence of tuple order preserving operators  Basically  the task boils down to a simple merge like sequential scan over two columns  re  sulting in low data access costs through all levels of modern hierarchical memory systems  Let us go through some of the basic operators of MonetDB s two column physical algebra  Operator select A v1 v2  searches all  key attr  pairs in base column A for attribute values between v1 and v2  For each qualifying attribute value  the key value  position  is included in the result  Since selections are mostly performed on base columns  the underlying implementation preserves the key order also in the intermediate results  Operator join j1 j2  performs a join between attr1 of j1 and attr2 of j2  The result contains the qualifying  key1 key2  pairs  In general  this operator can maintain the tuple order only for the outer join input  Similarly  groupby and orderby operators cannot main  tain tuple order for any of their inputs  Operator reconstruct A r  returns all  key attr  pairs of base column A at the positions speci ed by r  If r is the result of a tuple order preserving operator  then iterating over r  it uses cache friendly in order positional lookups into A  Otherwise  it requires expensive random access patterns  2 2 Selection based Cracking Let us now brie y recap selection cracking as introduced in  7   The  rst time an attribute A is required by a query  a copy of column A is created  called the cracker column CA of A  Each selection operator on A triggers a range  based physical reorganization of CA based on the selection of the current query  Each cracker column  has a cracker index  AVL tree  to maintain partitioning information  Fu  ture queries bene t from the physically clustered data and do not need to access the whole column  Cracking continues with every query  Thus  the system continuously re nes its  knowledge  about how values are spread in CA  Physical reorganization happens on CA while the original column is left as is  i e   tuples are ordered according to their insertion sequence  This order is exploited for tuple reconstruction  The operator crackers select A v1 v2  replaces the orig  inal select operator  First  it creates CA if it does not exist  It searches the index of CA for the area where v1 and v2 fall  If the bounds do not exist  i e   no query used them in the past  then CA is physically reorganized to cluster all qualifying tuples into a contiguous area  The result is again a set of keys positions  However  due to physical reorganization  cracker columns are no longer aligned with base columns and consequently the selection results are no longer ordered according to the tuple inser tion sequence  For queries with multiple selections  we need to perform the intersection of individual selection results on unordered  key value  sets  Without either sequential access or positional lookups  performance worsens signi cantly for tuple reconstruction  Thus  for conjunctive queries   7  uses crackers select only for the  rst selection  introducing the crackers rel select for all subsequent selections  It per  forms the tasks of select and reconstruct in one go  The terminology  database cracking  re ects the fact that the database is conceptually cracked into pieces  It aims at unpredictable dynamic environments  An extensive discus  sion on the pros and cons of cracking against traditional in  dices can be found in  7  8   In  8   cracking has been shown to also maintain its properties under high volume updates  3  SIDEWAYS CRACKING In this section  we introduce the basic sideways cracking technique using fully materialized maps  To motivate and il  lustrate the e ect of our choices  we build up the architecture starting with simple examples before continuing with more  exible and complex ones  The section closes with an exper  imental analysis of sideways cracking with full maps against the selection cracking and non cracking approaches  The addition of adaptive storage management through partial sideways cracking is discussed and evaluated in Section 4  Section 5 shows the bene ts on the TPC H benchmark  3 1 Basic De   nitions We de ne a cracker map MAB as a two column table over two attributes A and B of a relation R  Values of A are stored in the left column  while values of B are stored in the right column  called head and tail  respectively  Values of A and B in the same position of MAB belong to the same relational tuple  All maps that have been created using A as head are collected in the map set SA of R A  Maps are created on demand  only  For example  when a query q needs access to attribute B based on a restriction on attribute A and MAB does not exist  then q will create it by performing a scan over base columns A and B  For each cracker map MAB  there is a cracker index  AVL tree  that maintains information about how A values are distributed over MAB  Once a map MAB is created by a query q  it is used to evaluate q and it stays alive to speed up data access in future queries that need to access B based on A  Each such query triggers physical reorganization  cracking  of MAB based on the restriction applied to A  Reorganization happens in such a way  that all tuples with values of A that qualify the re  striction  are in a contiguous area in MAB  We use the two algorithms of  7  to physically reorganize maps by splitting a piece of a map into two or three new pieces  We introduce sideways select A v1 v2 B  as a new se  lection operator that returns tuples of attribute B of relation R based on a predicate on attribute A of R as follows   1  If there is no cracker map MAB  then create one   2  Search the index of MAB to  nd the contiguous area w of the pieces related to the restriction   on A  If   does not match existing piece boundaries   3  Physically reorganize w to move false hits out of the contiguous area of qualifying tuples   4  Update the cracker index of MAB accordingly   5  Return a non materialized view of the tail of w  Example  Assume a relation R A  B  shown in Figure 1  The  rst query requests values of B where a restriction on A 12 3 5 9 15 22 7 26 4 2 24 11 16 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b11 b12 b13 A B select B from R where 10 A 15 4 3 5 9 2 7 12 11 15 22 24 26 16 b9 b2 b3 b4 b10 b7 b1 b12 b5 b6 b11 b8 b13 AB Position 1 Piece 1 value   10 Position 7 Piece 2 value  10 Position 9 Piece 3 value   15 select B from R where 5  A 17 4 3 2 9 5 7 12 11 15 16 24 26 22 b9 b2 b10 b4 b3 b7 b1 b12 b5 b13 b11 b8 b6 Position 1 Piece 1 value  5 Position 7 Piece 3 value  10 Position 9 Piece 4 value   15 Position 4 Piece 2 value   5 Position 11 Piece 5 value   17 Cracker index M Cracker index MAB Initial state Figure 1  A simple example holds  The system creates map MAB and cracks it into three pieces based on the selection predicate  Via cracking the qualifying B values are already clustered together aligned with the qualifying A values  Thus  no explicit join like operation is needed for tuple reconstruction  the tail column of the middle piece forms the query s result  Then  a similar second query arrives  From the index  we derive that  1  the entire middle piece belongs to the result  and hence   2  only Pieces 1 and 3 must be analyzed and further cracked  As more queries are being processed  the system  learns    purely based on incoming queries   more about how data is clustered  and hence  can reduce data access  3 2 Multi projection Queries Let us now discuss queries with multiple tuple reconstruc  tion operations  We start with queries over a single selec  tion and multiple projections  Then  Section 3 3 discusses queries with multiple selections  The Problem  Non aligned Cracker Maps  A single  selection query q that projects n attributes requires n maps  one for each attribute to be projected  These maps MAx be  long to the same map set SA  However  a naive use of the maps can lead to incorrect query results  Consider the ex  ample depicted in the upper part of Figure 2  The  rst query triggers the creation of MAB and it physically reorganizes it based on A   3  Similarly  the second query triggers the creation of MAC and cracks it according to A 5  Then  the third query needs both MAB and MAC  Re ning both maps according to A   4 of the third query creates correct con  secutive results for each map individually  However  since these maps had previously been cracked independently using di erent restrictions on A  the result tuples are not position  ally aligned anymore  prohibiting e cient positional result tuple reconstruction  Maintaining the tuple identity explic  itly by adding a key column to the maps is not an e cient solution  either  It increases the storage requirements and allows only expensive join like tuple reconstruction requiring random access due to non aligned maps  The Solution  Adaptive Alignment  To overcome this problem  we extend the sideways select operator with an alignment step that adaptively and on demand restores the alignment of all maps used in a query plan  The basic idea is to apply all physical reorganizations  due to selections on an attribute A  in the same order to all maps in SA  Due to the deterministic behavior of the cracking algorithms  7   this approach ensures alignment of the respective maps  Obviously  in an unpredictable environment with no idle system time  we want to invest in this extra work only if it7 4 1 2 8 3 6 B b1 b2 b3 b4 b5 b6 b7 A c1 c2 c3 c4 c5 c6 c7 C select B from R where A 3 select C from R where A 5 select B C from R where A 4 2 1 4 7 8 3 6 b4 b3 b2 b1 b5 b6 b7 v 3 v  3 3 4 1 2 8 7 6 c6 c2 c3 c4 c5 c1 c7 v 5 v  5 2 1 3 7 8 4 6 M b4 b3 b6 b1 b5 b2 b7 v 3 v  4 v  3 3 2 1 4 8 7 6 c6 c4 c3 c2 c5 c1 c7 v 4 v  5 v  4 Result b4 b3 Result c6 c2 c3 c4 Result b4 b3 b6 c6 c4 c3 Initial state Without  Alignment 2 1 4 7 8 3 6 b4 b3 b2 b1 b5 b6 b7 v 3 v  3 b4 b3 Result With Alignment 2 1 4 3 8 7 6 c4 c3 c2 c6 c5 c1 c7 v 3 v  5 Result c4 c3 c2 c6 2 1 4 7 8 3 6 c4 c3 c2 c1 c5 c6 c7 v 3 v  3 Crack A 5 v  3 2 1 4 3 8 7 6 b4 b3 b2 b6 b5 b1 b7 v 3 v  5 v  3 2 1 3 4 8 7 6 b4 b3 b6 b2 b5 b1 b7 v 3 v  5 v  3 v  4 2 1 3 4 8 7 6 c4 c3 c6 c2 c5 c1 c7 v 3 v  5 v  3 v  4 Result b4 b3 b6 c4 c3 c6 Wrong  allignment Correct  allignment Crack A 3 Crack A 3 Crack A 5 Crack A 3 Crack A 5 Crack A 4 Crack A 4 MAB MAC AB MAC MAB MAC MAC MAB MAB MAC Crack A 4 Crack A 4 Figure 2  Multiple tuple reconstructions in multi projection queries pays back  i e   only once a map is required  In fact  per  forming alignment on line is not an option  On line align  ment would mean that every time we crack a map  we also forward this cracking to the rest of the maps in its set  This is prohibitive for several reasons  First  in order to be able to align all maps in one go we need to actually materialize and maintain all possible maps of a set  even the ones that the actual workload does not require  Most importantly ev  ery query would have to touch all maps of a set  i e   all attributes of the given relation  This immediately overshad  ows the bene t of using a column store in touching only the relevant attributes every time  The overhead of hav  ing adaptive alignment is that each map MAx in a set SA needs to materialize the head attribute A so that MAx can be cracked independently  We will remove this restriction with partial sideways cracking in the next section  To achieve adaptive alignment  we introduce a cracker tape TA for each set SA  which logs  in order of their occur  rence  all selections on attribute A that trigger cracking of any map in SA  Each map MAx is equipped with a cursor pointing to the entry in TA that represents the last crack on MAx  Given a tape TA  a map MAx is aligned  synchro  nized  by successively forwarding its cursor towards the end of TA and incrementally cracking MAx according to all se  lections it passes on its way  All maps whose cursors point to the same position in TA  are physically aligned  To ensure that alignment is performed on demand only  we integrate it into query processing  When a query q needs a map M  then and only then  q aligns M  We further extend the sideways select A v1 v2 B  operator with three new steps that maintain and use the cracker tapes as follows   1  If there is no TA  then create an empty one   2  If there is no cracker map MAB  then create one   3  Align MAB using TA   4  Search the index of MAB to  nd the contiguous area w of the pieces related to the restriction   on A  If   does not match existing piece boundaries   5  Physically reorganize w to move false hits out of the contiguous area of qualifying tuples   6  Update the cracker index of MAB accordingly   7  Append predicate v1  A v2 to TA   8  Return a non materialized view of the tail of w  For a query with one selection and k projections  the query plan contains k sideways select operators  one for each projection attribute  For example  assume a query that se  lects on A and projects B and C  Then  one sideways select operator will operate over MAB and another over MAC  With the maps aligned and holding the projection attributes in the tails  the result is readily available  The bottom part of Figure 2 demonstrates how queries are evaluated using aligned maps yielding the correctly aligned result  Sideways cracking performs tuple reconstruction by ef   ciently maintaining aligned maps via cracking instead of using  random access  position based joins  The alignment step follows the self organizing nature of a cracking DBMS  Aligning a map M becomes less expensive the more queries use M  as incremental cracking successively reduces the size of pieces and hence the data that needs to be accessed  Moreover  the more frequently M is used  the fewer alignment steps are required per query to bring it up  to date  Unused maps do not produce any processing costs  3 3 Multi selection Queries The  nal step is to generalize sideways cracking for queries that select over multiple attributes  One approach is to cre  ate wider maps that include multiple attributes in di er  ent orderings  However  the many combinations  orderings and predicates in queries lead to huge storage and main  tenance requirements  Furthermore  wider maps are not compatible with the cracker algorithms of  7  and the up  date techniques of  8   Instead  we propose a solution that exploits aligned two column maps in a way that enables ef   cient cache friendly operations  The Problem  Non aligned Map Sets  Let us  rst consider conjunctive queries  e g   the query of Figure 3  A query plan could use maps MAD  MBD and MCD  These maps belong to di erent sets SA  SB and SC  respectively  However  the alignment techniques presented before apply only to multiple maps within the same set  Keeping maps of di erent sets aligned is not possible at all  as each attribute requires determines its own individual order for its maps  Thus  using the above map sets for the example query inher  ently yields non aligned individual selection results requiring expensive operations for subsequent tuple reconstructions  The Solution  Use a Single Aligned Set  The chal  lenge for multi selections is to  nd a solution that uses maps of only one single set  and thus can exploit their alignment  We postpone the discussion about how to choose this one set till later in this section  To sketch our approach using the query of Figure 3 as example  we arbitrarily assume that set SA is chosen  i e   we use maps MAB  MAC and MAD  Each map is  rst aligned to the most recent crack opera 12 3 5 9 8 22 7 26 4 2 7 9 2 6 10 7 11 16 2 5 8 3 A B select create bv A 3 10 B 4 8  2 3 5 9 8 7 7 4 26 12 22 8 2 6 10 7 3 16 5 2 9 11 MAB v  3 v 3 v  10 1 0 1 0 0 1 Bit  vector bv 3 6 2 1 6 9 12 2 11 17 3 C select re   ne bv A 3 10 C 1 7 bv  2 3 5 9 8 7 7 4 26 12 22 17 6 2 1 6 3 12 11 2 3 9 v  3 v 3 v  10 1 0 1 0 0 0 Bit  vector bv 9 4 2 10 12 19 3 6 5 8 1 D reconstruct A 3 10 D bv  2 3 5 9 8 7 7 4 26 12 22 8 4 2 10 12 1 3 5 6 9 19 v  3 v 3 v  10 2 12  Result MAC MAD select D from R   where 3 A 10 and  4 B 8 and 1 C 7 Query Initial state 1 0 1 0 0 0 Bit  vector bv Figure 3  Multiple tuple reconstructions in multi selection queries tion on A and only then it is cracked given the current predi  cate on A  Given the conjunctive predicate  we know that we just created contiguous areas wB  wC and wD aligned across the involved maps that contain all result candidates  These areas are aligned since all maps were  rst aligned and then cracked based on the same predicate  Thus  all areas have also the same size k  To  lter out the  false candidates  that ful ll the predicate on A  but not all other predicates  we use bit vector processing  X100  4  and the study of  2  also exploit bit vectors for  ltering multiple predicates   Using a single bit vector of size k  if a tuple ful lls the predicate on B  the respective bit is set  otherwise cleared  Successively iterating over the aligned result areas in the remaining maps  wC in our example   the bits of tuples that do not ful ll the respective predicate are cleared  Finally  the bit vector in  dicates which wD tuples form the result  An example in Figure 3 illustrates the details using the following three new operators  sideways select create bv A v1 v2 B v3 v4   1 7  Equal to sideways select in Section 3 2   8  Create and return bit vector bv for w with v3  B  v4  sideways select refine bv A v1 v2 B v3 v4 bv   1 7  Equal to sideways select in Section 3 2   8  Re ne bit vector bv with v3  B  v4 and return bv  sideways reconstruct A v1 v2 B bv   1 7  Equal to sideways select in Section 3 2   8  Create and return a result that contains the tail value of all tuples from w in MAB whose bit is set in bv  Given the alignment of the maps and the bit vector  only positional lookups and sequential access patterns are in  volved  In addition  by clustering and aligning relevant data via cracking  the system needs to analyze only a small por  tion of the involved columns  equal to the size of the bit vector  for selections and tuple reconstructions  Map Set Choice  Self organizing Histograms  The remaining issue is to determine the appropriate map set  Our approach is based on the core of the  cracking philos  ophy   i e   in an unpredictable environment with no idle system time  always perform the minimum investment  Do just enough operations to boost the current query  Do not invest in the future unless the bene t is known  or there is the luxury of time and resources to do so  In this way  for a query q  a set SA is chosen such that the restriction on A is the most selective in q  yielding a minimal bit vector size in order to load and analyze less data in this query plan  The most selective restriction can be found using the cracker indices  for they maintain knowledge about how values are spread over a map  The size of the various pieces gives the exact number of tuples in a given value range  E ec  tively  we can view a cracker index as a self organizing his  togram  In order to estimate the result size of a selection over an attribute A  any available map in SA can be used  In case of alternatives  the most aligned map is chosen by looking at the distance of its cursor to the last position of TA  The bigger this distance  the less aligned a map is  A more aligned cracked map can lead to a more accurate es  timation  Using the cracker index of the chosen map MAx  we locate the contiguous area w that contains the result tuples  In case the predicate on A matches with the bound  aries of existing pieces in MAx  the result size is equal to the size jwj of w  Otherwise  we assume that w consists of n pieces W1          Wn  and derive jwj   Pn i 1 jWij and jw 0 j   Pn1 i 2 jWij as upper and lower bounds respectively  We can further tighten these bounds by estimating the qual  ifying tuples in W1 and Wn  e g   using interpolation  Disjunctive Queries  Disjunctive queries are handled in a symmetrical way  This time the  rst selection creates a bit vector with size equal to the size of the map and not to the size of the cracked area w  as with conjunctions   The rest of the selections need to analyze the areas outside w for any unmarked tuples that might qualify and re ne the bit vector accordingly  The choice of the map set is again symmetric  we choose a set based on the least selective attribute  In this way  the areas that need to be analyzed outside the cracked area are as small as possible  3 4 Complex Queries Until now we studied multi selections projections queries  The rest of the operators are not a ected by the physical reorganization step of cracking as no other operator  other than tuple reconstruction  depends on tuple insertion order  Thus  joins  aggregations  groupings etc  are all performed e ciently using the original column store operators  e g   see our experimental analysis   Potentially  many operators can exploit the clustering information in the maps  e g   a max can consider only the last piece of a map or a join can be performed in a partitioned like way exploiting disjoint ranges in the input maps  We leave such directions for future work consideration as they go beyond the scope of this paper  3 5 Updates Update algorithms for a cracking DBMS have been pro  posed and analyzed in detail in  8   An update is not applied immediately  Instead  it remains as a pending update and it is applied only when a query needs the relevant data as  sisting the self organizing behavior  This way  updates are applied while processing queries and a ect only those tuples relevant to the query at hand  For each cracker column  there exist a pending insertions and a pending deletions col  umn  An update is merely translated into a deletion and an insertion  Updates are applied merged in a cracker column without destroying the knowledge of its cracker index which o ers continual reduced data access after an update  Sideways cracking is compatible with  8  as follows  Each map MAB has a pending insertions table holding  A B  pairs  Insertions are handled independently and on demand for each map using the Ripple algorithm  8   The extension is that the  rst time an insertion is applied on a map of set SA  it is also logged in tape TA so that the rest of the SA maps can apply the insertions in the correct order during alignment  For deletions we only need one pending dele  tions column for each set SA as we only need  A key  pairs to identify a deletion  Since maps do not contain the tuple keys  as cracker columns do  we maintain a map MAkey for each set SA  This map  when aligned and combined with the pending deletions column  gives the positions of the rel  evant deletes for the current query in the currently aligned maps  The Ripple algorithm  8  is used to move deletes out of the result area of the maps used in a plan  3 6 Experimental Analysis In this section  we present a detailed experimental analy  sis  We compare our implementation of selection and side  ways cracking on top of MonetDB  against the latest non  cracking version of MonetDB and against MonetDB on pre  sorted data  We use a 2 4 GHz AMD Athlon 64 processor equipped with 2 GB RAM  The operating system is Fedora Core 8  Linux 2 6 23   Unless mentioned otherwise  all ex  periments use a relational table of 9 attributes  A1 to A9   each containing 10 7 randomly distributed integers in  1  10 7    Exp1  Varying Tuple Reconstructions  The  rst experiment demonstrates the behavior in query plans with one selection  but with multiple tuple reconstructions   q1  select max  A2   max  A3      from R where v1  A1 v2 We test with queries with 2 to 8 attributes in the select clause  For each case we run 100 queries requesting random ranges of 20  of the tuples  Figure 4 a  shows the results for the 100th query  full query sequence behavior is shown in next experiments   The structures in both cracking ap  proaches have been reorganized by the previous 99 queries  For all systems  increasing the number of tuple reconstruc  tions increases the overall cost while presorted MonetDB and sideways cracking signi cantly outperform the others  Presorted Sid  Cracking Sel  Cracking MonetDB Tot TR Sel Tot TR Sel Tot TR Sel Tot TR Sel 43 0 2 0 02 47 0 4 0 5 771 725 0 3 483 211 229 The above table breakes down the cost  in milli secs  for the case of 8 tuple reconstructions  It shows the contribu  tion of tuple reconstruction  TR  and selection  Sel  to the total cost  Tot   For presorted data  the table is already sorted on the selection attribute  Naturally  selections hap  pen very fast  using binary search   Tuple reconstructions are also extremely fast since the projection attributes are al  ready aligned with the selection result  given that only tuple order preserving operators are involved in these queries  we show more complex examples later on   Sideways cracking achieves similar performance to presorted data by contin  uously aligning and physically clustering relevant data to  gether both for selections and for tuple reconstructions  On the contrary  selection cracking improves over Mon  etDB signi cantly on selections but su ers from tuple re   0  200  400  600  800 2 4 8 Response time  milli secs    of tuple reconstructions  a  Exp1  Multiple tuple reconstructions Presorted MonetDB Sideways Cracking MonetDB Selection Cracking  1 5  0 01  0 1  1  1 10 100 Response time relative to MonetDB Query sequence  b  Exp2  Varying selectivity 1000 3 MonetDB Sid  point Sid  10  Sid  30  Sid  50  Sid  70  Sid  90  Figure 4  Improving tuple reconstruction construction costs  With MonetDB  the select operator is order preserving  hence  tuple reconstruction is performed using in order positional key lookups into the projection at  tribute s base column  The resulting sequential access pat  tern is very cache friendly ensuring that each page or cache  line is loaded at most once  On the contrary  with selection cracking  the result of crackers select is no longer aligned with the base columns due to physical reorganization  Con  sequently  the tuple reconstruction is performed using ran  domly ordered positional key lookups into the base column  Lacking both spatial and temporal locality  the random ac  cess pattern causes signi cantly more cache  page misses  making tuple reconstruction more expensive  Exp2  Varying Selectivity  We repeat the previous experiment for 2 tuple reconstructions  but this time we vary selectivity factors from point queries up to 90  se  lectivity  We run 10 3 queries selecting randomly located ranges points  Figure 4 b  shows the response time relative to the performance of non cracking MonetDB  per selec  tivity factor   Sideways cracking signi cantly outperforms MonetDB on all selectivity ranges  In general  the  rst query is slightly slower for sideways cracking since the appropriate maps are created  After only a few queries  the maps are physically reorganized to an extent that signi cantly fewer non qualifying tuples have to be accessed  allowing side  ways cracking to quickly outperform MonetDB  As queries become less selective  sideways cracking outperforms Mon  etDB sooner  in terms of processed queries   With less se  lective queries  more tuples have to be reconstructed pro  ducing higher costs for the non cracking column store  For the same reason  with more selective queries  the relative bene t of cracking is smaller for the initial queries as the  smaller  bene ts in tuple reconstruction are being partially shadowed by the higher cracking costs of the  rst queries   0  1  2 1 2 4 8 TR cost  seconds    Tuple reconstructions  TR  plain MonetDB  ord  TR  Sel Cracking  unord  TR  sort   ordered TR radix cluster   clust  TR Exp3  Reordering  A natural direction to improve tuple reconstruction with se  lection cracking is to reorder unordered intermediate results  For the graph on the left  we use both sorting and a cache  friendly radix clustering algo  rithm  10  that restricts random access during reconstruction to the cache  This achieves similar 0  500  1000  1500  2000  2500  1 10 100 Response time  milli secs  Query sequence Exp4      a  Total Cost Selection Cracking MonetDB Sideways Cracking Presorted MonetDB Presorting cost 12 secs  0  200  400  600  800  1000  1 10 100 Query sequence  b  Select and TR cost before join MonetDB Selection Cracking Sideways Cracking Presorted MonetDB  0  200  400  600  800  1000  1 10 100 Query sequence  c  TR cost after join Selection Cracking MonetDB Sideways Cracking Presorted MonetDB Figure 5  Join queries with multiple selections and tuple reconstructions  TR  1e 03 1e 04 1e 05 1e 06  1 10 100 Response time  micro secs  Query sequence 1000 MonetDB Selection Cracking Sideways Cracking Presorted MonetDB Presorting cost 3 5 secs Figure 6  Skewed workload reconstruction performance as purely sequential access at a lower investment than sorting  We see that the investment in clustering  sorting  pays o  with 4  8  or more projec  tions  In this way  reordering intermediate results pays o  when multiple projections share a single intermediate result  However  it is not bene cial with only a few projections or with multiple selections  where individual intermediate re  sults prohibit the sharing of reordering investments  Also  as seen in Figure 4 presorted MonetDB and sideways cracking signi cantly outperform plain MonetDB even with just a few tuple reconstructions by having columns already aligned  Exp4  Join Queries  We proceed with join queries that both select and project over multiple attributes  Two tables of 7 attributes and the following query are used   q2  select max  R1  max  R2  max  S1  max  S2  from R S where v1  R3 v2 and v3  R4 v4 and v5  R5 v6 and k1  S3 k2 and k3  S4 k4 and k5  S5 k6 and R7   S7 We run 10 2 randomly created queries  with  xed selec  tivity factors of 50   30  and 20  for the conjunctions of each table  Figure 5 a  shows the results  All systems evaluate the queries starting from the most selective pred  icate  Sideways cracking and presorted MonetDB achieve similar performance and signi cantly outperform the other approaches  Presorting of course has a high preparation cost  12 secs   Figure 5 b  shows separately the selections and tuple reconstruction costs before the join  For a fair compar  ison both MonetDB and MonetDB on presorted data use the faster rel select operator of selection cracking for the tu  ple reconstructions prior to the join  Figure 5 c  also shows separately the tuple reconstruction costs after the join  For both cost components  presorted data and sideways cracking signi cantly improve the column store performance by providing both very fast selections due to value ranges knowledge  but also very fast tuple reconstructions due to more e cient access patterns  Qualifying data is already aligned and clustered in smaller areas  i e   equal to the re  sult size of the most selective predicate  On the contrary  MonetDB and selection cracking have to reconstruct the re  quired attributes from the full base columns  Selection cracking loses its advantage in selections due to random access patterns in tuple reconstruction  even before the join  Also  the order of tuple keys in cracker columns becomes more distorted  as more queries contribute to crack  ing  resulting in further increasing reconstruction costs  For all systems  tuple order in the intermediate result of the inner input is lost after the join  Thus  all systems per  1e 05 1e 06  1 10 100 1000 10000 Response time  micro secs  Query sequence  Exp6     a  LFHV scenario Non cracking MonetDB Selection Cracking Sideways Cracking  1 10 100 1000 10000 Query sequence  b  HFLV scenario Non cracking MonetDB Selection Cracking Sideways Cracking Figure 7  E ect of updates form tuple reconstruction for this table using random access patterns  However  plain MonetDB and selection cracking need to prompt the base columns as the tuples to be re  constructed for each attribute A are scattered through the whole base column of A  On the other hand  MonetDB on presorted data and sideways cracking have the qualifying data clustered in a smaller area in each column and thus can improve signi cantly by loading and analyzing less data  Naturally  more selections or more tuple reconstructions  either before or after the join   further increase the bene ts of presorted data and sideways cracking  Exp5  Skewed Workload  Sideways cracking grace  fully adapts to the workload and exploits any opportunity to improve performance  To illustrate this powerful property  assume a 3 attribute table and the following query type   q3  select max  B  max  C  from R where v1  A v2 We choose v1 and v2 such that 9 10 queries request a ran  dom range from the  rst half of the attribute value domain  while only 1 10 queries request a random range from the rest of the domain  All queries request 20  of the tuples  Figure 6 shows that sideways cracking achieves high per  formance similar to presorted data by always using cache  friendly patterns to reconstruct tuples  Skew a ects the  learning  rate of sideways cracking  making it reach the best performance quickly for the hot set  Since most of the queries focus on a restricted area of the maps  cracking can analyze this area faster  in terms of query requests  and break it down to smaller pieces  which are faster to process   With queries outside the hot set  we have to analyze a larger area  though not the whole column   This is why we see the peaks roughly every 10 queries  However  as more queries1 5 8 1 9 6 1 1 2 1 4 5 1 2 1 8 4 9 1 3 7 b 1 b 2 b 3 b 4 b 5 b 6 b 7 b 8 b 9 b 1 0 b 1 1 b 1 2 b 1 3 b 1 4 A B Initial state Select B from R whe re 9 A  15 Chunk map  H    A id  v  9 U v 9 F v 15 U v  9 E v 9 M C 1 v 15 E Partial map M Select B from R whe re 9 A 13 v  9  E v 9 M C 1 v 15   E v  13 M C 1 Select B from R wher e 5  A 8 v 5  U v 9  F v 15  U v 15   E v 9 M C 1 v  13 M C 1 v  5 F v  8  U v 5  E v  5 M C 2 v  8  E 7 8 9 6 4 2 5 1 4 1 2 1 3 1 5 1 1 1 9 1 8 1 4 2 1 2 4 1 1 6 8 7 9 1 3 1 5 3 1 0 1 4 1 2 1 3 1 5 1 1 b 7 b 9 b 1 3 b 1 b 5 1 1 1 2 1 3 1 5 1 4 b 5 b 9 b 1 3 b 1 b 7 4 2 7 6 5 9 8 1 4 1 2 1 3 1 5 1 1 1 9 1 8 1 1 6 1 4 1 1 8 1 2 2 7 9 1 3 1 5 3 1 0 1 1 1 2 1 3 1 5 1 4 b 5 b 9 b 1 3 b 1 b 7 7 6 5 b 1 4 b 1 1 b 8 c 1 c 2 c 3 c 4 c 5 c 6 c 7 c 8 c 9 c 10 c 11 c 12 c 13 c 14 C Select C from R whe re 8  A 15 v 5  U v 9  F v 15  U v 15   E v 9 M C 1 v  13 M C 1 v  5 F v  8  F v 8  E v  8 M C 3 4 2 7 6 5 9 8 1 4 1 2 1 3 1 5 1 1 1 9 1 8 1 1 6 1 4 1 1 8 1 2 2 7 9 1 3 1 5 3 1 0 1 1 1 2 1 3 1 4 1 5 c 5 c 9 c 13 c 7 c 1 9 8 c 12 c 2 v 15 M C 1 A AB Partial map MAB Chunk map  H     A A id  Partial map MAB Chunk map  H     A A id  Partial map MAC 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 Keys Figure 8  Using partial maps  U Unfetched  F Fetched  E Empty  M Materialized  C ChunkID  touch the non hot area  in a self organizing way  sideways cracking improves performance also for the non hot set  Exp6  Updates  Two scenarios are considered for up  dates   a  the high frequency low volume scenario  HFLV   every 10 queries we get 10 random updates and  b  the low frequency high volume scenario  LFHV   every 10 3 queries we get 10 3 random updates  Random q3 queries are used  Figure 7 shows that sideways cracking maintains high per  formance and a self organizing behavior through the whole sequence of queries and updates demonstrating similar per  formance as in  8   We do not run on presorted data  here  since to the best of our knowledge there is no e cient way to maintain multiple sorted copies under frequent updates in column stores  6   This is an open research problem  Ob  viously  resorting all copies with every update is prohibitive  4  PARTIAL SIDEWAYS CRACKING The previous section demonstrated that sideways cracking enables a column store to e ciently handle multi attribute queries  It achieves similar performance to presorted data but without the heavy initial cost and the restrictions on up  dates and workload prediction  So far  we assumed that no storage restrictions apply  As any other indexing or caching mechanism  sideways cracking imposes a storage overhead  This section addresses this issue via partial sideways crack  ing  An extensive experimental analysis shows that it signif  icantly improves performance under storage restrictions and enables e cient workload adaptation by partial alignment  4 1 Partial Maps The motivation for partial maps comes from a divide and conquer approach  The main concepts are the following   1  Maps are only partially materialized driven by the workload   2  A map consists of several chunks   3  Each chunk is a separate two column table and  4  contains a given value range of the head attribute of this map   5  Each chunk is treated independently  i e   it is cracked separately and it has its own tape  Figure 8 illustrates a simpli ed example  Basic De nitions  A map set SA of an attribute A con  sists of  a  a collection of partial maps and  b  a chunk map HA  HA contains A values along with the respective tu  ple key  Its role is to provide the partial maps of SA with any missing chunks when necessary  Each partial and chunk map has an AVL tree based index to maintain partitioning information  Di erent maps in the same set do not necessar  ily hold chunks for the same value ranges  A partial map is created when a query needs it for the  rst time  The chunk map for a set S  is created along with the creation of the  rst chunk of the  rst partial map in S  An area w of a chunk map is de ned as fetched if at least one partial map has fetched all tuples of w to create a new chunk  Otherwise  w is called unfetched  Similarly  an area c of a partial map is de ned as materialized if this map has created a chunk for c  Otherwise  c is called empty  Figure 8 shows some simple examples  For each fetched area w  the index of a chunk map main  tains  i  a list of references to w  i e   the IDs of the partial maps that currently hold a chunk created by fetching w  and  ii  a tape where all the cracks that happen on the chunks created by w are logged  If all these chunks are dropped  discussed below under  Storage Management    then w is marked again as unfetched and its tape is removed  Creating Chunks  New chunks for a map MAx are cre  ated on demand  i e   each time a query q needs tuples from an empty area c of MAx  The area c corresponds to an area w of HA  We distinguish two cases depending on whether w is fetched or not  Firstly  if w is unfetched  then currently no other map in SA holds any chunks created from w  In this case  depending on the value range that q requires  we either make a new chunk using all tuples of w or crack w in smaller areas to materialize only the relevant area  see ex  amples in Figure 8   Secondly  in case w is already marked as fetched  it must not be cracked further  as this might lead to incorrect alignment as described in Section 3 2  For ex  ample  if multiple maps are used by a single query q that requires chunks created from an area w  then these chunks will not be aligned if created by di erently cracked instances of w  Hence  a new chunk is created using all tuples in w  To actually create a new chunk for a map MAB  we use the keys stored in w to get the B values from B s base column  Storage Management  A partial map is an auxiliary data structure  i e   without loss of primary information  any chunk of any map can be dropped at any time  if storage space is needed  e g   for new chunks  In the current imple  mentation  chunks are dropped based on how often queries access them  After a chunk is dropped  it can be recreated at any time  as a whole or only in parts  if the query work  load requires it  This is a completely self organizing behav  ior  Assuming there is no idle time in between  no available storage  and no way to predict the future workload  this ap  proach assures that the maximum available storage space is exploited  and that the system always keeps the chunks that are really necessary for the workload hot set  Before creating a new chunk  the system checks if there is su cient storage available  If not  enough chunks aredropped to make room for the new one  Dropping a chunk c involves operations to update the corresponding cracker index I  To assist the learning behavior lazy deletion is used  i e   all nodes of I that refer to c are not removed but merely marked as deleted and hence can be reused when c  or parts of it  is recreated in the future  Dropping the Head Column  The storage overhead is further reduced by dropping the head column of actively used chunks  at the expense of loosing the ability of further cracking these chunks  We consider two opportunities  First  we drop the head column of chunks that have been cracked to an extend that each piece  ts into the CPU cache  In case a future query requires further cracking of such pieces  it is cheap to sort a piece within the CPU cache  This action is then logged in the tape to ensure future align  ment with the corresponding chunks of other maps  Second  we drop the head column of chunks that have not been cracked recently as queries use their pieces  as is   Once we need to crack such a chunk c in the future  we only need to recover and align the head as follows  If a chunk c 0  of the same area as c  in an other map still holds the head and is less or equally aligned to the state that c was when it lost its head  then the head is recovered from c 0   Otherwise  the head is taken from the chunk map  The  rst case is cheaper as less e ort is needed to align the head  Chunk wise Processing  As seen in Section 3 3  each sideways cracking operator O  rst collects  using proper cracking and alignment  in a contiguous area w of the used map M all qualifying tuples based on the head attribute of M  Then  it runs the speci c operator O over the area w  e g   create or re ne a bit vector based on a conjunctive predicate  perform a projection  etc  With partial sideways cracking we have the opportunity to improve access patterns even more by allowing chunk wise processing  Each opera  tor O handles one chunk c of a map at a time  i e   load c  create c if the corresponding area is empty  crack or align c if necessary and  nally run O over c  Partial Alignment  Partial maps allow signi cant opti  mizations during alignment  The key observation is that we do not always need to perform full alignment  i e   align a chunk c up to the last entry of its tape  If c is not going to be cracked  it only needs to be aligned with respect to the corresponding chunks of the other maps of the same map set used in this query  i e   up to the maximum cursor of these chunks  We call this partial alignment  When performing any operator over a map  only the boundary chunks might need to be cracked  i e   the  rst chunk where the lower bound falls and the last chunk where the upper bound falls  all other chunks in between bene t from partial alignment  Even for boundary chunks  partial sideways cracking can in many cases avoid full alignment as follows  Assume a chunk c as a candidate for cracking based on a bound b  First  we perform partial alignment on c and monitor the alignment bounds  If b matches one of the past cracks  then cracking and thus full alignment of c is not necessary  Other  wise  full alignment starts  However  even then  if b is found on the way  then alignment stops and c is not cracked  Updates  The Ripple algorithm of  8  is already de  signed to update only the parts  value ranges  necessary for the running query  Thus  the update strategy and perfor  mance in partial maps remains the same as in Section 3  Chunk maps are treated in the same way  i e   a chunk map Hx has its own pending updates structures and areas on Hx are updated only on demand  Thus  before making a new chunk from an unfetched area w  w is updated if necessary  Naturally  updates applied in a chunk map are also removed from the pending updates of all partial maps in this set  4 2 Experimental Analysis We proceed with a detailed assessment of our partial side  ways cracking implementation on top of MonetDB  Using the same platform as in Section 3 6  we show that partial maps bring a signi cant improvement and a self organizing behavior under storage restrictions and during alignment  For storage management with full maps  we use the same approach as for partial maps  i e   existing maps are only dropped if there is not su cient storage for newly requested maps  We always drop the least frequently accessed map s   For the experiments throughout this section we use a re  lation with 11 attributes containing 10 6 tuples and 5 multi  attribute queries  Qi  i 2 f1          5g  of the following form   Qi  select Ci from R where v1  A v2 and v3  Bi  v4 All queries use the same A attribute but di erent Bi and Ci attributes  i e   each query requires two di erent maps  A fully materialized map needs 10 6 tuples  All queries select random ranges of S tuples  We run 10 3 queries in batches of 100 per type  i e    rst 100 Q1 queries  then 100 Q2 queries  and so on  while enforcing a storage threshold of T tuples  Handling Storage Restrictions  We use S   10 4 and three di erent storage restrictions   a  no limit  in practice  all 10 maps used by the 5 queries  t within T   10 7     b  T   6 5 10 6   i e   slightly more than required to keep 6 full maps concurrently   c  T   2 10 6   i e   only 2 full maps can co exist  the minimum to run one query using full maps   Figures 9 a    b  and  c  show the per query cost for each case separately  In all three plots  full maps show the same pattern  Once every 100 queries  very high peaks  i e   per query costs  severely disturb the otherwise good perfor  mance  These peaks relate to the workload changes between query batches  The  rst 5 peaks re ect the costs of initially creating the cracker maps for each batch  plus aligning them with the cracks of the preceding batch  Requiring no align  ment  the  rst peak is smaller than the next 4  As of query 500  the batch cycle is repeated  With unlimited storage  all created maps are still available for reuse  requiring only alignment but no recreation with peaks 5 10 in Figure 9 a   With limited storage  the  rst maps had to be dropped to make room for later batches  requiring complete recreation of the maps once the cycle restarts  Figure 9 d  shows full maps allocating storage in blocks of two full maps per batch  In contrast  partial maps do not penalize single queries  but distribute the map creation and alignment costs evenly across all queries  using chuck wise granularity to more ac  curately adapt to changing workloads  Due to slightly in  creased costs for managing individual chunks instead of mono  lithic maps  partial maps do not quite reach the minimal per query cost of full maps  However  this investment results in a much smoother and more predictable performance behav  ior due to more  exible storage management  cf   Fig  9 d    Additionally  partial maps reduce the overall costs of query sequences  cf   Fig  11   12 and discussion below   Adaptation  Partial maps can fully exploit the workload characteristics to improve performance  To demonstrate this  we re run the basic experiment with two variations   a  we keep the uniform workload  but increase the selectiv  ity using S   10 3    b  we keep S   10 4   but use a skewed1e 01 1e 02 1e 03 1e 04 1e 05 1e 06  0 2 4</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9cs4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09_column_stores"/>
        <doc>GPU Based Speculative Query Processing for Database Operations ### Peter Benjamin Volk Technisch Universi  t Dresden peter benjamin volk inf tudresden de Dirk Habich Technisch Universi  t Dresden dirk habich tudresden de Wolfgang Lehner Technisch Universi  t Dresden wolfgang lehner tudresden de ABSTRACT With an increasing amount of data and user demands for fast query processing  the optimization of database opera  tions continues to be a challenging task  A common op  timization method is to leverage parallel hardware archi  tectures  With the introduction of general purpose GPU computing  massively parallel hardware has become avail  able within commodity hardware  To e ciently exploit this technology  we introduce the method of speculative query processing  This speculative query processing works on  but is not limited to  a pre x tree structure to e ciently support heavily used database index operations  Fundamentally  our developed approach traverse a pre x tree structure in a spec  ulative  parallel way instead of a step by step traversing  To show the bene ts and opportunities of our novel approach  we present an exhaustive evaluation on a graphical process  ing unit  1  ###  INTRODUCTION The increasing availability of structured data managed by relational databases emphasizes the need for fast query processing on large amounts of data  The performance of a database query can be improved in multiple ways and represents a permanent research focus in the database com  munity  1   The use of indexes is one of the most common and e ective methods to enhance the speed of a query in a row oriented database  15   In this kind of database  a data tuple contains of multiple columns and the data tuples are stored and processed with all associated column  One com  mon access method realized by such an index is the search for a speci c data tuple within a dataset that satis es cer  tain properties  such as the value of a speci c column  For this reason  multiple approaches have been developed and optimized for a variety of di erent data skews and hard  ware architectures  16   Such indexes are one of the most commonly applied data structures  further optimizations of these structures are essential  Furthermore  indexes can be used as a core component of column store database systems  9  19   While row stores keep all columns physically close to each other  column stores partition the table vertically such that each column of a ta  ble is stored in a separate location  To be able to recreate a row  each column contains a row identi er next to its value  A column store is commonly the foundation for database systems performing a large number of analytical queries  These analytical queries access only a few columns but al  most all rows of a table  Thus  column stores only access the needed data for this query type  while row stores need to access all columns since they are tightly bound to each other and cannot be skipped  Therefore  index structures are heavily used in column as well as row storage database system to e ciently support query processing  The main data structures used for indexes are tree based structures  The trees store tuples  where each tuple consists of a key and a payload  A key is the identi   er for a tuple  and the payload may be a link or the ac  tual value  Multidimensional indexes store a combination of keys to a payload such as  key1      keyn  payload   For spe  ci c data skews and requirements  multiple algorithms and structures have been proposed to store and query data most e ciently under speci c side conditions  22   Aside from tree based structures  pre x trees are a well  known structure in many areas of research  They are used in various ways  e g   for the translation of virtual to physical addresses in operating systems  12  21  or within commercial database systems for index compression in an Oracle system  Furthermore  they are widely applied to optimize relational joins of tables  as introduced in  10   Therefore  it is essential to optimize this kind of structure as far as possible and to explore new ways for query processing with this structure  Current CPU designs o er an increasing single digit num  ber of cores creating opportunities for database systems to leverage parallel algorithms  With the introduction of Graph  ical Processing Units  GPU  as general purpose processors  a massively parallel architecture has become available to perform database operations in parallel  A common method to speed up the access to data is to partition the data and to perform operations on it in parallel  This can only be done if the underlying data structures contain no data dependen  cies  To e ciently exploit the technology of a highly parallel system for a pre x tree  we introduce the concept of spec  ulative computing for database operations  With this new concept  we partially pre create results that can potentially be used to answer database queries  Since these partial re  sults are computed in parallel  the complete runtime of the query may bene t from this approach  Our Contribution and Outline In the following section  we brie y review the general idea of pre x trees and the basic structure of Graphical Processing Units  GPU   Then  we introduce our novel developed ap  proach to traverse a pre x tree structure on tightly coupled system in parallel in Section 3  In order to enable this ap  proach  we describe a method to resolve data dependencies for tress structures in Section 3 1  Moreover  we present an analytical study of our approach in Section 3 2  Section 4        Keys Payload INT   793910 0001 1111 0000 00112  1       7       0       310 Figure 1  Pre x tree with a depth of 4 and the search for key 7939  includes a description of how to leverage speci c hardware features for our approach  Before we conclude the paper with a short summary in Section 7  we review related work in Section 5 and we present an exhaustive experimental eval  uation of our developed approach in Section 6  2  PREREQUISITES As a basis for our new method  we use a pre x tree 1   In this section  we will introduce and discuss the concept of pre x trees and give a detailed description of our hardware model  2 1 Pre   x Tree Figure 1 shows an example of a pre x tree  The idea is that a tuple consists of a key Ki and a payload Pi  where the key Ki may be of any type of integer  32 or 64 bit  or a string with variable length  Every key in a tree has the same data type  The key is inserted into the tree and the payload is added as a linked list to the leaf nodes of the tree  The path within the tree for a key Ki is de ned by the absolute value of the key instead of being de ned by the relation of the key to the other keys  A key Ki is split into N equally sized sub keys Kn i consisting of b   jKij N bits with jKij being the number of bits in the key  Each node contains an array of 2 b child node pointers  On the n  th tree level  the sub key Kn i determines the child node to be used by interpreting the Kn i th sub key as an integer value and accessing the Kn i  th element within the child node array  Therefore  the tree has a depth of N  Figure 1 illustrates how key 7939 is located within the tree  The elements that are accessed in each node are shaded  With a  xed tree depth  a key lookup can be executed in constant time independent of the tree load  A further bene   t of this structure is that the keys are kept in sorted order  Therefore  sorting and  lter operations can strongly bene t from this tree type since the operations can be performed directly on the structure of the tree without expensive com  parison functions  One of the main points of criticism of this approach is the possible tree depth  Since the tree depth de  pends directly on the key length  this structure is unsuitable for long keys  Short keys can be very e ciently managed though  Thus  to be able to use this structure as a core 1 The original idea was submitted to the  First Annual SIG  MOD Programming Contest   Main Memory Transactional Index  by the DEXTER team and was chosen as one of the 5 fastest submitted implementations  component of a database system  it must be optimized with regard to its handling and support of longer keys that are evident in database systems  A database index is a key component of a database sys  tem and widely used to optimize data access  Before a data structure is able to serve as a database index  it has to pro  vide the following primitives  1  get key   This operation returns the  rst payload to a speci c key  2  getNext    This operation returns the next payload from a position determined by a get key  function  Once all payloads for a speci c key have been returned  the function moves on to the next key in alphabetical order  3  insert key  payload   The insertion consists of two steps because the correct position for the key must be found  rst  This can be resolved by a modi ed get key  op  eration that creates new nodes within the tree if the required nodes in the tree do not yet exist  4  delete key  payload   As a reversed operation to the insert  the delete operation removes the key payload tuple from the index or makes it invisible to the user  These are the most common and basic operations on an index within a database system  14   For more enhanced features of a database system  the index may provide more sophisticated access functions  such as methods to determine the number of distinct items or a  ll factor  Furthermore  we focus on the get key  function  since it is used in all other functions  and therefore  an optimization within this function also enhances the speed of others  2 2 GPU The original purpose of Graphical Processing Units  GPUs  is to perform mathematical calculations to determine the color of a speci c pixel in a picture  13   With increasing requirements from the game and movie industries  GPUs have become a source for great computing power  This is achieved by highly specialized hardware consisting of tightly coupled parallel processors  Figure 2 illustrates a strongly abstracted architecture view on a GTX285 from NVIDIA  Other vendors  products  such as those from AMD  di er only slightly from this architectural view  The GPU consists of a two level hierarchy that can be found on processor and memory level  The GTX285 has 24   CPU Global Memory PCIe 16x Local Local Local Local Figure 2  Schematic overview of a GPU architec  ture  multiprocessors  MP  and a global memory  Each MP con  sists of 8 processors and a local memory  resulting in a total of 240 processors  While all data in the global memory can be accessed from all processors  data in the local memory is accessible only from within the same thread  For synchro  nization purposes  the global memory also includes a shared memory region  This region can be accessed by all threads that are executed within one block  The local and shared memory can be leveraged by software code only and will be  ushed when the function being executed on the GPU is complete  Therefore  the shared and local memory repre  sent a software controlled cache  The size of shared memory is limited to 16kBytes  compared to a maximum of 4GB for the global memory  Data in the global memory is persis  tent throughout the application runtime  Furthermore  lo  cal memory can be accessed within 4 6 cycles  compared to 400 600 cycles of latency on the global memory  With such high latency di erences  it is important to leverage the local memory as much as possible  To transfer data  the GPU is connected with a PCIExpress connection  The current theoretical maximum throughput is 8GByte s  version 2   To hide the high latency of the global memory  the GPU o ers two important features  The  rst is coalesced mem  ory access  This feature bundles multiple memory requests of processors into one request  This way  high memory band  width of 86GByte s can be used more e ciently  The sec  ond feature is fast thread switching  While a thread switch is very costly on the CPU  the GPU can handle thread switch  ing with more ease  It is therefore encouraged to create more threads than are available as physical execution units  This overload can then be used to schedule threads for execution  while others wait for a memory transfer  This is especially important since databases are more likely I O bound  not CPU bound  and it is thus one of the most important fea  tures for implementing database operations on the GPU  We will come back to this in later chapters  The GPU can be employed in multiple ways  Previously  the data has been hidden in structures for graphical pro  cessing  such as triangles  and then graphical programming languages like GLSL or DirectX have been used to perform image manipulation functions on the data  resulting in the desired mathematical operation on the data  This has re  quired good knowledge of graphics processing and graphical programming languages  CUDA and Stream are extensions to the C programming language that enable the program  mer to write native C code and execute it on the GPU  The methods introduced in this paper are implemented using the CUDA extension exclusively  3  SPECULATIVE TREE TRAVERSAL As described in Section 2 1  the performance of a database index structure depends on the e ciency of the get key  op  eration  However  this operation normally corresponds to sequential traversals of the underlying tree from the root to a leaf  To increase the performance and to leverage the high number of available cores on a many core processor  like a GPU   we introduce our speculative approach for tree traversal  Our goal is not to minimize the number of instructions used to the  nd the result but to utilize the high number of cores in an e cient and novel way  In this case  we assume that many core architectures o er enough com  putation capabilities to allow us to perform some speculative  and sometimes redundant  tasks that we would normally avoid in today s algorithms  Our concept of speculative tree traversal consists of two steps  1  Parallel traversal of  all  partitions of the tree  and 2  Aggregation of intermediate results to the  nal result  The parallel traversal of the tree uses the high number of cores on a GPU  In its initial implementation  all possible partial results are created  Then  the intermediate results are used to determine the  nal result  The second step is  again  a traversal of the intermediate results  which is imple  mented as a serial traversal  To further enhance the perfor  mance  we also introduce a method to create the  nal result in parallel  3 1 Partition Based Traversal To leverage the high number of cores of a GPU  we parti  tion the tree into multiple computational trees  Each parti  tion Cj starts at a speci c tree level k and covers m levels  At each leaf of a computational tree  the next computational tree starts  until the leaf nodes of the complete tree have been reached  Each computational tree is assigned to one thread on the GPU  Therefore  each level may consist of one or multiple computational trees  and each thread is respon  sible for traversing its tree for a given sub key Ki according to its starting level  To traverse the tree for a key K  each partition provides one of three di erent types of results  The result may con  tain a reference to yet another computational tree  This is the case if the partition is in the middle of the respective tree and if it contains a path for its substring  The result of a computational tree can also be NULL  meaning that the computational tree does not contain any matching path for its substring  The third possible result is a pointer to a payload  This is the case if the computational tree contains leaf nodes of the original pre x tree  The result of the partition is then entered into a global list for the intermediate results  Since the start addresses of a partition are known  the result of a partition can be used to determine which partition the pointer leads to  Finally  the global list has to be traversed starting from the  rst partition  Compared to a traversal of the original tree  only a fraction of traversal steps are needed here  Furthermore  the bene ts of the computational trees are that they are independent of each other and can be computed in parallel  Figure 3 illustrates an example for this parallel traversal  It shows a tree for a 12 bit integer  Each node has 8 entries  resulting in a sub key length of 3 bits and in a tree depth of1 2 3 4 INT   356910 110 111 110 0012 Partition 1 Partition 2 Partition 3 Partition 4 Figure 3  Speculative Traversal of a tree with a depth of 4 for the 12 bit integer 3569  4 nodes  The tree is  rst partitioned into 4 computational trees  Note that only those partitions are created that are needed  This can be implemented by keeping a list of start pointers for a speci c tree level that is maintained during the insertion of data  Subsequently  each of the 4 compu  tational trees is traversed in parallel  and the intermediate results are entered into a global list  as illustrated in Table 1  To determine the  nal result  this intermediate result is traversed in two steps  Computational tree no  3 did not lead to any result compared to trees 1  2 and 4  They con  tain a pointer to a payload or to a subsequent partition  The traversal of the intermediate result starts with the  rst com  putational tree  since it contains the root element  Its result is a pointer to the fourth computational tree  Finally  the result of the fourth computational tree is the pointer to the resulting payload  Therefore  we need a total of 4 traversal steps to retrieve the  nal result  Sub Tree Result 1 4 2 Payload 3 NULL 4 Payload Table 1  Result of the traversal from Figure 3  To quantify the possible bene ts of this approach  we will analyze it on a theoretical level  showing that for larger trees  this method reduces the number of traversal steps tremen  dously and  therefore  speeds up all operations on this tree  Further  we will use N as the number of levels within the tree  If all computational trees are executed completely in parallel  the number of tree levels each thread has to pass is equal to m  The number of steps required to build the  nal result can be determined by k   N m  1  There  fore  the total number of nodes that need to be traversed is n   m   k   m   N m  1  the maximum speedup w r t the number of nodes traversed is  s   N n   N m   N m  1    1  The global maximum of equation 1 is at m   p  N   There  fore  the maximum theoretical speedup compared to a se  quential traversal of the tree can be determined by  smax   N  2   p  N    1     2  This maximum speedup can only be reached if memory re  quests of each thread are coalesced and therefore answered simultaneously  Furthermore  it is necessary that the hard  ware can hide the remaining memory latency by switching threads into hardware that is not waiting for a memory transfer to be completed  These features are available and very well developed on the NVIDIA GPUs  The possible speedup is limited by equation 2  by the max  imum throughput between memory and processors  and by the number of available processors  Furthermore  the last step of building the  nal address is executed in serial  This is one of the bottlenecks of this approach  which we will reconsider later on  Furthermore  the possible maximum speedup via equation 2 is only very limited for larger trees and decreases with an increasing number of nodes  Since one of the main goals is the ability to apply a pre x tree to larger keys  it is necessary to perform further optimizations  To increase the speedup of this approach further  more fea  tures of the GPU must be exploited  like the local memory with very low latency to the processor  This is especially necessary since equation 1 shows that small computational trees result in a large table with intermediate results that can only be traversed in serial fashion  3 2 Hierarchical Traversal To increase the possible speedup further  we adopt the method of parallel traversal to include the fast local memory and to reduce the number of traversal steps on the global memory to build the  nal result from the intermediate one  To achieve this  we introduce another hierarchy similar to the processor hierarchy on the GPU  Threads that are executed on the GPU are grouped into blocks  All threads within a block can share data via the shared memory  Furthermore  shared memory can be ac  cessed within 4 6 processor clock cycles  meaning a low la  tency compared to the global memory  To utilize this shared memory  we group computational trees into blocks  Within each block  the threads write the intermediate results of their computational tree into the shared memory  Then  after all threads of one block have completed their traversal  the in    Figure 4  Legend of variables within the tree architecture  termediate result of the block is traversed  This  nal result of the block is then entered into the result list in the global memory to build the  nal result from the intermediate re  sults from the blocks  similar to the previous approach  This approach results in an additional traversal step but increases the speed by leveraging the fast shared memory of the GPU  The challenge with this additional step is to en  sure that the computational trees within one block depend on each other  meaning that no or only very few partitions link to a partition located in a di erent block  To guar  antee this  we split the size of the computational tree and create sub partitions within this tree  Each sub partition is then traversed by a separate thread  similar to the previous approach  To analyze the maximum speedup compared to the pre  vious approach  we need to incorporate more variables into our model  An illustration of the variables is given in Figure 4  The speedup depends on the number of blocks B gener  ated  the height of a block jBj in the number of nodes  the height of computational trees  and the latency relationship between global G and shared memory R   G L  The number of intermediate results generated in the shared memory to determine  nal results per block depends on the block depth jBj and can be determined by jBj m  Succeed  ing this is the number of traversal steps in global memory   N  jBj   The traversal in global memory is needed to create the  nal result  One of the limiting factors of the previous approach was the bandwidth between global memory and the processors  As the number of parallel traversals within one block is still equal to m  the bandwidth usage is still a limiting factor  In the following  we will analyze the possible speedup compared to the previous approach  The number of memory accesses khierarchical of the parallel node traversal must be split into shared and global memory accesses  k shared hierarchical   jBj m  3  and k global hierarchical   m   N jBj  4  Access to the global memory is only necessary to determine the  nal results from the intermediate block results  As a next step in our analysis  we incorporate the latency factor R  Since the latency to the global memory is a lot higher than to the shared memory  we add a penalty to the global memory access  khierarchical   k local hierarchical   k global hierarchical   R  5    jBj m    m   N jBj     R  6  Therefore  the speedup of the approach with shared mem  ory compared to the approach without shared memory can be determined by  s    m   N m  1    R jBj m    m   N jBj     R    7  The maximum speedup can be reached by using jBj    N  meaning that the complete tree is traversed within one block  In this case  the  nal result would be determined completely in the fast local memory  This would result in a maximum speedup compared to the non hierarchical ap  proach of  smax   R   8  This theoretical maximum could only be reached if we were able to dispatch unlimited numbers of threads in each block to calculate all computational trees  Since architecture lim  itations of the GPU prohibit unlimited numbers of threads within one block and since there are limitations on the size of the shared memory  this speedup can only be reached for small trees  as our evaluation will show  3 3 Parallel result building As the analysis of the previous section has shown  the determination of the  nal result is a sequential task and requires k   N m 1 steps  If this fraction can be parallelized  the total number of sequential steps would be reduced to the number of traversals within a partition N m  In special cases  the number of steps required within the  nal result can be reduced  If the intermediate result table contains exactly one pointer to a payload  then this pointer is the  nal result  No other partitions containing leaf nodes have found a path within their containing nodes  Furthermore  keys containing a common beginning  for example  Web URLs  only need to be compared in the last traversal steps  Therefore  we propose a new method to  nd the  nal re  sult  First  we start a thread for each pointer to a payload  Sub Tree Result Payload 1 NULL NULL 2 NULL payload 3 NULL NULL 4 1 payload Table 2  Reversed intermediate result table from Table 1  within the result table  Then  each thread checks the entries within the table that reference this entry  This is repeated until no further referencing entry is found or until the ref  erencing entry is the root node  If  during the traversal  all threads except for one are terminated  then the thread must have started at the pointer to the payload that is the  nal result  Given the example in Table 1  a thread is created for every entry containing a pointer to a payload  In this example  2 are created  Further  the threads determine the entries in the table that point to this element  For the payload in partition 2  no parent partition can be found  and therefore  this thread can terminate  Sub tree 4 is referenced by the root partition  Since the thread from partition 4 is the only thread running and the only one that has found a parent  it must be started at the pointer to the  nal result  The challenge with this approach is to  nd the referencing partitions  The na  ve implementation of such a search would be to scan the complete intermediate result table  Since this is very ine cient  we propose to change the intermediate result table to a reversed result table  The entries within it do not point to the successor but to the predecessor  The reversed intermediate result table from Table 1 is illustrated in Table 2  As the reversed table shows  it is necessary to save the pointer to the payload if the result of a partition is a payload  4  IMPLEMENTATION Our implementation of speculative pre computation for the GPU is based on CUDA 3 1 developed by NVidia  Using this technology  it is possible to write c c   style functions  called device or global kernels  that are compiled with nvcc and are executed via the runtime environment on the GPU  It is only possible to execute these functions on the GPU  Functions written for the CPU have to be rewritten  such as functions for memory management  Since the insertion of new data requires the allocation of new tree nodes  a mem  ory management must be implemented to function on the GPU  Furthermore  to leverage such functions as coalscaled memory access the data layout must be optimize to avoid con icts  In this section  we will illustrate the basic archi  tecture of the index system and outline how we optimized the memory layout  4 1 Basic Architecture Index systems of database systems do not only consist out of the index but also requires metadata and links to the actual payload  Since the result of the search within an index is used to perform further operations on the result  the data must be located where the next operation will take place  Figure 5 illustrates our main architecture  The dic  tionary contains metadata to the index  such as name and Tree API L1 L2 GPU CPU Figure 5  Architecture and location overview  type  Parts of the dictionary  such as the name  are located on the GPU and on the CPU memory  This makes changes and creation expensive but minimizes data transfer between the host and the device  The metadata on the GPU is used to search for a speci c index  If the user opens an index with a speci c name the GPU is used to locate it within the sys  tem  For this one thread per existing index is created which compares its name with the name that is being searched for  Once the index is found a pointer is returned to the CPU to perform further operations  This is especially useful if many indexes exist within the system  As Figure 5 shows  the complete index resides on the GPU  The insert   or get   functions are executed solely on the GPU  Therefore  only minimal data transfer is needed  The limiting factor fur such a design is the amount of avail  able GPU memory  Current systems provide up to 4GB of global memory  NVidia Tesla S1060   Since a pre x tree performs a type of compression we can store strings exceed  ing the 4GB limit  Furthermore  through our memory lay  out it is possible to page a complete index from the GPU to CPU ram if necessary  As depicted in the previous chapter  leafs of the tree con  tain the payload  These L1 items are located in CPU RAM together with the payload list  The leaf nodes on the GPU only contain the address of this L1 item within the address space of the CPU RAM  This way only the address of the L1 item and the key must be transferred to the GPU when inserting a tuple  The L1 Items contain a pointer to the actual payload  To support e cient scan operations  the payloads are within a globally linked list reducing the need to  nd the next lexical element within the tree  4 2 Memory and Thread Layout Finding an e cient memory layout is essential to leverage coalscaled access capabilities of the GPU  The most critical part is the creation of the node structure  Current NVidia GPU can coalscaled memory access from 16 parallel running threads  half warp  on the same multiprocessor  Therefore  it is essential that within a half warp the memory layout is build such that the memory requests can be coalscaled by the hardware  For each partition level we create a multi  ple of 16 threads and partitions  Each thread traverses one partition of the same level  Each of these partitions is  lled in parallel during the insertion of the data into the index  The only di erence between the partitions is the value of the leaf node of the partition  Hence each partition has the sameFigure 6  Illustration of parallel  lled partitions with the same layout  number of nodes and the same layout as Figure 6 illustrates  This method creates a higher memory footprint than an allo  cation of memory only when necessary  If a dataset contains many common substrings with di erent pre xes then only minimal overhead is created compared to an allocation on request  An allocation on request would randomly create addresses and hence limit coalscaled access  The only exception is the root partition since there is only exactly one root partition we create only one partition but dispatch 16 threads to traverse it  From these threads 15 terminate immediately  This is necessary to guarantee that all other partition levels are aligned with 16 threads  Besides coalscaled access this method of allocating the partitions also minimizes the number of divergent Branches  Divergent branches are created when a thread within a half warp has a di erent execution  ow as the other thread  When for example an if condition evaluates true within one thread and false in another thread then the threads are partially serialized reducing the parallelism  Since each thread within a half warp traverses the same path the eval  uation of the conditions are the same  Divergent branches create a signi cant overhead since the scheduler needs to  nd a new schedule for this new set of operations  Since dynamic memory allocation can only be performed from the CPU the insertion function requires their own mem  ory management  Memory management is based on pages  Each page consist of G number of bytes  A pointer deter  mines where the next free byte is located within this page  Before an insert operation is executed the system determines the number of free bytes within the current page  If not enough memory is available then another page is allocated and is added to the index  Therefore  the index maintains a list of memory pages it may use  For this setup deletes of nodes are not considered  Each page can be considered as an array of bytes  The actual address that is written into a node of the tree is not the physical address but a reference on into a speci c page  With this relative addressing we can move the complete index from and to any GPU device  in a multi device setting  without having to modify the pointers in the tree  5  RELATED WORK Multiple approaches to perform relational database oper  ations on a GPU  join  have been proposed in  6  7   The approach used to speed up database operations is to parti  tion the data and then to compute each partition in parallel  With this method  strong data dependencies still exist and only a limited parallelization usage of the high number of cores is possible  For spatial databases  similar approaches have been proposed in  2  3   Furthermore  all approaches focus on complete operations of databases and not on single elements such as indexes  Index structures for the GPU have A   k e y P a r t i a l U n s o r t     512 Key length Figure 7  Naive index  found attention in  5  4   The goal of  5  is to create a com  pression and evaluation strategy for bitmap indexes for mas  sively multi threaded platforms  One of the major points of criticism of this approach is that bitmap indexes are costly to maintain  In  11   the authors use a special layout of a binary tree to optimize the search strategy  Compared to our approach  their concept is update unfriendly since a complete rebuild of the tree is necessary to insert a tuple  Furthermore   20  proposes a SIMD based traversal of a tree to optimize the tree traversal for multiple parallel searches  This di ers from our approach  since we use a data parallel traversal  Furthermore  the previous approaches use a bi  nary tree as a base  compared to our pre x tree using N bits as a pre x  A binary search tree results in a deep tree for long key values  reducing its performance  In contrast  our approach uses multiple bits per node and  therefore  re  sults in shorter trees and a reduction of the total number of traversal steps required to resolve a search query  Beyond of the  eld of databases the method of specu  lative computation is widely adopted for parallel memory machines as  8  17  18  illustrate  Here the authors also com  pute partial results although they might not contribute to the  nal result  Precomputation is here applied to a domain containing data dependencies within complex calculations  Distributed memory machines have no global memory and o er di erent optimization possibilities  The research focus lies on when to push data from what computing node to another  Yet the computing node consist only of a single computing instance and not  like a GPU  of a highly thread parallel system  The GPU furthermore o ers other possibil  ities to optimize the parallel execution and parallel memory access as the previous section illustrates  6  DISCUSSION AND EVALUATION We evaluated our proposed approaches against the se  quential traversal of the tree  performing N steps on one core on the GPU and CPU  and against a na  ve approach  The na  ve approach creates a thread for every key within the index  To ensure that the na  ve implementation also uses coalesced memory access  we have arranged the data in a matrix containing 512 keys  as Fig  7 shows  With this organization  each thread accesses a character in ascending address order  Once a matrix is full  another matrix is cre  ated  Each matrix is executed as one block on the GPU  All data is unsorted within the matrixes  For the evaluation  we used a single GTX285 GPU with 240 cores with 1 48GHz per core and a memory clock rate of 1 2GHz running on a Windows 2008 PC and a CUDA driverof version 3 1  To compare the GPU performance with a CPU implementation  we used an Athlon 64 X2 Dual Core 4200  with a clock rate of 2 2GHz  To measure the number of serialized blocks  we used NVIDIA s Parallel NSight  First  we evaluated the performance of the approaches with multiple di erent arti cial data sets containing di er  ent data skews in addition to a real dataset  For the fol  lowing experiments  we use strings as keys with a length of 128 characters  Since the speculative approach creates one thread per partition  the  rst dataset creates a new partition with every insert  This is the worst case for our speculative approach  since it maxes out the number of threads used  Figure 8 a  illustrates the result of this experiment  As we can see  the speculative approach performs better than the linear traversal and the na  ve approach  Since the number of tuples within the index determines the number of partitions  and thus the number of threads used to traverse the tree  the performance degrades  As soon as the GPU hardware has more threads that can fully run in parallel  threads and blocks are serialized  The second data skew is optimal for the tree construc  tion  Every insert creates a new leaf node by incrementing the least signi cant byte by one for every new string  With this data skew  only the minimal number of partitions is created  since the tree is only expanded on the very last level  Figure 8 b  illustrates the result of this experiment  Clearly  it shows that the performance is almost always bet  ter than with the na  ve approach and the linear traversal of the tree  The performances of the na  ve approach and the linear traversal are equal to the performance with the pre  vious data skew  This shows that the performances of these approaches are independent of the data skew  Furthermore  it can be seen from the two experiments that the linear tree traversal is independent of the number of tuples compared to the na  ve approach  This is due to the tree structure  The number of levels within the tree is independent from the number of tuples within the tree  Since a new thread is created for every new tuple in the na  ve approach  it su ers from the same e ects as the parallel traversal does  As soon as a speci c number of tuples is reached  the GPU hardware starts to serialize the threads and blocks  The third data skew contains URL like strings  Their ini  tial segments are the same  but their last segments are very heterogeneous  We used the scaped URL data from the facebook directory  This dataset contains more than 100 000 urls with an average length of 60 characters and a maximum length of 147  It consists out of the Facebook URL  the user name and an integer value being the user ID within Face  book  This type of dataset can be used to show the e ects of the parallel traversal of the intermediate result  Figure 8 c  shows the result of this experiment  The performance gain that can be achieved by this additional approach is only very limited but adds another 10  performance increase for this speci c data skew  As the previous experiments have shown  the parallel traversal of the intermediate results shows only very little to no performance degradation compared to the sequential traversal of the intermediate result  Aside from the data skew  the performance of our ap  proach depends on the number of levels within a partition  In the previous examples  we used the theoretical optimum  Figure 8 d  illustrates the performance of our approach with the  rst dataset with di erent numbers of levels within a partition  Clearly  the experiment shows that the theoreti  cal optimum is also the optimum for the implementation  Due to the method how the memory layout is orches  trated  it is important to measure the memory consump  tion  Figure 8 e  shows an experiment for di erent dataset sizes from the di erent data skews from above and compared to a na  ve implementation  The na  ve implementation allo  cates data on demand and therefore creates a randomized memory layout minimizing the possibility of coalesced ac  cess  The modi ed memory allocation creates for the Face  book dataset only minimal overhead since the lower parti  tions are very heterogeneous and new partitions are created very often  The minimal tree in comparison creates a larger overhead since the partitions are completely  lled and the parallel partitions for the levels are also  lled  One of the main points of criticism for the usage of the GPU as a co processor for database operators is the limited bandwidth between CPU and GPU memory  which is cur  ren</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware"/>
        <doc>Dependency Aware Reordering for Parallelizing Query Optimization in Multi Core CPUs  ###  Wook Shin Han Department of Computer Engineering Kyungpook National University wshan knu ac kr Jinsoo Lee Department of Computer Engineering Kyungpook National University jslee www db knu ac kr ABSTRACT The state of the art commercial query optimizers employ cost based optimization and exploit dynamic programming  DP  to    nd the optimal query execution plan  QEP  without evaluating redundant sub plans  The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially  as the number of joins in the query increases  Recently  by exploiting the coming wave of multi core processor architectures  a state of the art parallel optimization algorithm  14   referred to as PDPsva  has been proposed to parallelize the    time consuming    DP query optimization process itself  While PDPsva signi   cantly extends the practical use of DP to queries having up to 20 25 tables  it has several limitations  1  supporting only the size driven DP enumerator  2  statically allocating search space  and 3  not fully exploiting parallelism  In this paper  we propose the    rst generic solution for parallelizing any type of bottom up optimizer  including the graph traversal driven type  and for supporting dynamic search allocation and full parallelism  This is a challenging problem  since recently developed  state of art DP optimizers such as DPcpp  21  and DPhyp  22  are very dif   cult to parallelize due to tangled dependencies in the join pairs they generate  Unless the solution is very carefully devised  a lot of synchronization con   icts are bound to occur  By viewing a serial bottom up optimizer as one which generates a totally ordered sequence of join pairs in a streaming fashion  we propose a novel concept of dependency aware reordering  which minimizes waiting time caused by dependencies of join pairs  To maximize parallelism  we also introduce a series of novel performance optimization techniques  1  pipelining of join pair generation and plan generation  2  the synchronization free global MEMO  and 3  threading across dependencies  Through extensive experiments with various query topologies  we show that our solution supports any type of bottom up optimization  achieving linear speedup for each type  Despite the fact that our solution is generic  due to sophisticated optimization techniques  our generic parallel optimizer outperforms PDPsva tailored to size driven enumeration  Experimental results also show that our solution is much more robust than PDPsva with respect to search space allocation  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  Categories and Subject Descriptors H 2 4  DATABASE MANAGEMENT   Systems General Terms Algorithms Keywords Multi cores  Parallel databases  Query optimization 1  ###   INTRODUCTION For the last few decades  the CPU performance has been signi     cantly improved by increasing the clock rate according to Moore   s law  However  fundamental physical limitations such as power consumption and heat generation clearly prevent us from relying on this trend any more  11  12  29  30   Instead  the industry has been improving the CPU performance by integrating more execution cores into each processor  The number of cores is expected to grow signi   cantly over time  11  35   Recently  by exploiting this new wave of multi core processor architectures  Han et al   14  have proposed a novel framework referred to here as PDPsva  to parallelize the    time consuming    dynamic programming  DP  query optimization process itself  The DP query optimizer enumerates many alternative query execution plans  QEPs  for evaluating a declarative SQL query  while estimating the cost of each QEP  and then chooses the one with lowest estimated cost  The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially  as the number of joins in the query increases  In fact  PDPsva signi   cantly extends the practical use of DP to queries having up to 20 25 tables  We otherwise would have to depend on sub optimal  randomized or greedy  heuristics  4  19  23  31  32  to complete query optimization in a reasonable time  However  PDPsva has three limitations  First  it supports only one speci   c bottom up optimizer  the size driven DP optimizer  That is  it does not support recently developed  state of the art DP optimizers such as DPcpp  21  and DPhyp  22   which directly traverse a query graph to generate join pairs  Such optimizers have advantages over the size driven enumeration  They can support early termination  since they can generate QEPs for all tables  more precisely  all quanti   ers 1   without generating QEPs for all smaller quanti   er sets  i e   not size driven   Thus  as soon as we obtain a suf   ciently good QEP or the estimated execution time of the obtained QEP is less than the expected remaining enumeration time  we can terminate the enumeration process early  DPhyp can handle 1 Quanti   ers correspond to the tuple variables seen in the FROM clause of the SQL query  24   45the widest class of non inner joins very ef   ciently  22   Therefore  there is a need for a generic framework that can parallelize any type of bottom up optimizer so that it can support both existing and future bottom up optimizers  Secondly  assuming all cores are evenly loaded  PDPsva employs static search space allocation  Although the best allocation strategy of PDPsva can allocate search space to threads evenly  14   the slowest thread holds up all the other  faster threads  resulting in seriously unbalanced workloads  Therefore  the search allocation strategy must be dynamic to resolve this situation  Lastly  PDPsva does not fully exploit parallelism since it merges per thread MEMOs to the global MEMO in serial execution for each size of resulting quanti   er sets  Here  each MEMO entry stores QEPs for a given quanti   er sets  Thus  the best version of PDPsva achieves only up to 6 1 speedup for star queries 2 using 8 threads  14   Therefore  in order to achieve linear speedup  all such serial steps must be executed by exploiting full parallelism  In this paper  we propose the    rst generic solution for parallelizing any type of bottom up optimizer  including the graph traversal driven type  and for supporting dynamic search allocation and full parallelism  This is a challenging problem  since DPcpp and DPhyp are very dif   cult to parallelize  14   Unless the solution is very carefully devised  a lot of synchronization con   icts are bound to occur  Figure 1 shows a motivating example using a sequence of join pairs  more precisely  a sequence of pairs of quanti   er sets  generated by DPcpp or DPhyp  Note that DPcpp and DPhyp generate the same sequence for equi join  An arrow from one pair to another pair represents a dependency  As opposed to size driven enumeration  the sizes of resulting quanti   er sets do not monotonically increase  This leads to tradeoff between early termination and tangled dependencies  That is  since we obtain some QEPs for all quanti   ers at the 17th pair of quanti   er sets  q1  q2q3q4   we might be able to terminate the optimization process early  if the best QEP obtained thus far is good enough  On the other hand  the resulting tangled dependencies in the pairs of quanti   er sets hinder parallelizing DPcpp  For example  if a thread Ta processes the eighth pair  q2  q3q4   and a thread Tb processes the    fth pair  q3  q4   Ta must wait until Tb    nishes the processing of  q3  q4     rst   since the quanti   er set q3q4 has a dependency on the pair  q3  q4   If we change the order of the eighth and the eleventh pairs  Ta can process  q1  q4  without waiting  The overview of our solution is as follows  To parallelize any type of bottom up enumeration  we view a serial bottom up optimizer as one which generates a totally ordered sequence of pairs of quanti   er sets in a streaming fashion  We buffer a    xed number of pairs and delay plan generation for the pairs buffered  Then on the    y  we convert the total order over these buffered pairs into a partial order over unordered groups of pairs  where threads can generate QEPs independently for all pairs within a group without waiting  These steps correspond to reordering of the original sequence so that the tangled dependencies in the original sequence are unraveled  We repeat these steps until we consume all pairs of quanti   er sets  Our contributions are as follows  1  We propose the    rst generic framework for parallelizing any type of bottom up optimization  2  We propose a novel concept of dependency aware reordering  which minimizes waiting time caused by dependencies of pairs of quanti   er sets and propose a generic algorithm DPEGeneric for parallelizing query optimization  3  To maximize parallelism  we propose a series of optimization techniques for DPEGeneric  2 A star query containing N quanti   ers consists of a hub quanti   er  N 1 neighboring quanti   ers  and N 1 edges  where each neighboring quanti   er is connected only to the hub quanti   er  q1 q2 q3 q4 Query graph G SELECT     FROM R1 q1  R2 q2  FROM R3 q3  R4 q4 WHERE q1 a2   q2 a1 and WHERE q1 a3   q3 a1 and WHERE q1 a4   q4 a1 and WHERE q2 a3   q3 a2 and WHERE q2 a4   q4 a2 and WHERE q3 a4   q4 a3 SQL query statement 1   q4  2   q3  3   q2  4   q1  5   q3  q4  6   q2  q4  7   q2  q3  8   q2  q3q4  9   q4  q2q3  10   q3  q2q4  11   q1  q4  12   q1  q3  13   q1  q3q4  14   q1  q2  15   q1  q2q3  16   q1  q2q4  17   q1  q2q3q4  18   q4  q1q2  19   q3  q1q2  20   q1q2  q3q4  21   q4  q1q3  22   q2  q1q3  23   q1q3  q2q4  24   q4  q1q2q3  25   q3  q1q4  26   q2  q1q4  27   q1q4  q2q3  28   q3  q1q2q4  29   q2  q1q3q4  Figure 1  Pairs of quanti   er sets generated by DPcpp  pipelining of join pair generation and plan generation  the synchronization free global MEMO  and threading across dependencies  4  Through extensive experiments  we show that DPEGeneric supports any type of bottom up optimizer  achieving linear speedup for each type  Our algorithm is even better than the state of the art parallel optimizer tailored to size based enumeration  PDPsva  Our algorithm is also much more robust than PDPsva with respect to search space allocation  The rest of this paper is organized as follows  Section 2 reviews the current bottom up join enumeration algorithms and the state of the art parallel algorithm for the size based enumeration  The next two sections give the details of two generic parallel enumeration algorithms  Section 3 gives a basic parallel enumeration algorithm that can support any type of bottom up enumeration algorithms  and Section 4 gives a theoretical framework for the dependencyaware reordering and an enhanced parallel enumeration algorithm exploiting the dependency aware reordering  Section 5 presents a series of performance optimization techniques to maximize parallelism  Section 6 presents the results of performance evaluation  We compare our contributions with related work in Section 7  and conclude in Section 8  2  BOTTOM UP ENUMERATION An enumeration algorithm is called bottom up if it processes all smaller quanti   er sets of both qs1 and qs2 before processing a pair of quanti   er sets  qs1  qs2   To avoid evaluating redundant subplans  the bottom up enumerator exploits the principle of optimality and stores the optimal QEPs in an in memory quanti   er set table  a k a  MEMO   18   Each entry in MEMO contains a list of QEPs for a quanti   er set  and MEMO is typically implemented by using a hash table with the quanti   er set as the key  Existing bottom up enumerators can be classi   ed into the following three categories based on how they generate pairs of quanti     er sets  1  size driven enumeration  2  subset driven enumeration  and 3  graph traversal driven enumeration  We omit explanation of subset driven enumeration since it is far slower than the graphtraversal driven enumeration  21  and is not used by commercial optimizers  For a more detailed description on the subset driven enumeration  refer to reference  21   462 1 Size Driven Enumeration The size driven optimizer iteratively increases the size of the resulting quanti   er set until it obtains the optimal QEP for all quanti     ers in the query  starting from sets containing only a single quanti     er  The join enumerators of conventional optimizers  such as those of DB2 and PostgreSQL  26   belong to this category  At each iteration  to produce all QEPs representing quanti   er sets of size S Z  the optimizer uses    nested loops    between quanti   er sets of smallSZ and quanti   er sets of largeSZ such that largeSZ   S Z   smallSZ  Here  for each pair of quanti   er sets  the optimizer must check whether the two quanti   er sets can form a feasible join  the two quanti   er sets are disjoint and connected using at least one join predicate between them  If the connectivity check is disabled  Cartesian products in the resulting QEPs are permitted  Note that  to avoid unnecessary generations of infeasible pairs  i e   overlapped pairs  of quanti   er sets  a special index called the skip vector array  SVA  can be used  14   We omit explanation of how the SVA can be exploited during enumeration since this is not our focus  Algorithm 1 outlines the state of the art parallel optimization algorithm for size driven enumeration  PDPsva  For each size of the resulting quanti   er sets  we    rst allocate parts of the search space to m threads  Line 3   each of which then executes its allocated nested loops in parallel  Line 4   In order to merge per thread MEMOs and prune expensive QEPs in the global MEMO  we need to wait until all threads    nish their processing  Line 5   After completing the parallel QEP generation for each size of resulting quanti   er sets  PDPsva merges per thread MEMOs to the global MEMO in serial execution  Line 6   To speed up the process of    nding feasible join pairs  the SVA must be built over MEMO entries we just constructed  Line 7   Algorithm 1 PDPsva Input  a connected query graph with quanti   ers q1             qN Output  an optimal bushy join tree 1  create table access plans and prune expensive QEPs for each quanti   er  2  for SZ     2 to N 3  allocate to m threads portions of nested loops for QEPs representing quanti   er sets of size SZ  4  each thread generates QEPs in parallel by using its allocated nested loops    5  wait until all threads    nish generating QEPs representing quanti   er sets of size SZ   6  merge per thread MEMOs into a global MEMO  7  build the skip vector array for MEMO entries corresponding to quanti   er sets of size SZ  8  return MEMO q1          qN    2 2 Graph Traversal Driven Enumeration By directly traversing the query graph  the graph traversal based optimizer generates a pair of quanti   er sets that are disjoint and connected  Two state of the art algorithms   DPcpp  21  and DPhyp  22    belong to this category  They have been developed very recently  Both behave similarly except that DPhyp can handle non inner and anti join predicates as well  Both generate pairs of quanti   er sets  qs1  qs2  such that qs1 is generated by enumerating all connected subgraphs of the query graph  and qs2 is generated by enumerating all other connected subgraphs that are disjoint and connected to qs1  As shown in Figure 1  the resulting dependencies in the pairs of quanti   er sets generated by DPcpp or DPhyp would prevent cleanly parallelizing DPcpp or DPhyp  This motivates us to dynamically reorder pairs of quanti   er sets on the    y to exploit parallelism  3  BASIC PARALLEL ENUMERATION In this section  we propose a basic parallel enumeration algorithm  BPEGeneric  for parallelizing any type of bottom up optimizer  Algorithm 2 outlines the algorithm of BPEGeneric  Each thread invokes BPEGeneric concurrently  At each iteration in BPEGeneric  the algorithm obtains a pair of quanti   er sets by calling the subroutine GetNextQSPair  Line 2   When GetNextQSPair generates a pair of quanti   er sets  qs1  qs2   qs2 is set to empty if GetNextQSPair generates a singleton set  If qs2 is empty  BPEGeneric invokes the subroutine CreateTableAccessPlans to generate QEPs for accessing a single table  Line 9   Otherwise  it invokes the subroutine CreateJoinPlans  Line 11  to generate various join QEPs by trying out different access paths  join methods  and join orders  BPEGeneric then calls PrunePlans to prune any plan QEP1 if there is another plan QEP2 such that cost QEP1    cost QEP2   and whose properties  e g   ordering of rows and partitioning  etc   subsume those of QEP1  Line 12   Algorithm 2 BPEGeneric  Basic Parallel Enumeration  Input      G  a query graph with quanti   ers q1             qN     MEMO  a concurrent global MEMO Output  an optimal bushy join tree 1  loop 2  atomic   qs1  qs2  e      GetNextQSPair  G    3  if e   NO MORE PAIR then 4  return  5  atomic   6  if CheckDependency qs1   qs2    true then retry  7    8  if qs2       then   qs1 must be a singleton    9  newP lans     CreateTableAccessPlans qs1    10  else 11  newP lans    CreateJoinPlans  MEMO qs1   MEMO qs2    12  PrunePlans MEMO qs1     qs2   newP lans     Here  we assume that GetNextQSPair always returns a feasible join pair of quanti   er sets  With the state of the art graph traversal driven enumerators such as DPcpp and DPhyp  we can directly generate feasible pairs of quanti   er sets only  On the other hand  in other join enumerators  we must execute a series of    lters to    nd a feasible join pair  BPEGeneric is generic in that any type of join enumerator can be employed by calling the overloaded subroutine GetNextQSPair of a speci   c join enumerator  We note that BPEGeneric must use a concurrent global MEMO table  since each thread can 1  concurrently access the MEMO table and 2  concurrently operate on MEMO entries to add and remove QEPs  We use the concurrent hash map in Intel Threading Building Block  28   which is known for its good scalability  Before calling CreateJoinPlans  BPEGeneric invokes the subroutine CheckDependency to ensure that neither qs1 nor qs2 is dependent on any pair of quanti   er sets  qs   1  qs   2  being processed by all the other threads  To do so  it checks whether either qs1 or qs2 is a superset of qs   1     qs   2  For example  suppose that  q1q2q3  q4  is about to be processed in thread Ta  and  q1q2  q3  is being processed in thread Tb  In this case  thread Ta must wait until thread Tb    nishes the processing of  q1q2  q3  since the quanti   er set q1q2q3 is dependent on  q1q2  q3   To minimize memory allocation deallocation synchronization con     icts  each thread uses a per thread memory manager  Thus  when 47a thread allocates a memory region or deallocates its own previously allocated memory region  it does not need any synchronization efforts  However  when a thread Ta tries to deallocate memory regions  e g   QEPs  allocated by another thread Tb as in P runeP lans  Ta adds a free request to the queue maintained by Tb  and Tb periodically deallocates its free requests  BPEGeneric employs    dynamic    search space allocation as opposed to static search space allocation employed in  14   That is  each thread consumes only one join pair at a time by calling GetNextQSPair  However  in  14   for each size of the resulting quanti   er set  the search space  i e   nested loops  for that size is    rst evenly divided  Then  each divided search space is allocated to a thread  Thus  although perfectly evenly divided search spaces are allocated to threads  the slowest thread can hold up all the other  faster threads  leading to severely biased workloads  Since the unit of search space allocation in BPEGeneric is a pair of quanti   er sets  the maximum delay incurred by BPEGeneric is the processing time of one pair of quanti   er sets  which is negligible  However  BPEGeneric pays the price for dynamic search space allocation  incurring a lot of synchronization overhead by using GetNextQSPair  CheckDependency  and the concurrent MEMO  Only one thread executes GetNextQSPair at a time  so the other threads must wait if they try to invoke it concurrently  Due to tangled dependencies  when a pair of quanti   er sets qs is being processed by a thread Ta  Ta must execute CheckDependency to check whether there is another thread Tb currently processing any other pair that qs depends on  This presents the question     Can we avoid such synchronization overhead while using dynamic search space allocation     This motivates us to develop a completely new approach that exploits 1  separation of join pair generation and plan generation  and 2  dependency aware reordering  That is  to avoid the synchronization overhead incurred by GetNextQSPair  we separate join pair generation from plan generation  That is  we    rst generate pairs of quanti   er sets by using one thread  and then  perform plan generation using multiple threads  We explain dependency aware reordering in the next section  4  SCALABLE PARALLEL ENUMERATION The formal foundation of our dependency aware reordering technique is presented in Section 4 1  More speci   cally  we propose the novel concepts of valid reordering  partial orders over search spaces  and dependency aware reordering based on group topological sort  In Section 4 2  we propose an enhanced generic algorithm called DPEGeneric that exploits dependency aware reordering as well as separation of join pair generation from plan generation  and we show the validity of the join pair sequence reordered by DPEGeneric  4 1 Partial Orders over Search Spaces To avoid synchronization con   icts  we convert the total order over join pairs into a partial order over unordered groups of pairs  The rational for grouping is that threads can generate QEPs independently for all pairs within a group without synchronization con     icts  However  grouping may reduce the opportunity for early termination  Thus  a grouping method supporting early termination must be devised  Grouping also may increase waiting time  For example  although a group G1 is dependent on a group G2  we may process some entries in G1 before completely processing all entries of G2  This phenomenon is explained in detail in Section 5 3  Before explaining detailed grouping methods  we formally de     ne several important concepts  When we reorder an incoming sequence of pairs of quanti   er sets  called join pair sequence   we make sure that the reordered sequence is valid as well  Otherwise  we can not guarantee that the reordered sequence can generate the same    nal QEP as the original sequence  The following de   nition formally de   nes validity of a join pair sequence  De   nition 1  A join pair sequence S is valid if any pair in S depends solely on its preceding pairs  Otherwise  the join pair sequence is invalid  We now de   ne the important property of valid reordering in Definition 2  De   nition 2  A reordering is valid if it transforms one valid join sequence into another valid join sequence  In order to    nd the valid reorderings for a streaming join pair sequence  we construct inherent partial orders over a set of unordered groups from the streaming join pair sequence  where no dependencies exist among entries in such a group  Then  we generate a totally ordered sequence by using group topological sort over the partial order  The formal de   nition of group topological sort is in De   nition 3  Theorem 1 states the validity of a reordered sequence generated by group topological sort  De   nition 3  For a given partial order P over a set of unordered groups  group topological sort performs topological sort  8  over P and obtains a totally ordered group sequence  Then  for each group G obtained  group topological sort generates a permutation for G  Theorem 1  For a given partial order P over a set of unordered groups  any totally ordered sequence generated by group topological sort from P is valid  PRO O F  We prove by contradiction  Assume that group topological sort generates an invalid sequence S     Let S   be s1s2   sm  Then  by De   nition 1  there exist two entries si and sj in the sequence such that i   j  and si depends on sj   Let unordered group G1 and G2 be the unordered groups that contain si and sj   respectively  Note that there is no order in entries in a group  Thus  due to the property of the partial order  G1 must depend on G2  Since group topological sort performs topological sort over the set of unordered groups  all entries in G2 must precede all entries in G1  Thus  it contradicts the assumption above that si precedes sj   Since there can exist several partial orders over a set of unordered groups from the original join pair sequence  the one which best maximizes parallelism should be chosen  To measure goodness of a partial order  we use the following three criteria  1  early termination is supported  2  the cost of maintaining the partial order over streaming join pairs is minimized in the multi threaded environment  and 3  waiting time due to dependencies is minimized  Therefore  if a partial order satis   es all three criteria  then parallelism is maximized while early termination is supported  Different partial orders can be formed depending on how we group join entries as follows  We note that  in order to use Theorem 1  each group must have no dependencies among entries within the group  1  Group by the resulting quanti   er set  Each group is called an RQS group  2  Group by the size of the resulting quanti   er set  Each group is called an SRQS group  3  Group by the size of the larger quanti   er set in the join pair  Each group is called an SLQS  484 1 1 Group by the resulting quanti   er set Let URQS be a set  of RQSs  We can de   ne a binary relation  RQS on URQS  where for any  rqs  rqs     in  RQS  rqs is a subset of rqs     The binary relation  RQS is a partial order since it is 1  re   exive  rqs  RQS rqs   2  anti symmetric  if rqs  RQS rqs   and rqs    RQS rqs  then rqs   rqs      and 3  transitive  if rqs  RQS rqs   and rqs    RQS rqs      then rqs  RQS rqs       for all rqs  rqs     rqs    in URQS  Figure 2 shows the partial order  RQS for the sequence of pairs of quanti   er sets in Figure 1  Each rounded box represents an RQS  q1  q1  q2  q2  q3  q3  q4  q4   q1  q2  q1q2  q1  q3  q1q3  q1  q4  q1q4  q2  q3  q2q3  q2  q4  q2q4  q3  q4  q3q4  q1  q2q3    q2  q1q3    q3  q1q2  q1q2q3  q1  q2q4    q2  q1q4    q4  q1q2  q1q2q4  q1  q3q4    q3  q1q4    q4  q1q3  q1q3q4  q2  q3q4    q3  q2q4    q4  q2q3  q2q3q4  q1  q2q3q4   q2  q1q3q4   q3  q1q2q4   q4  q1q2q3    q1q2  q3q4   q1q3  q2q4   q1q4  q2q3  q1q2q3q4 Figure 2  An example of partial order  RQS  We explain how to construct  RQS from the incoming join pair sequence  For each entry  qs1  qs2   we create an RQS corresponding to qs1     qs2 if the RQS has not been created  and add two incoming edges to that RQS  one from the RQS corresponding to qs1 and the other from the RQS corresponding to qs2  Due to the transitivity property of  RQS  one might think that we don   t need to add any edge between an RQSi and an RQSj when RQSi is a subset of RQSj and  RQSi      RQSj     1  For example  the addition of the direct edge between RQS q1q2q3 and RQS q3 doesn   t seem to be necessary  However  in general  its omission is not allowed  Consider a query where there is only one join predicate q1 a q2 b   q3 c between q1q2 and q3  This leads to a hyperedge  22  in the query graph  Thus  the direct edge between RQS q1q2q3 and RQS q3 must not be removed  To use Theorem 1 for the partial order  RQS  all entries of each group in  RQS have no dependencies among them  Lemma 1 guarantees this property  Lemma 1  No dependencies exist among entries in an RQS  PRO O F  Refer to  15   We now discuss the pros and cons of the partial order  RQS  1  In terms of early termination   RQS does not support early termination  since the topmost RQS q1q2q3q4 depends on all RQSs underneath  2  In terms of maintenance cost  maintaining  RQS would be expensive due to concurrent edge removals in multi thread environments  That is  when thread Ta    nishes processing an RQS  Ta must remove all of its outgoing edges  This can incur signi   cant synchronization cost  3  In terms of waiting time  as soon as the processing of an RQS is completed  we can    nd a set of RQSs that have no incoming edges  and thus  can be processed concurrently  The entries in an RQS can also be processed concurrently according to Lemma 1  minimizing waiting time  4 1 2 Group by the size of the resulting quanti   er set Let USRQS be a set of SRQSs  We denote SRQSi as an SRQS that contains every join pair whose resulting quanti   er set size is i  We can de   ne a binary relation  SRQS on USRQS  where for any  SRQSi   SRQSj   in  SRQS  i     j  The binary relation  SRQS is a partial order since it is 1  re   exive  SRQSi  SRQS SRQSi   2  anti symmetric  if SRQSi  SRQS SRQSj and SRQSj  SRQS SRQSi  then SRQSi   SRQSj    and 3  transitive  if SRQSi  SRQS SRQSj and SRQSj  SRQS SRQSk  then SRQSi  SRQS SRQSk   for all SRQSi  SRQSj   SRQSk in USRQS  Figure 3 shows the partial order  SRQS for the sequence of pairs of quanti   er sets in Figure 1  Each rounded box represents an SRQS   q1   q2   q3   q4  SRQS1  q1  q2   q1  q3   q1  q4   q2  q3   q2  q4   q3  q4  SRQS2  q1  q2q3   q1  q2q4   q1  q3q4   q2  q3q4    q2  q1q3   q2  q1q4   q3  q1q4   q3  q2q4    q3  q1q2   q4  q1q2   q4  q1q3   q4  q2q3  SRQS3  q1  q2q3q4   q2  q1q3q4   q3  q1q2q4   q4  q1q2q3    q1q2  q3q4   q1q3  q2q4   q1q4  q2q3  SRQS4 Figure 3  An example of partial order  SRQS  Now  we explain how to construct  SRQS from the incoming join pair sequence  Given a query having n quanti   ers  we create in advance n SRQSs and add outgoing edges from SRQSi to SRQSj where i   1     j     n  For each entry  qs1  qs2  in the join pair sequence  we add that entry to SRQS qs1   qs2    Lemma 2  No dependencies exist among entries in an SRQS  PRO O F  Refer to  15   Next  we discuss the pros and cons of the partial order  SRQS  Like  RQS   SRQS does not support early termination either  since the topmost SRQS depends on all SRQSs underneath  However  the cost of maintaining  SRQS is negligible  since we only need to access SRQSs in the increasing order of the size of the resulting quanti   er sets  No synchronization is needed to process entries within the same SRQS  thus minimizing waiting time  4 1 3 Group by the size of the larger quanti   er set in the join pair Let USLQS be a set of SLQSs  We denote SLQSi as an SLQS that contains every join pair whose larger quanti   er set size is i  To represent a set of single quanti   ers  not join quanti   ers   SLQS0 is used  We can de   ne a binary relation  SLQS on USLQS  where for any  SLQSi   SLQSj   in  SLQS  i     j  We can easily verify that the binary relation  SLQS over USLQS is a partial order  Figure 4 shows the partial order  SLQS for the sequence of pairs of quanti   er sets in Figure 1  Each rounded box represents an SLQS  The construction of  SLQS from the incoming join pair sequence is similar to that of  SRQS  That is  we create the partial order for a given query in advance  For each pair  qs1  qs2  in the incoming join sequence  we add this pair to SLQSmax  qs1   qs2   if qs2 is not empty  If qs2 is empty  we add the pair to SLQS0  49SLQS0 SLQS1 SLQS2 SLQS3 SLQS 1 0  SLQS 1 1  SLQS 1 2  SLQS 2 2  SLQS 1 3   q1   q2   q3   q4   q1  q2   q1  q3   q1  q4    q2  q3   q2  q4   q3  q4   q1  q2q3    q1  q2q4    q1  q3q4    q2  q1q3    q2  q1q4    q2  q3q4    q3  q1q2    q3  q1q4    q3  q2q4    q4  q1q2    q4  q1q3    q4  q2q3   q1q2  q3q4    q1q3  q2q4    q1q4  q2q3   q1  q2q3q4   q2  q1q3q4   q3  q2q1q4   q4  q1q2q3  Figure 4  An example of partial order  SLQS  Lemma 3  No dependencies exist among entries in an SLQS  PRO O F  Refer to  15   We can further divide SLQSi into a set of subgroups by the size of the smaller quanti   er set of the join pair  SLQS j i   j     i  denotes a subgroup of SLQSi that contains every join pair whose smaller quanti   er set size is j  As a special case  SLQS 1 0  represents SLQS0  Let us discuss pros and cons of the partial order  SLQS  Unlike  RQS and  SRQS   SLQS supports early termination  Suppose that a given query has N quanti   ers  Then  we have N levels in  SLQS  When we reach level  N 2    we can obtain QEPs containing all quanti   ers by processing join entries in SLQS N    N 2    N 2      For example  when we process entries in SLQS 2 2  in Figure 4  we can obtain QEPs containing all four quanti   ers in the query  Thus  we may terminate the optimization process if the obtained QEPs are good enough  Like  SRQS  the cost of maintaining  SLQS is negligible  and waiting time is minimized since it has the same number of edges in the partial order as  SRQS  It is clear that the partial order  SLQS is the best one among the three we have considered   Using the same experimental  setup in Section 6  we empirically veri   ed that  with  SLQS  the    rst join entry that contains all quanti   ers is processed just after processing 33 4      62 5  of the total join entries  depending on query topologies  Thus  hereafter we use  SLQS as a default partial order  We will empirically show that we can achieve linear speedup using  SLQS in Section 6  4 1 4 Discussion on grouping We proposed three grouping methods and concluded that grouping by SLQS was the best grouping method of the three  However  there might exist better approaches than SLQS  In fact  an SLQS may be partitioned into smaller groups  In this case  parallelism and the partial order management overhead could grow as we allow more groups  Thus  an interesting future topic would be to    nd the optimal grouping method  4 2 Dependency Aware Parallel Enumeration To convert a totally ordered join pair sequence to a partial order  SLQS 3   we allocate a concurrent dependency buffer B j i  for each 3 For ease of exposition  we use SLQS as a default partial order  SLQS j i    We denote B     i  as a set of dependency buffers corresponding to SLQSi  The dependency buffer B j i  is implemented as a concurrent queue that supports two core operations  Push and Pop  Here  we do not need any synchronization for Push  since we separate join pair generation from plan generation  We need synchronization for Pop since multiple threads can consume entries from a dependency buffer concurrently  However  the overhead for such synchronization is negligible as we will see in our extensive experiments in Section 6  Note that we do not use monitoringbased load balancing techniques as in  25   since such monitoring can incur non negligible overhead in CPU bound jobs  We also note that we have to buffer only a    xed size of join pairs to avoid a huge memory footprint size  Algorithm 3 shows a dependency aware parallel enumeration algorithm called DPEGeneric  The main thread invokes DPEGeneric  It then invokes the subroutine EnumAndBuildPartialOrder to convert a    xed number of join pairs into a partial order over SLQSs  To do so  EnumAndBuildPartialOrder repeatedly invokes GetNextQSPair to generate a join pair and pushes the pair to the corresponding dependency buffer  delaying plan generation for the pairs generated  Line 2   Here  to control the maximum number of join pairs to generate  MAXENUMCNT is used  More speci   cally  in EnumAndBuildPartialOrder  if a join pair generated is  qs1  qs2   this pair is pushed to B  qs1   qs2     After that  for each SLQSi  Line 4   all threads concurrently consume join pairs in B j i  for all j by executing GenerateQEPs  Lines 5 and 6   This step corresponds to group topological sort in parallel  After the processing of SLQSi is completed  Line 7   the main thread merges per thread MEMOs into the global MEMO  Line 8   We repeat these steps until all join pairs are consumed  Algorithm 3 DPEGeneric  Dependance Aware Parallel Enumeration  Input      G  a connected query graph with quanti   ers q1              qN     MEMO  a non concurrent global MEMO     memot  a local memo for thread t Output  an optimal bushy join tree Variable enumeration buffer B 1  loop 2  e     EnumAndBuildPartialOrder G  B  MAXENUMCNT   3  if e   NO MORE PAIR then break  4  for i     0 to N     1   increase the larger quanti   er set size   5  for t     1 to m   Execute m threads in parallel   6  pool SubmitJob GenerateQEPs B     i   memot    7  pool Sync    8  MergeAndPrunePlans MEMO   memo1            memom    9  return MEMO q1          qN   Function GenerateQEPs Input      B     i    a set of enumeration buffers large size is i     memot  a local memo for thread t 1  j     1  2  repeat 3  loop 4  atomic   qs1  qs2  e      Pop  B j i      5  if e   NO MORE PAIR then break  6  if qs2       then   qs1 must be a singleton    7  newP lans     CreateTableAccessPlans qs1    8  else 9  newP lans    CreateJoinPlans MEMO qs1   MEMO qs2    10  PrunePlans memot  qs1     qs2   newP lans   11  j     j   1  12  until j     i and j   i     N However  other partial orders can easily applied to Algorithm 3  50Like BPEGeneric  DPEGeneric employs dynamic search space allocation  That is  in GenerateQEPs  each tread only consumes one join pair at a time by calling Pop  Line 4   Since the unit of search space allocation in DPEGeneric is a pair of quanti   er sets  the maximum delay among threads incurred by GenerateQEPs is the processing time of one pair of quanti   er sets  which is negligible  As opposed to BPEGeneric  DPEGeneric has no GetNextQSPair and CheckDependency con   icts  Theorem 2  For a given valid sequence of join pairs  DPEGeneric always generates a valid reordered sequence of the join pairs  PRO O F  Let the original join sequence generated by the serial join pair enumerator be S   s1          sn  S must be valid  The main thread divides S into a series of subsequences by buffering each subsequence in EnumAndBuildPartialOrder at a time  Here  the size of each subsequence except the last subsequence is MAXENUMCNT  Thus  it is suf   cient to show that DPEGeneric generates the valid reordered sequence for each subsequence buffered using Lines 4   6 in DPEGeneric  The for loop in Line 4  corresponding to topological sort over SLQSs  and the repeat until loop in GenerateQEPs  corresponding to a permutation using multiple threads  correspond to group topological sort  Due to Theorem 1  DPEGeneric generates a valid reordered sequence for each buffered subsequence  This completes the proof  Example 1  Figure 5 depicts how DPEGeneric operates using an example  Suppose that MAXENUMCNT is set to 15  The main thread    rst invokes EnumAndBuildPartialOrder G  B  15  in serial  Line 2   Figure 5 a  shows a snapshot of a set of dependency buffers corresponding to a partial order over SLQSs after buffering the    rst 15 entries from the incoming sequence of join pairs of Figure 1  For each dependency buffer in increasing order of the larger quanti   er set size  each thread concurrently executes GenerateQEPs using its per thread MEMO  Lines 4     6   That is  each thread Ta concurrently consumes entries in B     i  using memoa  After that  the main thread merges all per thread MEMOs into the global MEMO  We repeat these above steps until we consume all join entries  That is  the main thread invokes EnumAndBuildPartialOrder G  B  15  again  Line 2   Figure 5 b  shows a snapshot of a set of dependency buffers after the remaining 14 entries are buffered  Line 2   5  MAXIMIZING PARALLELISM  IN DEPTH Although DPEGeneric solves fundamental problems of BPEGeneric  GetNextQSPair and CheckDependency con   icts   two steps in DPEGeneric still run in serial  1  join pair generation and 2  the merging of per thread MEMOs into the global MEMO  The third problem with DPEGeneric is that the dependency buffers must be processed in sequence  so buffered join pairs in other dependency buffers may be ready for processing  but the threads can   t process such pairs in advance of the current dependency buffer  This is especially problematic when the current dependency buffer does not have a suf   cient number of join pairs for all threads  since some threads are not fully utilized  In this section  we propose three optimization techniques resolving all three problems  5 1 Pipelining of Join Pair Generation and Plan Generation Suppose that there are m threads available  In order to utilize the other m   1 threads while the main thread generates join pairs  we leverage pipeline parallelism  13   To do so  we regard the main B 1 0   q4   q3   q2   q1  B 1 1   q3  q4   q2  q4   q2  q3   q1  q4   q1  q3   q1  q2  B 1 2   q2  q3q4   q4  q2q3   q3  q2q4   q1  q3q4   q1  q2q3  B 2 2  B 1 3   a  After the    rst 15 entries are buffered  B 1 0  B 1 1  B 1 2   q1  q2q4   q4  q1q2   q3  q1q2   q4  q1q3   q2  q1q3   q3  q1q4   q2  q1q4  B 2 2   q1q2  q3q4   q1q3  q2q4   q1q4  q2q3  B 1 3   q1  q2q3q4   q4  q1q2q3   q3  q1q2q4   q2  q1q3q4  front  b  After the remaining 14 entries are buffered  Figure 5  An example of DPEGeneric  thread generating join pairs as a producer and the other threads generating QEPs for the join pairs generated as consumers  More speci   cally  the main thread    rst generates and buffers a    xed number of join pairs  After that  all threads except the main thread consume the buffered join pairs  At the same time  the main thread generates a    xed number of join pairs for the next iteration  Note that the time needed to generate a    xed number of join pairs is much smaller than the time needed to generate QEPs for those join pairs generated  After the main thread    nishes join pair generation  together with the other threads  it participates in consuming the remaining buffered sequence  This way  all m threads are fully utilized  To avoid synchronization con   icts on dependency buffers  we use dual buffers for each dependency buffer B j i    5 2 Synchronization Free Global MEMO Unlike BPEGeneric  DPEGeneric uses per thread MEMOs to avoid synchronization con   icts during plan generation  However  it needs the additional step of merging per thread MEMOs to the global MEMO  Another problem with per thread MEMOs is that the total memory footprint size  in the worst case  can grow in proportion to the number of threads  By carefully analyzing operational semantics for the global MEMO  we develop a synchronization free global MEMO resolving these problems  To this end  we propose two novel concepts  1  equivalence class grouping and 2  earmark and pinning  Before explaining these concepts in detail  we brie   y explain the structure of the MEMO table  The MEMO table of conventional optimizers  such as those of DB2 and PostgreSQL  26   is typically implemented as a chained hash table with the quanti   er set as the key  The MEMO table can be dynamically reconstructed using a different hash function and a different hash table size  e g   when the length of a hash chain is larger than a prede   ned threshold  Each hash bucket corresponds to a MEMO entry that contains a resulting quanti   er set and a pointer to the plan chain for the quanti   er set  Figure 6 depicts a MEMO table  As shown in Figure 6  we note that there are two different types of linked lists in the MEMO table  the hash chain and the plan chain  Thus  two different types of synchronization con   icts can 51Hash table q1q2q3q4 MGJN HSJN q1q2 NLJN q3q4 HSJN   a MEMO entry   a QEP node   a hash chain   a plan chain Figure 6  A MEMO table  occur if threads manipulate these chains concurrently  To remove synchronization con   icts for the plan chain  we propose the concept of equivalence class grouping  To remove synchronization con   icts for the hash chain  we propose the concept of earmarkand pinning  Equivalence class grouping  We    rst de   ne an equivalence class in De   nition 4  and describe how equivalence classes are used to remove synchronization con   icts for the plan chain  De   nition 4  An equivalence class in the dependency buffer B j i  is a set of join pairs in B j i  whose resulting quanti   er sets are the same  Equivalence class grouping for B j i  subgroups join pair entries in B j i  by their equivalence classes  The set of equivalence classes in B j i  de   nes a partition over the set of join pairs in B j i    To support equivalence class grouping  each entry in B j i  stores an equivalence class  rather than a single join pair  The following lemma states that  with equivalence class grouping  threads incur no synchronization con   icts for any plan chain in the global MEMO  Lemma 4  Given dependency buffers B     i  currently being processed by threads  if all entries in an equivalence class are consumed by only one thread  there occurs no synchronization con   ict for plan chains  PRO O F  A plan chain is updated only by the subroutine PrunePlans  where newly created QEPs are added to the plan chain  and expensive QEPs are pruned from the plan chain  Thus  it is clear that  unless any two threads invoke PrunePlans for the same MEMO entry  no synchronization con   ict for plan chains occurs  Suppose that two threads Ta and Tb process two different join pairs  qs1  qs2  in B j i  and  qs   1  qs   2  in B j    i  concurrently  When j    j     Ta and Tb invoke PrunePlans for different MEMO entries  since  qs1     qs2      qs   1     qs   2   Even when j   j     Ta and Tb invoke PrunePlans for different MEMO entries  since both join pairs are from different equivalence classes  i e   qs1     qs2    qs   1     qs   2  We now discuss the ratio of the maximum size of an equivalence class over the total number of join pairs 4 according to the topology of the query graph  The ratio must be small  since the unit of allocation to threads is an equivalence class  For example  if we have a clique query 5 having 18 quanti   ers  the ratio is only 0 023   Furthermore  since we buffer a    xed number of join pairs at a time  the size of a buffered equivalence class is much smaller than these theoretical values  Earmark and Pinning  In order to remove synchronization con     icts for the hash chain  the main thread generating join pairs pre  4 The join pairs here include all singleton quanti   er sets  5 Each quanti   er in a clique query with N quanti   ers is connected to all the other quanti   ers using N 1 edges  allocates a MEMO entry for each equivalence class buffered if such MEMO entry has not been previously created  This step is called earmarking  To pin the MEMO entry earmarked  a join pair entry in the dependency buffer stores a pointer to it  Thus  to support earmark and pinning  each entry in the dependency buffer is changed to the form of  m qs1     qs2   m qs1   m qs2    where m qs  represents the memory address of the MEMO entry corresponding to qs  This way  during plan generation  threads directly access MEMO entries without accessing any hash chain or hash array in the  MEMO  table  Example 2  Figure 7 depicts a synchronization free global MEMO that exploits equivalence grouping and earmark and pinning  Recall Example 1  Suppose that the main thread invokes EnumAndBuildPartialOrder G  B  15  for the second time  As shown in this    gure  each entry in a dependency buffer is an equivalence class  For example  in B 2 2    the MEMO entry for the equivalence class q1q2q3q4 has not been created in the previous iteration  The main thread earmarks a MEMO entry for the equivalence class  During plan generation  when an equivalence class is popped by Thread Ta  Ta directly accesses the MEMO entry corresponding to the equivalence class using m q1q2q3q4  without accessing any hash chain or hash array  Note that all the other threads except Ta never access m q1q2q3q4  concurrently according to Lemma 4  Hash table q1q2q3q4 earmark and pinned q1q2 NLJN q3q4 HSJN A synchronization free global MEMO B 1 2   m q1q2q3   m q2   m q1q3    m q1q2q3   m q3   m q1q2    m q1q2q4   m q1   m q2q4    m q1q2q4   m q2   m q1q4    m q1q2q4   m q4   m q1q2    m q1q3q4   m q3   m q1q2    m q1q3q4   m q4   m q1q3   B 2 2   m q1q2q3q4   m q1q2   m q3q4    m q1q2q3q4   m q1q3   m q2q4    m q1q2q3q4   m q1q4   m q2q3   Figure 7  A synchronization free global MEMO exploiting equivalence grouping and earmark and pinning  5 3 Threading Across Dependencies When there is an insuf   cient number of join pairs at the current level to fully utilize all threads  the threading across dependencies technique enables threads to process dependency buffers in upper levels without waiting  More speci   cally  when a thread processes entries in B     i    the other threads can process an entry e in B     k   k   i  without waiting whenever all entries upon which e depends are already consumed either in the current or previous iterations  Such entries are called dependency free entries  In order to determine which join entry is dependency free  each MEMO entry e has an additional    eld named numEntry that stores the number of buffered join entries for e  Given a join entry  m qs1    qs2   m qs1   m qs2   in a dependency buffer  if the values of the numEntry    elds of both m qs1  and m qs2  are 0  this join entry is dependency free  To maintain the correct number of buffered join entries in each MEMO entry  we perform the following operations  Before the main thread pushes an entry  m qs1     qs2   m qs1   m qs2   to a 52dependency buffer  it increases the numEntry    eld of the MEMO entry m qs1     qs2  by one  using a hardware atomic instruction  For example  in Figure 7  the MEMO entry for q1q2q3q4 has three buffered entries in B 2 2    The MEMO entry for q1q2 has no buffered entries  During plan generation  after a thread consumes an entry  m qs   1     qs   2   m qs   1   m qs   2   in a dependency buffer  the thread decreases the numEntry    eld of the MEMO entry m qs   1     qs   2  by one  also using a hardware atomic instruction  In this way  each MEMO entry maintains the correct number of remaining buffered entries  During plan generation  when a thread consumes a join entry  m qs1     qs2   m qs1   m qs2   in the dependency buffer  it    rst checks  using hardware atomic instructions  whether both this entry is dependency free  If not  the thread must wait until the entry has no dependency  To minimize such wait time due to threading across dependencies  we move dependency free entries to the front of each dependency buffer during join pair generation  Thus  when some threads start to process dependency buffers in upper levels during plan generation  they access dependency free entries    rst  With equivalence grouping  an entry in B     i  is an equivalence class which has a set of join pairs whose resulting quanti   er sets are the same  Thus  if all join pairs in the equivalence class are dependency free  we move the equivalence class to the front of the dependency buffer  Example 3  Figure 8 depicts an example of how the threading across dependencies technique operates  Buffers on the lefthand side show a snapshot of dependency buffers before applying the technique  The right hand side shows dependency buffers after dependency free entries are moved to the front  With threading across dependencies  thread Tb can concurrently consume dependencyfree entries from B     i 1  without any synchronization con   ict while Ta processes entries from B     i    B     i  front B     i 1  B     i 2  Threading Across Dependencies B     i  Ta B     i 1  Tb B     i 2    dependency free entries Figure 8  An example of threading across dependencies  Theoretically  with threading across dependencies  threads can process entries of dependency buffers in many different levels  and thus  the threads may access the plan chain of the same MEMO entry concurrently  Thus  to guarantee the correctness of the threading across dependencies technique  we need synchronization when we access the plan chain  However  in reality  synchronization con     icts on the same plan chain due to threading across dependencies rarely occur  Through extensive experiments in Section 6  the ratio of the number of synchronization con   icts over the total number of join pairs is only 0      0 00064   depending on query topologies  We will empirically show in Section 6 that the wait time due to synchronization overhead of the threading across dependencies technique is negligible  6  PERFORMANCE EVALUATION The goals of our experiments are as follows      To determine how much the three optimization techniques proposed in Section 5 contribute to maximize the parallelism of DPEGeneric  in Section 6 1      To show that our parallel algorithm supports any type of bottom up enumeration  achieving linear speedup for each type  in Section 6 2       To show that our parallel algorithm is even better than the state of the art parallel optimizer tailored to size driven enumeration  PDPsva  in Section 6 3       To show that our parallel algorithm is much more robust than PDPsva with respect to search space allocation  especially when a core is heavily loaded   in Section 6 4  We evaluated the same four representative query topologies as  14   linear  cycle  star  and clique  In order to ensure that our solution never slows down optimization  our parallel optimizer is invoked only when the number of join pairs buffered exceeds a certain threshold  i e   when there is a suf   cient number of joins pairs to fully utilize multiple threads  All the experiments were conducted on a PC with two Intel Xeon Quad Core E5310 1 6GHz CPUs   8 cores  and 8 GB RAM  running Windows Vista  Each CPU has two 4Mbyte L2 caches  each of which is shared by two cores  We implemented all algorithms in PostgreSQL 8 3  26  to see the performance trends in a full    edged DBMS  Since the optimization component in PostgreSQL was not thread safe  we modi   ed it signi   cantly in order to be thread safe  Furthermore  there were many places in the original code where memory was not released during query optimization  We also    xed all such problems by calling memory deallocation functions ef     ciently  Since    xed sized structures  such as list cells  are extensively used  we used two types of memory managers to minimize the total memory allocation size  one for variable sized structures and the other for    xed sized structures  Unlike a commercial DBMS  during plan pruning  PostgreSQL uses a fuzzy costing comparison function that considers both the total cost and the startup cost of a plan  which tends to accumulate unnecessary plans in the plan chain  However  this costing mechanism is only useful for top k plans having the LIMIT clause  Since we focus on nontop k plans  we only exploited the total cost of a plan during plan pruning  The performance metrics are the speed up and the elapsed time  where the speedup is de   ned as the ratio of the elapsed time of the serial algorithm over that of its parallel counterpart  Table 1 summarizes the experimental parameters and their values  Note that we used the same parameter values as  14   We omit all experimental results for linear and cycle queries  because the total number of join pairs are generally too small to bene   t from parallelization  Our main focus is to reduce compilation times for large data warehouse OLAP queries which are typically star shaped  OLTP queries are not our focus  since they can be optimized very fast  An experimental study on a real DB2 query workload  18  veri   ed that compilation time is dominated by the number of join re orderings  Thus  we believe that star queries  for varying the number of joins  indeed model representative real data warehouse queries and are suf   cient to show our claim  For the same reason as  14  21  22   we used large clique queries to show the worst case scenario  which we believe is theoretically meaningful  53Table 1  Experimental parameters and their values  Parameter Default Range join pair enumerator DPcpp DPcpp  DPhyp  DPsva query topology star  clique star  clique   of quanti   ers 20  18 10  12  14  16  18  20   of threads 8 1     8 6 1 Impact of the Three Optimization Techniques We are interested in learning the impact of the three optimization techniques  in Section 5  on DPEGeneric  with respect to query topologies and the number of threads  The three optimization techniques are called P  S  and T  P stands for the pipelining of join pair generation and plan generation  S for synchronizationfree global MEMO  and T for threading across dependencies  For example  DPEGeneric PST denotes the DPEGeneric algorithm with the three optimization techniques  Figure 9 shows the experimental results for varying the number of quanti   ers for star and clique queries using 8 threads  The results are presented using the speedup between the serial DPcpp algorithm and the DPEGeneric algorithm with the speci   ed optimization technique s   For star queries  only DPEGeneric PST  which exploits the three optimization techniques  achieves linear speedup when the number of quanti   ers is 20  This is because the number of join pairs is large enough to exploit 8 parallel threads  and the three optimization techniques maximize parallelism  DPEGeneric PS achieves 7 1 times speedup  and DPEGeneric Pachieves 3 9 times speedup  while DPEGeneric alone achieves 3 3 times speedup  Clique queries achieve higher overall speedups than comparable star queries having the same number of quanti   ers  because the number of join pairs in clique queries are much larger than those in equally sized star queries  For clique queries  DPEGeneric PST also achieves linear speedup due to the sophisticated optimization techniques  Note that the threading across dependency technique is more effective in star queries than in clique queries  since join entries buffered in star queries are spread across many dependency buffers  and thus  there exist dependency buffers that have insuf     cient number of pairs to fully utilize all threads  DPEGeneric PST DPEGeneric PS DPEGeneric P DPEGeneric  1  2  3  4  5  6  7  8  10 12 14 16 18 20 speedup   of quantifiers  a  Star queries   1  2  3  4  5  6  7  8  10 12 14 16 18 speedup   of quantifiers  b  Clique queries  Figure 9  Effect of three optimization techniques by varying the number of quanti   ers  8 threads   Figure 10 a  shows the impact of the three optimization techniques on DPEGeneric for varying the number of threads over the serial DPcpp for star queries  Figure 10 b  shows the same for clique queries  Regardless of query topologies  DPEGeneric PST achieves linear speedup as the number of threads increases  The speedup rates for DPEGeneric and DPEGeneric P diminish as the number of threads increases  This is because  as the per thread MEMOs increase  the cost of merging them into the global MEMO also increases  In some cases  we notice that slightly superlinear speedups happen  Note that this can often happen in multi core applications due to cache sharing  That is  threads in different cores can access shared data structures  such as the MEMO table  catalog structures  and enumeration codes  in 4 Mbytes L2 cache shared by two cores  DPEGeneric PST DPEGeneric PS DPEGeneric P DPEGeneric  1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  a  Star query with 20 quanti     ers   1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  b  Clique query with 18 quanti   ers  Figure 10  Effect of three optimization techniques by varying the number of threads  In the following experiments  we apply all three optimization techniques to DPEGeneric  6 2 Generality of Our Framework The generality experiment is to show that our parallel algorithm supports any type of bottom up enumeration  achieving linear speedup for each type  To this end  we use the three state of the art serial bottom up enumerators  DPcpp  DPhyp  and the size driven enumerator using the skip vector array called DPsva  Note that DPsva is the fastest size driven enumerator which avoids generating infeasible join pairs using the skip vector array  14   We    rst perform experiments using the three serial enumerators before showing speedup using their parallel counterparts  since there have been no experimental comparisons reported of DPcpp  DPhyp  and DPsva  Figure 11 shows experimental results for varying the number of quanti   ers for star and clique queries  As shown in this    gure  the elapsed time for all three enumerators is nearly the same  although DPcpp and DPhyp that generate join pairs by directly traversing a query graph are marginally faster than DPsva for clique queries having 16 and 18 quanti   ers  about 6    DP DPccp ccp DP DPhyp hyp DP DPsva sva  0 01  0 1  1  10  100  1000  10 12 14 16 18 20 elapsed time  sec    of quantifiers  a  Star queries   0 1  1  10  100  1000  10000  10 12 14 16 18 elapsed time  sec    of quantifiers  b  Clique queries  Figure 11  Experimental results of DPcpp  DPhyp  and DPsva by varying the number of quanti   ers  Figure 12 shows the generality of our framework  As we see here  DPEGeneric supports all three state of the art enumerators  We achieve perfect linear speedup for DPcpp  and nearly linear speedups for DPhyp and DPsva  Figure 13 shows the generality of our framework by varying the number of threads  Again  DPEGeneric achieves  nearly  linear speedup for all three enumerators  54DPEGeneric DP DPEGeneric DPccp ccp DPEGeneric DP DPEGeneric DPhyp hyp DPEGeneric DP DPEGeneric DPsva sva  0  1  2  3  4  5  6  7  8  10 12 14 16 18 20 speedup   of quantifiers  a  Star queries   1  2  3  4  5  6  7  8  10 12 14 16 18 speedup   of quantifiers  b  Clique queries  Figure 12  Effect of various enumerator by varying the number of quanti   ers  8 threads   DPEGeneric DP DPEGeneric DPccp ccp DPEGeneric DP DPEGeneric DPhyp hyp DPEGeneric DP DPEGeneric DPsva sva  1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  a  Star query with 20 quanti     ers   1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  b  Clique query with 18 quanti   ers  Figure 13  Effect of various enumerator by varying the number of threads  6 3 Comparison with Direct Competitors This experiment provides comparison of DPEGeneric with BPEGeneric  in Section 3   and PDPsva  the state of the art parallel enumerator for DPsva   The three state of the art serial enumerators are applied to DPEGeneric  Figure 14 shows experimental results for varying the number of quanti   ers for star and clique queries  BPEGeneric performs the worst due to serious synchronization overhead  DPEGeneric achieves linear speedup  while PDPsva only achieves  up to 6 1 speedup for star queries using 8 threads  That is  23  of the total cores are not utilized  For clique queries  the overall trend is similar  BPEGeneric DPccp DPEGeneric DPccp DPEGeneric DPhyp DPEGeneric DPsva BPEGeneric DPccp PDPsva DPEGeneric DPccp DPEGeneric DPhyp DPEGeneric DPsva PDPsva  0  1  2  3  4  5  6  7  8  10 12 14 16 18 20 speedup   of quantifiers  a  Star queries   1  2  3  4  5  6  7  8  10 12 14 16 18 speedup   of quantifiers  b  Clique queries  Figure 14  Experimental results for speed up by varying the number of quanti   ers  8 threads   Note that DPEGeneric DPsva and PDPsva use the same enumerator  but DPEGeneric DPsva consistently outperforms PDPsva due to the three sophisticated optimization techniques  Thus  in spite of being a generic solution  DPEGeneric is highly effective  Figure 15 shows experimental results for varying the number of threads  Again  DPEGeneric achieves linear speedup as the number of threads increases  consistently outperforming PDPsva and BPEGeneric  The performance curve of BPEGeneric increases very slowly as the number of threads increases  due to substantial synchronization overhead  BPEGeneric DPccp DPEGeneric DPccp DPEGeneric DPhyp DPEGeneric DPsva BPEGeneric DPccp PDPsva DPEGeneric DPccp DPEGeneric DPhyp DPEGeneric DPsva PDPsva  1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  a  Star query with 20 quanti     ers   1  2  3  4  5  6  7  8  1 2 3 4 5 6 7 8 speedup   of threads  b  Clique query with 18 quanti   ers  Figure 15  Experimental results for speed up by varying the number of threads  6 4 Robustness This experiment demonstrates the robustness of our parallel algorithm in a controlled setting  with respect to search space allocation  For this  we place a CPU intensive process on a speci   c core  This process loops in   nitely  executing two steps in each loop  the    rst step executes 100 000    oating point operations and the second sleeps for 1 millisecond  To increase loads on a speci   c core  we increase the number of    oating point operations accordingly  We use two thread scheduling policies  One policy  called static placement denoted as SP  places thread i to core i  The other policy relies on the underlying operating system thread scheduling policy denoted as OS  Figure 16 shows experimental results f</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware"/>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware"/>
        <doc>Query Processing Techniques for Solid State Drives  ###  Dimitris Tsirogiannis University of Toronto Toronto  ON  Canada dimitris cs toronto edu Stavros Harizopoulos HP Labs Palo Alto  CA  USA stavros hp com Mehul A  Shah HP Labs Palo Alto  CA  USA mehul shah hp com Janet L  Wiener HP Labs Palo Alto  CA  USA janet wiener hp com Goetz Graefe HP Labs Palo Alto  CA  USA goetz graefe hp com ABSTRACT Solid state drives perform random reads more than 100x faster than traditional magnetic hard disks  while o ering comparable sequential read and write bandwidth  Because of their potential to speed up applications  as well as their reduced power consumption  these new drives are expected to gradually replace hard disks as the primary permanent storage media in large data centers  However  although they may bene t applications that stress random reads immedi  ately  they may not improve database applications  espe  cially those running long data analysis queries  Database query processing engines have been designed around the speed mismatch between random and sequential I O on hard disks and their algorithms currently emphasize sequential accesses for disk resident data  In this paper  we investigate data structures and algo  rithms that leverage fast random reads to speed up selection  projection  and join operations in relational query process  ing  We  rst demonstrate how a column based layout within each page reduces the amount of data read during selections and projections  We then introduce FlashJoin  a general pipelined join algorithm that minimizes accesses to base and intermediate relational data  FlashJoin s binary join kernel accesses only the join attributes  producing partial results in the form of a join index  Subsequently  its fetch kernel retrieves the attributes for later nodes in the query plan as they are needed  FlashJoin signi cantly reduces mem  ory and I O requirements for each join in the query  We implemented these techniques inside Postgres and experi  mented with an enterprise SSD drive  Our techniques im  proved query runtimes by up to 6x for queries ranging from simple relational scans and joins to full TPC H queries  Categories and Subject Descriptors H 2 4  H 2 2  Database Management   Systems   query processing  Physical Design   access methods  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  General Terms Algorithms  Design  Performance  Keywords Flash memory  SSD  columnar storage  join index  late ma  terialization  semi join reduction  1  ###   INTRODUCTION Solid state drives  SSDs  are quickly penetrating the mar  ketplace with promises of improved performance and energy e ciency for both desktop and enterprise applications  SSDs perform random reads more than 100x faster than tradi  tional magnetic hard disks  while o ering comparable se  quential read and write bandwidth  Although current of  ferings are more expensive than magnetic hard disk drives  HDDs   a 2007 IDC study  13  showed that SSD prices are dropping much faster than HDD prices and predicted an annual decline of 50  for SSDs  in   GB  through at least 2012  Due to their improved performance  low power consumption  small footprint  and predictable wearing  we expect SSDs to gradually replace HDDs as the primary per  manent storage media in large data centers  Such a shift in enterprise computing infrastructure has the potential to a ect all storage intensive applications  Database workloads may not bene t immediately from SSDs  however  since several DBMS architectural decisions  including data structures  algorithms and tuning parameters  were made based on the fundamental performance characteristics of HDDs  The common wisdom behind these decisions is to avoid random I O as much as possible and instead empha  size sequential accesses  which are orders of magnitude faster on HDDs  SSDs  in contrast  perform random reads nearly as fast as sequential reads  The transition from HDDs to SSDs thus forces us to reexamine database design decisions  1 1 Query Processing Techniques for SSDs Previous work has mainly focused on quantifying instant bene ts from the fast random reads of SSDs  16  20  and ad  dressing their slow random writes  15  17  21  23   Our focus instead is on investigating query processing techniques that improve the performance of complex data analysis queries  which are typical in business intelligence  BI  and data ware  housing workloads  Towards this goal  we evaluate data structures and algorithms that leverage fast random reads to speed up selection  projection  and join operations  Along0 20 40 60 80 100 120 140 160 0  20  40  60  80  100  Time  sec  Projectivity NSMScan  all SEL  FlashScan 100  SEL FlashScan 0 01  SEL Figure 1  Scans on SSDs  FlashScan is much faster than a traditional scan when either few attributes  small projectivity  or few rows  small selectivity   SEL  are selected  Data analysis queries contain many scans on large relations that project few at  tributes and select few rows  with sort  which can bene t from SSDs without algorithmic changes  16   and aggregation  which typically applies to in  memory data or to the output of scan or join operators and therefore cannot bene t from SSDs   these operations are the most common in complex query plans  We consider an  SSD only  DBMS in which all data  ta  bles  metadata  logs  etc   are stored in SSDs  Our intention is to  rst evaluate which query processing techniques best exploit the characteristics of SSDs and then build on these new techniques for hybrid SSD HDD con gurations  We advocate using a column based layout within each database page  such as PAX  3   While PAX was originally proposed to improve CPU cache performance  we show it can reduce the amount of data read from SSDs  In Section 3 3  we discuss the similarities between PAX layout on SSDs and column store layout  4  27  on both HDDs and SSDs  Us  ing a PAX based page layout  we implemented FlashScan  a scan operator that reads from the SSD only those attributes that participate in a query  FlashScan proactively evaluates predicates before fetching additional attributes from a given row  thus further reducing the amount of data read when few tuples are selected  Building on FlashScan s ability to e ciently extract needed attributes  we introduce FlashJoin  a general pipelined join algorithm that minimizes accesses to relation pages by re  trieving only required attributes  as late as possible  Flash  Join consists of a binary join kernel and a separate fetch kernel  Multiway joins are implemented as a series of two  way pipelined joins  The join kernel accesses only the join attributes  producing partial results in the form of a join index for each join node  Our current implementation uses a hash based join kernel that employs the hybrid hash join algorithm  25   any other join algorithm may be used in  stead  Subsequently  FlashJoin s fetch kernel retrieves the attributes for later nodes in the query plan as they are needed  using di erent fetching algorithms depending on the join selectivity and available memory  We show that Flash  Join signi cantly reduces the amount of memory and I O needed for each join in the query  1 2 Evaluation inside PostgreSQL We implemented the proposed techniques inside PostgreSQL  0 400 800 1200 1600 2000 3 4 5 6 Time  sec  N way joins N HNSM HPAX FLASHJOIN Figure 2  Multi way joins on SSDs  FlashJoin is at least 2x faster than hybrid hash join over either traditional row based  HNSM  or PAX layouts  In addition to incorporating a new scan and join opera  tor  we modi ed the bu er manager and the bulk loader to support both the original and PAX layouts  We ex  perimented with an enterprise SSD drive using synthetic datasets and TPC H queries  Figure 1 compares the per  formance of FlashScan to Postgres  original scan operator  NSMScan  as we vary the percentage of tuple length pro  jected from 4  to 100   experimentation details are in Sec  tion 3   FlashScan  100  selectivity  middle line  is up to 3X faster than the traditional scan for few projected attributes as it exploits fast random accesses to skip non projected at  tributes  For low percentages of selectivity  0 01   bottom line   FlashScan consistently outperforms the traditional scan by 3 4x  as it avoids reading projected attributes that be  long to non qualifying tuples  The original scan reads all pages regardless of selectivity  As we discuss in Section 3 3  column store systems already enjoy similar performance ben  e ts when scanning large relations on HDDs  Our tech  niques  however  are easier to integrate in existing row based database systems than building a column store from scratch  Figure 2 shows the performance improvements when run  ning multiway joins in Postgres using our FlashJoin algo  rithm  FlashJoin uses the drive more e ciently than hybrid  hash join over both traditional row based  NSM  and column  based  PAX  layouts  by reading the minimum set of at  tributes needed to compute the join  and then fetching only those attributes that participate in the join result  By ac  cessing only the join attributes needed by each join  Flash  Join also reduces memory requirements  which is bene cial in two ways  it decreases the number of passes needed in multi pass joins  hence speeding up the join computation   and it frees up memory space to be used by other operators  hence leading in improved overall performance and stability in the system   1 3 Contributions and Paper Outline The introduction of a new  fast primary storage technol  ogy in enterprise computing warrants a thorough reexamin  ination of database design choices  Towards this goal  we make the following contributions    We demonstrate the suitability of a column based page layout for accelerating database scan projections and selections on SSDs  through a prototype implementa  tion of FlashScan  a scan operator that leverages the columnar layout to improve read e ciency  inside Post greSQL    We present FlashJoin  a general pipelined join algo  rithm that minimizes memory requirements and I Os needed by each join in a query plan  FlashJoin is faster than a variety of existing binary joins  mainly due to its novel combination of three well known ideas that in the past were only evaluated for hard drives  using a column based layout when possible  creating a tem  porary join index  and using late materialization to re  trieve the non join attributes from the fewest possible rows    We incorporate the proposed techniques inside Post  greSQL  Using an enterprise SSD  we were able to speed up queries  ranging from simple scans to mul  tiway joins and full TPC H queries  by up to 6x  The rest of the paper is organized as follows  SSD char  acteristics and trends  along with related query processing techniques are presented in Section 2  In Section 3 we de  scribe and experiment with FlashScan and also discuss how column stores are related to our work  Section 4 describes in detail the join kernel and fetch kernel of FlashJoin  We present experiments with multi way joins and full TPC H queries in Section 5 and conclude in Section 6  2  SSD TRENDS AND RELATED WORK 2 1 SSD Characteristics  Costs  and Trends Most current commercial SSDs use NAND Flash for bulk data storage  NAND  ash is a purely electronic  non volatile store whose performance and price characteristics put it be  tween DRAM and traditional disks  Table 1 summarizes the relevant characteristics of current Flash SSDs compared to traditional magnetic hard disks  All of the SSDs provide or  ders of magnitude  10 100x  faster random read IO s than traditional drives and comparable sequential read and write bandwidth  Their random write performance  however  is much worse than read  especially on the consumer grade drives  Finally  SSDs well outperform traditional drives on the price performance metric IO s    random IO rate per dollar  and the energy e ciency metric IO s W  random IO rate per Watt consumed   The asymmetry between read and write performance is due to the underlying technology  NAND  ash is organized into large 128K erase blocks while read and write IOs are to 4K pages  However  a 4K page write succeeds only if the page has been previously erased  Erasing is expensive in terms of both time and power consumed  Thus  a naive random write requires an expensive 128K read  erase  and rewrite operation  Further  most NAND  ash is limited to about 100 000 erase write cycles per block  Fortunately  SSDs embed logic to hide these details  All SSDs include wear leveling logic that remaps writes to evenly update all blocks  With wear leveling  writing continuously to a 32GB drive at 40 MB s would cause the drive to wear  out after 2 5 years  which implies an acceptable lifespan of 5 10 years with average utilization  Moreover  as Table 1 shows  enterprise SSD vendors are improving random write performance by overprovisioning the underlying  ash capac  ity and embedding additional logic in the drive  Enterprise SSDs have so far been much more expensive than consumer SSDs  but these cost di erences are shrinking as enterprise SSD volumes increase  Overall   ash SSDs are seeing an annual   GB decline of 50  per year  13   which is a faster drop rate than for hard disks  We expect  ash SSDs to eventually be competitive with hard disks in terms of   GB and continue to outper  form them by orders of magnitude in read and write IO s   and IO s W  Thus  we expect that for many data analysis applications  SSDs will replace hard disks in the future  SATA SATA FC ioD Disk Flash Flash Flash GB 500 32 146 320   GB  0 12  15 62  85  30 Watts  W  13 2 8 4 6 seq  read  MB s  60 80 92 700 seq  write  MB s  55 100 108 500 ran  read  IO s  120 11 200 54 000 79 000 ran  write  IO s  120 9 600 15 000 60 000 IO s   2 0 11 2 4 4 8 3 IO s W 9 2 5 600 6 430 13 166 Table 1  Disk and Flash characteristics from manu  facturer specs or as measured where possible  Prices from online retailers as of Nov 25  2008  SATA disk  Seagate Barracuda  SATA Flash  Mtron  FC Flash  STech s ZeusIOps 3 5  FibreChannel  ioD  FusionIO ioDrive  2 2 Databases and SSDs Several recent studies have measured the read and write performance of  ash SSDs  5  20  22   uFLIP  5  de nes a benchmark for measuring sequential and random read and write performance and presents results for 11 di erent de  vices  Of particular note  they identify a  startup  phase where random writes may be cheaper on a clean SSD  since no blocks need to be erased  but quickly degrade as the disk  lls  Polte et al  perform a similar study using less sophisti  cated benchmarks  but focus on the behavior of  lesystems running on top of SSDs  They show the degraded mode is 3 7X worse than the  startup  phase but still an order of magnitude faster than any current HDD  22   Graefe  9  reconsiders the trade o  between keeping data in RAM and retrieving it as needed from non volative stor  age in the context of a three level memory hierarchy in which  ash memory is positioned between RAM and disk  The cited paper recommends disk pages of 256KB and  ash pages of 4KB to maximize B tree utility per I O time  Interest  ingly  these page sizes derive retention times of about 5 min  utes  reinforcing the various forms of the   ve minute rule   Both Myers  20  and Lee et al   16  measure the perfor  mance of unmodi ed database algorithms when the under  lying storage is a  ash SSD  Myers considers B tree search  hash join  index nested loops join  and sort merge join  while Lee et al  focus on using the  ash SSD for logging  sorting  and joins  Both conclude that using  ash SSDs provides better performance than using hard disks  For example  fast random reads allow a larger fan in during the merge phase of sorting  thus increasing the maximum size relation that can be sorted in two passes  Then  there are a set of papers that propose new database algorithms designed especially for  ash characteristics  thesealgorithms generally emphasize random reads and avoid ran  dom writes  where traditional algorithms stress sequential reads and writes and try to avoid any random I O   Lee et al   15  modify database page layout to make writing and logging more e cient  Ross  23  proposes new algorithms for counting  linked lists  and B trees that minimize writes  Shah et al   24  present a new hash based join algorithm that  in combination with a new page layout  uses random reads to retrieve less data than hybrid hash join  However  their algo  rithm focuses on binary joins and there is no implementation of the proposed join algorithm  Nath and Gibbons  21  de   ne a new data structure  the B  le  for maintaining large samples of tables dynamically  Their algorithm writes only completely new pages to the  ash and they observe that writes of pages to di erent blocks may be interleaved e   ciently on  ash SSDs  Li et al   17  propose a new index structure  it uses a B tree in memory to absorb writes and then several levels of sorted runs of the data underneath the tree  This structure uses only sequential writes to periodi  cally merge the runs and random reads to traverse the levels during a search  Finally  Koltsidas and Viglas  14  consider hybrid SSD HDD con gurations of databases systems and design a bu er man  ager that dynamically decides whether to store each page on a  ash SSD or a hard disk  based on its read and write access patterns  However  their approach assumes that all page accesses are random  2 3 Related Query Processing Techniques Our aim is to modify traditional query processing tech  niques to leverage the fast random reads of  ash SSDs  Our modi cations have three goals   1  avoid reading unneces  sary attributes during scan selections and projections   2  reduce I O requirements during join computations by min  imizing passes over participating tables  and  3  minimize the I O needed to fetch attributes for the query result  or any intermediate node in the query plan  by performing the fetching operation as late as possible  We discuss related techniques here and revisit some of those techniques in later sections  Traditionally  database systems use the N ary storage model  NSM   a page based storage layout in which tuples  or rows  are stored contiguously in pages  NSM may waste disk and memory bandwidth if only a small fraction of each row is needed  In contrast  the decomposition storage model  DSM   6   proposed in the 80s  decomposes relations ver  tically  allocating one sub relation per attribute  DSM had its own disadvantages  including storage overhead for stor  ing tuple IDs and expensive tuple reconstruction costs  With changing market needs and more favorable technology trends  newer DSM like  column store  commercial products and academic prototypes have recently appeared  such as Syba  seIQ  Vertica  C store  27   and MonetDB X100  4    These systems eliminate storage overhead through virtual IDs and o er fast scans of few attributes  10  at the cost of additional disk seeks to fetch non contiguous attributes  PAX  3   Partition Attribute Across  is a hybrid approach  essentially a DSM like organization within an NSM page  While the disk access pattern of PAX is indistinguishable from that of NSM  it improves on the memory bandwidth requirements  On SSDs  as we show in the next section  PAX  in combination with the much faster seek time  allows reading only those columns needed by the query  essentially enjoying the read e ciency of DSM while retaining existing NSM functionality  The ability of column stores to read only part of a tu  ple led to an examination of di erent tuple materialization strategies  2   A late materialization policy is particularly e ective in reducing the amount of data passed through the query operators and we discuss its use in FlashJoin in Sec  tion 4 3  Late materialization was originally implemented by Semijoin reducers  26  which  rst computed a semi join  using an index  if one was available  and then revisited base relations to obtain remaining attributes  without needing to redo predicate evaluation  Late materialization was also the essence behind TID hash joins  19   which compute a join in  dex  rst and then construct the  nal join result  Although TID hash joins waste disk bandwidth when reading the in  put relations and rely on an ine cient fetching strategy for constructing the  nal result  they were shown to outperform traditional hash joins for highly selective joins on large in  puts  A more e cient fetching strategy  given a pre existing join index  was described for Jive Slam joins  18   How  ever  these algorithms were limited to two way non pipelined joins  Our work builds on these ideas  re examining fundamental tradeo s in light of the di erent performance characteristics of SSDs  3  SCAN SELECTIONS   PROJECTIONS In this section  we demonstrate that a PAX like page orga  nization is a natural choice for database systems over SSDs  We  rst describe our implementation of FlashScan  a scan operator that leverages the PAX layout to improve selections and projections on  ash SSDs  We evaluate the implemen  tation of FlashScan inside Postgres in Section 3 2  Then  in Section 3 3 we discuss the similarities and di erences be  tween PAX layout and DSM or column layout and describe how FlashScan applies to column stores  3 1 FlashScan Operator Figure 3 shows the page organization of both NSM and PAX for a relation with four attributes  In general  for an n attribute relation  PAX divides each page into n mini  pages  where each minipage stores the values of a column contiguously  The size of each minipage is computed based on the average sizes of the attribute values  In our im  plementation  we do not handle over owing of minipages caused by variable length attributes  Ailamaki et al  de  scribe how PAX can support variable length attributes in  3   The column based layout of PAX o ers a physical sep  aration of values from di erent columns  within the same page  thus allowing an operator to access only the attributes that are needed in a query  Because regular scanners access a full page from disk  PAX does not have an impact on disk bandwidth  Once a page is brought into main memory though  PAX allows the CPU to access only the minipages needed by the query  thus reducing memory bandwidth re  quirements  3   Data transfer units in main memory can be as small as 32 to 128 bytes  the size of a cacheline   whereas a database page is typically 8K to 128K  With SSDs  the minimum transfer unit is 512 to 2K bytes  which allows us to selectively read only parts of a regular page  FlashScan takes advantage of the small transfer unit of SSDs to read only the minipages of the attributes that it needs  Consider a scan without selection predicates thatPage Header Page Header NSM PAX 1 100 000 ABC 96201 2 XYZ 90 000 93702 3 JKL MNO 85 000 91431 1 2 3 ABC XYZ JKLMNO 100 000 90 000 85 000 96201 93702 91431 minipage 1 minipage 2 minipage 3 minipage 4                          Figure 3  Disk pages in NSM  left  and PAX  right  storage layout of a relation with four attributes  projects the  rst and third column of the relation in Fig  ure 3  For each page  FlashScan initially reads the mini  page of the  rst attribute and then  seeks  to the start of the third minipage and reads it  Then it  seeks  again to the  rst minipage of the next page  This procedure contin  ues over the entire relation  resulting in a random  albeit strided  access pattern  In general  every  seek  results in a random read  The only exception is when the scan query requests contiguous minipages  in that case  FlashScan coalesces the reads and performs one random access for each set of contiguous mini  pages  Using random instead of sequential reads is bene cial only if the storage medium supports fast random access and the reduction in the amount of data read compensates for the overhead induced by the random I O  As we demon  strate in the next section  this is indeed the case for  ash SSDs  Note that we cannot expect the minipages to be exact multiples of the SSD minimum transfer unit  We also did not want to align minipages to OS page boundaries since that approach could result in severe fragmentation and poor space utilization  Instead  in our implementation of PAX and FlashScan  we decouple minipage size from SSD trans  fer units and compute the size of each minipage solely as a function of the page size and the average sizes of attribute values  Inside Postgres  we implemented PAX by dividing every database page into tightly packed minipages  There  fore  some unneeded data may be transferred when reading a single minipage  We modi ed the Postgres bu er manager and bulk loader to work with the new page layout  The bu er manager allo  cates space and performs replacements at the granularity of a database page  However  a page in the bu er pool may be partially full containing only the minipages transferred by FlashScan  Although this design wastes space in the bu er pool  it simpli es our implementation and ensures that the invocation of the bu er manager does not in uence our re  sults in the experimental evaluation  It also allows us to reuse existing Postgres functionality and utilities  We added FlashScan to Postgres as a new scan opera  tor that produces tuples in row format  after having read in the bu er pool all minipages containing the projected attributes  One of the features of Postgres that adds a con  siderable per tuple overhead  is multi version concurrency control  MVCC   Currently  our implementation does not support in place updates  and therefore we have no use of 0 20 40 60 80 100 120 140 160 0 01  0 10  1 00  10 00  Time  sec  Selectivity Projectivity 25  100 00  NSMScan FlashScan FlashScanOPT  U  FlashScanOPT  S  Figure 4  FlashScan avoids reading attributes that are not projected  FlashScanOpt additionally avoids reads of minipages without selected tuples  When run over sorted predicate attributes  Flash  ScanOpt S  is able to skip many minipages of pro  jected attributes  MVCC   as our goal was to evaluate read only queries  While certain database systems and several data warehousing ap  plications typically operate under alternating periods of read  only queries and bulk loading data  adding support for up  dates in PAX  if needed  could be performed without any penalty  as shown in  3   To ensure a fair comparison with Postgres  default page layout  NSM  that includes MVCC information  we added the same overhead to our PAX pages by reserving an extra minipage inside each page  Addition  ally  we removed any calls related to MVCC when NSM is used  3 1 1 Optimizations for Selection Predicates For selection predicates  FlashScan can improve perfor  mance even further by reading only the minipages that con  tribute to the  nal result  Consider a scan query that speci   es a set of projected attributes P  in the select clause in an SQL query  as well as a set of attributes S that participate in selection conditions  in the where clause   Assume for sim  plicity that S and P are disjoint  hence only the attributes in P are in the  nal result  FlashScan reads from each page only the minipages that correspond to the attributes in S and evaluates the selection conditions  If  for a given page  there is at least one tuple that passes the selection condi  tions  then FlashScan reads its minipages for the attributes in P  If none of the tuples of that page satisfy the selection conditions  FlashScan skips to the next page  As we demonstrate next  this technique is bene cial for highly selective conditions  with a small number of tuples in the result  and for selection conditions that are applied to sorted or partially sorted attributes  In general  the more clustered the attribute values that satisfy the conditions  the bigger the performance improvement  3 2 Scan Experiments For the experiments in this section we generated a relation0 20 40 60 80 100 120 140 160 0 01  0 10  1 00  10 00  Time  sec  Selectivity Projectivity 75  100 00  NSMScan FlashScan FlashScanOPT  U  FlashScanOPT  S  Figure 5  When more attributes are projected  FlashScan must read nearly as much data as NSM  Scan  The contrast between FlashScan and Flash  ScanOpt is therefore much greater  see Figure 4  when not all pages contain selected tuples  with 70 million tuples occupying about 10GB  The relation consists of 11 columns  eight 4 byte and three 32 byte at  tributes  for a tuple length of 128 bytes  NSM layout in Postgres includes a 23 byte header for every tuple  Hence  in order to ensure that relations stored in NSM and PAX layouts have the same size  we allocated an extra minipage to each PAX page for the tuple headers  The page size for both NSM and PAX was set to 64KB  PAX pages contained 12 minipages and the minimum transfer unit from the SSD was 4KB  All experiments were performed on a system with an Intel Core 2 Duo CPU at 2 33GHz and 4GB of RAM  running Ubuntu 8 04 Linux with kernel 2 6 24 21  We used an MTron 32GB SSD  the performance characteristics of which are presented in Table 1  formatted with the Linux ext2  le system  3 2 1 Varying the Number of Projected Attributes In the  rst experiment  we compare the performance of FlashScan to Postgres  original scan operator  NSMScan  as we vary the percentage of the tuple projected  its pro  jectivity  from 4  to 100   The results are in Figure 1 in Section 1  top two lines   NSMScan always reads the whole relation  regardless of projectivity  and exhibits con  stant performance in scan queries  FlashScan is up to 3X faster than NSMScan for low projectivity  it exploits fast random accesses to  seek  e ciently to the required mini  pages  As projectivity increases  FlashScan reads more data from every page  At the point where it reads the entire tuple  100  projectivity   FlashScan performs the same sequen  tial read of the relation as NSMScan  3 2 2 Varying Selectivity In this experiment  we consider a scan query with a sin  gle equality predicate and vary its selectivity from 0 01  to 100   We consider two versions of FlashScan  Plain Flash  Scan  rst reads all minipages of the projected attributes  discarding non qualifying tuples after evaluating the predi  cate  FlashScanOpt implements the optimization described in Section 3 1 1  minipages of projected attributes are read only if there is at least one tuple in the page that satis es the predicate  We experiment with an equality predicate on both sorted and unsorted attributes  When the attribute is sorted  then all matching tuples are stored contiguously  The results for 25  and 75  projectivities are in Figures 4 and 5 respectively  we use the letters U and S to distinguish between the runs of FlashScanOpt on unsorted and sorted attributes    The execution time for NSMScan and plain FlashScan re  mains  at across di erent selectivities  the small increase for FlashScan for large percentages of selectivity is due to the higher tuple reconstruction cost of PAX   The optimized version of FlashScan  however  performs signi cantly better with lower percentages of selectivity  For predicates on un  sorted attributes and selectivity below 1   FlashScanOpt skips entire minipages that do not contain any qualifying tuples  thus outperforming plain FlashScan by up to 3x  for 0 01  selectivity and 75  projectivity   For more than 1  selectivity there is at least one tuple that satis es the predicate in every page  in our relation there were 400 tu  ples per page   When applying the predicate on a sorted attribute  however  FlashScanOpt outperforms plain Flash  Scan for all selectivities below 100   only a few pages con  tain the contiguous matching tuples and all other minipages can be skipped  3 3 FlashScan and Column Stores We discuss next the relation of FlashScan to column ori  ented storage on both HDDs and SSDs  For scans with many qualifying tuples  FlashScan s behavior is in many ways similar to a column store scan on a traditional hard drive  FlashScan needs to  seek  between minipages  and that seek  although very fast  adds a small overhead  but is preferable over a full sequential scan   Column stores on HDDs read a large portion of a single column at a time  called chunk in MonetDB X100   to amortize the real disk head seek between di erent columns  For SSDs and HDDs with comparable bandwidths  these two scans would per  form similarly  For highly selective scans  however  Flash  Scan has an advantage  it can skip minipages that do not contain qualifying tuples  A column store on HDDs can only skip entire chunks which are two orders of magnitude larger than  ash SSD transfer units  This means that column  store scans on HDDs need to be 100 times more selective than FlashScan to witness similar bene ts  If we were to run a column store on SSDs  however  we would  nd similar behavior to FlashScan regardless of se  lectivity  In fact  by getting rid of the notion of a chunk  and instead rely on the transfer unit of SSDs  both Flash  Scan and column stores would exhibit the exact same SSD reading times  since  seeks  on SSDs are constant in time   While this assertion needs experimental validation  if true  it would be a step towards converging row store and column  store functionality  which  for current HDD systems  has been a subject of much debate  1  12    Several other aspects of row  and column stores would need to be reexamined  however  this is a subject of future  and much promising  research  Compression  for example  might be implemented di erently on a PAX based row store than in a pure column  store  One important aspect of our work is to demonstrate that the proposed techniques can be relatively easily inte grated inside a full blown relational DBMS  retaining much of the existing  rich functionality that otherwise would have to be re implemented  if one was to build a column store from scratch  Since in the next section we describe a general join method which is based on FlashScan for reading data o  disk  ex  isting column stores on SSDs could potentially adopt the same method  by plugging a column scanner in the place of PAX based FlashScan  Since the scope of the paper is limited to traditional  row based DBMS  we do not revisit column store applicability  4  FLASHJOIN In this section  we present FlashJoin  a multi way join algorithm tailored for solid state drives  We  rst give an overview of our algorithm in Section 4 1 and then describe its main components in the following sections  4 1 FlashJoin Overview FlashScan 1 FlashScan 2 FlashScan 3 A D K B  C  E  H  F R1  A  B  C  R2  D  E  G  H  R3  F  K  L  select R1 B  R1 C  R2 E  R2 H  R3 F from R1  R2  R3 where R1 A   R2 D AND R2 G   R3 K  G B  C F E  H Join  Kernel A   D Fetch  Kernel Join 1 Join  Kernel G   K Fetch Kernel Join 2 G Figure 6  Execution strategy for a three way join when FlashJoin is employed  To take advantage of the fast random reads of SSDs  FlashJoin uses the same principal techniques as FlashScan  It avoids reading unneeded attributes and postpones retriev  ing attributes in the result until absolutely necessary  FlashJoin is a multi way equi join algorithm  implemented as a pipeline of stylized binary joins  Each binary join in the pipeline is broken into two separate pieces  a join kernel and a fetch kernel  each of which is implemented as a separate operator  as shown in Figure 6  The join kernel computes the join and outputs a join index  Each join index tuple con  sists of the join attributes as well as the row ids  RIDs  of the participating rows from base relations  The fetch kernel re  trieves the needed attributes using the RIDs speci ed in the join index  FlashJoin uses a late materialization strategy  in which intermediate fetch kernels only retrieve attributes needed to compute the next join and the  nal fetch kernel retrieves the remaining attributes for the result  This ap  proach o ers some important bene ts over traditional joins  which use an early materialization strategy  First  FlashJoin reduces the amount of data retrieved from the input relations to compute the join result  It accesses the join attributes and accesses other projected attributes only from rows that participate in the result  Second  since join kernels process join indices instead of all projected at  tributes from the input relations  the indices on the build input are smaller  Thus  the join kernel is more memory  e cient  Moreover  when multiple passes are needed  the join kernel incurs lower partitioning costs than traditional joins  These bene ts come at the cost of additional random reads for retrieving the join attributes and other projected attributes separately  Moreover  they come at the cost of additional passes over data in the fetch kernel  In the ex  perimental section  we show that this tradeo  is worthwhile when using SSDs  4 2 Join Kernel The join kernel leverages FlashScan to fetch only the join attributes needed from base relations  The join kernel is im  plemented as an operator in the iterator model  In general  it can implement any existing join algorithm  block nested loops  index nested loops  sort merge  hybrid hash  etc  In this paper  we explore the characteristics of a join kernel that uses the hybrid hash algorithm  7   This kernel builds an index  hash table  on the join attribute and RID from the inner relation and probes the index in the order of the outer  Depending on the available memory and the size of the input  hybrid hash may make multiple passes over the input  Compared to a traditional hash join  this join kernel is more e cient in two important ways because it does not need to manage all of the projected attributes from the input  First  the join kernel needs less memory  thus many practical joins that would otherwise need two passes can complete in one pass  Second  when the join kernel spills to disk  materializing the runs is cheaper  Although we only explore hybrid hash in this paper  we believe a sort merge kernel would o er similar results because of the duality of hash and sort  8   4 3 Materialization Strategy To implement di erent materialization strategies  the query plan must also change at a logical level internal to FlashJoin  Typically in a query plan  associated with each node is a de  scription of the tuple the node produces  For example  in Figure 6  a traditional plan would indicate that the  rst join  Join 1   produces tuples with attributes  B  C  E  G  H   By adjusting these descriptions  which indicate to the fetch ker  nel the attributes to retrieve  we can specify any strategy we like for retrieving projected attributes  as long as it is con  sistent with the plan  For any individual binary join  there is a tension between the cost of retrieving projected attributes from base relations immediately after the join and the cost of retrieving the attributes further downstream  The former increases the cost for partitioning in subsequent join kernels  if those joins cannot complete in one pass  The latter forces the  nal fetch kernel to make additional read and write passes over the output  which can be expensive if the output cardinality is large  Choosing the optimal strategy requires estimating the cost of these options and minimizing the total cost across the whole multi way join  The number of possibilities is largesince at every node in the plan  we can choose to materialize any subset of the remaining needed attributes  To make matters more complicated  the cost for a particular strategy may be signi cant enough to a ect the total cost and thereby a ect the optimal join order  Instead of exploring this optimization space  in this paper  we use a simple heuristic  late materialization  This heuris  tic postpones retrieving projected attributes as far down  stream as possible  Every join produces the minimum set of attributes needed by the next operator and the  nal join produces the output needed by the remaining plan opera  tors  This heuristic works well as we show in Section 5 be  cause typical query plans reduce the output cardinality at each level of the plan  In that case  an extra pass over the output is usually cheaper than paying the additional mate  rialization cost through multiple levels of the plan  In order to perform the late materialization  the join kernel output carry forward the RIDs for each of the base relations needed downstream  Figure 6 shows an example of this strategy  Each tuple produced by FlashScan 1 in contains only attribute A  which is one of the join attributes of Join 1  and each tuple pro  duced by FlashScan 2 contains only attribute D  the second join attribute of Join 1  Similarly  the tuples produced by Join 1 contain only attribute G  which is one of the join attributes of Join 2  Finally  Join 2  the last join node  pro  duces tuples with all of the attributes needed in the result  B  C  E  H  F  Moreover  if there was a sort operator after Join 2 that sorted the results based on attribute L  R3   then each tuple produced by Join 2 would also contain L  4 4 Fetch Kernel The fetch kernel uses the join index  produced by the join kernel  to materialize the attributes of the join result from their base relations  For example  in Figure 6  the join ker  nel of Join 1 outputs pairs of RIDs  id1  id2  from relations R1 and R2  respectively  A RID speci es the page and o set within the page for that row  The fetch kernel uses id2 to locate and retrieve attribute G from relation R2  Similarly  Join 2 produces a join index containing  id1  id2  id3  point  ing to rows of R1  R2  and R3  The corresponding fetch kernel  uses id1 to retrieve attributes B  C  id2 to retrieve attributes E  H and id3 to retrieve attribute F  A straightforward strategy for the fetch kernel is to re  trieve projected attributes in a tuple at a time  non blocking fashion  For each join index tuple  the kernel locates the needed mini pages in the bu er pool or retrieves them from the underlying relation  and composes the result tuple  This approach is reasonable when all of the pages needed to gen  erate the result can  t in memory because random reads are cheap  When available memory is insu cient  however  this approach may result in reading some pages multiple times because the RIDs in the join index are usually unordered  The larger the result  the worse is the overhead for re reading pages  TID hash joins implemented this approach  and this overhead was their biggest weakness  19   To mitigate this overhead  we present a fetching strat  egy  inspired by Jive Join  18   that reads each containing page from each base relation only once at the cost of addi  tional passes over the join index  Algorithm 1 makes multi  ple passes over the join index fetching attributes in row order from one relation at a time  Roughly speaking  in each pass  it sorts the join index based on the RIDs of the current rela  Algorithm 1 Fetch Kernel to produce the join result with multiple passes  Input  R1          Rk  base relations  id1          idk  corre  sponding RID attributes  A1          Ak  Ai is the set of attributes to fetch from relation Ri  jA1j           jAkj  JI  join index Output  JR  join result 1  T   SortedStream JI  id1  2  for i   1 to k do 3  Z   fg 4  while T is exhausted do 5  for all memory resident tuples t of T do 6  Add GeneratePartialResultTuple t  idi  Ri  Ai  to Z 7  end for 8  Produce sorted run of Z on idi 1 9  end while 10  T   SortedStream Z  idi 1  11  end for 12  JR   T tion to be scanned  Then  it retrieves the needed attributes from that relation for each tuple and augments the join in  dex with those attributes  If the join index does not  t in memory  it uses an external merge sort  Sorting ensures that once a mini page from a relation has been accessed  it will not need to be accessed again  thus placing minimal demands on the bu er pool  Sorting  however  does not en  sure sequential access to the underlying relation because the needed pages can be far apart  Hence  this strategy of de  coupling and postponing materialization is better suited to SSDs than HDDs  We explain some important optimizations in Algorithm 1 above  First  the  nal merge of the sort and attribute re  trieval are pipelined to avoid the unnecessary  nal write  SortedStream sorts the join index but leaves out the last merge step that creates a  nal sorted run  line 1   As tuples are fetched from T  a sorted stream   it merges the under  lying runs on demand  For each tuple  GeneratePartial  ResultTuple retrieves the needed attributes and augments the join index  line 6   We ensure that only enough tuples are read so that the result Z  the new join index with the attributes Ai projected  can  t in memory  line 5   Finally  we sort Z on the RID for the next relation  line 8   The sorted Z runs are then merged into a single sorted stream T  line 10  which is then pipelined with attribute retrieval from the next relation  A second optimization is that our fetch kernel processes relations in order of the width of the attributes retrieved  from smallest to largest  This order reduces the data spilled to SSD when producing the intermediate runs  Third  if Z and JI  t in memory  we avoid spilling them to disk  Fourth  if a fetch kernel  esp  intermediate ones in the plan  already has its input sorted on the RIDs for one of the relations  it processes that relation  rst to avoid the sort  The cost for this fetch strategy depends on the cardinality of the result  the width of the attributes  and the number of relations that need to be accessed  Although we did not implement it  an analogous fetching strategy would be to use hashing instead of sorting  as done in RARE join  24   In each pass  we could hash join index tuples into buckets based on the page id in the RID  Thiswould ensure that all tuples that need the same page fall into the same bucket  The buckets could be sized such the number of distinct pages referenced  t in the available mem  ory  We could then process each bucket fetching the the ap  propriate pages from base relations  This strategy requires less CPU overhead than the sort based one  but would need more memory than the sort based one for the same number of partitions  In summary  if the optimizer estimates that all the pages needed to retrieve projected attributes can  t in memory  we opt for the naive  tuple at a time strategy  Otherwise  we resort to our sort based relation at a time fetching strategy  5  EXPERIMENTAL EVALUATION This section presents our experiments with FlashJoin  Our objective is to demonstrate the e ectiveness and e ciency of FlashJoin during the execution of multi way joins and com  plex BI queries  In Section 5 1  we describe the algorithms evaluated  the datasets and queries used  and implementa  tion details  The results are presented in the remainder of the section  5 1 Experimental Setting FlashJoin was implemented inside PostgreSQL as a new join operator  Signi cant changes had to be performed in PostgreSQL s planner component to support the late ma  terialization strategy  The planner employes a cost based optimizer to determine the most e cient way to execute a query  The output of the planner is a query execution plan that details which access methods to use and which at  tributes to access from each base relation  Additionally  it determines for each operator in the query plan the schema  attributes  of output tuples  We altered the planner in or  der to generate query execution plans which comply to the late materialization strategy  Speci cally  we added a recur  sive function that takes as input the query execution plan and process it in a top down fashion  recall that a query execution plan is a tree of operators   For every operator  node in the query execution plan  our function alters the schema of output tuples by removing unnecessary attributes  Overall  we added approximately 7K new lines of code in PostgreSQL  half of which was the implementation of Flash  Join and planner component and the rest divided between FlashScan  bulk loading utilities  the bu er manager and the storage layout  There are several limitations with respect to our imple  mentation of FlashJoin and the execution of complex queries  Currently  we support query execution plans that contain a pipeline of hash joins followed by other operators  such as sort and aggregation  Furthermore  the optimizer is not aware of the late materialization strategy and the cost of FlashJoin and so it produces the same plan that it would using the hybrid hash operator  In the future  we plan to integrate the cost of FlashJoin into the optimizer  In all of our experiments  we compare the performance of FlashJoin with the hybrid hash join algorithm implemented in PostgreSQL  We consider two variations of the hybrid  hash join algorithm  one for each storage layout  We re  fer to the default implementation over the NSM layout as HNSM  We call our version of hybrid hash over the PAX lay  out HPAX  HPAX and HNSM use the same materialization strategy  but when HPAX is used  the scan nodes use the FlashScan operator to read only the projected attributes of 0 20 40 60 80 100 120 140 160 180 0  20  40  60  80  Time  sec  Projectivity Two way join  one pass  100  HNSM 1  HPAX 1  FLASHJOIN 1  FLASHJOIN 0 1  FLASHJOIN 0 01  Figure 7  Performance comparison of join algo  rithms for two way joins when the join is computed in one pass  each relation  We conducted a series of experiments using synthetically generated datasets and queries  We also evaluated the per  formance of FlashJoin using full TPC H queries  All of the experiments were conducted with the same page sizes and relations and on the same system that we described in Sec  tion 3 2  The sizes of the relations and the amount of mem  ory allocated are described separately for each experiment  After the execution of each query  PostgreSQL was restarted and all cached pages were  ushed from memory using the drop caches utility of the Linux kernel  In all the experi  ments  the performance metric measured is wall clock time to produce the full join result  5 2 Two way Join Results In this section  we assess FlashJoin during the execution of two way joins  We consider the equijoin of two relations R  S on a single join attribute  R contains 70 million tuples with a total size of approximately 10GB and S contains 7 million tuples with a total size of 1GB  We consider queries of the following form   SELECT R an          R am  S an          S am FROM R  S WHERE R ai   S aj   We use projectivity to refer to the percentage of the tuple length projected from each join relation  When projectivity is 100   a join result tuple contains all of the attributes of R and S and has a total tuple length of 256 bytes  5 2 1 One pass Joins In the  rst experiment  we vary projectivity from 4  to 100   The amount of memory allocated to the join is 1GB to ensure that all algorithms compute the join in one pass  Figure 7 shows the runtimes of HNSM  HPAX  and Flash  Join with three di erent result cardinalities  The percentage next to each algorithm s label indicates the cardinality of the join result  expressed as a percentage of the cardinality of the larger join relation  In this experiment  join result car  dinality had a negligible e ect on the performance of HPAX and HNSM  so we only present one set of execution times0 200 400 600 800 1000 1200 1400 1600 0  20  40  60  80  Time  sec  Projectivity Two way join  two pass  100  HNSM 1  HPAX 1  FLASHJOIN 100  FLASHJOIN 10  FLASHJOIN 1  FLASHJOIN 0 1  Figure 8  Performance comparison of join algo  rithms for two pass two way joins  for them  The performance of HNSM is independent of projectivity  The input scans of HNSM read relations R and S entirely  regardless of how many attributes participate in the join re  sult  HPAX  however  uses the FlashScan operator to read from  ash disk only the attributes that are actually needed in a query  For low projectivities  4  in our experiment   HPAX is 3X faster than HNSM  However  as projectivity increases  more attributes are read and the performance dif  ference between HPAX and HNSM diminishes  The performance of FlashJoin depends on both the projec  tivity and the join result cardinality  When the cardinality is 1  or greater  FlashJoin and HPAX perform the same since every disk page of R and S contains at least one tuple that belongs to the join result and must be read  For car  dinalities less than 1   FlashJoin reads all of the minipages of the join attributes but only a fraction of the minipages containing projected attributes  Consequently  it reads less data than HPAX  As projectivity increases  the number of minipages that FlashJoin does not read increases accord  ingly and causes a more pronounced performance di erence of up to a factor of 3x  5 2 2 Two pass joins In the second experiment  we compare the performance of the join algorithms when they require two passes to compute the join result  The amount of memory allocated to the join is 100MB and the same query was used as above  Figure 8 shows results for HNSM and HPAX with a join result car  dinality of 1  and for FlashJoin with cardinalities  varying from 0 1  to 100   The execution times of HNSM and HPAX increase linearly with projectivity  since projections are performed early in the query execution plan  more data participate in the par  titioning phase as projectivity increases  Consequently  the partitioning cost increases  HPAX is faster than HNSM for lower projectivities because it reads only the attributes needed in the query  until at 100  projectivity  both HPAX and HNSM read all attributes and perform the same  0 100 200 300 400 500 600 700 800 100 150 200 250 300 350 400 450 Time  sec  Memory  MB  Two way join 500 550 600 HNSM HPAX FLASHJOIN Figure 9  Performance comparison of two way join algorithms as a function of memory size  FlashJoin  however  is much faster than HNSM and HPAX  and increasingly so as projectivity increases  FlashJoin ac  cesses only the join attributes during the expensive parti  tioning phase  when two passes are required for the com  putation of the join  Consequently  the partitioning cost of FlashJoin does not depend on projectivity  The performance of FlashJoin does depend on the join result cardinality  however  When the cardinality is low  FlashJoin reads only a few minipages of projected attributes in order to construct the join result tuples and performs up to 7X faster than HPAX and HNSM  As cardinality in  creases  FlashJoin reads a larger fraction of minipages  thus increasing the join result construction cost  Furthermore  when the join index does not  t in memory  FlashJoin pays the cost of materializing the join index  whose size depends on the result cardinality  When the result cardinality is 100   FlashJoin must read as many minipages as HPAX and is only marginally faster than HPAX  5 2 3 Memory impact on joins In the third experiment  we compare the performance of the join algorithms as we vary the amount of memory allo  cated  We set the join result cardinality at 1  of the car  dinality of R  larger relation  and the projectivity at 25   We vary the amount of memory from 100MB to 600MB  Figure 9 shows that FlashJoin is faster than both HPAX and HNSM for all memory sizes examined  HPAX and HNSM require at least 500MB to compute the join in one pass  The hash table on the join attribute of the build re  lation requires 270MB and the projected attributes of the build relation require 230MB  When memory is less than 500MB  HPAX and HNSM compute the join in two passes  Since they both use the hybrid hash join algorithm  they exploit any memory available to avoid writing all of the par  titions  When the allocated memory is between 250MB and 500MB  one partition  ts in memory and only the second partition gets written  By reading only the join attributes  FlashJoin has a smaller memory footprint and increases the range of memory sizes at which a two way join is executed in one pass  Flash  Join requires only 270MB of memory  for the hash table 0 200 400 600 800 1000 1200 1400 1600 0 01  0 10  1 00  10 00  Time  sec  Result Size    of larger relation s cardinality   Three way join   Projectivity 25  100 00  HNSM HPAX FLASHJOIN Figure 10  Performance comparison of join algo  rithms during three way joins as a function of join result cardinality  when projectivity is 25   to compute the join in one pass  Consequently  FlashJoin is 2X faster than the other algorithms when memory size is between 270MB and 500MB  When memory is larger than 500MB  FlashJoin is 1 2X faster  When memory is less than 270MB  FlashJoin computes the join in two passes  it is 3X faster than HPAX and HNSM due to the r</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s9dmh4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_modern_hardware"/>
        <doc>FlashLogging  Exploiting Flash Devices for Synchronous Logging Performance   ### Shimin Chen Intel Research Pittsburgh 4720 Forbes Avenue  Suite 410 Pittsburgh  PA 15213  USA shimin chen intel com  ABSTRACT Synchronous transactional logging is the central mechanism for ensuring data persistency and recoverability in database systems  Unfortunately  magnetic disks are ill suited for the small sequential write pattern of synchronous logging  Alternative solutions  e g   backup servers or sophisticated battery backed write caches in high end disk arrays  are either expensive or complicated  In this paper  we exploit    ash devices for synchronous logging based on the observation that    ash devices support small sequential writes well  Comparing a wide variety of    ash devices  we    nd that USB    ash drives are a good match for this task because of its unique characteristics  widely available USB ports  hot plug capability useful for coping with    ash wear  and low price so that multiple drives are a   ordable  We propose F lashLogging  a logging solution that exploits multiple  USB     ash drives for synchronous logging  We identify and address four challenges   i  e   ciently exploiting multiple    ash drives for logging   ii  coping with the large variance of write latencies because of device erasure operations   iii  e   cient recovery processing  and  iv  combining    ash drives and disks for better logging and recovery performance  We implemented our solution within MySQL InnoDB  Our real machine experiments running online transaction processing workloads  TPCC  show that F lashLogging achieves up to 5 7X improvements over magnetic disk based logging  and obtains up to 98 6  of the ideal performance  We further compare our design with one that uses Solid State Drives  SSDs   and    nd that although SSDs improve logging performance  multiple USB    ash drives can achieve comparable or better performance with much lower price  Categories and Subject Descriptors H 2 2  DATABASE MANAGEMENT   Physical Design  H 2 4  DATABASE MANAGEMENT   Systems   Transaction processing  H 2 7  DATABASE MANAGEMENT   Database Administration   Logging and recovery Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  General Terms Algorithms  Design  Performance  Reliability Keywords FlashLogging  Online Transaction Processing  Synchronous Logging  Flash Devices  Unconventional Array Organization  Outlier Detection and Hiding  Near Zero Delay Archival Disk  Recovery Processing 1   ###  INTRODUCTION Synchronous logging  in which log records are forced to stable media  is one of the most important techniques for achieving fault tolerant computation  It is used extensively in a wide variety of computer systems  In relational database systems  synchronous logging is the central mechanism to ensure transaction durability  12  17  22  27   Before a transaction commits  the redo log records associated with the transaction must be    ushed to the synchronous redo log  If later the database system fails and restarts  these redo log records guarantee that any data changes in the transaction persist across the failure  Similarly  synchronous logging  a k a  synchronous journal  can be used to guarantee the durability of    le I O operations in    le systems  26   and it can be employed in distributed systems  e g   multi tier systems providing web services  to restore server states upon crash recovery  2  8  36   This paper investigates e   cient solutions for synchronous logging  As DRAM capacity doubles every two years  14   an OLTP database that was considered    large    ten years ago can now    t into main memory  For example  in the TPCC benchmark  34   a warehouse  which represents 30 thousand users  occupies less than 100MB 1 space  Note that a low end server machine today is often equipped with 4   32GB of memory  Therefore  a database for 30 million users  which requires less than 100GB space  can easily    t into the aggregate main memory of a small cluster of low end server machines  Recent database studies have explored this trend to redesign database architecture for better transaction processing performance  13  33   In contrast  synchronous logging always requires to write to stable media  Therefore  the performance of synchronous logging is becoming increasingly important to the overall performance of transaction processing  However  e   cient solutions for synchronous logging are not straightforward  1 KB 10 3 B  MB 10 6 B  GB 10 9 B  KiB 2 10 B  MiB 2 20 B  GiB 2 30 B  735466 31762 0 5000 10000 15000 20000 25000 30000 35000 40000 NOTPM disabled required  enabled ideal  logging disk write cache Figure 1  TPCC transaction rates while a magnetic disk is used as the logging device for synchronous transactional logging   NOTPM  New Order Transactions Per Minute  1 1 Magnetic Disks Suffer from Small Sequential Writes of Synchronous Logging The    rst solution that comes to mind is to use magnetic disks as the stable media for synchronous logging  Figure 1 shows the TPCC transaction rates with disk based logging  Two Dell PowerEdge 1955 blade servers are used in the experiments  one is running a MySQL server with the InnoDB backend storage engine supporting a 20 warehouse TPCC database  the other is running a client driver that repeatedly issues TPCC transaction requests and measures the transaction rates  Each blade server is equipped with two 10k rpm SAS disk drives  The database server uses one disk for storing TPCC tables and indices  while the other is devoted to synchronous redo logging  We make sure that the entire TPCC database and its indices can    t into the main memory bu   er pool on the server  In each experiment  the client driver runs 32 driver threads for an hour  modeling 32 independent clients without thinking time   More experimental details and more results varying from 1 to 128 clients are described in Section 5   In Figure 1  the left bar corresponds to the required con     guration  The write cache in the logging disk is disabled so that log    ushes are guaranteed to be on stable media  In contrast  for the right bar  we enable the write cache in the logging disk  While an invalid con   guration  the latter is interesting because it models a logging device with ideal performance  modeling the cost of operating system calls  device driver scheduling  and device interface delays   As shown in Figure 1  compared to the ideal con   guration  the transaction rate of the required con   guration is a factor of 5 8 times lower  indicating that magnetic disk based logging is a major performance bottleneck  But why is this the case  By instrumenting the source code of MySQL InnoDB  we obtain the cumulative distribution of log    ush sizes  as shown in Figure 2  While log writes are often accumulated in an in memory log bu   er  log    ushes send actual I O writes to the logging device  As shown in Figure 2  the distribution is dominated by small I O write sizes  82 9  of the write sizes are less than 10KB large  90 5  of the write sizes are less than 15KB large  and 99 0  of the write sizes are less than 36KB  Compared to the track size of a typical disk  e g   8MB   the write size is very small  Since synchronous logging mainly sequentially appends to the log  the major access pattern is small sequential writes  It is common knowledge that magnetic disks  without write caching  are ill suited for handling small sequential writes  Because the 0  20  40  60  80  100  0 10 20 30 40 50 60 log flush size  KB  CDF Figure 2  The cumulative distribution of MySQLInnoDB log    ush sizes when running a TPCC test with the logging disk write cache disabled  corresponding to the left bar in Figure 1   platters in a disk are constantly spinning  a small write to a subsequent  sequential  location often incurs a full rotational delay  Even the highest performing 15k rpm disks experience 4ms rotational delays  Fundamentally  this penalty is determined by the mechanical capabilities of magnetic disks  which is di   cult to reduce  1 2 Alternatives are Expensive or Complicated In light of the above problem  high end disk arrays often provide battery backed write caches that are controlled by the caching mechanisms to prevent data loss  22   Upon a power failure  data is required to be maintained for up to several days until power is properly restored  Note that a UPS  Uninterruptible Power Supply  is insu   cient for this purpose  This solution achieves good logging performance by using caches but incurs the cost of purchasing expensive disk array systems  Another alternative approach is to replicate the state of a database server across one or more backup servers  a k a  replicas  for fault tolerance  If the states of all the replicas are kept consistent  then database requests can be served as long as at least one of the replicas does not fail  However  it is challenging to implement a working and scalable replication solution for transaction processing  1  4  7  11  21   Eagerly updating replicas as part of a transaction may incur signi   cant overhead  while lazily updating replicas after transactions commit may result in loss of transactions  stale data versions  and or complicated con   ict resolution problems  Even if a replication scheme could replace transactional logging  purchasing backup servers multiplies the cost of ownership  1 3 Our Approach  Exploiting Flash Devices We would like to    nd a synchronous logging solution that is e   cient  inexpensive  and simple  In this paper  we study    ash devices as a promising candidate for such a solution based on the following observations      E   ciency  There are no moving components in    ash devices  Therefore     ash devices do not su   er from the limitation of magnetic disks  We observe that for small sequential writes     ash devices are up to 9X faster than magnetic disks      Low Price  Flash memory capacity has been increasing and its price per GB decreasing exponentially  23  thanks to the demand in the mobile and embedded markets  Note that although the capacity of    ash de  74vices  e g   1   32GB  is often smaller than that of magnetic disks  it is typically su   cient for synchronous logging  whose requirements are detailed in Section 3 1       Simplicity  Unlike replication schemes discussed in the above  the design impact of employing    ash devices for logging can be constrained to the logging subsystem  thereby avoiding signi   cant changes to the entire database system  Traditionally     ash devices have been favored in the mobile and embedded markets because of their energy e   ciency  shock resistance  small form factors  and good performance  Recently     ash memory has been regarded as an alternative  replacement or addition  to magnetic disk technology  with new products  such as Solid State Drives  SSDs   targeting the mainstream computing market  Recent research studies have investigated the use of    ash devices in various aspects of data management systems  3  5  10  18  19  20  24  25  29  30  32  35   We focus on exploiting    ash devices for synchronous logging in this paper  Comparing a wide variety of    ash devices  we    nd that USB    ash drives are particularly suitable for synchronous logging because of its unique characteristics   i  widely available USB ports on almost all modern computers   ii  hotplug capability useful for coping with    ash wear  and  iii  low prices so that multiple drives can be used for better performance  Previous study  20  shows that SSDs can significantly improve transactional logging performance  In Section 5  we compare our solution with designs using a stateof the art SSD and    nd that although SSDs improve logging performance  multiple USB    ash drives can achieve comparable or better performance with much lower price  We propose F lashLogging  a logging solution that exploits multiple  USB     ash drives and addresses the following four key challenges      E   ciently exploiting an array of    ash drives  We    nd that the conventional array organization  in disk arrays   which stripes data across all the drives in a round robin manner  results in sub optimal behaviors  such as request splitting or skipping  for synchronous logging  In fact  the goal of this round robin organization is to support random access pattern well  However  synchronous logging mainly performs sequential writes during normal operations and sequential reads during recovery  while random accesses for given Log Sequence Numbers  LSNs  are rare  Therefore  we can employ an unconventional array organization  which only enforces that the LSNs written to each individual device are non decreasing  thus enabling request scheduling to maximize logging performance      Coping with large variance of write latencies  We observe that most writes to    ash devices are fast but occasionally some outlier writes incur delays that are much longer   possibly orders of magnitudes longer for some devices  The behavior can be explained by the sophisticated    ash block erasure and management operations in    ash devices  9   To reduce its negative impact  we leverage multiple    ash drives to hide the outlier latency  Because of the di   culty in predicting outliers  we instead propose a solution that performs outlier detection  We    nd that an adaptive outlier detection and hiding scheme is desirable      E   cient recovery processing  Recovery processing consists of two steps   i  locating the log record corresponding to a given checkpoint LSN  and  ii  scanning the log sequentially until the crash point   i  is a rare random access to the log  Since we maintain the invariant of non decreasing LSNs for all devices  we can perform binary searches for random accesses  For further narrowing the search range  we construct a low cost index structure on each device during normal logging operations  For  ii   multiple    ash drives are scanned concurrently  providing larger read bandwidth      Combining USB    ash drives with disks for better logging and recovery performance  Although magnetic disks have poor performance for small sequential writes  they are competitive to or better than USB    ash drives for writes that are 32KiB or larger  Disks also support larger sequential read bandwidth  which is important for recovery processing  We propose to include a disk drive in F lashLogging as a nearzero delay archival log device  During normal processing  the disk performs log    ushes as soon as log data is more than a prede   ned size S  e g   32KiB   Logging performance is improved because the disk can serve requests when all the    ash drives are busy  Moreover  a large portion of the log can be read from the archival log disk  achieving good recovery scan performance  1 4 Contributions This paper makes the following main contributions  First  we identify USB    ash drives as a good match for synchronous logging  Section 2   To our knowledge  this is the    rst study to exploit USB    ash drives for synchronous logging  Second  we propose F lashLogging  a logging system design that addresses four key challenges and exploits multiple    ash drives for good performance  Section 3   Third  we implemented F lashLogging as the logging subsystem in MySQLInnoDB  Section 4  and performed extensive real machine experimental evaluations using TPCC workloads  Section 5   Experimental results show that F lashLogging achieves up to 5 7 times improvements over magnetic disk based logging  obtaining up to 98 6  of the ideal performance  while recovery times are similar  Finally  we compare F lashLogging with designs employing SSDs  Experimental results show that naively employing SSDs already signi   cantly improves logging performance  that F lashLogging can further improve the performance of a SSD by dividing it into multiple partitions and treating the partitions as virtual    ash devices  and that compared to a single SSD  multiple USB    ash drives can achieve comparable or better performance with much lower price  2 THE CASE FOR EXPLOITING USB FLASH DRIVES FOR SYNCHRONOUS LOGGING In this section  we make the case for exploiting USB    ash drives for synchronous logging  Section 2 1 compares the performance characteristics of    ash devices and magnetic disks  Then Section 2 2 compares a wide variety of    ash devices to show that USB    ash drives are a good match for the task of synchronous logging  2 1 Flash Devices vs  Magnetic Disks Both magnetic disks and    ash devices are non volatile  There are two types of    ash memory  NAND and NOR  NAND is targeted at mass storage devices and is the focus of 75sequential read 0 01 0 10 1 00 10 00 100 00 1000 00 512 1K 2K 4K 8K 16K 32K 64K 128K 256K 512K 1M 2M read request size  bytes  average time  ms  sequential write 0 01 0 10 1 00 10 00 100 00 1000 00 512 1K 2K 4K 8K 16K 32K 64K 128K 256K 512K 1M 2M write request size  bytes  average time  ms   a  Sequential read performance  b  Sequential write performance random read 0 01 0 10 1 00 10 00 100 00 1000 00 512 1K 2K 4K 8K 16K 32K 64K 128K 256K 512K 1M 2M read request size  bytes  average time  ms  random write 0 01 0 10 1 00 10 00 100 00 1000 00 512 1K 2K 4K 8K 16K 32K 64K 128K 256K 512K 1M 2M write request size  bytes  average time  ms   c  Random read performance  d  Random write performance disk flash A flash B flash C ssd disk wce Figure 3  Comparing the performance of four    ash devices and a 10k rpm magnetic disk drive   Flash A  B  and C  three USB    ash drives from three di   erent vendors  SSD  a state of the art SSD with 250MB s peak read bandwidth and 70MB s peak write bandwidth  disk wce  write cache enabled   this paper  NAND    ash devices support the logical block addressing interface similar to magnetic disks  read write I Os can be performed on 512 byte sized logical blocks  However  unlike magnetic disks     ash memory does not contain moving components  A NAND    ash memory chip is composed of    ash blocks  e g   128KiB   512KiB large   Every    ash block consists of    ash pages  e g   512B   4KiB large   Only clean pages can be directly written  a k a  programmed   Before overwriting a page that contains data  a    ash device has to erase the entire    ash block  Such an erase operation is often an order of magnitude slower than a write operation  To avoid confusions with the 512 byte sized logical blocks at the device interface  we refer to a    ash block as a    ash erase unit in the rest of the paper  Moreover     ash memory has a    nite number of erase write cycles  A    ash erase unit can typically endure 100 000 erase write cycles  31   A    ash device contains a controller and one or more    ash memory chips  For example  SSDs often contain tens of    ash memory chips  15   To cope with the erase write constraints     ash controllers employ erase unit mapping and wear leveling algorithms in order to reduce the number of erase operations and evenly distribute them to physical    ash erase units  9   As a result     ash devices often have very different performance characteristics from    ash memory chips  Figure 3 compares the sequential random read write performance of four    ash devices and a 10k rpm disk  The three USB    ash drives in the experiment represent low end    ash devices  while the SSD 2 represents high end    ash devices  The write caches of the disk and the SSD are disabled unless otherwise noted  All the experiments are per  2 The SSD was released in Fall  2008  It has been shown by third party to perform signi   cantly better than several previous SSD products  15   formed on a PowerEdge 1955 blade server using the block device interface on Linux  The devices are opened with the O DIRECT O SYNC    ag in order to avoid operating system cache e   ects as much as possible  Elapsed times are measured with the rdtsc  Read Time Stamp Counter  x86 instruction in cycles  then converted to wall clock times by dividing the CPU frequency  We vary the I O request size from 512B to 2MiB  The requests are all aligned on requestsize boundaries  A data point shown represents the average of 10 000 requests of the same type for 512B   32KiB sizes  and 1 000 requests for 64KiB   2MiB sizes  As shown in Figure 3  the low end    ash devices have better small sequential write performance and better random read performance compared to the disk  This is expected because    ash devices do not su   er from the mechanical limitations of magnetic disks  However  the low end    ash devices suffer from worse random write performance because random writes often incur expensive erase and erase unit management operations  Moreover  the sequential read performance of low end    ash devices is much lower than the disk  This is partly limited by the target use model and price range of the devices  In contrast  the SSD has better or comparable performance than the disk in all aspects  An explanation is that these high end    ash devices can exploit the parallelism of many    ash memory chips  Studying the characteristics more closely  we make the following observations related to synchronous logging  First  sequential write performance is the most important characteristics for synchronous logging  As shown in Figure 3 b   the 10k rpm magnetic disk su   ers from a full 6ms rotational delay for every small write  Compared to the disk  the    ash devices are 1 2   9 4 times faster when write sizes are 16KiB or smaller  Moreover  compared to the SSD with write cache disabled  the low end devices have compa  76Table 1  Comparing properties of    ash devices  Interface Interface Easily Price  availability bandwidth replaceable capacity     USB    ash drive good good yes  15 8GB PCMCIA card laptops good yes  80 1GB CompactFlash digital good yes  20 8GB MicroSD etc  cameras Solid state drive good excellent possible  350 80GB    Data were obtained from shopping web sites in April 2009  rable sequential write performance  suggesting the potential of using low end    ash devices for synchronous logging  Second  sequential read performance is important for recovery processing  As shown in Figure 3 a   the low end    ash devices are 2 6   11 4 times slower compared to the disk  posing a challenge for achieving good recovery performance  We will address this challenge in Section 3 6  Finally     ash devices have faster random read performance than disks  which we leverage in our solution  in Section 3 6   2 2 USB Flash Drives are a Good Match for Synchronous Logging There are many di   erent types of NAND    ash based storage devices  e g   USB    ash drives  PCMCIA cards  CompactFlash  MicroSD  and Solid State Disks  SSDs  3   Table 1 compares the properties of these    ash devices  We    nd that USB    ash drives are a good match for synchronous logging for the following reasons      USB ports are widely available  In contrast to interfaces of PCMCIA cards  CompactFlash  and MicroSD  USB ports are widely available in all kinds of computer systems  This is true even for blade servers with very compact designs  The PowerEdge 1955 blade servers in our experiments support two USB ports with an extension cable shipped with the machine  Therefore  solutions based on USB    ash drives can be readily employed in almost all modern computers      USB bus bandwidth is good for logging  From our experiences with running MySQL on blade servers  the USB 2 0 bus bandwidth  60MB s  is su   cient for synchronous logging purpose  Moreover  the upcoming USB 3 0 speci   cation  6  targets 600MB s  or 4 8Gbit s  bandwidth  which is greater than the logging bandwidth required by even the highest rated TPCC performance results  34       The hot plug capability allows easy replacement of drives  Even with wear leveling support  a    ash drive may wear out more quickly than magnetic disks due to the frequent log writes  For example  the peak write bandwidth of a USB    ash drive is typically 10    20MB s  Therefore  it takes at least 400 seconds to write an entire 8GB drive once  Given the 100 000 erase write cycles  an 8GB drive can last at least 462 days in the worst case  Note that this worst case lifetime increases proportionally with device capacity      The price of an individual drive is low  Compared to SSDs  the price of an individual USB    ash drive is much lower  In April 2009  an 8GB USB    ash drive costs about  15  The drive capacity is good for logging purpose  our experiences with MySQL show 3 For synchronous logging purpose  SSDs represent the performance and prices for all devices targeting the mainstream mass storage market  e g   including hybrid drives   that the active online log size is consciously kept small  e g   a few hundred MBs  in order to reduce recovery time  Log capacity can be easily provided by magnetic disks that archive the log  Because of the low prices  a logging solution can exploit multiple drives to achieve better performance  Our experimental results in Section 5 4 show that multiple USB    ash drives can achieve comparable or better performance with lower price than a state of the art SSD  3  FLASHLOGGING DESIGN We propose F lashLogging  a synchronous logging solution that exploits multiple  USB     ash drives  In this section  we    rst summarize the requirements of synchronous logging that we learned from MySQL InnoDB in Section 3 1  We present the overall architecture of F lashLogging in Section 3 2  Then we describe our techniques and algorithms for addressing the four challenges in Section 3 3   3 7  3 1 Synchronous Logging Requirements We study MySQL with InnoDB as the backend storage engine  MySQL provides a database frontend with SQL query parser  optimizer  and query processing algorithms  while InnoDB supports the in memory bu   er pool  transaction processing  and write ahead logging  The following summarizes the requirements of synchronous logging that we learned from the MySQL InnoDB implementation      The online log is circular  Its size is kept small  e g   a few hundred MBs  to reduce recovery time  Note that log capacity can be easily provided by archiving the online log with large sized writes  to disks       During normal processing  sequential log writes are the frequent operation  A 64 bit Log Sequence Number  LSN  is assigned to each write  which uniquely identi   es the log record  The LSN is monotonically increasing and represents the log o   set if the log size were in   nite      During normal processing  checkpoints are periodically written to well known    xed locations in the log  Compared to the normal log writes  checkpoint frequency is very low  A checkpoint contains a checkpoint LSN and a checkpoint number  This LSN corresponds to the oldest modi   ed data page in the database bu   er pool  all data changes before this LSN are already on disks  InnoDB guarantees that all committed log records since the last checkpoint LSN are in the online log by  i     ushing log records when transactions commit  and  ii  monitoring log wrap around margins and    ushing old dirty bu   er pool pages accordingly so that log wrap arounds are correct      During recovery processing  InnoDB    rst locates the log record corresponding to the LSN recorded in the last checkpoint  Then the log is sequentially scanned until the crash point  i e  log blocks containing inconsistent checksums or LSNs   After the log scan  necessary redo operations are carried out  3 2 FlashLogging Architecture Figure 4 depicts the overall architecture of a F lashLogging system  We employ a simple producer consumer design  A set of interface routines accept logging requests  Log records are cached in an in memory log bu   er  e g   16MiB in our 77Interface  write  flush   checkpoint  recovery Request queue In memory log buffer Archival  Worker Worker Worker Figure 4  FlashLogging architecture  experiments   Upon a    ush call  the system creates request node s  for all the log data since the last    ush call  and appends it to the request queue  A request node contains begin and end pointers to the actual log data in the log bu   er  Each logging device is serviced by a dedicated worker thread  The worker thread obtains new requests from the request queue and performs the requested I O operations  Optionally  a disk is employed for both archival purpose and for better logging and recovery performance  as will be described in Section 3 7   The architecture for recovery processing is similar except that the data    ow is reverse  The workers read data from the devices into the log bu   er  while the interface routines deliver log data from the log bu   er to upper level callers  As will be shown in Section 5  this simple design achieves good logging performance  3 3 Exploiting an Array of Flash Drives What   s wrong with the conventional array organization  In a conventional array design  data are striped across the array of devices in a round robin manner  If there are N devices and the stripe unit size is S  then data at address A are found on the k th device  where k      A S    mod N  In this way  the physical location of data can be computed easily  enabling fast random accesses  However  the major access pattern in synchronous logging is small sequential writes during normal processing and sequential reads during recovery  Random data access is rare  On the other hand  round robin addressing may result in behaviors such as request splitting and or request skipping  incurring unnecessary overhead for our purpose  First  request splitting occurs if the log data to    ush is larger than the stripe unit size S  Under conventional array organization  the log data has to be split into multiple request nodes to be serviced by multiple devices  This may occur frequently if S is small  Request splitting can be suboptimal because for small sequential writes  the latency TR of a request size R satis   es T2R   2TR  as evidenced in Figure 3 b   In other words  if there are two homogeneous devices and two outstanding requests of size 2R  processing two 2R sized requests are faster than splitting the requests and processing four R sized requests  To con   rm this observation  we perform a set of micro benchmark experiments  as shown in Figure 5  The experimental setting is similar to that described in Section 2 1  In each experiment  two USB    ash drives serve 2   32 concurrent requesters  who repeatedly sends 4KiB sized    ush requests  The X axis varies the number of requesters  The Y axis shows the aggregate writing bandwidth of the two drives  From Figure 5  we see that compared to request splitting  no splitting achieves 1 41   1 51X higher bandwidth for    ashA  1 79   1 83X for    ash B  and 1 13   1 85X for    ash C  0 2 4 6 8 10 2 4 8 16 32 num requesters bandwidth  MB s  0 2 4 6 8 10 2 4 8 16 32 num requesters bandwidth  MB s  no split split  b  flash B  c  flash C 0 2 4 6 8 10 2 4 8 16 32 num requesters bandwidth  MB s   a  flash A Figure 5  Micro benchmark results comparing two cases   left bars  processing 4KiB requests  and  right bars  splitting each 4KiB requests into two then processing the split requests  Two drives of each device are used in the experiments  Second  if the stripe unit size S is large  request splitting is less frequent  while request skipping occurs more frequently  Imagine that the request queue contains a number of small requests R1  R2       Rm  Because of the large stripe unit size  several consecutive requests may fall into a single stripe unit  For example  a worker starts working on R1 before the other requests arrive  R1         Ri     S  while Ri 1 starts a new stripe unit  As a result  a second worker may be constrained to skip many early requests in the queue because the requests do not belong to the corresponding device  There could be two consequences   i  response times for the skipped requests are longer than necessary  which may in turn impact throughput in a closed loop system  such as TPCC   ii  the request queue may not be long enough  e g   Ri 1 does not exist yet   and the device may be forced to be idle since no requests belong to the device  To avoid these sub optimal behaviors  we would like to exploit the special access pattern of synchronous logging to organize the array in an unconventional way  Unconventional Array Design  Because the major access patterns are sequential for logging and recovery  the conventional array organization is unnecessary  Instead  we propose an unconventional array organization  Log data can be appended to a device as long as the LSNs on the device are non decreasing  Here  we sacri   ce the ability to quickly compute the locations for random accesses  In return  the log data since the last    ush can always be formed to a single request node  and be processed by a single device  thereby avoiding the request splitting and skipping problems  The invariant of non decreasing LSNs is naturally satis   ed because of the property of logging 4   This invariant enables the system to support rare random accesses through binary searches on the    ash drives  3 4 Coping with Outlier Writes Sequential writes have large variance  Figure 6 shows the individual request latencies of 5000 back to back sequential writes of    xed request size for the three USB    ash drives in our study  Figure 6 a  and  b  show the results for 512B and 4KiB requests  respectively  The experimental setting is similar to that described in Section 2 1  5 4 The rare event that a 64 bit LSN wraps around can be easily detected and specially handled  5We use a pair of rdtsc   s to measure latencies  The measurement overhead is less than 1 10000 of the lowest reported latencies  78 a   b  Figure 6  Large variance of elapsed times for 5000 back to back sequential writes   Note the di   erent Y scales for di   erent devices   As shown in Figure 6  we see that the elapsed times display a bimodal distribution  Most writes are fast while a number of long latency writes take possibly orders of magnitude longer time to complete  We call these long latency writes outliers  An explanation for outlier writes is that these writes somehow trigger the device to perform management tasks such as wear leveling and erase unit management  9   Since outlier writes may have a negative impact on the response time and throughput of the logging system  we investigate how to minimize such impacts  Outlier prediction is di   cult  The    rst idea that comes to our mind is to accurately predict the outliers and proactively avoid them by sending dummy write requests to devices that are predicted to experience outliers at the next writes  However  as shown in Figure 6  we    nd that accurate outlier prediction is di   cult  First  outlier patterns are di   erent across di   erent devices  Second  outlier patterns may be di   erent for di   erent request sizes  which cannot be predicted by combining simple statistics  such as the number of requests and the aggregate request size since the last outlier  Third  the number of requests between outliers may change even if both the request size and the device are    xed  as evidenced with    ash A and    ash B in Figure 6  Finally  when back to back requests of di   erent sizes are measured  not shown in the    gure   we see that outlier pattern is much less obvious from what are shown in Figure 6  We believe these complications are caused by the large variety of sophisticated erase unit mapping and wear leveling algorithms implemented in di   erent devices  9   Outlier Detection and Hiding  The good news is that outliers can be easily detected because of the bi model distribution  Therefore  instead of outlier prediction  we propose to perform outlier detection  To do this  we measure the average sequential write latencies for di   erent request sizes for a device  and list the latencies in a table as part of the device description  In this way  a worker can estimate the average latency of a request by interpolation using the table  A request that takes longer than twice the average latency is considered an outlier  Upon detecting an outlier  we would like to re issue the same request to a di   erent ready device in hope that the re issued request completes faster than the outlier  thus hiding the outlier latency  0 0 0 5 1 0 1 5 2 0 2 5 3 0 3 5 flash A flash B flash C average latency  ms  normal stream1 stream2 Figure 7  Writing two separate sequential streams to a single USB    ash drive  However  a problem arises  At the time of detection  an outlier request must have existed for quite some time  and its LSN may be smaller than the LSNs of all ready devices  In such situations  the request cannot be re issued because of the invariant of non decreasing LSNs  Unfortunately  this frequently happens as will be shown in Section 5 2  Our solution to this problem is to take advantage of an observation of    ash devices  they often support a limited number of  separate  sequential write streams well  This property is called semi randomness  24   Micro benchmark experiments in Figure 7 con   rm this observation  In the experiments  we generate two sequential write streams of 4KiB sized writes  99  of the writes are in    stream1     which models the main logging use of the device  while the rest 1  of the writes go to    stream2     which models the area for re issuing outlier requests     normal    latency of a single streams of 4KiB writes is provided for comparison purpose  As shown in Figure 7  the latencies of stream2 are comparable or moderately worse than stream1  In summary  we reserve an area on each device for outlier hiding  Upon outlier detection  we re issue the outlier request to the outlier hiding area on a di   erent ready device  Note that we also maintain the invariant of non decreasing LSNs for the outlier hiding areas  In this way  during recovery  we can simply treat these areas as    pseudo    devices and perform the same processing as a normal device  3 5 Logging Algorithm Figure 8 shows the logging algorithm  The algorithm uses an in memory log bu   er  and three queues for new 79requests  new queue   busy requests  busy queue   and completed but not released requests  done queue   respectively  Several LSN  Log Sequence Number  variables record the last known LSN on stable media  LSN done   the end LSN of all    ush requests  LSN flushreq   and the end LSN of the in memory log bu   er  LSN write   Frontend Functions  A transaction calls frontend write to append log records to the in memory log bu   er  L 3   When the transaction commits  it calls frontend flush with the largest LSN of all its log records  frontend flush determines whether the request is already serviced  L 6  or being serviced  L 7   If not  it composes a new request  including all new log data  L 8   6 Then it enqueues and waits for the request to complete  L 10   12   Worker Algorithm  A worker is dedicated to serving requests for a device  In an in   nite loop  a worker checks and calls process req for new requests  L 9   12   process req sets the owner of the request  L 15  and enqueues the request into busy queue  L 18   The worker records the last LSN for the device  L 21  and performs blocking I O for the request  L 22   When completed  it checks to see if it still owns the request  both outlier hiding and archival disk worker in Section 3 7 may change the ownership   If yes  then the worker moves the request to done queue  L 24   Note that the worker noti   es a frontend transaction that is blocking on a request r only if request r and all previous requests have completed  L 25   28   Unconventional Array Organization  Our algorithm avoids request splitting because frontend flush composes a request from all the new log data  L 8   Request skipping is avoided because a worker can process any new request  L 10   12   The invariant of non decreasing LSNs is naturally achieved  New requests are generated with increasing LSNs in frontend flush  Therefore  the    rst in    rst out nature of a queue guarantees that at any worker  the LSNs of de queued requests  L 10  are non decreasing  Adaptive Outlier Detection and Hiding  Before performing I O for a request  a worker Wi computes an estimated maximum end time of the request  L 17   Later  another worker Wj may    nd that the maximum end time has passed  L 5   thus detecting an outlier  Upon outlier detection  worker Wj re issues the request to its outlier hiding area  L 7  if the invariant of non decreasing LSNs of this area holds  L 6   For simplicity  we do not check if a reissued request is again an outlier by setting the end time of the re issued request to in   nity  L 20   Note that an outlier request eventually takes at least three times of the average latency  for detection and re issuing   Moreover  outlier hiding incurs additional I Os  Therefore  the signi   cance of outlier hiding is device dependent  The larger the latency di   erence between outliers and normal requests  the more signi   cant impact outlier hiding can have  as will be shown in Section 5 2  Therefore  we propose to use an adaptive algorithm  In Figure 8  the global variable is outlier hiding enables disables outlier detection and hiding in the worker algorithm  L 2   8   The adaptive algorithm can initially enable outlier hiding  then measure the number of outliers successfully hidden to determine whether to continue or disable outlier hiding  6 Requests must be multiple of 512B blocks  Details are omitted for clarity and will be discussed in Section 4  Global variables  CircularBuffer log buf  UInt64   LSN write  LSN flushreq  LSN done  ReqQueue new queue  busy queue  done queue  Bool is outlier hiding  UInt64   LSN archived  Worker private variable  LogDevice stream1  LogDevice stream2  frontend write logdata  size  1  while  log buf avail   size  2    waiting for log buffer space  3  append logdata to log buf  4  LSN write    size   5  return LSN write  frontend flush LSN  6  if  LSN    LSN done  return  7  if  LSN    LSN flushreq  goto Wait done  8  compose a request r from all log data since  LSN flushreq till LSN write  9  LSN flushreq   LSN write  10 enqueue  new queue  r   11 wake up workers for handling new requests  12 Wait done  block waiting till r and all earlier  requests are done  worker   1  Infinite worker loop  2    if  is outlier hiding  3        curtime   get current time    4        foreach r in busy queue 5           if   curtime   r end threshold     6             r start lsn  stream2 last lsn   7               process req r  stream2   8               goto Infinite worker loop  9    if  new queue is not empty  10       r  dequeue new queue    11       process req r  stream1   12       goto Infinite worker loop  13   block waiting for notification  14   goto Infinite worker loop  process req r  stream  15  r owner  this worker  16  if  stream    stream1     new request 17      r end threshold  get current time     2 avg latency r size   18      enqueue busy queue  r   19  else    re issue outlier request 20      r end threshold  infinity  21  stream last lsn  r end lsn  22  perform blocking device I O for r  23  if  r owner    this worker  24      move r from busy queue to done queue  25      if  r start lsn    LSN done 1  26          release contiguous done requests and wake up frontend flush  27          release log buffer space   and wake up frontend write  28          update LSN done  Figure 8  Logging algorithm   Details of synchronization between the frontend and the workers  and 512B log block alignment are omitted for clarity   80Section 1 header 2 header Section 0  Section 1  common block start lsn prev section lsn   Section k Section 0 header 1 Section k header 0  a  A log device is divided into sections  each with a header block   b  Header blocks are non leaf nodes in the index structure  Figure 9  A simple index structure on a log device  3 6 Ef   cient Recovery Processing From the perspective of the logging subsystem  recovery processing consists of two main steps   i  locating the log record corresponding to the LSN in the last checkpoint   ii  scanning the log sequentially until the crash point  Essentially  step  i  performs a random read operation  As discussed in Section 3 3  we can support random accesses by performing binary searches on the    ash drives because of the invariant of non decreasing LSNs  We can further improve the performance by constructing a simple index structure to narrow the search scope  as shown in Figure 9  The space of a log device is divided into equal sized sections with wellknown starting o   sets  As shown in Figure 9 a   the    rst block  called header  in a section records meta information   i  the starting LSN of the current section  and  ii  a small array mapping equally strided logical blocks to LSNs for the previous section  During normal processing  when a worker is working on section i 1  it generates the content for the header of section i  After section i 1 is    lled  the worker moves onto section i  At this point  it can write out the header of section i while maintaining the sequential write pattern  As shown in Figure 9 b   the header blocks form a simple index structure  To locate the block for an random LSN  the system searches the section headers to narrow the search range before performing binary searches for the block  For step  ii   the F lashLogging architecture allows scanning multiple devices in parallel for good scan bandwidth  During recovery  workers pro actively read data in chunks  e g   64KiB  into the in memory log bu   er  The log blocks in the chunks are then merged according to the ascending LSN order and delivered to the upper level recovery processing requesters  A global variable  delivered lsn  is maintained  The workers compare delivered lsn with the LSNs of the last read chunks  and check the availability of inmemory bu   er in order to determine whether or not to perform new chunk read I Os 3 7 Combining USB Flash Drives and Disks for Better Performance We propose to optionally include a disk in F lashLogging  Our goal is three fold  First  the disk can serve as a large capacity archival device to complement our online logging design  Second  recovery scan described in Section 3 6 is limited by the maximum USB bus bandwidth  60MB s   while a magnetic disk may provide larger recovery scan bandwidth  Third  as shown previously in Figure 3 b   magnetic disks are competitive to USB    ash drives for writes that are archival worker   1  Infinite archival worker loop  2    while LSN write     LSN archived   32KiB  3        waiting for more frontend writes  4    perform blocking I O for up to LSN write  5    update LSN archived  6    if  LSN done   LSN archived             7        process all queues  release requests  wake up frontend flush  free buffer space  wake up frontend write  8        LSN done   LSN archived  9    goto Infinite archival worker loop  Figure 10  Archival worker algorithm  32KiB or larger  Therefore  we may improve the logging performance by including a disk in the design  For achieving the goal  we propose to    ush log data to the disk as soon as log data is more than a prede   ned size S  e g   32KiB in our experiments   We call such a disk a nearzero delay archival disk because the disk eagerly archives data larger than S  Figure 10 shows the archival worker algorithm  Whenever the unarchived log data  including log data in memory  is at least 32KiB  the archival worker    ushes the data to the archival disk  L 4   If log data is archived before the associated normal    ush requests complete  L 6   the archival worker will consider these requests as completed  set ownership of the requests  and wake up frontend transactions that are blocking on the requests  L 7   thereby potentially improving logging performance  During recovery processing  the starting LSN is mostly likely found on the archival disk  Therefore  the system    rst performs the log scan on the archival disk  followed by scanning  a small portion of  the    ash drives  The starting points of the latter can be found similarly using binary search as described in Section 3 6  4  IMPLEMENTATION We implemented F lashLogging as the logging subsystem in MySQL version 5 0 24a with InnoDB as the backend storage engine  We modi   ed MySQL to recognize a con   guration variable in MySQL   s my cnf con   guration    le to decide whether to use the original logging subsystem or the F lashLogging subsystem  The con   guration    le also speci   es the devices to be used in F lashLogging  F lashLogging supports an array of K  K     1  homogeneous devices and an optional archival device  The device paths  capacities  and starting o   sets are speci   ed in the con   guration    le  F lashLogging uses the Linux open system call to open the devices  In a typical setting  we will specify K    ash devices and zero or one archival magnetic disk device  In this way  raw devices are used  In contrast  MySQL InnoDB does not support raw devices as the logging media  The log is con   gured as a set of normal    le system    les  This may pose disadvantages for MySQLInnoDB  To compensate for this  we run two sets of experiments for disk based logging  The    rst sets use MySQLInnoDB with log    les  The second sets use F lashLogging and specify the disk device as a single logging device  The former may have better code quality  while the latter enjoys the raw device interface  In our experiments  we report the better of the two cases  Overwriting data at the same logical address is an expensive operation for USB    ash drives  which often triggers an 81erasure of an entire  128   512KiB     ash erase unit  incurring performance overhead and reducing devices    life time  We observe in micro benchmark experiments  not shown due to space limitation  that if subsequent writes overlap even a small part  512B  of the previous ones  the write performance can degrade drastically  up to 60 8X worse  for some USB    ash drives  In the original MySQL logging implementation  overlapping writes are common  A    ush request r may end in the middle of a 512B block x  Because of the block device interface  the entire x is    ushed  A subsequent request s will continue to use the unused portion in x  When s is to be    ushed  x has to be re sent to the device  resulting in an overlapping write  In F lashLogging  a worker always appends a    ush request to its device and avoids any overlapping writes  This entails that multiple log blocks  possibly partially full  with the same LSNs may exist  During recovery processing  F lashLogging merges multiple log streams in the LSN order  When it sees two 512B blocks with the same LSN in the header  it chooses the full block or the block with more data  For outlier hiding purpose  the con   guration    le can optionally contain a path to a characteristics    le with the device   s average sequential write latencies for di   erent request sizes  If such a    le is con   gured  F lashLogging will read the    le content to build a latency table and use it to implement avg latency in Figure 8  L 17   Otherwise  is outlier hiding is set to false to disable outlier hiding and avg latency always returns in   nity  5  PERFORMANCE EVALUATIONS In this section  we present real machine experimental evaluations for F lashLogging using TPCC workloads  We    rst describe the experimental setups in Section 5 1  Then we present experimental results for the logging and recovery experiments in Section 5 2 and Section 5 3  respectively  Finally  we compare F lashLogging with designs that employ SSDs in Section 5 4  5 1 Experimental Setup Machine Con   guration  Two Dell PowerEdge 1955 blade servers are used in the experiments  Each machine is equipped with a 3 0GHz Intel 5160 Woodcrest CPU  dual core  two hardware threads per core   4MB L2 cache  4GB DRAM  and two 73GB 10K RPM SAS  Serial Attached SCSI  disks  running Linux 2 6 17 10  The two machines are connected through 1Gbps Ethernet  Logging Devices  We use the same devices as in the characteristics study in Section 2 1  An extension cable supports up to two USB ports  Therefore  we use up to two USB    ash drives for    ash A     ash B  and    ash C  In every con   guration  the devices in a    ash array are homogeneous  The capacity of the USB    ash drives range from 1 to 4GB  For experiments in Section 5 4  we replace one of the disks in a blade server with the SSD  The write caches of the logging devices are disabled unless otherwise noted  The devices are opened with the O DIRECT O SYNC    ag to avoid operating system cache e   ects as much as possible  We report experimental results for the following logging device con   gurations  disk  ideal  i e  disk with write cache enabled   three con   gurations for each    ash device  1f  one drive  2f  two drives  2f 1d  two drives with an archival disk   and multiple designs that employ SSDs  TPCC Workload  We con   gure an MySQL 5 0 24a database server to run on one blade server and store data on one of its disks  while storing the circular transactional log on the other disk  or on    ash drives  Transaction isolation level is set to be serializable  We con   gure the database inmemory bu   er pool to use 2GB of the main memory  The number of warehouses in the TPCC benchmark is chosen to be 20  As reported by the    show table status    statement  the resulting database size  including database tables and indices  is 1803 6 MB  Therefore  the entire database and its indices are memory resident  The circular log size is set to be 900 MB  Note that as discussed in Section 3 1  MySQL InnoDB performs background writes for dirty bu   er pool pages to guarantee the correctness of the wrap rounds of the circular log  We enhanced MySQL InnoDB with  i  implementation of F lashLogging and  ii  instrumentation for statistics  The other blade server runs an open source TPCC driver  based on OSDL DBT2 version 0 39  28   Originally  the driver has a two level thread architecture  The    rst level threads generate TPCC requests  send the requests to the second level threads through Unix sockets  which connect and forward the requests to the database server through TCP connections  We simpli   ed the driver by combining the two levels  Each thread maintains a persistent TCP connection to the database server  and repeatedly generates and sends transaction requests to the server  TPCC think time is set to be zero to maximize the load at the database server  The number of client threads is an adjustable parameter  which we vary from 1 to 128 in our experiments  5 2 Logging Performance For every logging experimental result  we run TPCC test for the con   guration for 60 minutes and report the average transaction rate  NOTPM  New Order Transactions Per Minute   as was output from OSDL DBT2  Before every test run  we perform three preparation steps to ensure that the size of the TPCC database is roughly the same  and the MySQL InnoDB bu   er pool is warmed up  First  we rebuild the TPCC database by dropping the entire database  creating the database  creating tables and indices  loading the table data  and loading stored procedures for TPCC transactions  Second  we perform a    select count    from table    for every TPCC table  Third  we run 10 minute TPCC test with 20 clients  Overall performance  Figure 11 compares the transaction performance of disk based logging  F lashLogging  and the ideal case  From Figure 11  we can see the following points  First  disk based logging is a major bottleneck  As shown in Figure 11 a   disk based logging obtains only 11 2    29 9  of the ideal case performance  As the number of concurrent clients increases from 1 to 128  the transaction rates    rst increase then decrease  with the peak being around 32 clients  7  Figure 1 in Section 1 shows the 32 client points   Second  employing USB    ash drives naively in MySQLInnoDB achieves marginal gains at best  as shown in Figure 11 a   This is mainly because the original InnoDB logging system performs overlapping writes  which incurs poor performance for USB    ash drives  7 In our experiments  the number of clients is a power of 2  Therefore  the best transaction rates should be achieved in the range of  16  64   820 5000 10000 15000 20000 25000 30000 35000 1 2 4 8 16 32 64 128 number of clients NOTPM disk ideal flashA flashB flashC 0 5000 10000 15000 20000 25000 30000 35000 1 2 4 8 16 32 64 128 number of clients NOTPM disk ideal 2f 1d 2f 1f  a  Employing USB    ash drives naively  b  F lashLogging with USB    ash drive A 0 5000 10000 15000 20000 25000 30000 35000 1 2 4 8 16 32 64 128 number of clients NOTPM disk ideal 2f 1d 2f 1f 0 5000 10000 15000 20000 25000 30000 35000 1 2 4 8 16 32 64 128 number of clients NOTPM disk ideal 2f 1d 2f 1f  c  F lashLogging with USB    ash drive B  d  F lashLogging with USB    ash drive C Figure 11  Logging performance  disk  magnetic disk  ideal  disk with write cache enabled  1f  1    ash drive  2f  2    ash drives  2f 1d  2    ash drives and an archival disk   The same disk and ideal curves are included in all    gures for ease of comparisons  0 1 2 3 4 5 6 1f 2f 2f 1d 1f 2f 2f 1d 1f 2f 2f 1d flashA flashB flashC improvement over disk 0  20  40  60  80  100  1f 2f 2f 1d 1f 2f 2f 1d 1f 2f 2f 1d flashA flashB flashC compared to ideal  a  Normalized to disk  b  Normalized to ideal Figure 12  Normalized throughputs for the 32 client cases in Figure 11 b     d   Third  almost all F lashLogging con   gurations achieve signi   cant improvements over disk based logging  Using one    ash drive achieves signi   cant improvements for    ash A and    ash B  Using two    ash drives as an array leads to better performance for all three kinds of USB    ash drives  The best performance is achieved with two    ash drives and an archival disk  As shown in Figure 12  we see that F lashLogging with    ash A     ash B  and    ash C achieves up to 5 7X  5 7X  and 4 7X improvements over disk based logging  respectively  thus obtaining up to 98 6  of the ideal performance  Array Organization  Figure 13 shows the performance of the conventional striping array organization that employs round robin addressing  normalized to that of F lashLogging  2f  for each device  We vary the stripe unit size from 512B to 256KiB  Note that less than 100  means that the conventional scheme su   ers performance loss  From Figure 13  0  20  40  60  80  100  512B 1KiB 2KiB 4KiB 8KiB 16KiB 32KiB 64KiB 128KiB 256KiB stripe unit size normalized performance flash A flash B flash C Figure 13  TPCC performance with conventional array organization normalized to that of F lashLogging   2f is used  we see that small stripe unit sizes  512B   8KiB  and large unit size  256KiB  both result in sub optimal performance across all devices  As discussed in Section 3 3  this is because of request splitting and request skipping  respectively  The best performance of conventional scheme is achieved with 64KiB   128KiB stripe units  However  even in these cases  conventional scheme can incur signi   cant performance loss compared to F lashLogging    up to 26 6  performance loss for    ash C and up to 9 8  loss for    ash A  This is because of remaining request splitting and skipping behaviors  e g   due to log requests not aligned on stripe unit boundaries  In contrast  F lashLogging   s unconventional array design avoids request splitting and skipping  thereby achieving good performance without parameter tuning for the stripe unit sizes  Outlier Detection and Hiding  Figure 14 and Figure 15 study the e   ects of outlier detection and hiding  As 830 0 0 5 1 0 1 5 2 0 2 5 3 0 1KiB 2KiB 4KiB 8KiB 16KiB request size 0 bandwidth  MB s  2 4 6 8 10 1KiB 2KiB 4KiB 8KiB 16KiB request size 0 bandwidth  MB s  5 10 15 20 1KiB 2KiB 4KiB 8KiB 16KiB request size bandwidth  MB s  no hiding naive hiding 2streams  a  flash A 2f  b  flash B 2f  c  flash C 2f Figure 14  Potential study of outlier hiding with micro benchmarks   naive hiding  outlier requests are re issued to the main logging area  2streams  outlier requests are re issued to the separate outlier area   0 5000 10000 15000 20000 25000 30000 1 2 4 8 16 32 64 128 number of clients NOTPM 2f no hide 2f 2f 1d no hide 2f 1d Figure 15  E   ects of outlier detection and hiding on overall TPCC performance for    ash C  described in Section 3 4  the e   cacy of the outlier hiding scheme depends on the latency di   erence between outliers and normal requests  Therefore  we    rst perform a set of micro benchmarks in Figure 14 to understand the potentials of outlier hiding  Here  we build a stand alone program on F lashLogging  where four threads repeatedly issue log requests of a given size via F lashLogging to logging devices  We report the aggregate logging bandwidth  As shown in Figure 14  we see that outlier hiding achieves up to 1 37X improvements with    xed sized requests for    ashC  while it has almost no e   ect for    ash A and    ash B  Note that    ash C has the largest latency di   erence between outliers and normal writes  Figure 6   Therefore  it is reasonable to expect outlier hiding to have the most signi   cant impact on    ash C  Moreover  from Figure 14 c   we see that the choice of where to re issue outlier requests is important  Upon detection  naive hiding is hardly able to re issue outlier requests  because of the invariant of non decreasing LSNs  while 2  streams hiding works well with the invariant  As a result  2 streams hiding re issues up to 18 6 times more outlier requests than naive hiding  Furthermore  Figure 15 shows the impact of outlier hiding on the overall TPCC performance for    ash C  Outlier hiding achieves up to 1 66X improvements over the no hiding cases  Given the above results  we believe that for addressing the large variance across USB    ash drive types  a logging system should employ an adaptive scheme that detects the e   cacy of outlier hiding  and dynamically enable disable it  as described in Section 3 5  8 In our TPCC experiments  8 It happens that    ash C has the largest latency di   erence between outliers and normal requests and it is also the lowest performing    ash drive  In general  this may not be true  We prefer a we choose no hiding for    ash A and    ash B  and 2 streams hiding for    ash C  Archival Disk  As shown in Figure 11  adding a nearzero delay archival disk to 2f signi   cantly improves logging performance  As described in Section 3 7  we    ush log data to the disk as soon as un archived log data in memory is more than 32KiB  Logging performance can be improved because an archival disk    ush may cover pending requests while both    ash drives are busy serving earlier requests  For example  the two    ash drives are serving requests R1 and R2 when R3 arrives  It is possible that an archival    ush including data up to R3 is initiated  Later     ash drive 1 may complete R1 and start serving R3  If the time to serve R1 then R3 on the    ash drive is longer than the time to serve the archival    ush  the archival    ush may return before    ash drive 1 completes R3  thus improving the logging performance for R3  Such situations are possible mainly because  i  disk performance is competitive for writes of 32KiB or larger   ii  a single write of large size can be faster than multiple writes of small sizes  and  iii  the    ash drive may encounter an outlier  As a result  the archival disk approach is more e   ective for lower performing    ash drives  as shown in Figure 11 b     d   5 3 Recovery Performance Recovery experiments are performed as follows  First  we carry out the same preparation steps as in a logging experiment  Second  we run 30 minute TPCC workloads with 32 clients  Third  we kill the MySQL server process  Finally  we restart the MySQL server and collect statistics for recovery processing  For each con   guration  we report the results of    ve recovery experiments  Figure 16 shows the recovery scan performance of diskbased logging and F lashLogging  Figure 16 a  is a scatter plot  where each point represents a single recovery run  The X axis is the log data size scanned during recovery  The Y axis reports the elapsed time for the recovery scan  Figure 16 b  compares the average recovery scan bandwidth of all the con   gurations  The error bars show the standard deviation of the    ve runs of each scheme  From the    gures  we can see the following points  First  all F lashLogging schemes achieve comparable or better recovery scan times compared to disk based logging  Second  it is e   ective to scan multiple    ash drives in parallel  Scanning two    ash drives in parallel achieves 1 19   1 81X higher bandwidth than using a single    ash drive  self contained algorithmic solution to a solution that requires all DBAs to use certain types of USB    ash drives for avoiding large outliers  840 5 10 15 20 25 30 35 40 45 0 100 200 300 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing"/>
        <doc>Efficiently Evaluating Complex Boolean Expressions Marcus Fontoura ### Suhas Sadanandan Jayavel Shanmugasundaram Sergei Vassilvitski Erik Vee Srihari Venkatesan Jason Zien Yahoo  Research  701 First Ave   Sunnyvale  CA 94089  marcusf  suhas  jaishan  sergei  erikvee  venkates  jasonyz  yahoo inc com ABSTRACT The problem of e   ciently evaluating a large collection of complex Boolean expressions     beyond simple conjunctions and Disjunctive Conjunctive Normal Forms  DNF CNF      occurs in many emerging online advertising applications such as advertising exchanges and automatic targeting  The simple solution of normalizing complex Boolean expressions to DNF or CNF form  and then using existing methods for evaluating such expressions is not always e   ective because of the exponential blow up in the size of expressions due to normalization  We thus propose a novel method for evaluating complex expressions  which leverages existing techniques for evaluating leaf level conjunctions  and then uses a bottom up evaluation technique to only process the relevant parts of the complex expressions that contain the matching conjunctions  We develop two such bottom up evaluation techniques  one based on Dewey IDs and another based on mapping Boolean expressions to one dimensional intervals  Our experimental evaluation based on data obtained from an online advertising exchange shows that the proposed techniques are e   cient and scalable  both with respect to space usage as well as evaluation time  Categories and Subject Descriptors H 2 4  Systems   Query processing General Terms Algorithms Performance Keywords Boolean expressions  Pub sub  Dewey  Interval 1 ###  INTRODUCTION We consider the problem of e   ciently evaluating a large collection of arbitrarily complex Boolean expressions  given Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     5 00  an assignment of attributes to values  The problem of ef     ciently evaluating Boolean expressions has many applications  including  1  publish subscribe systems  8   where a subscription can be modeled as a Boolean expressions and an event can be modeled as a collection of attribute value pairs  and the goal is to rapidly return the subscriptions that match an event  and  2  online display advertising systems  10  13  18   where an advertiser campaign can be modeled as a Boolean expression targeting user visit features  and a user visit can be modeled as a collection of attributevalue pairs  and the goal is to rapidly return the set of advertiser campaigns that are eligible for the user visit  Current solutions for evaluating Boolean expressions  however  are primarily limited to simple Boolean expressions such as conjunctions  1  9  14  20  21  and conjunctive disjunctive normal form expressions  5  17   While these restrictions are reasonable for many of the above applications  some emerging applications require support for arbitrarily complex Boolean expressions  as discussed below  1 1 Motivating Applications Online Display Advertising Exchanges  One of the emerging trends in online display advertising is the notion of an advertising exchange  or simply  an ad exchange  An ad exchange is essentially an electronic hub that connects online publishers to advertisers  either directly or through intermediaries  An ad exchange can thus be represented as a directed graph  where the nodes are publishers  advertisers and intermediaries  and an edge exists between a node X and a node Y if X agrees to sell advertisement slots associated with user visits to Y  note that X may either be a publisher who generates advertisement slots through user visits  or an intermediary who obtains user visits from publishers or other intermediaries   In addition  each edge is typically annotated with a Boolean expression that restricts the set of user visits that can be sold through that edge  for various business reasons such as protecting certain user visits for sales through speci   c channels  edges   For instance  a node X may only want to sell to Y male user visits to Sports pages  and female user visits that are not to Finance pages  this could be represented as a Boolean expression on the X Y edge   Gender      Male      Category      Sports        Gender      F emale      Category 6     F inance   Given the ad exchange graph  an advertiser A can decide to book an advertising campaign with a publisher P  even though A may only be indirectly connected to P  For instance  if an advertiser A wants to book a campaign with P that targets users in age category 4 who are interested in NFL  and age category 5 who are interested in NBA  1this campaign can also be represented as a Boolean expression   Age      4      Interest      NF L        Age      5      Interest      NBA    Note  however  that any user visit from P that satis   es the above Boolean expression cannot simply be served the ad from A  the user visit should also satisfy all the Boolean expressions on the path from P to A 1   Consequently  the campaign booked with P is the conjunction of the advertiser campaign Boolean expression  and the Boolean expressions along the path from P to A  Since advertiser and targeting constraints can themselves be complicated DNF or other expressions  conjunctions of such expressions quickly leads to fairly complex Boolean expressions which are booked to publishers  When a user visits a publisher Web page  the user visit can be viewed as an attribute value assignment such as the one below   Gender   Male  Interest   NF L  Category   Sports  and the goal is to rapidly    nd all the ad campaigns booked to P that can be satis   ed by the user visit  so that the best ads can then be selected to be shown to the user  In other words  the exchange has to rapidly evaluate complex Boolean expressions to determine which one satisfy the given assignment of attributes to values  Automatic user targeting  A related application that generates complex Boolean expressions is automatic user targeting for display advertising  Unlike manual user targeting where advertisers specify the Boolean expression of interest  as in the examples above   in automatic user targeting  the system automatically generates targeting expressions that try to maximize advertiser objectives such as clicks or conversions  For instance  an advertiser who is interested in obtaining clicks on ads may specify a fairly broad targeting constraint  and allow the system to explore the high dimensional targeting space to generate boolean targeting expressions that optimize desired measurement objectives  Clearly these can get quite complex very quickly because they are automatically generated  Given many such advertiser campaigns  the advertising system again needs to rapidly evaluate these Boolean expressions given an attribute assignment  user visit   1 2 Contributions Given the above motivating applications  we now turn to the issue of e   ciently evaluating arbitrarily complex Boolean expressions  A simple evaluation method  of course  is to sequentially loop over all the Boolean expressions  and do a recursive top down evaluation of the expression tree given the attribute value assignment  This method has the obvious downside of having to evaluate every single expression  even though the assignment may only match a small fraction of them  Another simple method is convert the Boolean expressions to DNF or CNF form  and leverage state of the art techniques  e g    5  17   to e   ciently evaluate these expressions  Again  this method has the downside of an exponential blow up in the size the expressions due to normalization  this issue is exacerbated by the fact that most online ad systems are entirely memory resident  for latency reasons   which leads to excessive memory requirements  Given the limitations of the obvious approaches  the question that 1 There might be multiple paths from P to A and the best path is usually chosen based on revenue and other constraints that are not germane to the current discussion  arises is the following  is there a way to evaluate Boolean expressions that does not require evaluating every expression  and that does not result in an exponential space blow up  The main technical contribution of this paper is a novel evaluation method that addresses the above issues  The method consists of two key steps  In the    rst step  existing conjunction matching techniques  9  17  21  are used to    nd the leaf level conjunctions of the  un normalized  Boolean expressions that match the given assignment  In addition  each conjunction is annotated with a compact description of where it occurs in the Boolean expressions  In the second step  the matching conjunctions along with the information on where they occur is used to perform a bottom up evaluation of the Boolean expressions  The bottom up evaluation is performed in a single pass over the matching conjunctions  and only selectively evaluates the expressions     and parts of these expressions     that have at least one matching conjunction  The above two step approach thus leverages existing conjunction matching techniques without blowing up the size of the expressions  and also avoids explicitly evaluating all expressions by using selective bottom up evaluation of only those  parts of  expressions that can possibly be satis     ed  As mentioned above  the key idea that enables the bottomup evaluation of Boolean expressions is the annotation that identi   es the position of each conjunction within the Boolean expression  There are two annotation variants that we consider  both of which work on the Boolean tree representation of expressions  In the    rst variant  each conjunction is identi   ed based on a Dewey labeling of the Boolean expression tree  similar to the Dewey labeling of an XML tree  16    Given this labeling  the bottom up evaluator uses a stackbased algorithm to e   ciently    nd the ids of the contracts that evaluate to true  In the second variant  each Boolean expression tree is mapped to a one dimensional space  and the bottom up evaluator uses a simple interval matching technique to    nd the ids of the matching contracts  While both approaches are e   cient  one of the advantages of the one dimensional mapping is that the conjunction annotations are    xed length  as compared to variable length Dewey labels  We have implemented the proposed methods and evaluated them using data obtained from an online display advertising exchange  Our performance results show that the proposed methods signi   cantly outperform existing methods  both in terms of latency and memory requirements  1 3 Roadmap The rest of the paper is organized as follows  In Section 2  we describe the problem and the system architecture  In Section 3  we present the evaluation method based on Dewey labeling  and in Section 4  we present the evaluation method based on the one dimensional interval mapping  In Section 5  we present our experimental results and  in Section 6  we discuss related work  Finally  in Section 7  we present our conclusions  2  PROBLEM DESCRIPTION Our problem is to e   ciently    nd which Boolean expressions from a large set are satis   ed by an input assignment  An assignment is a set of attribute name and value pairs  A1   v1  A2   v2          For example  a woman in California may have the assignment  Gender   F  State   CA   2An assignment does not necessarily specify all the possible attributes  Allowing unspeci   ed attributes is important to support high dimensional data where the number of attributes may be in the order of hundreds  Consequently  our model does not restrict assignments to use a    xed set of possible attributes known in advance  A Boolean expression  BE  is a tree in which intermediate nodes are of two types  AND nodes and OR nodes  Leaf nodes in the tree are simple conjunctions of basic     and 6    predicates  The predicate State      CA  NY    for example  means that the state can either be California or New York while the predicate State 6     CA  NY   means the state cannot be either of the two states  Notice that the     and 6    primitives subsume simple   and   predicates  6 Without loss of generality  we restrict our BE trees to have alternating AND OR nodes in every path from the root to the leafs  Any arbitrarily complex BE can be represented by these alternating AND OR trees with conjunction leafs  including DNFs  i e   disjunctive normal form   CNFs  i e   conjunctive normal form   ANDs of DNFs  ORs of CNFs  and so on  2 1 System Architecture The overall system architecture is presented in Figure 1  In an o   ine process  before query evaluation starts  BEs are annotated and indexed  The Conjunction Annotator module is responsible for annotating each conjunction with a compact description of where it occurs in the BE  These annotations are stored in a Conjunction Annotations database  The conjunctions are then indexed by the Conjunction Index Builder  Our approach works with any existing scheme for indexing and evaluating conjunctions  e g   1  9  14  17   During runtime  given an assignment  the index is used to retrieve the matching conjunctions  Given these set of matching conjunctions  the Expression Evaluator uses the Conjunction Annotations database to retrieve the annotations for the conjunctions that need to be evaluated  The job of the Expression Evaluator is to e   ciently verify if the entire BE tree can be satis   ed from the conjunctions retrieved by the index  We highlighted components Expression Evaluator and Expression Annotator since these are the main contributions of the paper  Sections 3 and 4 describe two di   erent annotation schemes and evaluation strategies for these components  Scalability  latency and updates  The main focus points of this paper are the Expression Evaluator and Conjunction Annotator components and we can reuse any conjunction indexing scheme  Each of these di   erent schemes will handle scalability  latency and updates di   erently  Our driving applications are online advertising systems  which have to process billions of requests a day  However  the update volume is typically many orders of magnitude less than the read volume  Fortunately  there are several conjunction indexing schemes optimized for this scenario  e g    17   For instance  scalability can be solved by index replication and partitioning  latency can be solved by keeping the indexes in main memory  while updates can handled by keeping small    delta    indexes in addition to the main indexes  17   3  DEWEY ID MATCHING Our    rst algorithm uses the notion of Dewey IDs to perform boolean expression matching  We    rst describe how Dewey IDs are generated  and then how they are used in Conjunc on       Annotator    Conjunc on    Index   Builder    Conjunc on    Index    Boolean    Expression    O   ine    Assignment    Index    Evaluator    Conjunc on    Annota ons    Expression    Evaluator    Online    Matching    Conjunc ons    Matching   Boolean   Expression    Figure 1  Online and o   ine architectural view of the system  We focus on the highlighted components  Expression Evaluator and Conjunction Annotator  evaluating BEs  The main challenge comes from the fact that we do not store the Boolean Expression Tree for evaluation  Rather  we reconstruct the relevant parts of the tree only from the information encoded in the matching Dewey IDs and decide whether the overall tree evaluates to true  3 1 Conjunction Annotator We    rst describe the information stored with each conjunction  As we mentioned earlier  any BE can be expressed as an alternating AND OR Boolean tree  We label each leaf node of the tree with its corresponding Dewey ID as follows  1  Without loss of generality let the root of the tree be an AND node  We can always add an arti   cial AND at the top if needed  2  Let edges to the children of a node be sequentially numbered starting from 1  with the last child marked with a special symbol      The root to node numbering  based on those edge numbers  is referred to as the Dewey ID of a node  3  The length of a Dewey ID is the number of edges from the root to the node  Observe that a node with an odd length Dewey ID is beneath an AND  and a node with an even length Dewey ID is beneath an OR  For example  consider the BE tree in Figure 2  The label of D is 1    3 1     to reach D from the root  one takes the    rst branch  which happens to be the last branch as well  as denoted by       then the third branch  and then the    rst branch again  The Dewey IDs of other leaves are given in the Figure  3 2 Expression Evaluator We    rst give intuition on the functioning of the algorithm  We then describe it in more detail  The algorithm acts recursively on the Boolean expression tree  Note that this tree is not actually available during online processing  We only have the list of matching dewey IDs  However  these dewey IDs implicitly encode this tree  or more precisely  the portion of the tree where the dewey IDs lie   As we process the IDs  we create this    virtual    tree on the    y  3C D A B OR AND AND F E AND A 1  1 1 B C D E F 1  1 2  1  2 1  3 1 1  3 2 1  3 3  Figure 2  An example of a BE tree with Dewey ID labels  The special symbol     indicates the last child of an AND node  Algorithm 1 The Dewey Evaluation Algorithm Require  deweyList  a list of dewey IDs in sorted order  1  Initialize curr     deweyList getFirst    curr and deweyList are global variables  2  return EvaluateAND Empty DeweyId  Throughout the running of the algorithm  we alternate calls to EvaluateAND and EvaluateOR  Each call takes as input a dewey label  which we call dewLabel  We think of each call as corresponding to exploring a node in the Boolean expression tree  and this label corresponds precisely to the dewey ID of that node  A call to EvaluateAND is like exploring an AND node of the tree  while EvaluateOR corresponds to exploring an OR node  We iterate through the list of dewey IDs  in sorted order  The value of curr is the dewey ID we are currently considering  Note the curr corresponds to a leaf node in the Boolean expression tree  At this point  it is helpful to imagine a depth    rst traversal of the virtual nodes of the Boolean expression tree  If we are exploring a node in the virtual tree that is an ancestor of the leaf node corresponding to curr which is equivalent to the dewey label in the current call being a pre   x of curr   then we move down toward that leaf  When we reach a leaf  we evaluate it true  since the index only returns those conjunctions which evaluate to true  We then pop up a level  partially evaluating that node  The curr dewey ID is updated to the next dewey ID in the list  We continuing popping up levels  evaluating nodes as we go   until we reach an ancestor of the newly updated curr  Continuing in this manner allows us to evaluate the entire expression  We now walk through the algorithm  Pseudo code is shown in Algorithms 1 2  and  3  We use several helper functions  First  Child  which takes as input a dewey pre     x and a dewey ID   The input pre   x must be a pre   x of the input dewey ID   It returns the dewey ID of entry where they    rst di   er  For example  Child 0 1 2  0 1 2 3 4  returns 0 1 2 3  Note that Child returns the Dewey label for one of the children of the dewey pre   x  The other two functions  work on the dewey IDs  Last returns the value of the last Algorithm 2 The EvaluateOR Algorithm Require  dewLabel  current position in the tree  1  if dewLabel   curr then  We are at a leaf   2  return true 3  end if 4  Initialize result     false 5  while dewLabel is a pre   x for curr do  curr is a descendant of this node  6  Let child     Child dewLabel  curr  7  result     result     EvaluateAND child  8  curr     deweyList next   9  end while 10  curr     deweyList prev   11  return result Algorithm 3 The EvaluateAND Algorithm Require  dewLabel  current position in the tree  1  if dewLabel   curr then  We are at a leaf   2  return true 3  end if 4  Initialize result     true  lastExplored     0  and lastChild   false 5  while dewLabel is a pre   x for curr do  curr is a descendant of this node  6  Let child     Child dewLabel  curr  7  lastExplored     lastExplored   1 8  if Last child   6 lastExplored then 9  result     false 10  end if 11  lastChild   Marked child  12  result     result     EvaluateOR child  13  curr     deweyList next   14  end while 15  curr     deweyList prev   16  return result   lastChild id  For example Last 0 1 2  returns 2  Finally  Marked returns true if the last node of the Dewey id is marked with a     and false otherwise  The algorithm is initialized by setting curr to the    rst element in the sorted deweyID list  It then calls EvaluateAND  with input dewey label of    Empty DeweyID     We think of this    rst call as moving to the root node of the Boolean expression tree  Now  within the EvaluateAND call  our base case  Steps 1 to 3  corresponds to being at a virtual leaf node  in which case we return true  The while loop  Step 5  iterates through all ancestors in the dewey list of the node currently being explored  It does this by    rst exploring the child under which the curr dewey ID lies  Thus  we recursively call EvaluateOR with its label corresponding to the child of the currently explored node  After this evaluation takes place  we AND its result with our result so far  Note that curr may have been updated during the EvaluateOR call  We continue to iterate through each of the children  In the call to EvaluateAND  we need every child to evaluate to true  We ensure that every child is explored my maintaining lastExplored  and checking that we never jump over a child  Steps 7 to 10   We also check that we encounter a starred dewey ID along the way  43 3 Example We walk through an example of the Evaluation algorithm  Let the set of Dewy IDs presented to the expression evaluator be  1    1 1  1    3 1  1    3 2  1    3 3    Is the BE satis   ed  The Dewey IDs represent nodes A  D  E and F in Figure 2  so it is easy to see that the expression is satis   ed  However  remember that the evaluation algorithm does not know what the tree for the BE was  it only sees the matching Dewey IDs  The Dewey evaluation algorithm    rst looks at id 1    1 1  and recursively calls EvaluateAND and EvaluateOR until it reaches the leaf  A   It then pops up a level to the AND at position 1    1 and increments curr  Since the next id  1    3 1 does not have 1    1 as a pre   x  the evaluation stops  and the AND is evaluated to false since the lastChild was never set to true  The algorithm then proceeds to evaluate the AND at position 1    3  It successfully evaluates all of the leaves  at which point the result is set to true and so is lastChild  the latter being set to true during the evaluation of 1    3 3     since the id ends in the special symbol      Therefore the OR at position 1    is set to true as well  Finally  the original call to EvaluateAND returns with true  3 4 Correctness Theorem 1  The Dewey Evaluation algorithm is correct  Proof  The proof of correctness follows quickly from the recursive nature of the algorithm  We sketch the proof that EvaluateAND and EvaluateOR both evaluate correctly  and further  the value of curr after the call is set to the last dewey ID for which the dewey label of the call is a pre   x  Clearly  the algorithm produces the correct result when the Boolean expression tree is a single node  By induction  assume that the algorithm works on a tree of depth d     1  and consider a tree of depth d  There are two cases  whether the top level node is an AND or an OR  Suppose it is an AND  the OR case is even simpler   In the call to EvaluateAND  we call EvaluateOR iteratively for each child of the explored node  By induction  each of these calls returns the correct result  The method returns false if one of these subroutines returned false  since all of the results are ANDed together   one of the children is skipped  the check of lastExplored   or if the    nal child was not seen  the check of lastChild   Otherwise we can conclude that all of the children of the AND returned true  and thus this node evaluates to true as well  Finally  we note the running time  Theorem 2  Let   be the set of leaves returned  and for a leaf n       denote by len n  the length of the Dewey ID of n  Then the Dewey Evaluation algorithm runs in time O  P n     len n    Proof  The running time follows from the fact that we evaluate each of the returned Dewey IDs one level at a time  so the time to process an id n is proportional to len n   We note that while the pseudocode presented is not optimized  one can  for example exit the EvaluateAND loop as soon as the result is set to false   this does not change the worst case running time of the algorithm  4  INTERVAL IDS We describe an alternative algorithm for evaluating BE trees  At a high level the algorithm works by mapping each leaf node of the BE tree onto a 1 dimensional interval on the real line  A contract is satis   ed if there is a subset of intervals that cover the real line without overlap  At    rst glance it sounds like we have made our problem more di     cult  however  the matching algorithm is simpler and more intuitive than the Dewey ID evaluation algorithm  Moreover  for each conjunction we need to store only two    xed length values  namely the beginning and the end of the corresponding interval  Hence  the amount of the stored information is constant  4 1 Intuition Consider an arbitrary BE tree  we will map the leaf nodes of the tree to intervals on the real line  We denote an interval  s  t  as hs  ti   In what follows s and t will always be integers   Fix M to be the maximum number of leaves in a BE tree  We will represent each contract by a line segment h1  Mi  Each leaf of the tree will be mapped to a subinterval of h1  Mi  The key to the algorithm lies in the mapping of conjunctions to intervals  We aim to    nd a mapping so that a contract is satis   ed if and only if there exists a subset of satis   ed leaves covering the entire segment h1  Mi without overlap  To develop the intuition  we    rst describe two simple cases  Consider a hypothetical Boolean expression  A     B shown in Figure 3  The contract is satis   ed if either of the two leaves is satis   ed  Therefore  the interval mapping scheme assigns the same interval h1  Mi to both A and B  Invariant 1  Consider a BE tree  and a node n corresponding to an OR  Then every child of n has the same interval as n  OR A B 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 A B Figure 3  The children of an OR node inherit the same interval as the parent  The situation is the opposite for an AND node  Consider a hypothetical Boolean expression  A     B shown in Figure 4  The contract is satis   ed only if both of the leaves are satis   ed  The interval mapping scheme splits the interval h1  Mi of the parent node into two non overlapping intervals  h1  xi and hx  Mi for the children  We describe the exact choice for x later  More generally  this leads to a second invariant  Invariant 2  Consider a BE tree  and a node n corresponding to an AND  Then the interval corresponding to n is partitioned among its children  5AND A B 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 A B Figure 4  The children of an AND node partition the interval  It is straightforward to apply these two invariants recursively  A more complicated example  corresponding to  A     B     C     D     E    F  is shown in Figure 5  Notice that there are three ways the tree can be satis   ed  Either both A and B are satis   ed  C is satis   ed  or all three of D  E and F are satis   ed  This example also demonstrates why we must    nd a set of non overlapping intervals that fully cover h1  10i  If B and D are returned  then h1  10i     h1  4i     h2  10i  that is  together B and D cover the whole interval  However  since B and D overlap  this is not an eligible assignment  1 C D A B OR AND AND F E A B 2 3 4 5 6 7 8 9 10 C 1 2 3 4 5 6 7 8 9 10 D 1 2 3 4 E 5  F 6 7 8 9 10 Figure 5  A more complex example of intervals  Another potential pitfall in assigning intervals is shown in Figure 6  Here because B and E share the same starting point  an assignment of B and D would evaluate to true  together B and D cover the whole interval h1  10i without overlap  Thus we have the third invariant   1 C D A B OR AND AND F E A B 2 3 4 5 6 7 8 9 10 C 1 2 3 4 5 6 7 8 9 10 D E 2 3 4 5 F 6 7 8 9 10 1 Figure 6  An invalid labeling  If B and D are true  the full interval is covered without overlap  Invariant 3  Given a boolean expression tree  and let C be the set of children of AND nodes  Let F be the set of    rst  left most  children of AND nodes  Then no two nodes in C   F have the same starting point for their intervals  The last invariant precisely precludes the case in Figure 6  Since both B and E are second children of an AND node  their corresponding intervals must start at di   erent positions  We will refer to the assignment of intervals to the leaves of a tree as the labeling of the tree  Definition 1  We call a labeling valid if Invariants 1  2 and 3 are maintained  We describe an algorithm for generating a valid labeling in the next section  and in Section 4 3 show how we can quickly evaluate whether the full BE is true  based only on the intervals corresponding to the satis   ed conjunctions  4 2 Conjunction Annotation In this section we describe an algorithm for providing a valid labeling of the nodes of the tree  Let the size of a node be the total number of children in its subtree  with size of a leaf node set to 1   Further  for each node n  let n leftLeaves denote the total number of leaves appearing before n in a pre order traversal of the tree  The algorithm is recursive  it starts by labeling the root node with interval h1  Mi where M is the maximum number of leaves supported  and then calling the subroutine presented in Algorithm 4  Algorithm 4 The Label algorithm Require  Node n  1  if n is a leaf then 2  return 3  else if n is an OR node then 4  for all children c of n do 5  c begin     n begin 6  c end     n end 7  Label c  8  end for 9  else if n is an AND node then 10  for    rst child c do 11  c begin     n begin 12  c end     n leftLeaves   c size 13  Label c  14  curr     c end 1 15  end for 16  for all intermediate children c of n do 17  c begin     curr  18  c end     curr   c size  1  19  Label c  20  curr     c end 1 21  end for 22  for last child c do 23  c begin     curr  24  c end     n end  25  Label c  26  end for 27  end if For example  consider the Tree in Figure 5 with M   10  When labeling node A  we have n leftLeaves   0  since there are no prior leaf nodes in an in order traversal of the tree  Therefore the interval for A is h1  1i  B is relegated the rest 6Interval matched 0 1 2 3 4 5 6 7 8 9 10 Initial 1 0 0 0 0 0 0 0 0 0 0 A   h1  1i 1 1 0 0 0 0 0 0 0 0 0 D   h1  4i 1 1 0 0 1 0 0 0 0 0 0 E   h5  5i 1 1 0 0 1 1 0 0 0 0 0 F   h6  10i 1 1 0 0 1 1 0 0 0 0 1 Table 1  The matched array during the evaluation of the algorithm on A  D  E  F  For instance  row D shows the array matched after adding the interval for D  of the interval  h2  10i  The interval for C is easy  since it is a child of an OR node  it must be h1  10i  Now consider the label for D  For D   s parent node  n leftLeaves is set to three  therefore the endpoint of the interval for D is 1 3   4  The intervals for E and F follow  Note that the labeling of the tree can be constructed in a single in order traversal of the BE tree  4 3 Expression Evaluation The input to the evaluation algorithm is a set of intervals  one for each matching conjunction  The algorithm attempts to    nd a non overlapping set of intervals that cover the range h1  Mi  To do so  the algorithm will maintain a Boolean array matched  where matched i  is true if there is a nonoverlapping set of intervals that ends in i  We give the full matching algorithm below  Algorithm 5 Match Algorithm  Require  I  set of intervals hbegin  endi sorted by begin 1  matched     Boolean Array of length M   1 2  Initialize matched i  to false for all i 3  matched 0    true 4  for all intervals hbegin  endi in I do 5  if matched begin 1  then 6  matched end      true 7  end if 8  end for 9  if matched M  then 10  return true 11  else 12  return false 13  end if Consider again the example in Figure 5  and suppose that A  D  E  F were returned as matching conjunctions  The algorithm maintains the state of the matched array  with the individual steps presented in Table 1  Note that processing each interval requires only two probes into the boolean array  4 4 Correctness In this section we prove that both the labeling algorithm and the label evaluation algorithms are correct  Theorem 3  The Label Algorithm produces a valid labeling  Proof  It is easy to see that Invariants 1 and 2 are trivially satis   ed  that is every child of an OR node has the same interval as the parent  and the children of an AND node partition the interval among themselves  It remains to show invariant 3  Let C be the set of children of AND nodes  and F be the set of    rst  or leftmost children  We show that the interval corresponding to every node n     C  F starts at n leftLeaves   1  Recall that n leftLeaves is the number of leaves occurring before n in a pre order traversal of the tree  If this holds than no two nodes in C   F can have the same starting points for intervals  We prove the claim by induction on the depth of the node  For the base case  consider the root node n  and let cP1  c2          ck be its k children  Observe that ci leftLeaves   i   1 j 1 cj  size  and the base case follows  Suppose that the claim holds for nodes at depth d  Let n be a node at depth d  1 and c1  c2          ck be its k children  Since ci leftLeaves   n leftLeaves   Pi   1 j 1 cj  size  the theorem follows  We now show that the matching algorithm is correct  We begin with a theorem about valid labelings  Theorem 4  Consider a BE tree with a valid labeling  Let I be the set of intervals corresponding to leaf nodes in N that evaluate to true  Then the BE is satis   ed i    there exists a subset I 0     I such that    i   I 0 hbegini  endii   h1  Mi and any two intervals i  i 0     I 0 are disjoint  hbegini  endii     hbegini 0   endi 0 i        To prove the theorem  we    rst show that given a satisfying assignment to the BE we can    nd a set of intervals I 0 as described above  We then prove the converse  that is  given a set of intervals satisfying the condition above  we show that the BE must be satis   ed  Proof  To prove the forward direction  consider the minimal set of leaves of the tree that lead to a satis   ed assignment  a set is minimal if removing any element would lead to the BE evaluating as false   Then the set of corresponding intervals covers the whole segment h1  Mi and is non overlapping  The formal proof is by induction on the height of the tree  since an AND partitions the interval  to be satis   ed all of its children must be satis   ed  therefore its interval would be fully covered by its children  On the other hand  since all of the children of an OR inherit the same interval  only one of the children needs to be satis   ed  otherwise the initial set of trees is not minimal   Therefore the intervals corresponding to the minimal set of leaves satisfy the conditions of the theorem  To prove the reverse direction  the main obstacle is to show that an interval corresponding to an AND node can only be fully covered without overlap by the nodes corresponding to its children  This is guaranteed by Invariant 3  Since the starting points of the intervals of all intermediate AND children are disjoint  and AND node can only be satis   ed by its children because no subset of other children would result in a continuous and non overlapping interval  The formal proof again proceeds by induction on the height of the tree rotted at the AND node  Finally  we can prove the correctness of the Match algorithm  Let   be the number of leaves of the tree returned by the indexing system  Theorem 5  The Match algorithm is correct and runs in time O     Proof  To prove correctness  we show that the Matching algorithm    nds a non overlapping subset of intervals covering the whole interval if one exists  The invariant maintained by the algorithm is that matched i  is set to true only if there exists a non overlapping set of intervals that cover 7the interval h1  ii  When processing an individual interval i   hbegin  endi  we check to see if begin continues a previous interval  in which case matched end  is set to true  Since the intervals are sorted by the begin position when processing  all intervals that end before begin will have been processed before  since they must begin even earlier   To show the running time  observe that each evaluation of an interval results in at most two lookups into a boolean array  4 5 Discussion The Interval evaluation algorithm has a number of appealing properties that improve upon the Dewey ID algorithm      It can handle trees of very large depth with a    xed size Interval ID  In contrast  the Dewey ID grows linearly with the depth of the tree  If the total number of leaves is n  the space taken by storing the interval is O log n   whereas the space required for some Dewey ids may be as large is     n       Although we described the algorithm in the context of AND OR trees  it can naturally handle arbitrary BE trees without any change to the code      Finally  the Interval algorithm is faster  requiring only two memory look up calls for each interval  instead of being linear in the size of the Dewey IDs  5  EXPERIMENTS In this section  we evaluate our Dewey and Interval matching algorithms for BE evaluation on synthetic datasets from an online advertising application  We compare their performance to other e   cient algorithms for BE evaluation and study how the algorithms behave for di   erent scenarios  such as BE tree depth and selectivity  All algorithms were implemented in C    and our experiments were run on a 2 5GHz Intel R  Xeon R  processor with 16GB of RAM  5 1 Data set In order to test the e   ciency of the Dewey evaluation  Interval evaluation and other algorithms  we used a synthetic dataset generated from statistics of real advertising contracts  To gather the statistics we looked at individual conjunctions appearing in each contract  For example  a contract looking for Males from California would specify Gender      Male      State      CA   We    rst collected statistics over the size of these conjunctions  We denote by ci the probability of seeing a conjunction on i elements  We then recorded how often each attribute i  e g  gender  state  etc  is present in the contracts  which we denote by ai  For example  if half the contracts targeted on gender then agender   1 2  For each attribute i  we collected statistics on the targeting values for this attribute  We denote the distribution by pattribute target   For example  if  from the set of contracts targeting on gender  2 3 targeted males  and 1 3 targeted females  then we say pgender male    2 3 and pgender female    1 3  These statistics served as input to the BE generator  We    rst generated the logical BE tree  namely an alternating AND OR tree  Without loss of generality the root node was selected to be an AND  The tree was then generated recursively  For each node  we    rst decide how many children the node will have  If it has 0  then we stop and mark the node as a conjunction  otherwise we generate the children  mark them as OR nodes  or in the case of processing an OR node  we mark the children as AND nodes   and recurse  The input to the tree generator speci   ed the minimum and maximum depths of the desired tree  and the probability distribution on the number of children  If the node being generated is below the minimum depth  then the number of children is set to be non zero  If the node is at the maximum depth  then the number of children is set to 0  Otherwise  we select from the distribution  In our experiments  the number of children was 2 with probability 0 7  3 with probability 0 2  and 1 or 4 with probability 0 05  This resulted in a wide distribution on the trees  Given the tree structure  we then generated a conjunction for each leaf node of the tree  To generate a</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing"/>
        <doc>Answering Why not Questions on Top k Queries ### Zhian He Eric Lo Department of Computing  Hong Kong Polytechnic University fcszahe  ericlog comp polyu edu hk Abstract   After decades of effort working on database performance  the quality and the usability of database systems have received more attention in recent years  In particular  the feature of explaining missing tuples in a query result  or the so called    why not    questions  has recently become an active topic  In this paper  we study the problem of answering why not questions on top k queries  Our motivation is that we know many users love to use top k queries when they are making multi criteria decisions  However  they often feel frustrated when they are asked to quantify their feeling as a set of numeric weightings  and feel even more frustrated after they see the query results do not include their expected answers  In this paper  we use the queryre   nement method to approach the problem  Given as inputs the original top k query and a set of missing tuples  our algorithm returns to the user a re   ned top k query that includes the missing tuples  A case study and experimental results show that our approach returns high quality explanations to users ef   ciently  I ###  INTRODUCTION Although the performance of database systems has gained dramatic improvement in the past decades  they also become more and more dif   cult to use than ever  1   In recent years  there is a growing effort to improve the usability of database systems  For example  to help end users query the database more easily  the features of keyword search  2  or form based search  3  could be added to a database system to assist users to    nd their desired results  As another example  the features of query recommendation  4  or query autocompletion  5  could be added to a database system in order to help users to formulate SQL queries  Among all the studies that focus on improving databases    usability  the feature of explaining missing tuples in database queries  or the so called    whynot     6  questions  has recently received growing attentions  In practice  users often feel frustrated when they    nd their expected tuples are not in the query results without any explanation  and intuitively  they will ask such a question  why are my expected tuples not in the results  If a database system can give a good explanation for it  it would be very useful for users to understand and re   ne the query  There are different ways to answer    why not    questions  For example   6  answers a user   s why not question on SelectProject Join  SPJ  queries by telling her which query operator s  eliminated her desired tuples  In  7  and  8   the missing answers of SPJ  7  and SPJUA  SPJ   Union   Aggregation  queries  8  are explained by telling the user how the data should be modi   ed  e g   adding a tuple  if she wants the missing answer back to the result  Another work that goes one step further is  9   which tells the user how to revise her original SPJA queries so that the missing answers can return to the result  These works  however  have been concentrated on only traditional SPJUA queries  and none of these can answer why not questions on preference queries such as topk queries yet  In fact  answering why not questions on top k queries is very useful because while users love to use top k queries when making multi criteria decisions  they often feel frustrated when they are forced quantify their preferences as a set of numeric weightings  Moreover  they would feel even more frustrated when their expected answers are missing in the query result  For instance  a customer Mary is going to Paris for a holiday  and this is her    rst time to be there  so she needs to choose a hotel carefully from a list of hotels in Paris  Yet  the list is so long to read  therefore  Mary decides to look at the top 3 hotels based on a set of weightings she sets on the attributes    Price        Distance to Downtown     and    Ratingby other customers     To her surprise  hotel Hilton  which is very famous around the world  and Mary   s favorite  is not in the result  Now  Mary may feel frustrated     Why is my favorite hotel not in the top 3 result  Is that because there is no Hilton Hotel in Paris  Should I revise my weightings  Or my Hilton Hotel can be back to result if I simply revise my query to look for top 5 hotels instead of top 3 hotels     In this paper  we present methods to answer why not questions on top k queries  We follow the latest query re   nement  9  approach that suggests users how to re   ne their top k query in order to get their expected answers back  Note that the result of a top k query depends on two sets of parameters   1  the number of objects to be included in the result  i e   the k value  and  2  the set of weightings  w speci   ed by the user  In some cases  the missing object can easily be back in the result if the k value is slightly increased  e g   re   ning a top 3 query to be a top 5 query   However  there can be cases where the missing object will not be included in the result unless the k value is dramatically increased  e g   re   ning a top 3 query to be a top 10000 query   and for those cases  modifying the set of weightings  w instead of k may make more sense from the users perspective  Furthermore  there could be cases in which the best option is to slightly modify both the k value and the set of weightings  w together  instead of signi   cantly modify one of them alone  To address this problem  we    rst de   ne a penalty model to capture a user   s tolerance to the changes of weightings  w and k on her original query  Next  we show that    nding the re   ned top k query with the least penalty is actually computational expensive  Afterwards  we present an ef   cient algorithm that uses sampling to obtain the best approximate answer in reasonable time  We evaluate the quality and the performance of our method using real data and synthetic data As an interesting preview  when we attempted to    nd the top 3 players in the NBA history  we found that Michael Jordan was amazingly missing in the result  So  we immediately posed a why not question    why is Michael Jordon not in top 3      In 0 15 seconds  our method returned a suggestion that we could re   ne the query to be a top 5 query without changing our weightings  The result told us that our initial set of weightings was actually reasonable  but we should have asked for more players in the top k query initially  The rest of the paper is organized as follows  Up next in Section II  we formulate the problem and illustrate that it is computationally expensive to obtain the exact answer  In Section III  we present our method to solve the problem ef   ciently with provable quality guarantee  In Section IV  we report experimental results  In Section V  we discuss related work and we conclude the paper in Section VI  II  PRELIMINARY In this section  we give a formal de   nition to the problem of answering why not questions on a top k query  Afterwards  we discuss the dif   culty of solving the problem exactly  A  Problem Statement Given a database of n objects  each object  p with d attribute values can be represented as a point  p   jp 1  p 2        p d j in a d dimensional data space  For simplicity  we assume that all attribute values are numeric and a smaller value means a better score  A top k query is composed of a scoring function  a result set size k  and a weighting vector  w   jw 1  w 2        w d j  In this paper  we accept the scoring function score as a linear function  where score  p   w     p    w  k as any positive integer  Pand we assume the weighting space subject to the constraints w i    1 and 0   w i    1  The query result would then be a set of k objects whose scores are the smallest  in case objects with the same scores are tie at rank k th  only one of them is returned   Initially  a user speci   es a top k query qo ko   wo   After she gets the result  she may pose a why not question on qo with a set of missing objects M   f m1           mjg  and hope the system to return her a re   ned top k query q 0  k 0    w 0   such that all objects in M appear in the result of q 0 under the same scoring function   A special case is that a missing object  mi is indeed not in the database  we describe more on this later   We use  k and  w to measure the quality of the re   ned query  where  k   max 0  k 0  ko  and  w   jj  w 0   wojj2  We de   ne  k this way is to deal with the possibilities that a re   ned query may obtain a k 0 value smaller than the original ko value  For instance  assume a user has issued a top 10 query and the system returns a re   ned top 3 query with a different set of weightings  We regard  k as 0 in this case because the user essentially does not need to change her original k  In order to capture a user   s tolerance to the changes of k and  w on her original query qo  a basic penalty model that sets the penalties  k and  w to  k and  w  respectively  where  k    w   1  is as follows  P enalty k 0    w 0      k k    w w  1  Note that the basic penalty model favors changing weightings more than changing k because  k could be a large integer whereas  w is generally small  One possible way to mitigate this discrimination is to normalize them respectively  To do so  we normalize  k using  k m o  ko   where k m o is the rank of the missing object  m under the original weighting vector  wo  To explain this  we have to consider the answer space of why not queries  which consists of two dimensions   k and  w  Obviously  a re   ned query q 0 1 is better than  or dominates  10   another re   ned query q 0 2   if both its re   nements on k  i e    k  and w  i e    w  are smaller than that of q 0 2   For a re   ned query q 0 with  w   0  its corresponding  k must be k m o  ko  Any other possible re   ned queries with  w   0 and  k    k m o  ko  must be dominated by q 0 in the answer space  In other words  a re   ned query with  w   0 must have its  k values smaller than k m o  ko or else it is dominated by q 0 and could not be the best re   ned query  Therefore  k m o  ko is the largest possible value for  k and we use that value to normalize  k  Similarly  let the original weighting qvector  wo   jwo 1  wo 2        wo d j  we normalize  w using 1   P wo i  2   because  Lemma 1  In our concerned weighting space  given  wo and an arbitrary weighting vector  w   jw 1  w 2        w d j   w   q 1   P wo i  2   pPProof  First  we have  w   k  w   wok2    w i   wo i   2   Since w i  and wo i  are both nonnegative  then we can have pP  w i   wo i   pP 2    w i  2   wo i  2     pP w i  2   P wo i  2   It is easy to know that P w i  2     P w i   2   As P w i    1  we know that P w i  2   1  Therefore  we have  w   pP w i  2   P wo i  2   p 1   P wo i  2   Now  we have a normalized penalty function as follows  P enalty k 0    w 0      k  k  km o  ko     w  w p 1   P wo i  2  2  The problem de   nition is as follows  Given a why not question fM  qog  where M is a non empty set of missing objects and qo is the user   s initial query  our goal is to    nd a re   ned top k query q 0  k 0    w 0   that includes M in the result and with the smallest penalty  In this paper  we use Equation 2 as the penalty function  Nevertheless  our solution indeed works for all kinds of monotonic  with respect to both  k and  w  penalty functions  For better usability  we do not explicitly ask users to specify the values for  k and  w  Instead  users are prompted to answer a simple multiple choice question 1 illustrated in Figure 1  Let us give an example  Figure 2 shows a 2 D data set with    ve data objects  p1   p2   p3   p4  and  m  Assume a user 1 The number of choices and the pre de   ned values for  k and  w  of course  could be adjusted  Furthermore  one might prefer to minimize the difference between the new result set and the original one  instead of minimizing the difference between the re   ned query and the original query  In that case  we suggest the user to choose the option where  w is a big value  i e   not prefer modify the weightings  because  11  has pointed out that similar weightings generally lead to similar top k results              Choice           Question Prefer modifying k or your weightings  Prefer modify k  k   0 1   w   0 9 Prefer modify weightings  k   0 9   w   0 1 Never mind  Default   k   0 5   w   0 5 Fig  1  A multiple choice question for freeing users to specify  k and  w d 2  0 d 1  p   3    2 5    p   4    6 6    p   1    2 2    p   2    5 2    m       4 4    ranking ID score 1  p1 2 2  p2 3 5 2  p3 3 5 Top 3 query qo ko   3   wo   j0 5 0 5j  Fig  2  A 2 D example has issued a top 3 query qo ko   3   wo   j0 5 0 5j  and wonders why the point  m is missing in the query result  So  she would like to    nd the reason by declaring a why not question ff mg  qog  using the default penalty preference    Never mind      k    w   0 5   In the example   m ranks 4 th under  wo  so we get k m o   4  k m o  ko   4  3   1  and p 1   P w2 oi   1 2  Table I lists some example re   ned queries that include  m in their results  Among those  re   ned query q 0 1 dominates q 0 2 because both its  k and  w are smaller than that of q 0 2   The best re   ned query in the example is q 0 3  penalty 0 12   At this point  readers may notice that the best re   ned query is located on the skyline of the answer space  Later  we will show how to exploit properties like this to abtain better ef   ciency in our algorithm  TABLE I EXAMPLE OF CANDIDATE REFINED QUERIES Re   ned Query  ki  wi Penalty q 0 1  4  j0 5 0 5j  1 0 0 5 q 0 2  5  j0 6 0 4j  2 0 14 1 06 q 0 3  3  j0 7 0 3j  0 0 28 0 12 q 0 4  3  j0 2 0 8j  0 0 42 0 175 q 0 5  3  j0 21 0 79j  0 0 41 0 17 q 0 6  3  j0 22 0 78j  0 0 4 0 167 B  Problem Analysis First  let us consider there is only one missing object  m in the why not question  In the data space  we say an object  a dominates object   b  if a i    b i  for i    1          d  and there exists at least one a i    b i   In a data set  if there are kd objects dominating  m and n objects incomparable with  m  then the ranking of  m could be  kd   1           kd   n   1   For these n 1 possible rankings r1  r2          rn 1 of  m  each ranking ri has a corresponding set Wri of weighting vectors  where each weighting vector  wri 2 Wri makes  m ranks rith  As such  any re   ned queries q 0  ri    wri   are also candidate answers  because when k   ri   the missing tuple  m is in the result  with rank ri   Recall that our objective is to    nd a combination of k and weighting  wri that minimizes Equation 2  However  the weighting vector set Wri is actually either empty or a set of convex polytopes  Lemma 2   That means when Wri is not empty  then there are an in   nite number of points  wri 2 Wri   which also makes the number of candidate answers in   nite  Lemma 2  Wri is either empty or a set of convex polytopes  Proof  Given a data space and a missing object  m  assume there are kd objects dominate  m  and the number of incomparable objects with  m is n  To    nd the set of weighting vectors Wri   where ri   kd   j and j 2  1  n   1   we have to solve a set of linear inequality systems  We use I to stand for the set of incomparable objects with respect to  m  Now  we arbitrarily put j  1 objects from I into a new set E  and put the rest into another set F  Let any object  e 2 E satisfy the following inequality   e    wri    m    wri  3  which means E is the set of objects that have scores better than  m  Similarly  any object   f 2 F is an object that has score not better than  m    f    wri    m    wri  4  Now we have a set of linear inequalities for all objects in E and F  Together with the constraints P wri  i    1 and wri  i  2  0  1   we can get a linear inequality system  The solution of this inequality system is a set of weighting vectors that make  m rank  kd   j  th  In fact  there are C n j1 such linear inequality systems and Wri is essentially the union of the solutions of them  The boundary theory of linear programming  12  shows that a linear inequality system is a polyhedron  which is the intersection of a    nite set of half spaces  A polyhedron is either empty  unbounded  or a convex polytope  In our case  the polyhedrons are either empty or convex polytopes  because they are additionally bounded by the constraints P w i    1 and w i  2  0  1   Therefore  the weighting vectors Wri is the union of a set of convex polytopes or an empty set if there are no solutions for all the inequality systems  As Wri is a set of convex polytopes with in   nite points if it is not empty  the number of candidate answers in the answer space is also in   nite  Therefore  we conclude that searching the optimal re   ned query for one missing object in an in   nite answer space is unrealistic  2 Moreover  the problem would not become easier when M has multiple missing objects  III  ANSWERING WHY NOT In this section  we present our method to answer why not questions on top k queries  According to the problem analysis 2One exact solution that uses a quadratic programming  QP  solver  13  is as follows  For each ranking value ri   kd   j  j 2  1  n   1   we can compute  k   max ri  ko  0   In order to make Equation 2 as small as possible under this ri  we have to    nd a  wri 2 Wri such that jj  wri   wojj2 is minimized  Since general QP solver requires the solution space be convex  we have to    rst divide Wri into Cn j1 convex polytopes  Each convex polytope corresponds to a quadratic programming problem  After solving all these quadratic programming problems  the best  wri could then be identi   ed  For all ranking ri to be considered  there are Pn 1 j 1 Cn j1   2 n  n is the number of incomparable objects with  m  quadratic programming problems at the worst case  so this exact solution is impractical         presented above     nding the best re   ned query is computationally dif   cult  Therefore  we trade the quality of the answer with the running time  Speci   cally  instead of considering the whole in   nite answer space  we propose a special samplingbased algorithm that    nds the best approximate answer  A  Basic Idea Let us start the discussion with an assumption that there is only one missing object  m  First  suppose we have a list of weighting vectors S      wo   w1   w2           ws   where  wo is the weighting vector in the user   s original query qo  For each weighting vector  wi 2 S  we formulate a progressive top k query q 0 i using  wi as the weighting  Each query q 0 i is executed by a progressive top k algorithm  e g    14    15    16    which progressively reports each top ranking object one by one  until the missing object  m comes forth to the result set with a ranking ri   If  m does not appear in the result of the    rst executed query  we report to the user that  m does not exist in the database and the process terminates  Assuming  m exists in the database  then after s 1 progressive top k executions  we have s   1 re   ned queries q 0 i  ri    wi   where i   o  1  2          s  with missing object  m known to be rank ri th exactly  Finally  the re   ned query q 0 i  ri    wi  with the least penalty is returned to the user as the answer  In the following  we discuss where to get the list S of weighting vectors  Section III B   Then  we discuss how large the list S should be  Section III C   Afterwards  we present the algorithm  Section III D   Finally  we present how to deal with multiple missing objects  Section III E   B  Where to get weighting vectors  In the basic idea of the algorithm  the size of S plays a crucial role in the algorithm ef   ciency and the solution quality  Having one more sample weighting in S  on the one hand  may increase the chance of having a better quality solution  on the other hand  that would de   nitely increase the number of progressive top k operations by one and thus increase the running time  So  one of our objectives is to keep S as small as possible and at the same time put only high quality weightings  e g   only those that may yield the optimal solution  into S  Recall that if there are kd objects dominate the missing object  m and there are n objects incomparable with  m  the best and the worst ranking of  m are kd   1 and n   kd   1  respectively  For these n 1 possible rankings r1  r2          rn 1 of  m  each ranking ri is associated with a weighting vector set Wri such that for each weighting vector  wri 2 Wri    m ranks ri th in the corresponding re   ned query q 0  ri    wri    So altogether there is a set W that contains n 1 weighting vector sets  W   fWr1           Wri           Wrn 1 g  In the following  we are going to show that if the re   ned query q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt that minimizes Equation 2 in terms of  k and  w has a weighting vector  wopt on the boundaries of the weighting vector sets W  Theorem 1   Furthermore  re   ned queries with weightings on the boundaries of the weighting vector sets W would make missing object  m have a ranking no worse than other re   ned queries whose weightings not on the boundaries  Lemma 3   Therefore  in addition to the original weighting vector  wo  the rest of the weighting vectors  w1   w2           ws in S should be sampled from the space formed by the boundaries of those n   1 weighting vector sets in W  Theorem 1  If q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt   which minimizes Equation 2  has a weighting  wopt on the boundaries of the n 1 weighting vector sets in W  Proof  According to Lemma 2  if a weighting vector set Wri 2 W is not empty  then it is the union of a set of convex polytopes CPri   So  the boundaries of Wri are essentially the union of the boundaries of each convex polytope in CPri   Let CP   S CPri   Theorem 1 can be re stated as follow  if q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of CP  Let CPn  wo 2 CP be the set of convex polytopes that do not contain  wo  Further  let CP  wo   CP  CPn  wo be the set of convex polytopes that contain  wo  Since all the convex polytopes are disjoint  as no weighting can satisfy two different linear inequality systems described in Lemma 2 at the same time    wo is in only one convex polytope  As such  CP  wo essentially contains only one convex polytope  Now  to prove the theorem  we need to prove the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of   1   CPn  wo or   2   CP  wo   We now start with proving   1    To do so  we    rst assume the optimal re   ned query q 0 opt has a weighting  wopt NOT in CP  wo or on its boundaries  and show that  Lemma 3  For all convex polytopes cp 2 CP  any re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp  has  m ranks r b  th under  wr b   which is not worse than rank r 0  th  i e   r b   r 0    where r 0 is the ranking of  m under another re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 not on the boundaries of cp  Lemma 3   Lemma 4  For all convex polytopes cpn  wo 2 CPn  wo   there exists a re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cpn  wo   has its  w   jj  wr 0   wojj2  where  wr 0 is the weighting vector of any other re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 not on the boundaries of cpn  wo  Lemma 4   If both Lemma 3 and Lemma 4 hold  then   1   is true  Proof of Lemma 3  We prove it by induction  First  recall that kd is the number of objects that dominate the missing object  m  In the base case  we want to show that  when there is only one object  p1 incomparable with  m in the data space  the Lemma is true  When there is only one incomparable point  p1  the whole weighting space is divided by the hyperplane H1   p1   m    w   0 into two convex polytopes cp   and cp     cp   is the convex polytope at the side   p1   m     w   0  cp   is the convex polytope at the side   p1   m     w   0  and hyperplane H1 is the boundary of cp   and cp     We use CP1 to denote the whole set of convex polytopes at this moment  Now  consider a re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp      m   s ranking r 0   kd   2 because  m is dominated by  p1 under  wr 0   Consider another     re   ned query q 0 b  r b    wr b    whose  wr b on the boundary H1   m   s ranking r b   kd   1 because  m and  p1 have the same score  Finally  if the re   ned query q 0  r 0    wr 0   has its weighting vector  wr 0 in cp   but not on the boundary of cp      m dominates  p1 under  wr 0 and thus  m   s ranking remains as r 0   kd   1  In the above  we can see that r b   r 0   thus the base case holds  Assume Lemma 3 is still true when there are i objects incomparable with  m in the data space and we use CPi to denote the set of convex polytopes constructed by the corresponding i hyperplanes  Now  we want to show that the lemma is true when there are i   1 incomparable objects in the data space  When the  i 1  th incomparable object  pi 1 is added  CPi is divided into three sets of convex polytopes  CP     CP     and CP G   CP   consists of any convex polytope cp   that is completely at the side of   pi 1   m     w   0  CP   consists of any convex polytope cp   that is completely at the side of   pi 1   m    w   0  and CP G consists of any convex polytope cp G that intersects the hyperplane Hi 1    pi 1   m     w   0  We want to show that the lemma is true for all the three sets of convex polytopes above  For any cp   2 CP     the re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp     the addition of  pi 1 makes  m   s ranking r 0 increments by one  i e   r 0   r 0 1  because  pi 1 has a better score when the weighting vectors are at the side of   pi 1   m     w   0  For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp     the addition of  pi 1 makes  m   s ranking r b increments by one  i e   r b   r b   1  In this case  r b   r 0   the lemma still holds after  pi 1 is added  For any cp   2 CP     the re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp     the addition of  pi 1 does not change  m   s ranking r 0   because  pi 1 has score worse than  m when the weighting vectors at the side of   pi 1  m    w   0  For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp     the addition of  pi 1 also does not change  m   s ranking r b   So  in this case  r b   r 0   the lemma still holds after the pi 1 is added  For any cp G 2 CP G   since it intersects hyperplane Hi 1  cp G is divided into two new convex polytopes cp G   and cp G    with Hi 1 as their boundaries  The proof for the case of cp G   is similar to the case of any cp   2 CP     with both r b and r 0 get increased by one  so r b   r 0 is still true  For the case of cp G    we also have r b   r 0 like the case of any cp   2 CP     For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the hyperplane Hi 1  as the added object  pi 1 has the same score as  m at this moment   m   s ranking remains the same  And for those q 0  r 0    wr 0    whose wr 0 not on Hi 1  may either keep  m   s ranking or increase its ranking by one just like the discussion above  Therefore  we can assert that r b   r 0   is true in all cases  Proof of Lemma 4  Let  wr  be the weighting vector that is closest to  wo  i e   the one with the optimal  w   we prove the lemma by showing that  among all the weighting vectors in cpn  wo    wr  must be on the boundaries of cpn  wo   Assume  wr   is in cpn  wo but not on its boundaries  So   wr   is an interior point of cpn  wo and there exists an open ball B   cpn  wo centered at  wr    13   Since B is convex  we can    nd two points  wa   wb in B such that  wr      wa  wb 2   As  wr   is closest to  wo  we know that    wa  wr        wo  wr       0  see Lemma 5   Because  wa   2  wr     wb  we can substitute  wa into the inequality above and get    wr     wb     wo   wr       0  Now we have    wb   wr        wo   wr       0  which contradicts Lemma 5  below   Therefore   wr   must be on the boundaries of cpn  wo   Lemma 5  Let C   R n be a non empty closed convex set  Then  for every point  x 62 C   z   2 C is a closest point to  x iif   z  z         x  z       0 for all  z 2 C   13  Now  let us prove   2    i e   the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of CP wo   To do so  we assume the optimal re   ned query q 0 opt has a weighting  wopt NOT in CPn  wo or its boundaries  We    rst show that  for any re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 in CP  wo but not on its boundaries  is dominated by q 0 o  k m o    wo   Given that  wr 0 is in CP  wo but not on its boundaries  and also given that  wo is in CP  wo we consider two cases  in which  wo is  i  not on and  ii  on CP  wo    s boundaries  respectively  In case  i   since both  wr 0 and  wo are in CP  wo but not on its boundaries  they satisfy the same inequality system and thus we have r 0   k m o   In case  ii   in which  wo is on CP  wo    s boundaries  we can apply Lemma 3 and thus we have k m o   r 0   Combining two cases together we have k m o   r 0 always holds  Note that when  wr 0  6  wo   w 0 r    wo  Hence  among all re   ned queries q 0  r 0    wr 0    whose  wr 0 in CP  wo   only those with  wr 0 on the boundaries of CP  wo have chances not to be dominated by q 0 o  k m o    wo   Finally  as q 0 o  k m o    wo  is not the optimal answer  the given condition in Theorem 1   so we know that if the optimal weighting  wopt is not on the boundaries of CPn  wo  from   1     then it is on the boundaries of CP  wo  from   2     Thus Theorem 1 is proven  Consider the dataset in Figure 2 as an example  Point  m  who is missing in the top 3 results of the original query qo  has two incomparable points  p2 and  p3  To    nd out the set of weighting vectors Wri that makes m ranks third  i e   ri   3  we can solve the following inequality systems separately     p2    wri    m    wri  p3    wri    m    wri  5     p2    wri    m    wri  p3    wri    m    wri  6  The union of the results above is Wri 3  In the systems above  we have two hyperplanes H1    p3   m     wri   0 and H2     p2   m     wri   0 that divide the weighting space like Figure 3  The two hyperplanes intersect the weighting constraint planes P wri  i    1 and wri  i  2  0  1  and results in three convex polytopes cp1  cp2  and cp3  in 2 d case the polytopes are line segments   The union of cp1 and cp3 is the corresponding weighting vector set Wri 3  and cp2 is the weighting vector set Wri 4  In this 2 D example  the                intersections  w1   j1 3 2 3j and  w2   j2 3 1 3j between the two hyperplanes and the weighting constraint planes are the boundaries of the polytopes  Note that by some back ofenvelop calculation  we can derive that m ranks third under weightings  w1 and  w2  This aligns with Lemma 3  which states that the ranking of m on the boundaries  rank third  is not worse than its ranking not on the boundaries  rank fourth in cp2 and third in cp1 and cp3   w 2  w 1  1 0 1   2 3 1 3  cp1 H1 H2 cp3 cp2 w   1  1 3 2 3  w   o w   2 r 3 r 4 r 3 Fig  3  Convex polytopes for m shown under the weighting space C  How large the list of weighting vectors should be  Having known that the list of weighting vectors S should be obtained from the boundaries of the weighting sets in W  the next question is  given that there are still an in   nite number of points  weightings  on the boundaries of the weighting sets in W  how many sample weightings from the boundaries should we put into S in order to obtain a good approximation answer  Recall that more sample weightings in S will increase the number of progressive top k executions and thus the running time  Therefore  we hope S to be as small as possible while maintaining good approximation  We say a re   ned query is the best T  re   ned query if its penalty is smaller than  1  T   re   ned queries in the whole  in   nite  answer space  and we hope the probability of getting at least one such re   ned query is larger than a certain threshold P r  1   1  T   s   P r  7  Equation 7 is general  In our algorithm  we use it based on a smaller sample space that contains high quality weightings  The sample size s is independent of the data size but controlled by two parameters  T  and P r  Following our usual practice of not improving usability  i e   why not queries  by increasing users    burden  e g   specifying parameter values for  k  T   and P r   we make T   and P r as system parameters and let users to override their values only when necessary  D  Algorithm To begin  let us    rst outline the three core phases of the algorithm  which is slightly different from the basic idea mentioned in Section III A for better ef   ciency   PHASE 1  It    rst samples s weightings  w1   w2           ws from the boundaries of the weighting vector sets W and add them into S  which initially contains  wo   PHASE 2  Next  for some weighting vectors  wi 2 S  it executes a progressive top k query using  wi as the weighting until a stopping condition is met  Let us denote that operation as ri   PROGRESS q 0 i    wi   STOPPING CONDITION   In the basic idea mentioned in Section III A  we have to execute s   1 progressive top k queries for all s   1 weightings in S  In this section  we present a technique to skip many of those progressive top k operations so as to improve the ef   ciency  Section III D Technique  ii    In addition  the stopping condition in the basic idea is to proceed until the missing object  m comes forth to the result  However  if  m ranks very poorly under some weighting  wi   the corresponding progressive top k operation may be quite slow because it has to access many tuples in the database  In this section  we present a much more aggressive and effective stopping condition that makes most of those operations stop early even before  m is seen  Section III D Technique  i    These two techniques together can signi   cantly reduce the overall running time of the algorithm   PHASE 3  Using ri as the re   ned k 0    wi as the re   ned weighting  w 0   the  k 0    wi  combination with the least penalty is formulated as a re   ned query q 0  k 0    wi  and returned to the user as the why not answer  We    rst provide the details of PHASE 1  First   wo  the weighting vector in the user   s original query qo  is added to S  Next  we use the method in  17  to locate the set I of objects incomparable with  m  After that  we randomly pick a point pi from I and use Gaussian Jordan method  18  to ef   ciently    nd the intersection between the hyperplane   pi   m     w   0 and the constraint plane P w i    1  Then  we randomly pick a point  weighting  from the intersection  If all components of this weighting are non negative  w i    0   we add it to S  The process repeats until s weightings have been collected  As we show in the experiments  this phase can be done very ef   ciently because    nding I for one  or a few  missing object s  and solving plane intersections using Gaussian Jordon method incur almost negligible costs  Technique  i      Stopping a progressive top k operation earlier In PHASE 2 of our algorithm  the basic idea is to execute the progressive top k query until  m appears in the result  with rank ri   Denoting that operation as r1   PROGRESS q 0 1    w1   UNTIL SEE   m   In the following  we show that it is actually possible for a progressive top k execution to stop early even before  m shows up in the result  Consider an example that a user speci   es a top 2 query qo ko   2   wo  and a why not question about missing object  m is posed  Assume that the list of weightings S is    wo   w1   w2   w3   Furthermore  assume that PROGRESS q 0 o    wo   UNTIL SEE   m  is    rstly executed and  m   s actual ranking under  wo is 6  Now  we have our    rst candidate re   ned query q 0 o  k m o   6   wo   with  ko   6  2   4 and  wo   0  The corresponding penalty  denoted as  Pq 0 o   could be calculated using Equation 2  Remember that we want to    nd the re   ned query with the least penalty Pmin  So  at this     moment  we set a penalty variable Pmin   Pq 0 o   According to our basic idea  we should execute another progressive top k using weighting vector  say   w1  until  m shows up in the result set with a ranking r1  However  we notice that the skyline property in the answer space can help to stop that operation earlier  even before  m is seen  Given the    rst candidate re   ned query q 0 o  k m o   6   wo  with  wo   0 and  ko   4  any other candidate re   ned queries q 0 i with  ki   4 must be dominated by q 0 o   In our example  since the    rst executed progressive top k execution  all the subsequent progressive top k executions can stop once  m does not show up in the top 6 tuples     k    w q  o q  2 q  1    w1 q  3 0 3 4    w2    w    w3  2  1    k   4    k T    k q   2 L f Slope Pmin Fig  4  Example of answer space Figure 4 illustrates the answer space of the example  The idea above essentially means that all other progressive top k executions with m does not show up in top 6  i e    ki   4  see the dotted region   e g   q 0 1   can stop early at top 6  because after that  they have no chance to dominate q 0 o anymore  While useful  we can actually be even more aggressive in many cases  Consider another candidate re     ned query  say  q 0 2   in Figure 4  Assume that r2   PROGRESS q 0 2    w2   UNTIL SEE   m    5  i e    k2   5  2   3   which is not covered by the above technique  since  k2  6 4   However  q 0 2 can also stop early  as follows  In Figure 4  we show the normalized penalty Equation 2 as a slope Pmin that passes through the best re   ned query so far  currently q 0 o    All re   ned queries lie on the slope have the same penalty value as Pmin  In addition  all re   ned queries that lie above the slope actually have penalty larger than Pmin  and thus dominated by q 0 o in this case  Therefore  similar to the skyline discussion above  we can determine an even tighter threshold ranking r T   for stopping the subsequent progressive top k operations even earlier  r T    k T   ko  where  k T   b Pmin   w  w p 1   P w2 oi   k m o  ko  k c  8  Equation 8 is a rearrangement of Equation 2  with P enalty k 0    w 0     Pmin  and with the original top k value ko added  Back to our example in Figure 4  given that the weighting of candidate re   ned query q 0 2 is  w2  we can    rst compute its  w2 value  Then  we can project  w2 onto the slope Pmin  currently Pmin   Pq 0 o   to obtain the corresponding  k T value  which is 2 in Figure 4  That means  if we carry out a progressive top k operation using  w2 as the weighting  and if  m still does not appear in result after the top 4 tuples  r T    k T   ko   2   2   4  are seen  then we can stop it early because the penalty of q 0 2 is worse than the penalty Pmin of the best re   ned query  q 0 o   seen so far  Following the discussion above  we now have two early stopping conditions for the progressive topk algorithms  UNTIL SEE   m and UNTIL RANK r T   Except the    rst progressive top k operation in which PROGRESS q 0 o    wo   UNTIL SEE   m  must be used  the subsequent progressive top k operations can use    UNTIL SEE   m OR UNTIL RANK r T     as the stopping condition  We remark that the conditions UNTIL RANK r T and UNTIL SEE   m are both useful  For example  assume that the actual ranking of  m under  w2 is 3  which gives it a  k2   1  see q 00 2 in Figure 4   Recall that by projecting  w2 on to the slope of Pmin  we can stop the progressive top k operation after r T   2   2   4 tuples have been seen  However  using the condition UNTIL SEE   m  we can stop the progressive top k operation when  m shows up at rank three  This drives us to use    UNTIL SEE   m OR UNTIL RANK r T     as the stopping condition  Finally  we remark that the pruning power of this technique increases when the algorithm proceeds  For example  after q 00 2 has been executed  the best re   ned query seen so far should be updated as q 00 2  because its penalty is better than q 0 o    Therefore  Pmin now is updated as Pq 00 2 and the slope Pmin should be updated to pass through q 00 2 now  the dotted slope in Figure 4   Because Pmin is continuously decreasing   k T and thus the threshold ranking r T would get smaller and smaller and the subsequent progressive top k operations can terminate even earlier and earlier when the algorithm proceeds  With the same token  we also envision that the pruning power of this technique is stronger when we have a large  w  or small  k  because they make  k T decreases at a faster rate  see Equation 8   Technique  ii   Skipping progressive top k operations In PHASE 2 our algorithm  the basic idea is to execute progressive top k queries for all weightings in S  We now illustrate how some of those executions could be entirely skipped  so that the overall running time can be further reduced  The    rst part of the technique is based on the observation that similar weighting vectors may lead to similar top k results  11   Therefore  if a weighting  wj is similar to a weighting  wi and if operation PROGRESS q 0 i    wi   for  wi has already been executed  then the query result Ri of PROGRESS q 0 i    wi   could be exploited to deduce the highest ranking of the missing object  m under  wj   If the deduced highest ranking of  m is worse than the threshold ranking rT   then we can skip the   entire PROGRESS q 0 j    wj    operation  We illustrate the above by reusing our running example  Assume that we have cached the result sets of executed progressive top k queries  Let Ro be the result set of the    rst executed query PROGRESS q 0 o    wo   UNTIL SEE   m  and Ro     p1   p2   p3   p4   p5   m   Then  when we are considering the next weighting vector  say   w1  in S  we    rst follow Technique  i  to calculate the threshold ranking r T   In Figure 4  projecting  w1 onto slope Pq 0 o we get r T   3   2   5  Next we calculate the scores of all objects in Ro using  w1 as the weighting  Assume that the scores of  p1   p2   p3   p4  and  p5 are also smaller  better  than  m under  w1  in this case  we know the rankings of  p1   p2   p3   p4  and  p5 are all smaller  better  than the ranking of m  i e   the ranking of  m is at least 5   1   6  which is worse than r T   5  So  we can skip the entire PROGRESS q 0 1    w1   operation even without starting it  The above caching technique is shown to be the most effective between similar weighting vectors  11   Therefore  we design the algorithm in a way that the list of weightings S is sorted according to their corresponding  wi values  of course   wo is the in the head of the list since  wo   0   The second part of the technique is to exploit the best possible ranking of m  under all possible weightings  to set up an early termination condition for the whole algorithm  so that after a certain number of progressive top k operations have been executed  the algorithm can terminate early without executing the subsequent progressive top k operations  Recall that the best possible ranking of  m is kd   1  where kd is the number of objects that dominate  m  Therefore  the lower bound of  k  denoted as  kL  equals to max kd   1  ko  0   By de   nition in Section II   k   0   So  this time  we project  kL onto slope Pmin in order to determine the corresponding maximum feasible  w value  Naming that value as  w f   For any  w    w f   it means     m has  k    kL     which is impossible  As our algorithm is designed to examine the weightings in their increasing order of  w values  when a weighting wi 2 S has jjwi  wojj    w f   PROGRESS q 0 i    wi   and all subsequent progressive top k operations PROGRESS q 0 i 1    wi 1            PROGRESS q 0 s    ws   could be skipped  Reusing Figure 4 as an example and assume that the number of objects that dominated  m is 2  By projecting  kL   max kd   1  ko  0    1 onto slope the Pq 0 o   we could determine the corresponding  w f value  So  when the algorithm    nishes executing progressive top k operation for weighting  w2  PHASE 2 of the algorithm can terminate at that point because all the subsequent  wi are larger than  w f   As a    nal remark  we would like to point out that the pruning power of this technique also increases when the algorithm proceeds  For instance  in Figure 4  if q 00 2 has been executed  slope Pmin is changed from slope Pq 0 o to slope Pq 00 2   Projecting  kL onto the new Pmin slope would result in a smaller  w f   which in turns increases the chance of terminating PHASE 2 earlier  The pseudo code of the complete idea is presented in Algorithm 1  It is self explanatory and mainly summarizes what we have discussed above  so we do not give it a walkthrough here  Algorithm 1 Answering Why not Question on a Top K Query Input  The dataset D  original top k query qo ko   wo   missing object  m  penalty settings  k   w  T  and P r Output  A re   ned query q 0  k 0    w 0   1  Set list of weighting S      wo   2  Result of a top k query Ro  3  Rank of missing object k m o   4   Ro  k m o     PROGRESS q 0 o   wo   UNTIL SEE   m  5  if k m o     then 6  return     m is not in the D    7  end if Phase 1  8  Use  17  to determine the number of points kd that dominate  m and the set of points I incomparable with  m  9   kL   max kd   1  ko  0   10  Determine s from T  and P r using Equation 7  11  Sample s weightings from the hyperplanes boundaries constructed by I and  m and add them to S  12  Sort S according to their  wi values  Phase 2  13  R    Ro   wo     Cache the results 14  Pmin   P enalty k m o    wo   15   kL   max kd   1  ko  0     Calculate the lower bound ranking of m 16   w f    Pmin k  kL km o ko   p 1  P w2 oi  w     Project  kL to determine early termination point  wf 17  for all  wi 2 S do 18  if  wi    wf then 19  break    Technique  ii      early algorithm termination 20  end if 21   k T  b Pmin   w p  wi 1  P w2 oi   km o ko  k c  22  r T   k T   ko  23  if there exist r T objects in some Ri 2 R having scores better  m under  wi then 24  continue    Technique  ii      use cached result to skip a progressive top k 25  end if 26   Ri  ri    PROGRESS q 0 o   wi   UNTIL SEE   m OR UNTIL RANK r T      Technique  i      stopping a progressive top k early 27  Pi   P enalty ri   wi   28  R   R    Ri   wi   29  if Pi   Pmin then 30  Pmin   Pi  31   w f    Pmin   k  kL km o ko   p 1  P w2 oi  w   32  end if 33  end for Phase 3  34  return the best re   ned query q 0  k 0    w 0   whose penalty Pmin  E  Multiple Missing Objects To deal with multiple missing objects in a why not question  we have to modify our algorithm a little bit  First  we do a simple    ltering on the set of missing objects M  Speci   cally  among all the missing objects in M  if there is a missing object  mi dominates another one  mj in the data space  then we can           remove the dominating object  mi from M for the reason that every time  mj appears into the top k result   mi is certainly in the result as well  So  we only need to consider  mj   Let M0 be the set of missing objects after the    ltering step  The next modi   cation to the algorithm is related to PHASE 1        nding good weightings and put them into S  First  the set I should now consist of incomparable points of all objects in M0   Second  we should randomly select a hyperplane   pi   mi   w   0  where  pi is a point in I  After that  as usual  we sample a point on the intersection of the hyperplanes plus the constraint plane P w i    1  0   w i    1   That way  the whole method still obeys Theorem 1  The modi   cation related to Technique  i  is as follows  For the condition UNTIL SEE   m  it should now be UNTIL SEE ALL OBJECTS IN M0   For example  k m o in Algorithm 1 Line 4 should now refer to the ranking of the missing object with the highest score  The threshold ranking r T for the condition UNTIL RANK r T should also be computed based on the above k m o instead  The modi   cations related to Technique  ii  is as follows  We now have to identify the lower bound of  kL for a set of missing objects M0 instead of a single missing object  With a set of missing objects M0   f m1         mng  we use DOMi to represent the set of objects that dominate  mi   So   kL for M0 is max jDOM1   DOM2           DOMn   M0 j  ko  0   IV  EXPERIMENTAL STUDY We evaluate our proposed solution using both synthetic and real data  By default  we set the system parameters T  and P r as 0 2  and 0 8  respectively  resulting in a sample size of 800 weightings   The algorithms are implemented in C   and the experiments are run on a Ubuntu PC with Intel 2 67GHz i5 Dual Core processor and 4GB RAM  We adopt  16  as our progressive top k algorithm  A  Case Study We use the NBA data set in the case study  The NBA data set contains 21961 game statistics of all NBA players from 1973 2009  Each record represents the career performance of a player  player name  Player   points per game  PTS   rebounds per game  REB   assists per game  AST   steals per game  STL   blocks per game  BLK      eld goal percentage  FG   free throw percentage  FT   and three point percentage  3PT   For comparison  we also implemented a version of our algorithm in which weightings are randomly sampled from the whole weighting space  We refer to that version as WWS  In the following  we present several interesting cases  Case 1  Finding the top 3 centers in NBA history   The    rst case was to    nd the top 3 centers in the NBA history  Therefore  we issued a top 3 query q1 with equal weighting  0 2  on    ve attributes PTS  REB  BLK  FG  and FT  The initial result was  fW  Chamberlain  Abdul Jabbar  Shaquille O   nealg  which means Chamberlain ranked    rst  followed by Jabber  and O   neal   Because we were curious why Yao Ming was not in the result  we issued a why not question ffYao Mingg  q1g using the    Prefer modify weighting    option  since we wanted to see Yao in top 3  In 156ms  our algorithm returned a re   ned query q 0 1 with k 0 1   3 and  w 0 1   j0 0243 0 0024 0 0283 0 0675 0 8775j  The re   ned query essentially indicated that we should have put more weights on a center   s free throw  FT  ability if we wish to see Yao in the top 3 result  The corresponding result of q 0 1 was  fAbdul Jabbar  Hakeem Olajuwon  Yao Mingg  where Yao Ming ranked third  The penalty value of q 0 1 was 0 069  As a comparison  WWS returned another re   ned query q 0WWS 1   using 154ms  However  q 0WWS 1 was a top 7 query that uses another set of weighting  Yao ranked 7 th   The penalty of q 0WWS 1 was 0 28  which was four times worse than q 0 1   Case 2  Finding the top 3 guards in NBA history   The second case was to    nd the top 3 guards in the NBA history  Therefore  we issued a top 3 query q2 with equal weighting   1 6   on six attributes PTS  AST  STL  FG  FT  and 3PT  The initial result was  fMichael Jordon  LeBron James  Oscar Robertsong  We were surprised why Kobe Bryant was not in the result  So  we posed a why not question ffKobe Bryantg  q2g using the    Prefer modify weighting    option  since we wanted to see Kobe Bryant in top 3  In 163ms  our algorithm returned a re   ned query q 0 2 with k 0 2   3 and  w 0 2   j0 0129 0 0005 0 0416 0 2316 0 3769 0 3364j  The corresponding result of q 0 2 was  fMichael Jordon  Pete Maverich  Kobe Byrantg  The penalty of q 0 2 was 0 035  As a comparison  WWS returned a re   ned query q 0WWS 2   in 161ms  However  q 0WWS 2 was a top 4 query  Kobe Bryant ranked 4 th   which con   icted with our    Prefer modify weighting    option  Thus  q 0WWS 2    s penalty was 0 2  which was more than    ve times worse than q 0 2   Case 3  Finding the top 3 players in NBA history   The third case was to    nd the top 3 players in the NBA history  Therefore  we issued a top 3 query q3 with equal weighting   1 8   on all eight numeric attributes  The initial result was  fW  Chamberlian  LeBron James  Elgin Baylorg  Amazingly  Michael Jordan was missing in the result  To understand why  we issued a why not question ffMichael Jordang  q3g  using the    Prefer modify k    option  because we insisted that Michael Jordan should have a top ranking without twisting the weightings much  Using 150ms  our algorithm returned a re   ned query q 0 3 with k 0 3   5 and  w 0 3    wo  with the following result  fW  Chamberlain  LeBron James  Elgin Baylor  Bob Pettit  Michael Jordang  The re   ned query q 0 3 essentially means that our initial weightings were reasonable but we should have looked for the top 5 players instead  In this case  both versions of our algorithms came up with the same re   ned query in 150ms  As a follow up  we were also interested in understanding why both Michael Jordan and Shaquille O   neal were not the top 3 players in the NBA history  Therefore  we issued another why not query ffMichael Jordan  Shaquille O   nealg  q3g  using the    Never mind    option  In 166ms  our algorithm returned a re   ned query q 00 3 with k 0   5 and  w 0 3   j0 138 0 0847 0 0639 0 1066 0 2481 0 1143 0 1231 0 1212j  The corresponding result of q 00 3 was  fW  Chamberlian  Michael Jordon  LeBorn James  Abudl   Jabber  Shaquille  O   nealg  The penalty of q 00 3 was 0 2  WWS returned another re   ned query q 00WWS 3   in 164ms  However  q 00WWS 3 was a top 5 query with penalty 0 27  which was higher than q 00 3   B  Performance We next turn our focus to the performance of our algorithm  We present experimental results based on three types of synthetic data  uniform  UN   correlated  CO  and anti correlated  AC   Since  the experiment results between UN and CO are very similar  we only present the results of UN and AC here  Table II shows the parameters we varied in the experiments  The default values are in bold faces  The default top k query qo has a setting of  k   ko   wo   j 1 d       1 d j  where d is the number of dimensions  attributes involved   By default  the why not question asks for a missing object that is ranked  10   ko   1  th under  wo  TABLE II PARAMETERS SETTING Parameter Ranges Data size 100K  500K  1M  1 5M  2M Dimension 2  3  4  5 ko 5  10  50  100 Actual ranking of  m under qo 11  101  501  1001 T  0 3   0 25   0 2   0 15   0 1  P r 0 5  0 6  0 7  0 8  0 9 jMj 1  2  3  4  5 Varying Data Size  Figure 5 shows the running time of our algorithm under different data sizes  using different penalty options  PMK stands for    Prefer modifying k     PMW stands for    Prefer modifying weighting     NM stands for    Never mind     We can see our algorithm for answering why not questions scales linearly with the data size  The running time scales linearly  but at a faster rate  on AC data because of the general fact that progressive top k operations on anticorrelated data takes a longer time to    nish  16    0  0 2  0 4  0 6  0 8  1 1 5 10 15 20 Total running time  sec  Data size 100K  PMW NM PMK  a  Uniform Data  0  0 5  1  1 5  2  2 5  3 1 5 10 15 20 Total running time  sec  Data size 100K  PMW NM PMK  b  Anti correlated Data Fig  5  Varying data size Varying Query Dimension  Figure 6 shows the running time of our algorithm using top k queries in different number of query dimensions  In general  answering why not questions for top k queries in a higher query dimension needs more time because the execution time of a progressive top k operation increases if a top k query involves more attributes  From the    gure  we see that our algorithm scales well with the number of dimensions  Varying ko  Figure 7 shows the running time of our algorithm using top k queries with different ko values  In this  0  0 2  0 4  0 6  0 8  1 2 3 4 5 Total running time  sec  Dimensionality PMW NM PMK  a  Uniform Data  0  0 5  1  1 5  2  2 5  3  3 5  4 2 3 4 5 Total running time  sec  Dimensionality PMW NM PMK  b  Anti correlated Data Fig  6  Varying query dimension experiment  when a top 5 query  ko   5  is used  the corresponding why not question is to ask why the object in rank 51 th is missing  Similarly  when a top 50 query  ko   50  is used  the corresponding why not question is to ask why the object in rank 501 th is missing  Naturally  when ko increases  the time to answer a why not question should also increase because the execution time of a progressive top k operation also increases with k  Figure 7 shows that our algorithm scales well with ko  The running time of our algorithm increases very little under the PMK option  Recall that in Section III D we mentioned that the effectiveness of our pruning techniques is more pronounced when the PMK option is used  In this experiment  when we scaled up ko to a large value  the algorithm running time became higher  As such  the stronger pruning effectiveness of the PMK option became more obvious in the experimental result   0  0 5  1  1 5  2  2 5  3  3 5  4 10 50 100 Total running time  sec  k o PMW NM PMK  a  Uniform Data </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>SkimpyStash  RAM Space Skimpy Key Value Store on Flash based Storage ### Biplob Debnath     1 Sudipta Sengupta     Jin Li         Microsoft Research  Redmond  WA  USA    EMC Corporation  Santa Clara  CA  USA ABSTRACT We present SkimpyStash  a RAM space skimpy key value store on    ash based storage  designed for high throughput  low latency server applications  The distinguishing feature of SkimpyStash is the design goal of extremely low RAM footprint at about 1     0 5  byte per key value pair  which is more aggressive than earlier designs  SkimpyStash uses a hash table directory in RAM to index key value pairs stored in a log structured manner on    ash  To break the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key  it    moves  most of the pointers that locate each key value pair from RAM to    ash itself  This is realized by  i  resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and  ii  storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  hence incurring multiple    ash reads per lookup  Two further techniques are used to improve performance   iii  two choice based load balancing to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and a bloom    lter in each hash table directory slot in RAM to disambiguate the choice during lookup  and  iv  compaction procedure to pack bucket chain records contiguously onto    ash pages so as to reduce    ash reads during lookup  The average bucket size is the critical design parameter that serves as a powerful knob for making a continuum of tradeoffs between low RAM usage and low lookup latencies  Our evaluations on commodity server platforms with real world data center applications show that SkimpyStash provides throughputs from few 10 000s to upwards of 100 000 get set operations sec  Categories and Subject Descriptors H 3 Information Storage and Retrieval  H 3 1 Content Analysis and Indexing   Indexing Methods General Terms Algorithms  Design  Experimentation  Performance  1Work done while Biplob Debnath was at Microsoft Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  Keywords Key value store  Flash memory  Indexing  RAM space ef   cient index  Log structured index ###  1  INTRODUCTION A broad range of server side applications need an underlying  often persistent  key value store to function  Examples include state maintenance in Internet applications like online multi player gaming and inline storage deduplication  as described in Section 3   A high throughput persistent key value store can help to improve the performance of such applications  Flash memory is a natural choice for such a store  providing persistency and 100 1000 times lower access times than hard disk  Compared to DRAM     ash access times are about 100 times higher  Flash stands in the middle between DRAM and disk also in terms of cost     it is 10x cheaper than DRAM  while 20x more expensive than disk     thus  making it an ideal gap    ller between DRAM and disk  It is only recently that    ash memory  in the form of Solid State Drives  SSDs   is seeing widespread adoption in desktop and server applications  For example  MySpace com recently switched from using hard disk drives in its servers to using PCI Express  PCIe  cards loaded with solid state    ash chips as primary storage for its data center operations  5   Also recently  Facebook released Flashcache  a simple write back persistent block cache designed to accelerate reads and writes from slower rotational media  hard disks  by caching data in SSDs  6   These applications have different storage access patterns than typical consumer devices and pose new challenges to    ash media to deliver sustained and high throughput  and low latency   These challenges arising from new applications of    ash are being addressed at different layers of the storage stack by    ash device vendors and system builders  with the former focusing on techniques at the device driver software level and inside the device  and the latter driving innovation at the operating system and application layers  The work in this paper falls in the latter category  To get the maximum performance per dollar out of SSDs  it is necessary to use    ash aware data structures and algorithms that work around constraints of    ash media  e g   avoid or reduce small random writes that not only have a higher latency but also reduce    ash device lifetimes through increased page wearing   In the rest of the paper  we use NAND    ash based SSDs as the architectural choice and simply refer to it as    ash memory  We describe the internal architecture of SSDs in Section 2  Recently  there are several interesting proposals to design keyvalue stores using    ash memory  10  11  15  16   These designs use a combination RAM and    ash memory     they store the full key value pairs on    ash memory and use a small amount of metadata per key value pair in RAM to support faster insert andlookup operations  For example  FAWN  11   FlashStore  16   and ChunkStash  15  each require about six bytes of RAM space per key value pair stored on    ash memory  Thus  the amount of available RAM space limits the total number of key value pairs that could be indexed on    ash memory  As    ash capacities are about an order of magnitude bigger than RAM and getting bigger  RAM size could well become the bottleneck for supporting large    ashbased key value stores  By reducing the amount of RAM bytes needed per key value pair stored on    ash down to the extreme lows of about a byte  SkimpyStash can help to scale key value stores on    ash on a lean RAM size budget when existing current designs run out  Our Contributions In this paper  we present the design and evaluation of SkimpyStash  a RAM space skimpy key value store on    ash based Storage  designed for high throughput server applications  The distinguishing feature of SkimpyStash is the design goal of extremely low RAM footprint at about 1 byte per key value pair  which is more aggressive than earlier designs like FAWN  11   BufferHash  10   ChunkStash  15   and FlashStore  16   Our base design uses less than 1 byte in RAM per key value pair and our enhanced design takes slightly more than 1 byte per key value pair  By being RAM space frugal  SkimpyStash can accommodate larger    ash drive capacities for storing and indexing key value pairs  Design Innovations  SkimpyStash uses a hash table directory in RAM to index key value pairs stored in a log structure on    ash  To break the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key  it    moves  most of the pointers that locate each key value pair from RAM to    ash itself  This is realized by  i  Resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and  ii  Storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  hence incurring multiple    ash reads per lookup  Two further techniques are used to improve performance   iii  Two choice based load balancing  12  to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and a bloom    lter  13  in each hash table directory slot in RAM for summarizing the records in that bucket so that at most one bucket chain on    ash needs to be searched during a lookup  and  iv  Compaction procedure to pack bucket chain records contiguously onto    ash pages so as to reduce    ash reads during lookup  The average bucket size is the critical design parameter that serves as a powerful knob for making a continuum of tradeoffs between low RAM usage and low lookup latencies  Evaluation on data center server applications  SkimpyStash can be used as a high throughput persistent key value storage layer for a broad range of server class applications  We use real world data traces from two data center applications  namely  Xbox LIVE Primetime online multi player game and inline storage deduplication  to drive and evaluate the design of SkimpyStash on commodity server platforms  SkimpyStash provides throughputs from few 10 000s to upwards of 100 000 get set operations sec on the evaluated applications  Figure 1  Internal architecture of a Solid State Drive  SSD  The rest of the paper is organized as follows  We provide an overview of    ash memory in Section 2  In Section 3  we describe two motivating real world data center applications that can bene   t from a high throughput key value store and are used to evaluate SkimpyStash  We develop the design of SkimpyStash in Section 4  We evaluate SkimpyStash in Section 5  We review related work in Section 6  Finally  we conclude in Section 7  2  FLASH MEMORY OVERVIEW Figure 1 gives a block diagram of an NAND    ash based SSD  In    ash memory  data is stored in an array of    ash blocks  Each block spans 32 64 pages  where a page is the smallest unit of read and write operations  A distinguishing feature of    ash memory is that read operations are very fast compared to magnetic disk drive  Moreover  unlike disks  random read operations are as fast as sequential read operations as there is no mechanical head movement  A major drawback of the    ash memory is that it does not allow in place updates  i e   overwrite   Page write operations in a    ash memory must be preceded by an erase operation and within a block  pages need be to written sequentially  The in place update problem becomes complicated as write operations are performed in the page granularity  while erase operations are performed in the block granularity  The typical access latencies for read  write  and erase operations are 25 microseconds  200 microseconds  and 1500 microseconds  respectively  9   The Flash Translation layer  FTL  is an intermediate software layer inside SSD  which makes linear    ash memory device act like a virtual disk  The FTL receives logical read and write commands from the applications and converts them to the internal    ash memory commands  To emulate disk like in place update operation for a logical page  Lp   the FTL writes data into a new physical page  Pp   maintains a mapping between logical pages and physical pages  and marks the previous physical location of Lp as invalid for future garbage collection  Although FTL allows current disk based application to use SSD without any modi   cations  it needs to internally deal with    ash physical constraint of erasing a block before overwriting a page in that block  Besides the in place update problem     ash memory exhibits another limitation     a    ash block can only be erased for limited number of times  e g   10K 100K   9   FTL uses various wear leveling techniques to even out the erase counts of different blocks in the    ash memory to increase its overall longevity  17   Recent studies show that current FTL schemes are very effective for the workloads with sequential access write patterns  However  for the workloads with random access patterns  these schemes show very poor performance  18  20  22   One of the design goals of SkimpyStash is to use    ash memory in FTLfriendly manner 3  KEY VALUE STORE APPLICATIONS We describe two real world applications that can use SkimpyStash as an underlying persistent key value store  Data traces obtained from real world instances of these applications are used to drive and evaluate the design of SkimpyStash  3 1 Online Multi player Gaming Online multi player gaming technology allows people from geographically diverse regions around the globe to participate in the same game  The number of concurrent players in such a game could range from tens to hundreds of thousands and the number of concurrent game instances offered by a single online service could range from tens to hundreds  An important challenge in online multi player gaming is the requirement to scale the number of users per game and the number of simultaneous game instances  At the core of this is the need to maintain server side state so as to track player actions on each client machine and update global game states to make them visible to other players as quickly as possible  These functionalities map to set and get key operations performed by clients on server side state  The real time responsiveness of the game is  thus  critically dependent on the response time and throughput of these operations  There is also the requirement to store server side game state in a persistent manner for  at least  the following reasons   i  resume game from interrupted state if and when crashes occur   ii  of   ine analysis of game popularity  progression  and dynamics with the objective of improving the game  and  iii  veri   cation of player actions for fairness when outcomes are associated with monetary rewards  We designed SkimpyStash to meet the high throughput and low latency requirement of such get set key operations in online multi player gaming  3 2 Storage Deduplication Deduplication is a recent trend in storage backup systems that eliminates redundancy of data across full and incremental backup data sets  28   It works by splitting    les into multiple chunks using a content aware chunking algorithm like Rabin    ngerprinting and using SHA 1 hash  24  signatures for each chunk to determine whether two chunks contain identical data  28   In inline storage deduplication systems  the chunks  or their hashes  arrive one ata time at the deduplication server from client systems  The server needs to lookup each chunk hash in an index it maintains for all chunk hashes seen so far for that backup location instance     if there is a match  the incoming chunk contains redundant data and can be deduplicated  if not  the  new  chunk hash needs to be inserted into the index  Because storage systems currently need to scale to tens of terabytes to petabytes of data volume  the chunk hash index is too big to    t in RAM  hence it is stored on hard disk  Index operations are thus throughput limited by expensive disk seek operations  Since backups need to be completed over windows of half a day or so  e g   nights and weekends   it is desirable to obtain high throughput in inline storage deduplication systems  RAM prefetching and bloom    lter based techniques used by Zhu et al   28  can avoid disk I Os on close to 99  of the index lookups  Even at this reduced rate  an index lookup going to disk contributes about 0 1msec to the average lookup time     this is about 10 3 times slower than a lookup hitting in RAM  SkimpyStash can be used as the chunk hash index for inline deduplication systems  By reducing the penalty of index lookup misses in RAM by orders of magnitude by serving such lookups from    ash memory  SkimpyStash can help to increase deduplication throughput  99019 94500 16064 5948 0 20000 40000 60000 80000 100000 seq reads rand reads seq writes rand writes IOPS Figure 2  IOPS for sequential random reads and writes using 4KB I O request size on a 160GB fusionIO drive  4  SkimpyStash DESIGN We present the system architecture of SkimpyStash and the rationale behind some design choices in this section  4 1 Coping with Flash Constraints The design is driven by the need to work around two types of operations that are not ef   cient on    ash media  namely  1  Random Writes  Small random writes effectively need to update data portions within pages  Since a  physical     ash page cannot be updated in place  a new  physical  page will need to be allocated and the unmodi   ed portion of the data on the page needs to be relocated to the new page  2  Writes less than    ash page size  Since a page is the smallest unit of write on    ash  writing an amount less than a page renders the rest of the  physical  page wasted     any subsequent append to that partially written  logical  page will need copying of existing data and writing to a new  physical  page  To validate the performance gap between sequential and random writes on    ash  we used Iometer  3   a widely used performance evaluation tool in the storage community  on a 160GB fusionIO SSD  2  attached over PCIe bus to an Intel Core 2 Duo E6850 3GHz CPU  The number of worker threads was    xed at 8 and the number of outstanding I Os for the drive at 64  The results for IOPS  I O operations per sec  on 4KB I O request sizes are summarized in Figure 2  Each test was run for 1 hour  The IOPS performance of sequential writes is about 3x that of random writes and worsens when the tests are run for longer durations  due to accumulating device garbage collection overheads   We also observe that the IOPS performance of  random sequential  reads is about 6x that sequential writes   The slight gap between IOPS performance of sequential and random reads is possibly due to prefetching inside the device   Given the above  the most ef   cient way to write    ash is to simply use it as an append log  where an append operation involves a    ash page worth of data  typically 2KB or 4KB  This is the main constraint that drives the rest of our key value store design  Flash has been used in a log structured manner and its bene   ts reported in earlier works  11  14  15  16  19  23  27   4 2 Design Goals The design of SkimpyStash is driven by the following guiding principles      Support low latency  high throughput operations  This requirement is extracted from the needs of many server classapplications that need an underlying key value store to function  Two motivating applications that are used for evaluating SkimpyStash are described in Section 3      Use    ash aware data structures and algorithms  This principle accommodates the constraints of the    ash device so as to extract maximum performance out of it  Random writes and in place updates are expensive on    ash memory  hence must be reduced or avoided  Sequential writes should be used to the extent possible and the fast nature of random sequential reads should be exploited      Low RAM footprint per key independent of key value size  The goal here is to index all key value pairs on    ash in a RAM space ef   cient manner and make them accessible using a small number of    ash reads per lookup  By being RAM space frugal  one can accommodate larger    ash drive capacities and correspondingly larger number of keyvalue pairs stored in it  Key value pairs can be arbitrarily large but the RAM footprint per key should be independent of it and small  We target a skimpy RAM usage of about 1 byte per key value pair  a design point that is more aggressive than earlier designs like FAWN  11   BufferHash  10   ChunkStash  15   and FlashStore  16   4 3 Architectural Components SkimpyStash has the following main components  A base version of the design is shown in Figure 3 and an enhanced version in Figure 5  We will get to the details shortly  RAM Write Buffer  This is a    xed size data structure maintained in RAM that buffers key value writes so that a write to    ash happens only after there is enough data to    ll a    ash page  which is typically 2KB or 4KB in size   To provide strict durability guarantees  writes can also happen to    ash when a con   gurable timeout interval  e g   1 msec  has expired  during which period multiple key value pairs are collected in the buffer   The client call returns only after the write buffer is    ushed to    ash  The RAM write buffer is sized to 2 3 times the    ash page size so that key value writes can still go through when part of the buffer is being written to    ash  RAM Hash Table  HT  Directory  The directory structure  for key value pairs stored on    ash  is maintained in RAM and is organized as a hash table with each slot containing a pointer to a chain of records on    ash  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  The chain of records on    ash pointed to by each slot comprises the bucket of records corresponding to this slot in the HT directory  This is illustrated in Figure 3  The average number of records in a bucket  k  is a con     gurable parameter  In summary  we resolve hash table directory collisions by linear chaining and store the chains in    ash  In an enhancement of the design  we use two choice based load balancing to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and introduce a bloom    lter in each hash table directory slot in RAM for summarizing the records in that bucket so that at most one bucket chain on    ash needs to be searched during a lookup  These enhancements form the the core of our design and are discussed in detail in Section 4 5  Flash store  The    ash store provides persistent storage for the keyvalue pairs and is organized as a circular append log  Key value pairs are written to    ash in units of a page size to the tail of the log  When the log accumulates garbage  consisting of deleted or key value RAM Flash Memory key value key value key value             key value key value Hash table  directory Sequential log null null null Keys  ordered  by write  time in  log ptr Figure 3  SkimpyStash architecture showing the sequential log organization of key value pair records on    ash and base design for the hash table directory in RAM   RAM write buffer is not shown   older values of updated records  beyond a con   gurable threshold  the pages on    ash from the head of the log are recycled     valid entries from the head of the log are written back to the end of the log  This also helps to place the records in a given bucket contiguously on    ash and improve read performance  as we elaborate shortly  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its HT bucket chain  on    ash  4 4 Overview of Key Lookup and Insert Operations To understand the relationship of the different storage areas in our design  it is helpful to follow the sequence of accesses in key insert and lookup operations performed by the client application  A key lookup operation  get     rst looks up the RAM write buffer  Upon a miss there  it lookups up the HT directory in RAM and searches the chained key value pair records on    ash in the respective bucket  A key insert  or  update  operation  set  writes the key value pair into the RAM write buffer  When there are enough key value pairs in RAM write buffer to    ll a    ash page  or  a con   gurable timeout interval since the client call has expired  say 1 msec   these entries are written to    ash and inserted into the RAM HT directory and    ash  A delete operation on a key is supported through insertion of a null value for that key  Eventually the null entry and earlier inserted values of the key on    ash will be garbage collected  When    ash usage and fraction of garbage records in the    ash log exceed a certain threshold  a garbage collection  and compaction  operation is initiated to reclaim storage on    ash in a manner similar to that in log structured    le systems  26   This garbage collection operation starts scanning key value pairs from the  current  head of the log     it discards garbage  invalid or orphaned  as de   ned later  key value pair records and moves valid key value pair records from the head to the tail of the log  It stops when    oor thresholds are reached for    ash usage or fraction of garbage records remaining in the    ash log  The functionalities of  i  client key lookup insert operations   ii 0 1 2 3 4 5 6 7 0 4 8 12 16 20 24 28 32 RAM bytes per key value pair Avg  keys per bucket  k  Base design Enhanced Design Figure 4  RAM space usage per key value pair in SkimpyStash for the base and enhanced designs as the average number of keys per bucket  k  is varied  writing key value pairs to    ash store and updating RAM HT directory  and  iii  reclaiming space on    ash pages are handled by separate threads in a multi threaded architecture  Concurrency issues with shared data structures arise in our multi threaded design  which we address but do not describe here due to lack of space  4 5 Hash Table Directory Design At a high level  we use a hash table based index in RAM to index the key value pairs on    ash  Earlier designs like FAWN  11  and ChunkStash  15  dedicate one entry of the hash table to point to a single key value pair on    ash together with a checksum that helps to avoid  with high probability  following the    ash pointer to compare keys for every entry searched in the hash table during a lookup  The RAM overhead in FAWN and ChunkStash is 6 bytes per key value pair stored on    ash  With such a design  we cannot get below the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key value pair  even if we ignore the other    elds  like checksums  in the hash table entry   Our approach  at an intuitive level  is to move most of the pointers that locate each key value pair from RAM to    ash itself  We realize this by     Resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and     Storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  Because we store the chain of key value pairs in each bucket on    ash  we incur multiple    ash reads upon lookup of a key in the store  This is the tradeoff that we need to make with lookup times in order to be able to skimp on RAM space overhead per keyvalue pair  We will see that the average number of keys in a bucket  k  is the critical parameter that allows us to make a continuum of tradeoffs between these two parameters     it serves as a powerful knob for reducing RAM space usage at the expense of increase in lookup times  We    rst begin with the base design of our hash table based index in RAM  Thereafter  we motivate and introduce some enhancements to the design to improve performance  Base Design The directory structure  for key value pairs stored on    ash  is maintained in RAM and is organized as a hash table with each slot containing a pointer to a chain of records on    ash  as shown in Figure 3  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  The chain of records on    ash pointed to by each slot comprises the bucket of records corresponding to this slot in the HT directory  A hash function h is used to map keys to slots in the HT directory  The average number of records in a bucket  k  is a con   gurable parameter  Then  to accommodate up to some given number n key value pairs  the number of slots required in the HT directory is about n k  In summary  we resolve hash table directory collisions by linear chaining and store the chains in    ash  We next describe the lookup  insert update  and delete operations on this data structure  A lookup operation on a key uses the hash function h to obtain the HT directory bucket that this key belongs to  It uses the pointer stored in that slot to follow the chain of records on    ash to search the key  upon    nding the    rst record in the chain whose key matches the search key  it returns the value  The number of    ash reads for such a lookup is k 2 on the average  and at most the size of the bucket chain in the worst case  An insert  or  update  operation uses the hash function h to obtain the HT directory bucket that this key belongs to  Let a1 be the address on    ash of the    rst record in this chain  i e   what the pointer in this slot points to   Then a record is created corresponding to the inserted  or  updated  key value pair with its next pointer    eld equal to a1  This record is appended to the log on    ash and its address on    ash now becomes the value of the pointer in the respective slot in RAM  Effectively  this new record is inserted at the beginning of the chain corresponding to this bucket  Thus  if this insert operation corresponds to an update operation on an earlier inserted key  the most recent value of the key will be  correctly  read during a lookup operation  the old value being further down the chain and accumulating as garbage in the log   A delete operation is same as the insert  or  update  with null value for that key  Eventually the null entry on    ash and old values of the key will be garbage collected in the log  RAM Space Overhead for Base Design  Let us say that the pointer to    ash in each HT directory slot is 4 bytes   This accommodates up to 4GB of byte addressable log  If records are of a    xed size  say 64 bytes  then this can accommodate up to 256GB of 64 byte granularity addressable log  Larger pointer sizes  up to 8 bytes  can be used according to application requirements   Then  with a value of k   10 average bucket size  the RAM space overhead is a mere 4 k   0 4 bytes   3 2 bits per entry  independent of key value size  At this sub byte range  this design tests the extremes of low RAM space overhead per entry  The average number of    ash reads per lookup is k 2   5  with current SSDs achieving    ash read times in the range of 10  sec  this corresponds to a lookup latency of about 50   sec  The parameter k provides a powerful knob for achieving tradeoffs between low RAM space usage and low lookup latencies  The RAM space usage per key value pair for the base design as a function of k is shown in Figure 4  Design Enhancements We identify some performance inef   ciencies in the base design and develop techniques to address them with only a slight increase in the RAM space overhead per key value pair  The enhanced design is shown in Figure 5  Load Balancing across Buckets The hashing of keys to HT directory buckets may lead to skewed distributions in the number of keys in each bucket chain  thus creating variations in average lookup times across buckets  Thus  itkey value RAM Flash Memory key value key value key value             key value key value Hash table  directory Sequential log null null null Keys  ordered  by write  time in  log BF ptr Two choice load  balancing for key x Figure 5  SkimpyStash architecture showing the sequential log organization of key value pair records on    ash and enhanced design for the hash table directory in RAM   RAM write buffer is not shown   might be necessary to enforce fairly equal load balancing of keys across HT directory buckets in order to keep each bucket chain of about the same size  One simple way to achieve this is to use the power of two choice idea from  12  that has been applied to balance a distribution of balls thrown into bins  With a load balanced design for the HT directory  each key would be hashed to two candidate HT directory buckets  using two hash functions h1 and h2  and actually inserted into the one that has currently fewer elements  We investigate the impact of this design decision on balancing bucket sizes on our evaluation workloads in Section 5  To implement this load balancing idea  we add one byte of storage to each HT directory slot in RAM that holds the current number of keys in that bucket     this space allocation accommodates up to a maximum of 2 8     1   255 keys per bucket  Bloom Filter per Bucket This design modi   cation  in its current form  will lead to an increase in the number of    ash reads during lookup  Since each key will need to be looked up in both of its candidate buckets  the worst case number of    ash reads  hence lookup times  would double  To remove this latency impact on the lookup pathway  we add a bloom    lter  13  per HT directory slot that summarizes the keys that have been inserted in the respective bucket  Note that this bloom    lter in each HT directory slot can be sized to contain about k keys  since load balancing ensures that when the hash table reaches its budgeted full capacity  each bucket will contain not many more than k keys with very high probability  A standard rule of thumb for dimensioning a bloom    lter to use one byte per key  which gives a false positive probability of 2    hence the bloom    lter in each HT directory slot can be of size k bytes  The introduction of bloom    lters in each HT directory slot has another desirable side effect     lookups on non existent keys will almost always not involve any    ash reads since the bloom    lters in both candidate slots of the key will indicate that the key is not present  module false positive probabilities    Note that in the base design  lookups on non existent keys also lead to    ash reads and involve traversing the entire chain in the respective bucket   In an interesting reciprocity of bene   ts  the bloom    lters in each Bloom Filter Bucket  size pointer to key value  pair chain on flash                                 k bytes 1 byte 4 bytes Figure 6  RAM hash table directory slot and sizes of component    elds in the enhanced design of SkimpyStash  The parameter k is the average number of keys in a bucket  bucket not only help in reducing lookup times when two choice load balancing is used but also bene   t from load balancing  Load balancing aims to keep the number of keys in each bucket upper bounded  roughly  by the parameter k  This helps to keep bloom    lter false positive probabilities in that bucket bounded as per the dimensioned capacity of k keys  Without load balancing  many more than k keys could be inserted into a given bucket and this will increase the false positive rates of the respective bloom    lter well beyond what it was dimensioned for  The additional    elds added to each HT directory slot in RAM in the enhanced design are shown in Figure 6  During a lookup operation  the key is hashed to its two candidate HT directory buckets and the chain on    ash is searched only if the respective bloom    lter indicates that the key may be there in that bucket  Thus  accounting for bloom    lter false positives  the chain on    ash will be searched with no success in less than 2  of the lookups  When an insert operation corresponds to an update of an earlier inserted key  the record is always inserted in the same bucket as the earlier one  even if the choice determined by load balancing  out of two candidate buckets  is the other bucket  If we followed the choice given by load balancing  the key may be inserted in the bloom    lters of both candidate slots     this would not preserve the design goal of traversing at most one bucket chain  with high probability  on    ash during lookups  Moreover  the same problem would arise with version resolution during lookups if different versions of a key are allowed to be inserted in both candidate buckets  This rule also leads to ef   ciencies during garbage collection operations since all the obsolete values of a key appear in the same bucket chain on    ash  Note that this complication involving overriding of the load balancing based choice of insertion bucket can be avoided when the application does not perform updates to earlier inserted keys     one example of such an application is storage deduplication as described in Section 3  In summary  in this enhancement of the base design  two choice based load balancing strategy is used to reduce variations in the the number of keys assigned to each bucket  hence  chain lengths and associated lookup times   Each HT directory slot in RAM also contains a bloom    lter summarizing the keys in the bucket and a size  count     eld storing the current number of keys in that bucket  RAM Space Overhead for Enhanced Design  With this design modi   cation  the RAM space overhead per bucket now has three components  namely      Pointer to chain on    ash  4 bytes       Bucket size  1 byte   and     Bloom    lter  k bytes   This space overhead per HT directory slot is amortized over an average of k keys  in that bucket   hence the RAM space overhead per entry can be computed as  k   1   4  k   1   5 k whichis about 1 5 bytes for k   10  The average number of    ash reads per lookup is still k 2   5  with high probability   with current SSDs achieving    ash read times in the range of 10  sec  this corresponds to a lookup latency of about 50   sec  Moreover  the variation across lookup latencies for different keys is better controlled in this design  compared to the base design  as bucket chains are about the same size due to two choice based load balancing of keys across buckets  The RAM space usage per key value pair for the enhanced design as a function of k is shown in Figure 4  Storing key value pairs to    ash  Key value pairs are organized on    ash in a log structure in the order of the respective write operations coming into the system  Each slot in the HT directory contains a pointer to the beginning of the chain on    ash that represents the keys in that bucket  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  We use a 4 byte pointer  which is a combination of a page pointer and a page offset  The all zero pointer is reserved for the null pointer     in the HT directory slot  this represents an empty bucket  while on    ash this indicates that the respective record has no successor in the chain  RAM and Flash Capacity Considerations  We designed our RAM indexing scheme to use 1 byte in RAM per key value pair so as to maximize the amount of indexable storage on    ash for a given RAM usage size  Whether RAM or    ash capacity becomes the bottleneck for storing key value pairs on    ash depends further on the key value pair size  With 64 byte key value pair records  1GB of RAM can index about 1 billion key value pairs on    ash which occupy 64GB on    ash  This    ash memory capacity is well within the capacity range of SSDs shipping in the market today  from 64GB to 640GB   On the other hand  with 1024 byte key value pair records  the same 1GB of RAM can index 1 billion key value pairs which now occupy 1TB on    ash     at currently available SSD capacities  this will require multiple    ash drives to store the dataset  4 6 Flash Storage Management Key value pairs are organized on    ash in a log structure in the order of the respective write operations coming into the system  When there are enough key value pairs in the RAM write buffer to    ll a    ash page  or  when a pre speci   ed coalesce time interval is reached   they are written to    ash  The pages on    ash are maintained implicitly as a circular log  Since the Flash Translation Layer  FTL  translates logical page numbers to physical ones  this circular log can be easily implemented as a contiguous block of logical page addresses with wraparound  realized by two page number variables  one for the    rst valid page  oldest written  and the other for the last valid page  most recently written   We next describe two maintenance operations on    ash in SkimpyStash  namely  compaction and garbage collection  Compaction is helpful in improving lookup latencies by reducing number of    ash reads when searching bucket  Garbage collection is necessary to reclaim storage on    ash and is a consequence of    ash being used in a log structured manner  Compaction to Reduce Flash Reads during Lookups In SkimpyStash   a lookup in a HT directory bucket involves following the chain of key value records on    ash  For a chain length of c records in a bucket  this involves an average of c 2    ash reads  Over time  as keys are inserted into a bucket and earlier inserted keys are updated  the chain length on    ash for this bucket keeps Figure 7  Diagram illustrating the effect of compaction procedure on the organization of a bucket chain on    ash in SkimpyStash  increasing and degrading lookup times  We address this by periodically compacting the chain on    ash in a bucket by placing the valid keys in that chain contiguously on one or more    ash pages that are appended to the tail of the log  Thus  if m key value pairs can be packed onto a single    ash page  on the average   the number of    ash reads required to search for a key in a bucket of k records is k  2m  on the average and at most    k m    in the worst case  The compaction operations proceed over time in a bucket as follows  Initially  as key value pairs are added at different times to a bucket  they appear on different    ash pages and are chained together individually on    ash  When enough valid records accumulate in a bucket to    ll a    ash page  say  m of them   they are compacted and appended on a new    ash page at the tail of the log     the chain is now of a single    ash page size and requires one    ash read  instead of m  to search fully  Thereafter  as further keys are appended to a bucket  they will be chained together individually and appear before the compacted group of keys in the chain  Over time  enough new records may accumulate in the bucket to allow them to be compacted to a second    ash page  and so the process repeats  At any given time  the chain on    ash for each bucket now begins with a chained sequence of individual records followed by groups of compacted records  each group appearing on the same    ash page   This organization of a bucket chain as a result of compaction is illustrated in Figure 7  When a key value pair size is relatively small  say 64 bytes as in the storage deduplication application  there may not be enough records in a bucket to    ll a    ash page  since this number is  roughly  upper bounded by the parameter k  In this case  we may reap the same bene   ts of compaction by applying the procedure to groups of chains in multiple buckets at a time  The compaction operation  as described above  will lead to orphaned   or  garbage  records on the    ash log  Moreover  garbage records also accumulate in the log as a result of key update and delete operations  These need to be garbage collected on    ash as we describe next  Garbage Collection Garbage records  holes  accumulate in the log as a result of compaction and key update delete operations  These operations creategarbage records corresponding to all previous versions of respective keys  When a certain con   gurable fraction of garbage accumulates in the log  in terms of space occupied   a cleaning operation is performed to clean and compact the log  The cleaning operation considers currently used    ash pages in oldest    rst order and deallocates them in a way similar to garbage collection in log structured    le systems  26   One each page  the sequence of key value pairs are scanned to determine whether they are valid or not  The classi   cation of a key value pair record on    ash follows from doing a lookup on the respective key starting from the HT directory     if this record is the same as that returned by the lookup  then it is valid  if it appears later in the chain than a valid record for that key  then this record is invalid and corresponds to an obsolete version of they key  otherwise  the record is orphaned and cannot be reached by following pointers from the HT directory  this may happen because of the compaction procedure  for example   When an orphaned record is encountered at the head of the log  it is skipped and the head position of the log is advanced to the next record  From the description of the key insertion procedure in Section 4 5  it follows that the    rst record in each bucket chain  the one pointed to from the HT directory slot  is the most recently inserted record  while the last record in the chain is the earliest inserted record in that bucket  Hence  the last record in a bucket chain will be encountered    rst during the garbage collection process and it may be a valid or invalid  obsolete version of the respective key  record  A valid record needs to be reinserted at the tail of the log while an invalid record can be skipped  In either case  the next pointer in its predecessor record in the chain would need to be updated  Since we want to avoid in place updates  random writes  on    ash  this requires relocating the predecessor record and so forth all the way to the    rst record in the chain  This effectively leads to the design decision of garbage collecting entire bucket chains on    ash at a time  In summary  when the last record in a bucket chain is encountered in the log during garbage collection  all valid records in that chain are compacted and relocated to the tail of the log  This garbage collection strategy has two bene   ts      First  the writing of an entire chain of records in a bucket to the tail of the log also allows them to be compacted and placed contiguously on one or more    ash pages and helps to speed up the lookup operations on those keys  as explained above in the context of compaction operations  and     Second  since garbage  orphaned  records are created further down the log between the  current  head and tail  corresponding to the locations of all records in the chain before relocation   this helps to speedup the garbage collection process for the respective pages when they are encountered later  since orphaned records can be simply discarded   We investigate the impact of compaction and garbage collection on system throughput in Section 5  4 7 Crash Recovery SkimpyStash    s persistency guarantee enables it to recover from system crashes due to power failure or other reasons  Because the system logs all key value write operations to    ash  it is straightforward to rebuild the HT directory in RAM by scanning all valid    ash pages on    ash  Recovery using this method can take some time  however  depending on the total size of valid    ash pages that need to be scanned and the read throughput of    ash memory  If crash recovery needs to be executed faster so as to support    near  real time recovery  then it is necessary to checkpoint the Trace Total get  get set Avg  size  bytes  set ops ratio Key Value Xbox 5 5 millions 7 5 1 92 1200 Dedup 40 millions 2 2 1 20 44 Table 1  Properties of the two traces used in the performance evaluation of SkimpyStash  RAM HT directory periodically into    ash  in a separate area from the key value pair logs   Then  recovery involves reading the last written HT directory checkpoint from    ash and scanning key value pair logged    ash pages with timestamps after that and inserting them into the restored HT directory  During the operation of checkpointing the HT directory  all insert operations into it will need to be suspended  but the read operations can continue   We use a temporary  small in RAM hash table to provide index for the interim items and log them to    ash  After the checkpointing operation completes  key value pairs from the    ash pages written in the interim are inserted into the HT directory  Key lookup operations  upon missing in the HT directory  will need to check in these    ash pages  via the small additional hash table  until the later insertions into HT directory are complete  The    ash garbage collection thread is suspended during the HT directory checkpointing operation  since the HT directory entries cannot be modi   ed during this time  5  EVALUATION We evaluate SkimpyStash on real world traces obtained from the two applications described in Section 3  5 1 C  Implementation We have prototyped SkimpyStash in approximately 3000 lines of C  code  MurmurHash  4  is used to realize the hash functions used in our implementation to compute hash table directory indices and bloom    lter lookup positions  different seeds are used to generate different hash functions in this family  The metadata store on    ash is maintained as a    le in the    le system and is created opened in non buffered mode so that there are no buffering caching prefetching effects in RAM from within the operating system  The ReaderWriterLockSlim and Monitor classes in  NET 3 0 framework  1  are used to implement the concurrency control solution for multi threading  5 2 Evaluation Platform and Datasets We use a standard server con   guration to evaluate the performance of SkimpyStash  The server runs Windows Server 2008 R2 and uses an Intel Core2 Duo E6850 3GHz CPU  4GB RAM  and fusionIO 160GB    ash drive  2  attached over PCIe interface  We compare the throughput  get set operations per sec  on the two traces described in Table 1  We described two real world applications in Section 3 that can use SkimpyStash as an underlying persistent key value store  Data traces obtained from real world instances of these applications are used to drive and evaluate the design of SkimpyStash  Xbox LIVE Primetime trace We evaluate the performance of SkimpyStash on a large trace of get set key operations obtained from real world instances of the Microsoft Xbox LIVE Primetime online multiplayer game  7   In this application  the key is a dot separated sequence of strings with total length averaging 94 characters and the value averages around 1200 bytes  The ratio of get operations to set set operations is about 7 5 1 0 0 2 0 4 0 6 0 8 1 1 2 4 8 16 32 Stddev mean of bucket sizes Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 8  Xbox trace  Relative variation in bucket sizes  standarddeviation mean  for different values of average bucket size  k  for the base and enhanced designs   Behavior on dedup trace is similar   0 500 1000 1500 2000 2500 1 2 4 8 16 32 Avg  lookup time  usec  Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 9  Xbox trace  Average lookup  get  time for different values of average bucket size  k  for the base and enhanced designs  The enhanced design reduces lookup times by factors of 6x 24x as k increases  Storage Deduplication trace We have built a storage deduplication analysis tool that can crawl a root directory  generate the sequence of chunk hashes for a given average chunk hash size  and compute the number of deduplicated chunks and storage bytes  The enterprise data backup trace we use for evaluations in this paper was obtained by our storage deduplication analysis tool using 4KB chunk sizes  The trace contains 27 748 824 total chunks and 12 082 492 unique chunks  Using this  we obtain the ratio of get operations to set operations in the trace as 2 2 1  In this application  the key is a 20 byte SHA 1 hash of the corresponding chunk and the value is the meta data for the chunk  with key value pair total size upper bounded by 64 bytes  The properties of the two traces are summarized in Table 1  Both traces include get operations on keys that have not been set earlier in the trace   Such get operations will return null   This is an inherent property of the nature of the application  hence we play the traces    as is  to evaluate throughput in operations per second  5 3 Performance Evaluation We evaluate the performance impact of our design decisions and obtain ballpark ranges on lookup times and throughput  get set operations sec  for SkimpyStash on the Xbox LIVE Primetime online multi player game and storage deduplication application traces  from Table 1   We disable the log compaction procedure for all but the last set of experiments  In the last set of experiments  we investigate the impact of garbage collection  which also does compaction  on system throughput  0 20 40 60 80 100 1 2 4 8 16 32 Avg  lookup time  usec  Avg  keys per bucket  k   log scale  Dedup trace Base design Enhanced Design Figure 10  Dedup trace  Average lookup  get  time for different values of average bucket size  k  for the base and enhanced designs  The enhanced design reduces lookup times by factors of 1 5x 2 5x as k increases  Impact of load balancing on bucket sizes  In the base design  hashing keys to single buckets can lead to wide variations in the chain length on    ash in each bucket  and this translates to wide variations in lookup times  We investigate how two choice based load balancing of keys to buckets can help reduce variance among bucket sizes  In Figure 8  we plot the relative variation in the bucket sizes  standard deviation mean of bucket sizes  for different values of k   1  2  4  8  16  32 on the Xbox trace  the behavior on dedup trace is similar   This metric is about 1 4x 6x times better for the enhanced design than for the based design on both traces  The gap starts at about 1 4x for the case k   1 and increases to about 6x for the case k   32  This provides conclusive evidence that two choice based load balancing is an effective strategy for reducing variations in bucket sizes  Key lookup latencies  The two ideas of load balancing across buckets and using a bloom    lter per bucket help to signi   cantly reduce average key lookup times in the enhanced design  with the gains increasing as the average bucket size parameter k increases  At a value of k   8  the average lookup time in the enhanced design is 20   sec for the Xbox trace and 12   sec for the dedup trace  In Figures 9 and 10  we plot the average lookup time for different values of k   1  2  4  8  16  32 on the two traces respectively  As the value of k increases  the gains in the enhanced design increase from 6x to 24x for the Xbox trace and from 1 5x to 2 5x for the dedup trace  The gains are more for the Xbox trace since that trace has many updates to earlier inserted keys  while the dedup trace has none   hence chains accumulate garbage records and get longer over time and bloom    lters help even more to speedup lookups on non existing keys  by avoiding searching the entire chain on    ash   Throughput  get set operations sec   The enhanced design achieves throughputs in the range of 10 000 69 000 ops sec on the Xbox trace and 34 000 165 000 ops sec on the dedup trace  with throughputs decreasing  as expected  with increasing values of k  This is shown in Figures 11 and 12  The throughput gains for the enhanced design over the base design are in the range of 3 5x 20x on the Xbox trace and 1 3x 2 5x on the dedup trace  with the gains increasing as the parameter k increases in value  These trends are in line with that of average lookup times  The higher throughput of SkimpyStash on the dedup trace can be explained as follows  The write mix per unit operation  get or set  in the dedup trace is about 2 65 times that of the Xbox trace  However  since the key value pair size is about 20 times smaller for the dedup trace  the number of syncs to stable storage per write0 10000 20000 30000 40000 50000 60000 70000 1 2 4 8 16 32 Throughput  get set ops sec  Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 11  Xbox trace  Throughput  get set ops sec  for different values of average bucket size  k  for the base and enhanced designs  The enhanced design improves throughput by factors of 3 5x 20x as k increases  0 20000 40000 60000 80000 100000 120000 140000 160000 1 2 4 8 16 32 Throughput  get set ops sec  Avg  keys per bucket  k   log scale  Dedup trace Base design Enhanced Design Figure 12  Dedup trace  Throughput  get set ops sec  for different values of average bucket size  k  for the base and enhanced designs  The enhanced design improves throughput by factors of 1 3x 2 5x as k increases  operation is about 20 times less  Overall  the number of syncs to stable storage per unit operation is about 7 6 times less for the dedup trace  Moreover  bucket chains get longer in the Xbox trace due to invalid  garbage  records accumulating from key updates  For these reasons  SkimpyStash obtains higher throughputs on the dedup trace  In addition  since the dedup trace has no key update operations  the lookup operation can be avoided during key inserts for the dedup trace  this also contributes to the higher throughput of dedup trace over the Xbox trace  Garbage collection can help to improve performance on the Xbox trace  as we discuss next  Impact of garbage collection activity  We study the impact of garbage collection  GC  activity on system throughput  ops sec  and lookup times in SkimpyStash  The storage deduplication application does not involve updates to chunk metadata  hence we evaluate the impact of garbage collection on the Xbox trace  We    x an average bucket size of k   8 for this set of experiments  The aggressiveness of the garbage collection activity is controlled by a parameter g which is</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>Design and Evaluation of Main Memory Hash Join Algorithms for Multi core CPUs ### Spyros Blanas Yinan Li Jignesh M  Patel University of Wisconsin   Madison  sblanas  yinan  jignesh  cs wisc edu ABSTRACT The focus of this paper is on investigating e   cient hash join algorithms for modern multi core processors in main memory environments  This paper dissects each internal phase of a typical hash join algorithm and considers di   erent alternatives for implementing each phase  producing a family of hash join algorithms  Then  we implement these main memory algorithms on two radically di   erent modern multiprocessor systems  and carefully examine the factors that impact the performance of each method  Our analysis reveals some interesting results     a very simple hash join algorithm is very competitive to the other more complex methods  This simple join algorithm builds a shared hash table and does not partition the input relations  Its simplicity implies that it requires fewer parameter settings  thereby making it far easier for query optimizers and execution engines to use it in practice  Furthermore  the performance of this simple algorithm improves dramatically as the skew in the input data increases  and it quickly starts to outperform all other algorithms  Based on our results  we propose that database implementers consider adding this simple join algorithm to their repertoire of main memory join algorithms  or adapt their methods to mimic the strategy employed by this algorithm  especially when joining inputs with skewed data distributions  Categories and Subject Descriptors H 2 4   Database Management   Systems   Query processing  Relational databases General Terms Algorithms  Design  Performance Keywords hash join  multi core  main memory Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  ### 1  INTRODUCTION Large scale multi core processors are imminent  Modern processors today already have four or more cores  and for the past few years Intel has been introducing two more cores per processor roughly every 15 months  At this rate  it is not hard to imagine running database management systems  DBMSs  on processors with hundreds of cores in the near future  In addition  memory prices are continuing to drop  Today 1TB of memory costs as little as  25 000  Consequently  many databases now either    t entirely in main memory  or their working set is main memory resident  As a result  many DBMSs are becoming CPU bound  In this evolving architectural landscape  DBMSs have the unique opportunity to leverage the inherent parallelism that is provided by the relational data model  Data is exposed by declarative query languages to user applications and the DBMS is free to choose its execution strategy  Coupled with the trend towards impending very large multi cores  this implies that DBMSs must carefully rethink how they can exploit the parallelism that is provided by the modern multi core processors  or DBMS performance will stall  A natural question to ask then is whether there is anything new here  Beginning about three decades ago  at the inception of the    eld of parallel DBMSs  the database community thoroughly examined how a DBMS can use various forms of parallelism  These forms of parallelism include pure sharednothing  shared memory  and shared disk architectures  17   If the modern multi core architectures resemble any of these architectural templates  then we can simply adopt the methods that have already been designed  In fact  to a large extent this is the approach that DBMSs have haven taken towards dealing with multi core machines  Many commercial DBMSs simply treat a multi core processor as a symmetric multi processor  SMP  machine  leveraging previous work that was done by the DBMS vendors in reaction to the increasing popularity of SMP machines decades ago  These methods break up the task of a single operation  such as an equijoin  into disjoint parts and allow each processor  in an SMP box  to work on each part independently  At a high level  these methods resemble variations of query processing techniques that were developed for parallel shared nothing architectures  6   but adapted for SMP machines  In most commercial DBMSs  this approach is re   ected across the entire design process  ranging from system internals  join processing  for example  to their pricing model  which is frequently done by scaling the SMP pricing model  On the other hand  open source DBMSs havelargely ignored multi core processing and generally dedicate a single thread process to each query  The design space for modern high performance main memory join algorithms has two extremes  One extreme of this design space focuses on minimizing the number of processor cache misses  The radix based hash join algorithm  2  is an example of a method in this design class  The other extreme is to focus on minimizing processor synchronization costs  In this paper we propose a    no partitioning    hash join algorithm that does not partition the input relations to embody an example of a method in this later design space  A crucial question that we ask and answer in this paper is what is the impact of these two extreme design points in modern multi core processors for main memory hash join algorithms  A perhaps surprising answer is that for modern multi core architectures  in many cases the right approach is to focus on reducing the computation and synchronization costs  as modern processors are very e   ective in hiding cache miss latencies via simultaneous multi threading  For example  in our experiments  the    no partitioning    hash join algorithm far outperforms the radix join algorithm when there is skew in the data  which is often the case in practice   even while it incurs many more processor cache and TLB misses  Even with uniform data  the radix join algorithm only outperforms the    no partitioning    algorithm on a modern Intel Xeon when the parameters for the radix join algorithm are set at or near their optimal setting  In contrast  the nonpartitioned algorithm is    parameter free     which is another important practical advantage  Re   ecting on the previous work in this area  one can observe that the database community has focused on optimizing query processing methods to reduce the number of processor cache and TLB misses  We hope that this paper opens up a new discussion on the entire design space for multi core query processing techniques  and incites a similar examination of other aspects of query processing beyond the single hash join operation that we discuss in this paper  This paper makes three main contributions  First  we systematically examine the design choices available for each internal phase of a canonical main memory hash join algorithm     namely  the partition  build  and probe phases     and enumerate a number of possible multi core hash join algorithms based on di   erent choices made in each of these phases  We then evaluate these join algorithms on two radically di   erent architectures and show how the architectural di   erences can a   ect performance  Unlike previous work that has often focused on just one architecture  our use of two radically different architectures lets us gain deeper insights about hash join processing on multi core processors  To the best of our knowledge  this is the    rst systematic exploration of multiple hash join techniques that spans multi core architectures  Second  we show that an algorithm that does not do any partitioning  but simply constructs a single shared hash table on the build relation often outperforms more complex algorithms  This simple    no partitioning    hash join algorithm is robust to sub optimal parameter choices by the optimizer  and does not require any knowledge of the characteristics of the input to work well  To the best of our knowledge  this simple hash join technique di   ers from what is currently implemented in existing DBMSs for multi core hash join processing  and o   ers a tantalizingly simple  e   cient  and robust technique for implementing the hash join operation  Finally  we show that the simple    no partitioning    hash join algorithm takes advantage of intrinsic hardware optimizations to handle skew  As a result  this simple hash join technique often bene   ts from skew and its relative performance increases as the skew increases  This property is a big advancement over the state of the art methods  as it is important to have methods that can gracefully handle skew in practice  8   The remainder of this paper is organized as follows  The next section covers background information  The hash join variants are presented in Section 3  Experimental results are described in Section 4  and related work is discussed in Section 5  Finally  Section 6 contains our concluding remarks  2  THE MULTI CORE LANDSCAPE In the last few years alone  more than a dozen di   erent multi core CPU families have been introduced by CPU vendors  These new CPUs have ranged from powerful dual CPU systems on the same die to prototype systems of hundreds of simple RISC cores  This new level of integration has lead to architectural changes with deep impact on algorithm design  Although the    rst multi core CPUs had dedicated caches for each core  we now see a shift towards more sharing at the lower levels of the cache hierarchy and consequently the need for access arbitration to shared caches within the chip  A shared cache means better single threaded performance  as one core can utilize the whole cache  and more opportunities for sharing among cores  However  shared caches also increase con   ict cache misses due to false sharing  and may increase capacity cache misses  if the cache sizes don   t increase proportionally to the number of cores  One idea that is employed to combat the diminishing returns of instruction level parallelism is simultaneous multithreading  SMT   Multi threading attempts to    nd independent instructions across di   erent threads of execution  instead of detecting independent instructions in the same thread  This way  the CPU will schedule instructions from each thread and achieve better overall utilization  increasing throughput at the expense of per thread latency  We brie   y consider two modern architectures that we subsequently use for evaluation  At one end of the spectrum  the Intel Nehalem family is an instance of Intel   s latest microarchitecture that o   ers high single threaded performance because of its out of order execution and on demand frequency scaling  TurboBoost   Multi threaded performance is increased by using simultaneous multi threading  HyperThreading   At the other end of the spectrum  the Sun UltraSPARC T2 has 8 simple cores that all share a single cache  This CPU can execute instructions from up to 8 threads per core  or a total of 64 threads for the entire chip  and extensively relies on simultaneous multi threading to achieve maximum throughput  3  HASH JOIN IMPLEMENTATION In this section  we consider the anatomy of a canonical hash join algorithm  and carefully consider the design choices that are available in each internal phase of a hash join algorithm  Then using these design choices  we categorize various previous proposals for multi core hash join processing  In the following discussion we also present information about some of the implementation details  as they often have a signi   cant impact on the performance of the technique that is described A hash join operator works on two input relations  R and S  We assume that  R     S   A typical hash join algorithm has three phases  partition  build  and probe  The partition phase is optional and divides tuples into distinct sets using a hash function on the join key attribute  The build phase scans the relation R and creates an in memory hash table on the join key attribute  The probe phase scans the relation S  looks up the join key of each tuple in the hash table  and in the case of a match creates the output tuple s   Before we discuss the alternative techniques that are available in each phase of the join algorithm  we brie   y digress to discuss the impact of the latch implementation on the join techniques  As a general comment  we have found that the latch implementation has a crucial impact on the overall join performance  In particular  when using the pthreads mutex implementation  several instructions are required to acquire and release an uncontended latch  If there are millions of buckets in a hash table  then the hash collision rate is small  and one can optimize for the expected case  latches being free  Furthermore  pthread mutexes have signi   cant memory footprint as each requires approximately 40 bytes  If each bucket stores a few  key  record id  pairs  then the size of the latch array may be greater than the size of the hash table itself  These characteristics make mutexes a prohibitively expensive synchronization primitive for buckets in a hash table  Hence  we implemented our own 1 byte latch for both the Intel and the Sun architectures  using the atomic primitives xchgb and ldstub  respectively  Protecting multiple hash buckets with a single latch to avoid cache thrashing did not result in signi   cant performance improvements even when the number of partitions was high  3 1 Partition phase The partition phase is an optional step of a hash join algorithm  if the hash table for the relation R    ts in main memory  If one partitions both the R and S relations such that each partition    ts in the CPU cache  then the cache misses that are otherwise incurred during the subsequent build and probe phases are almost eliminated  The cost for partitioning both input relations is incurring additional memory writes for each tuple  Work by Shatdal et al   16  has shown that the runtime cost of the additional memory writes during partitioning phase is less than the cost of missing in the cache     as a consequence partitioning improves overall performance  Recent work by Cieslewicz and Ross  4  has explored partitioning performance in detail  They introduce two algorithms that process the input once in a serial fashion and do not require any kind of global knowledge about the characteristics of the input  Another recent paper  11  describes a parallel implementation of radix partitioning  2  which gives impressive performance improvements on a modern multi core system  This implementation requires that the entire input is available upfront and will not produce any output until the last input tuple has been seen  We experiment with all of these three partitioning algorithms  and we brie   y summarize each implementation in Sections 3 1 1 and 3 1 2  In our implementation  a partition is a linked list of output bu   ers  An output bu   er is fully described by four elements  an integer specifying the size of the data block  a pointer to the start of the data block  a pointer to the free space inside the data block and a pointer to the next output bu   er that is initially set to zero  If a bu   er over   ows  then we add an empty output bu   er at the start of the list  and we make its next pointer point to the bu   er that over   owed  Locating free space is a matter of checking the    rst bu   er in the list  Let p denote the desired number of partitions and n denote the number of threads that are processing the hash join operation  During the partitioning phase  all threads start reading tuples from the relation R  via a cursor  Each thread works on a large batch of tuples at a time  so as to minimize synchronization overheads on the input scan cursor  Each thread examines a tuple  then extracts the key k  and      nally computes the partitioning hash function hp k   Next  it then writes the tuple to partition Rhp k  using one of the algorithms we describe below  When the R cursor runs out of tuples  the partitioning operation proceeds to process the tuples from the S relation  Again  each tuple is examined  the join key k is extracted and the tuple is written to the partition Shp k    The partitioning phase ends when all the S tuples have been partitioned  Note that we classify the partitioning algorithms as    nonblocking    if they produce results on the    y and scan the input once  in contrast to a    blocking    algorithm that produces results after bu   ering the entire input and scanning it more than once  We acknowledge that the join operator overall is never truly non blocking  as it will block during the build phase  The distinction is that the non blocking algorithms only block for the time that is needed to scan and process the smaller input  and  as we will see in Section 4 3  this a very small fraction of the overall join time  3 1 1 Non blocking algorithms The    rst partitioning algorithm creates p shared partitions among all the threads  The threads need to synchronize via a latch to make sure that the writes to a shared partition are isolated from each other  The second partitioning algorithm creates p     n partitions in total and each thread is assigned a private set of p partitions  Each thread then writes to its local partitions without any synchronization overhead  When the input relation is depleted  all threads synchronize at a barrier to consolidate the p     n partitions into p partitions  The bene   t of creating private partitions is that there is no synchronization overhead on each access  The drawbacks  however  are  a  many partitions are created  possibly so many that the working set of the algorithm no longer    ts in the data cache and the TLB   b  at the end of the partition phase some thread has to chain n private partitions together to form a single partition  but this operation is quick and can be parallelized  3 1 2 Blocking algorithm Another partitioning technique is the parallel multi pass radix partitioning algorithm described by Kim et al   11   The algorithm begins by having the entire input available in a contiguous block of memory  Each thread is responsible for a speci   c memory region in that contiguous block  A histogram with p     n bins is allocated and the input is then scanned twice  During the    rst scan  each thread scans all the tuples in the memory region assigned to it  extracts the key k and then computes the exact histogram of the hash values hp k  for this region  Thread i      0  n     1  stores the number of tuples it encountered that will hash to partition j      0  p   1  in histogram bin j   n i  At the end of the scan  all the n threads compute the pre   x sum on the histogramin parallel  The pre   x sum can now be used to point to the beginning of each output partition for each thread in the single shared output bu   er  Finally  each thread performs a second scan of its input region  and uses hp to determine the output partition  This algorithm is recursively applied to each output partition for as many passes as requested  The bene   t of radix partitioning is that it makes few cache and TLB misses  as it bounds the number of output destinations in each pass  This particular implementation has the bene   t that  by scanning the input twice for each pass  it computes exactly how much output space will be required for each partition  and hence avoids the synchronization overhead that is associated with sharing an output bu   er  Apart from the drawbacks that are associated with any blocking algorithm when compared to a non blocking counterpart  this implementation also places a burden on the previous operator in a query tree to produce the compact and contiguous output format that the radix partitioning requires as input  E   ciently producing a single shared output bu   er is a problem that has been studied before  5   3 2 Build phase The build phase proceeds as follows  If the partition phase was omitted  then all the threads are assigned to work on the relation R  If partitioning was done  then each thread i is assigned to work on partitions Ri 0   n  Ri 1   n  Ri 2   n  etc  For example  a machine with four cores has n   4  and thread 0 would work on partitions R0  R4  R8       thread 1 on R1  R5  R9       etc  Next  an empty hash table is constructed for each partition of the input relation R  To reduce the number of cache misses that are incurred during the next  probe  phase  each bucket of this hash table is sized so that it    ts on a few cache lines  Each thread scans every tuple t in its partition  extracts the join key k  and then hashes this key using a hash function h      Then  the tuple t is appended to the end of the hash bucket h k   creating a new hash bucket if necessary  If the partition phase was omitted  then all the threads share the hash table  and writes to each hash bucket have to be protected by a latch  The build phase is over when all the n threads have processed all the assigned partitions  3 3 Probe phase The probe phase schedules work to the n threads in a manner similar to the scheduling during the build phase  described above  Namely  if no partitioning has been done  then all the threads are assigned to S  and they synchronize before accessing the read cursor for S  Otherwise  the thread i is assigned to partitions Si 0   n  Si 1   n  Si 2   n  etc  During the probe phase  each thread reads every tuple s from its assigned partition and extracts the key k  It then checks if the key of each tuple r stored in hash bucket h k  matches k  This check is necessary to    lter out possible hash collisions  If the keys match  then the tuples r and s are joined to form the output tuple  If the output is materialized  it is written to an output bu   er that is private to the thread  Notice that there is parallelism even inside the probe phase  looking up the key for each tuple r in a hash bucket and comparing it to k can be parallelized with the construction of the output tuple  which primarily involves shu   ing bytes from tuples r and s   See Section 4 10 for an experiment that explores this further   3 4 Hash Join Variants The algorithms presented above outline an interesting design space for hash join algorithms  In this paper  we focus on the following four hash join variations  1  No partitioning join  An implementation where partitioning is omitted  This implementation creates a shared hash table in the build phase  2  Shared partitioning join  The    rst non blocking partitioning algorithm of Section 3 1 1  where all the threads partition both input sources into shared partitions  Synchronization through a latch is necessary before writing to the shared partitions  3  Independent partitioning join  The second nonblocking partitioning algorithm of Section 3 1 1  where all the threads partition both sources and create private partitions  4  Radix partitioning join  An implementation where each input relation is stored in a single  contiguous memory region  Then  each thread participates in the radix partitioning  as described in Section 3 1 2  4  EXPERIMENTAL EVALUATION We have implemented the hash join algorithms described in Section 3 4 in a stand alone C   program  The program    rst loads data from the disk into main memory  Data is organized in memory using traditional slotted pages  The join algorithms are run after the data is loaded in memory  Since the focus of this work in on memory resident datasets  we do not consider the time to load the data into main memory and only report join completion times  For our workload  we wanted to simulate common and expensive join operations in decision support environments  The execution of a decision support query in a data warehouse typically involves multiple phases  First  one or more dimension relations are reduced based on the selection constraints  Then  these dimension relations are combined into an intermediate one  which is then joined with a much larger fact relation  Finally  aggregate statistics on the join output are computed and returned to the user  For example  in the TPC H decision support benchmark  this execution pattern is encountered in at least 15 of the 22 queries  We try to capture the essence of this operation by focusing on the most expensive component  namely the join operation between the intermediate relation R  the outcome of various operations on the dimension relations  with a much larger fact relation S  To allow us to focus on the core join performance  we initially do not consider the cost of materializing Intel Nehalem CPU Xeon X5650   2 67GHz Cores 6 Contexts per core 2 Cache size  sharing 12MB L3  shared Memory 3x 4GB DDR3 Sun UltraSPARC T2 CPU UltraSPARC T2   1 2GHz Cores 8 Contexts per core 8 Cache size  sharing 4MB L2  shared Memory 8x 2GB DDR2 Table 1  Platform characteristics 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 20 40 60 80 100 120 140 160 180 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 1  Cycles per output tuple for the uniform dataset  the output in memory  adopting a similar method as previous work  7  11   In later experiments  see Section 4 8   we consider the e   ect of materializing the join result     in these cases  the join result is created in main memory and not    ushed to disk  We describe the synthetic datasets that we used in the next section  Section 4 1   In Section 4 2 we give details about the hardware that we used for our experiments  We continue with a presentation of the results in Sections 4 3 and 4 4  We analyze the results further in Sections 4 5 through 4 7  We present results investigating the e   ect of output materialization  and the sensitivity to input sizes and selectivities in Sections 4 8 through 4 10  4 1 Dataset We experimented with three di   erent datasets  which we denote as uniform  low skew and high skew  respectively  We assume that the relation R contains the primary key and the relation S contains a foreign key referencing tuples in R  In all the datasets we    x the cardinalities of R to 16M tuples and S to 256M tuples 1   We picked the ratio of R to S to be 1 16 to mimic the common decision support settings  We experiment with di   erent ratios in Section 4 9  In our experiments both keys and payloads are eight bytes each  Each tuple is simply a  key  payload  pair  so tuples are 16 bytes long  Keys can either be the values themselves  if the key is numeric  or an 8 byte hash of the value in the case of strings  We chose to represent payloads as 8 bytes for two reasons   a  Given that columnar storage is commonly used in data warehouses  we want to simulate storing  key  value  or  key  record id  pairs in the hash table  and  b  make comparisons with existing work  i e   11  4   easier  Exploring alternative ways of constructing hash table entries is not a focus of this work  but has been explored before  15   For the uniform dataset  we create tuples in the relation S such that each tuple matches every key in the relation R with equal probability  For the skewed datasets  we added skew to the distribution of the foreign keys in the relation S   Adding skew to the relation R would violate the primary key constraint   We created two skewed datasets  for two di   erent s values of the Zipf distribution  low skew with s   1 05 and high skew with s   1 25  Intuitively  the most 1 Throughout the paper  M 2 20 and K 2 10   popular key appears in the low skew dataset 8  of the time  and the ten most popular keys account for 24  of the keys  In comparison  in the high skew dataset  the most popular key appears 22  of the time  and the ten most popular keys appear 52  of the time  In all the experiments  the hash buckets that are created during the build phase have a    xed size  they always have 32 bytes of space for the payload  and 8 bytes are reserved for the pointer that points to the next hash bucket in case of over   ow  These numbers were picked so that each bucket    ts in a single  last level cache line for both the architectures  We size the hash table appropriately so that no over   ow occurs  4 2 Platforms We evaluated our methods on two di   erent architectures  the Intel Nehalem and the Sun UltraSPARC T2  We describe the characteristics of each architecture in detail below  and we summarize key parameters in Table 1  The Intel Nehalem microarchitecture is the successor of the Intel Core microarchitecture  All Nehalem based CPUs are superscalar processors and exploit instruction level parallelism by using out of order execution  The Nehalem family supports multi threading  and allows two contexts to execute per core  For our experiments  we use the six core Intel Xeon X5650 that was released in Q1 of 2010  This CPU has a uni   ed 12MB  16 way associative L3 cache with a line size of 64 bytes  This L3 cache is shared by all twelve contexts executing on the six cores  Each core has a private 256KB  8 way associative L2 cache  with a line size of 64 bytes  Finally  private 32KB instruction and data L1 caches connect to each core   s load store units  The Sun UltraSPARC T2 was introduced in 2007 and relies heavily on multi threading to achieve maximum throughput  An UltraSPARC T2 chip has eight cores and each core has hardware support for eight contexts  UltraSPARC T2 does not feature out of order execution  Each core has a single instruction fetch unit  a single    oating point unit  a single memory unit and two arithmetic units  At every cycle  each core executes at most two instructions  each taken from two di   erent contexts  Each context is scheduled in a round robin fashion every cycle  unless the context has ini 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 50 100 150 200 250 300 350 400 450 500 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 2  Cycles per output tuple for the low skew dataset  tiated a long latency operation  such as a memory load that caused a cache miss  and has to wait for the outcome  At the bottom of the cache hierarchy of the UltraSPARC T2 chip lies a shared 4MB  16 way associative write back L2 cache  with a line size of 64 bytes  To maximize throughput  the shared cache is physically split into eight banks  Therefore  up to eight cache requests can be handled concurrently  provided that each request hits a di   erent bank  Each core connects to this shared cache through a non blocking  pipelined crossbar  Finally  each core has a 8KB  4 way associative write through L1 data cache with 16 bytes per cache line that is shared by all the eight hardware contexts  Overall  in the absence of arbitration delays  the L2 cache hit latency is 20 cycles  4 3 Results We start with the uniform dataset  In Figure 1  we plot the average number of CPU cycles that it takes to produce one output tuple  without actually writing the output  for a varying number of partitions   Note that to convert the CPU cycles to wall clock time  we simply divide the CPU cycles by the corresponding clock rate shown in Table 1   The horizontal axis shows the di   erent join algorithms  bars    No        Shared        Independent      corresponding to the    rst three hash join variants described in Section 3 4  For the radix join algorithm  we show the best result across any number of passes  bars marked    Radix best      Notice that we assume that the optimizer will always be correct and pick the optimal number of passes  Overall  the build phase takes a very small fraction of the overall time  regardless of the partitioning strategy that is being used  across all architectures  see Figure 1   The reason for this behavior is two fold  First and foremost  the smaller cardinality of the R relation translates into less work during the build phase   We experiment with di   erent cardinality ratios in Section 4 9   Second  building a hash table is a really simple operation  it merely involves copying the input data into the appropriate hash bucket  which incurs a lot less computation than the other steps  such as the output tuple reconstruction that must take place in the probe phase  The performance of the join operation is therefore mostly determined by the time spent partitioning the input relations and probing the hash table  As can be observed in Figure 1 a  for the Intel Nehalem architecture  the performance of the non partitioned join algorithm is comparable to the optimal performance achieved by the partition based algorithms  The shared partitioning algorithm performs best when sizing partitions so that they    t in the last level cache  This    gure reveals a problem with the independent partitioning algorithm  For a high number of partitions  say 128K  each thread will create its own private bu   er  for a total of 128K     12     1 5 million output bu   ers  This high number of temporary bu   ers introduces two problems  First  it results in poor space utilization  as most of these bu   ers are    lled with very few tuples  Second  the working set of the algorithm grows tremendously  and keeping track of 1 5 million cache lines requires a cache whose capacity is orders of magnitude larger than the 12MB L3 cache  The radix partitioning algorithm is not a   ected by this problem  because it operates in multiple passes and limits the number of partition output bu   ers in each pass  Next  we experimented with the Sun UltraSPARC T2 architecture  In Figure 1 b  we see that doing no partitioning is at least 1 5X faster compared to all the other algorithms  The limited memory on this machine prevented us from running experiments with a high number of partitions for the independent partitioning algorithm because of the signi     cant memory overhead discussed in the previous paragraph  As this machine supports nearly    ve times more hardware contexts than the Intel machine  the memory that is required for bookkeeping is    ve times higher as well  To summarize our results with the uniform dataset  we see that on the Intel architecture the performance of the no partitioning join algorithm is comparable to the performance of all the other algorithms  For the Sun UltraSPARC T2  we see that the no partitioning join algorithm outperforms the other algorithms by at least 1 5X  Additionally  the no partitioning algorithm is more robust  as the performance of the other algorithms degrades if the query optimizer does not pick the optimal value for the number of partitions  4 4 Effect of skew We now consider the case when the distribution of foreign keys in the relation S is skewed  We again plot the average time to produce each tuple of the join  in machine cycles  in Figure 2 for the low skew dataset  and in Figure 3 for the high skew dataset 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 100 200 300 400 500 600 700 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 3  Cycles per output tuple for the high skew dataset  Intel Sun Nehalem UltraSPARC T2 NO No   1 No   1 SN Indep    16 Indep    64 L2 S Shared   2048 Shared   2048 L2 R Radix   2048 Radix   2048 Table 2  Shorthand notation and corresponding partitioning strategy   number of partitions  By comparing Figure 1 with Figure 2  we notice that  when using the shared hash table  bar    No    in all graphs   performance actually improves in the presence of skew  On the other hand  the performance of the shared partitioning algorithm degrades rapidly with increasing skew  while the performance of the independent partitioning and the radix partitioning algorithms shows little change on the Intel Nehalem and degrades on the Sun UltraSPARC T2  Moving to Figure 3  we see that the relative performance of the non partitioned join algorithm increases rapidly under higher skew  compared to the other algorithms  The nonpartitioned algorithm is generally 2X faster than the other algorithms on the Intel Nehalem  and more than 4X faster than the other algorithms on the Sun UltraSPARC T2  To summarize these results  skew in the underlying join key values  data skew  manifests itself as partition size skew when using partitioning  For the shared partitioning algorithm  during the partition phase  skew causes latch contention on the partition with the most popular key s   For all partitioning based algorithms  during the probe phase  skew translates into a skewed work distribution per thread  Therefore  the overall join completion time is determined by the completion time of the partition with the most popular key   We explore this behavior further in Section 4 7 1   On the other hand  skew improves performance when sharing the hash table and not doing partitioning for two reasons  First  the no partitioning approach ensures an even work distribution per thread as all the threads are working concurrently on the single partition  This greedy scheduling strategy proves to be e   ective in hiding data skew  Second  performance increases because the hardware handles skew a lot more e   ciently  as skewed memory access patterns cause signi   cantly fewer cache misses  TLB TLB Cycles L3 Instruc  load store miss  tions miss miss partition 0 0 0 0 0 NO build 322 2 2 215 1 0 probe 15 829 862 54 762 557 0 partition 3 578 18 29 096 6 2 SN build 328 8 2 064 0 0 probe 21 717 866 54 761 505 0 partition 11 778 103 31 117 167 257 L2 S build 211 1 2 064 0 0 probe 6 144 35 54 762 1 0 partition 6 343 221 34 241 7 237 L2 R build 210 1 2 064 0 0 probe 6 152 36 54 761 1 0 Table 3  Performance counter averages for the uniform dataset  millions   4 5 Performance counters Due to space constraints  we focus on speci   c partitioning con   gurations from this section onward  We use    NO    to denote the no partitioning strategy where the hash table is shared by all threads  and we use    SN    to denote the case when we create as many partitions as hardware contexts  join threads   except we round the number of partitions up to the next power of two as is required for the radix partitioning algorithm  We use    L2    to denote the case when we create partitions to    t in the last level cache  appending     S    when partitioning with shared output bu   ers  and     R    for radix partitioning  We summarize this notation in Table 2  Notice that the L2 numbers correspond to the best performing con   guration settings in the experiment with the uniform dataset  see Figure 1   We now use the hardware performance counters to understand the characteristics of these join algorithms  In the interest of space  we only present our    ndings from a single architecture  the Intel Nehalem  We    rst show the results from the uniform dataset in Table 3  Each row indicates one particular partitioning algorithm and join phase  and each column shows a di   erent architectural event  First  notice the code path length  It takes  on average  about 55 billion instructions to complete the probe phase and an additional 50  to 65  of that for partitioning  depending on the algorithm of choice  The NO algorithm pays a high cost inTLB TLB Cycles L3 Instruc  load store miss  tions miss miss partition 0 0 0 0 0 NO build 323 3 2 215 1 0 probe 6 433 98 54 762 201 0 partition 3 577 17 29 096 6 1 SN build 329 8 2 064 0 0 probe 13 241 61 54 761 80 0 partition 36 631 79 34 941 67 106 L2 S build 210 5 2 064 0 0 probe 8 024 13 54 762 1 0 partition 5 344 178 34 241 5 72 L2 R build 209 4 2 064 0 0 probe 8 052 13 54 761 1 0 Table 4  Performance counter averages for the high skew dataset  millions   terms of the L3 cache misses during the probe phase  The partitioning phase of the SN algorithm is fast but fails to contain the memory reference patterns that arise during the probe phase in the cache  The L2 S algorithm manages to minimize these memory references  but incurs a high L3 and TLB miss ratio during the partition phase compared to the NO and SN algorithms  The L2 R algorithm uses multiple passes to partition the input and carefully controls the L3 and TLB misses during these phases  Once the cache sized partitions have been created  we see that both the L2 S and L2 R algorithms avoid incurring many L3 and TLB misses during the probe phase  In general  we see fewer cache and TLB misses across all algorithms when adding skew  in Table 4   Unfortunately  interpreting performance counters is much more challenging with modern multi core processors and will likely get worse  Processors have become a lot more complex over the last ten years  yet the events that counters capture have hardly changed  This trend causes a growing gap between the high level algorithmic insights the user expects and the speci   c causes that trigger some processor state that the performance counters can capture  In a uniprocessor  for example  a cache miss is an indication that the working set exceeds the cache   s capacity  The penalty is bringing the data from memory  an operation the costs hundreds of cycles  However  in a multi core processor  a memory load might miss in the cache because the operation touches memory that some other core has just modi   ed  The penalty in this case is looking in some other cache for the data  Although a neighboring cache lookup can be ten or a hundred times faster than bringing the data from memory  both scenarios will simply increment the cache miss counter and not record the cause of this event  To illustrate this point  let   s turn our attention to a case in Table 3 where the performance counter results can be misleading  The probe phase of the SN algorithm has slightly fewer L3 and TLB misses than the probe phase of the NO algorithm and equal path length  so the probe phase of the SN algorithm should be comparable or faster than probe phase of the NO algorithm  However  the probe phase of the NO algorithm is almost 25  faster  Another issue is latch contention  which causes neither L3 cache misses nor TLB misses  and therefore is not reported in the performance counters  For example  when comparing the uniform and high skew numbers for the L2 S algorithm  the number of the L3 cache misses during the high skew experiment is 0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of threads NO SN L2 S L2 R Figure 4  Speedup over single threaded execution  uniform dataset  30  lower than the number of the cache misses observed during the uniform experiment  However  partitioning performance worsens by more than 3X when creating shared partitions under high skew  The performance counters don   t provide clean insights into why the non partitioned algorithm exhibits similar or better performance than the other cache e   cient algorithms across all datasets  Although a cycle breakdown is still feasible at a macroscopic level where the assumption of no contention holds  for example as in Ailamaki et al   1    this experiment reveals that blindly assigning    xed cycle penalties to architectural events can lead to misleading conclusions  4 6 Speedup from SMT Modern processors improve the overall e   ciency with hardware multithreading  Simultaneous multi threading  SMT  permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures  We now evaluate the impact of SMT on the hash join algorithms  We    rst show a speedup experiment for the Intel Nehalem on the uniform dataset in Figure 4  We start by dedicating each thread to a core  and once we exceed the number of available physical cores  six for our Intel Nehalem   we then start assigning threads in a round robin fashion to the available hardware contexts  We observe that the algorithms behave very di   erently when some cores are idle  fewer than six threads  versus in the SMT region  more than six threads   With fewer than six threads all the algorithms scale linearly  and the NO algorithm has optimal speedup  With more than six threads  the NO algorithm continues to scale  becoming almost 11X faster than the single threaded version when using all available contexts  The partitioning based algorithms SN  L2 S and L2 R  however  do not exhibit this behavior  The speedup curve for these three algorithms in the SMT region either    attens completely  SN algorithm   or increases at a reduced rate  L2 R algorithm  than the nonSMT region  In fact  performance drops for all partitioning algorithms for seven threads because of load imbalance  a single core has to do the work for two threads   This imbalance can be ameliorated through load balancing  a technique that we explore in Section 4 7 1  Uniform 6 threads 12 threads Improvement NO 28 23 16 15 1 75X SN 34 04 25 62 1 33X L2 S 19 27 18 13 1 06X L2 R 14 46 12 71 1 14X High skew 6 threads 12 threads Improvement NO 9 34 6 76 1 38X SN 19 50 17 15 1 14X L2 S 38 37 44 87 0 86X L2 R 15 04 13 61 1 11X Table 5  Simultaneous multi threading experiment on the Intel Nehalem  showing billions of cycles to join completion and relative improvement  Uniform 8 threads 64 threads Improvement NO 37 30 12 64 2 95X SN 55 70 22 25 2 50X L2 S 51 62 23 86 2 16X L2 R 46 62 18 88 2 47X High skew 8 threads 64 threads Improvement NO 23 92 11 67 2 05X SN 70 52 49 54 1 42X L2 S 73 91 221 01 0 33X L2 R 66 01 43 16 1 53X Table 6  Simultaneous multi threading experiment on the Sun UltraSPARC T2  showing billions of cycles to join completion and relative improvement  We summarize the bene   t of SMT in Table 5 for the Intel architecture  and in Table 6 for the Sun architecture  For the Intel Nehalem and the uniform dataset  the NO algorithm bene   ts signi   cantly from SMT  becoming 1 75X faster  This algorithm is not optimized for cache performance  and as seen in Section 4 5  causes many cache misses  As a result  it provides more opportunities for SMT to ef     ciently overlap the memory accesses  On the other hand  the other three algorithms are optimized for cache performance to di   erent degrees  Their computation is a large fraction of the total execution time  therefore they do not bene   t signi   cantly from using SMT  In addition  we notice that the NO algorithm is around 2X slower than the L2 R algorithm without SMT  but its performance increases to almost match the L2 R algorithm performance with SMT  For the Sun UltraSPARC T2  the NO algorithm also bene   ts the most from SMT  In this architecture the code path length  i e  instructions executed  has a direct impact on the join completion time  and therefore the NO algorithm performs best both with and without SMT  As the Sun machine cannot exploit instruction parallelism at all  we see increased bene   ts from SMT compared to the Intel architecture  When comparing the high skew dataset with the uniform dataset across both architectures  we see that the improvement of SMT is reduced  The skewed key distribution incurs fewer cache misses  therefore SMT loses opportunities to hide processor pipeline stalls  4 7 Synchronization Synchronization is used in multithreaded programs to guarantee the consistency of shared data structures  In our join implementations  we use barrier synchronization when all the threads wait for tasks to be completed before they can proceed to the next task   For example  at the end of each pass of the radix partition phase  each thread has to wait until all other threads complete before proceeding   In this section  we study the e   ect of barrier synchronization on the performance of the hash join algorithm  In the interest of space  we only present results for the Intel Nehalem machine  Since the radix partitioning algorithm wins over the other partitioning algorithms across all datasets  our discussion only focuses on results for the non partitioned algorithm  NO  and the radix partitioning algorithm  L2 R   Synchronization has little impact on the non partitioned  NO  algorithm for both the uniform and the high skew datasets  regardless of the number of threads that are running  The reason for this behavior is the simplicity of the NO algorithm  First  there is no partition phase at all  and each thread can proceed independently in the probe phase  Therefore synchronization is only necessary during the build phase  a phase that takes less than 2  of the total time  see Figure 1   Second  by dispensing with partitioning  this algorithm ensures an even distribution of work across the threads  as all the threads are working concurrently on the single shared hash table  We now turn our attention to the radix partitioning algorithm  and break down the time spent by each thread  Unlike the non partitioned algorithm  the radix partitioning algorithm is signi   cantly impacted by synchronization on both the uniform and the high skew datasets  Figure 5 a  shows the time breakdown for the L2 R algorithm when running 12 threads on the Intel Nehalem machine with the high skew dataset  Each histogram in this    gure represents the execution    ow of a thread  The vertical axis can be viewed as a time axis  in machine cycles   White rectangles in these histograms represent tasks  the position of each rectangle indicates the beginning time of the task  and the height represents the completion time of this task for each thread  The gray rectangles represent the waiting time that is incurred by a thread that completes its task but needs to synchronize with the other threads before continuing  In the radix join algorithm  we can see    ve expensive operations that are synchronized through barriers   1  computing the threadprivate histogram   2  computing the global histogram   3  doing radix partitioning   4  building a hash table for each partition of the relation R  and  5  probing each hash table with a partition from the relation S  The synchronization cost of the radix partitioning algorithm accounts for nearly half of the total join completion time for some threads  The synchronization cost is so high under skew primarily because it is hard to statically divide work items into equally sized subtasks  As a result  faster threads have to wait for slower threads  For example  if threads are statically assigned to work on partitions in the probe phase  the distribution of the work assigned to the threads will invariably also be skewed  Thus  the thread processing the partition with the most popular key becomes a bottleneck and the overall completion time is determined by the completion time of the partition with the most popular keys  In Figure 5 a   this is thread 3 0 2 4 6 8 10 12 14 1 2 3 4 5 6 7 8 9 10 11 12 Cycles  billions  Thread ID work wait  a  High skew dataset 0 2 4 6 8 10 12 14 1 2 3 4 5 6 7 8 9 10 11 12 Cycles  billions  Thread ID work wait  b  High skew dataset with work stealing Figure 5  Time breakdown of the radix join  4 7 1 Load balancing If static work allocation is the problem  then how would the radix join algorithm perform under a dynamic work allocation policy and highly skewed input  To answer this question  we tweaked the join algorithm to allow the faster threads that have completed their probe phase to steal work from other slower threads  In our implementation  the unit of work is a single partition  In doing so  we slightly increase the synchronization cost because work queues need to now be protected with latches  but we balance the load better  In Figure 5 b  we plot the breakdown of the radix partitioning algorithm  L2 R  using this work stealing policy when running on the Intel Nehalem machine with the high skew dataset  Although the work is now balanced almost perfectly for the smaller partitions  the partitions with the most popular keys are still a bottleneck  In the high skew dataset  the most popular key appears 22  of the time  and thread 3 in this case has been assigned only a single partition which happened to correspond to the most popular key  In comparison  for this particular experiment  the NO algorithm can complete the join in under 7 billion cycles  Table 4   and hence is 1 9X faster  An interesting area for future work is load balancing techniques that permit work stealing at a    ner granularity than an entire partition with a reasonable synchronization cost  To summarize  under skew  a load balancing technique improves the performance of the probe phase but does not address the inherent ine   ciency of all the partitioning based algorithms  In essence  there is a coordination cost to be paid for load balancing  as thread synchronization is necessary  Skew in this case causes contention  stressing the cache coherence protocol and increasing memory tra   c  On the other hand  the no partitioning algorithm does skewed memory loads of read only data  which is handled very e   ciently by modern CPUs through caching  4 8 Effect of output materialization Early work in main memory join processing  7  did not take into account the cost of materialization  This decision was justi   ed by pointing out that materialization comes at a    xed price for all algorithms and  therefore  a join algorithm will be faster regardless of the output being materialized or Machine NO SN L2 S L2 R Intel Nehalem 23  4  7  10  Sun UltraSPARC T2 29  21  20  23  Table 7  Additional overhead of materialization with respect to total cycles without materialization on the uniform dataset  Scale 0 5 Scale 1 Scale 2 NO 7 65  0 47X  16 15  1 00X  62 27  3 86X  SN 11 76  0 46X  25 62  1 00X  98 82  3 86X  L2 S 8 47  0 47X  18 13  1 00X  68 48  3 78X  L2 R 5 82  0 46X  12 71  1 00X  DNF Table 8  Join sensitivity with varying input cardinalities for the uniform dataset on Intel Nehalem  The table shows the cycles for computing the join  in billions  and the relative di   erence to scale 1  discarded  Recent work by Cieslewicz et al   3  highlighted the trade o   s involved when materializing the output  In Table 7 we report the increase in the total join completion time when we materialize the output in memory for the uniform dataset and the partitioning strategies described in Table 2  If the join operator is part of a complex query plan  it is unlikely that the entire join output will ever need to be written in one big memory block  but  even in this extreme case  we see that no algorithm is being signi   cantly impacted by materialization  4 9 Cardinality experiments We now explore how sensitive our    ndings are to variations in the cardinalities of the two input relations  Table 8 shows the results when running the join algorithms on the Intel Nehalem machine  The numbers obtained from the uniform dataset  described in detail in Section 4 1  are shown in the middle column  We    rst created one uniform dataset where both relations are half the size  scale 0 5   This means the relation R has 8M tuples and the relation S has 128M tuples  We also created a uniform dataset where both relations are twice the size  scale 2   i e  the relation R has 32M tuples and the relation S has 512M tuples  The scale 2 dataset occupies 9GB out of the 12GB of memory our system has  Table 1  and leaves little working memory  but the serial0 100 200 300 400 500 600 700 800 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe Figure 6  Experiment on Intel Nehalem with uniform dataset and  R   S   access pattern allows performance to degrade gracefully for all algorithms but the L2 R algorithm  The main memory optimizations of the L2 R algorithm cause many random accesses which hurt performance  We therefore mark the L2 R algorithm as not    nished  DNF   We now examine the impact of the relative size of the relations R and S  We    xed the cardinality of the relation S to be 16M tuples  making  R     S   and we plot the cycles per output tuple for the uniform dataset when running on the Intel Nehalem in Figure 6  First  the partitioning time increases proportionally to  R     S   Second  the build phase becomes signi   cant  taking at least 25  of the total join completion time  The probe phase  however  is at most 30  slower  and less a   ected by the</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Database Engines on Multicores  Why Parallelize When You Can Distribute   ###   Tudor Ioan Salomie Ionut Emanuel Subasu Jana Giceva Gustavo Alonso Systems Group  Computer Science Department ETH Zurich  Switzerland  tsalomie  subasu  gicevaj  alonso  inf ethz ch Abstract Multicore computers pve to having to redesign the database eose a substantial challenge to infrastructure software such as operating systems or databases  Such software typically evolves slower than the underlying hardware  and with multicore it faces structural limitations that can be solved only with radical architectural changes  In this paper we argue that  as has been suggested for operating systems  databases could treat multicore architectures as a distributed system rather than trying to hide the parallel nature of the hardware  We    rst analyze the limitations of database engines when running on multicores using MySQL and PostgreSQL as examples  We then show how to deploy several replicated engines within a single multicore machine to achieve better scalability and stability than a single database engine operating on all cores  The resulting system offers a low overhead alternatingine while providing signi   cant performance gains for an important class of workloads  Categories and Subject Descriptors H 2 4  Information Systems   DATABASE MANAGEMENT   Systems General Terms Design  Measurement  Performance Keywords Multicores  Replication  Snapshot Isolation ### 1  Introduction Multicore architectures pose a signi   cant challenge to existing infrastructure software such as operating systems  Baumann 2009  Bryant 2003  Wickizer 2008   web servers  Veal     This work is partly funded by the Swiss National Science Foundation under the programs ProDoc Enterprise Computing and NCCR MICS Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  EuroSys   11  April 10   13  2011  Salzburg  Austria  Copyright  c 2011 ACM 978 1 4503 0634 8 11 04       10 00 2007   or database engines  Hardavellas 2007  Papadopoulos 2008   In the case of relational database engines  and in spite of the intense research in the area  there are still few practical solutions that allow a more    exible deployment of databases over multicore machines  We argue that this is the result of the radical architectural changes that many current proposals imply  As an alternative  in this paper we describe a solution that works well in a wide range of use cases and requires no changes to the database engine  Our approach is intended neither as a universal solution to all use cases nor as a replacement to a much needed complete redesign of the database engine  Rather  it presents a new architecture for database systems in the context of multicores relying on well known distributed system techniques and proving to be widely applicable for many workloads  1 1 Background and Trends Most commercial relational database engines are based on a decades old design optimized for disk I O bottlenecks and meant to run on single CPU computers  Concurrency is achieved through threads and or processes with few of them actually running simultaneously  Queries are optimized and executed independently of each other with synchronization enforced through locking of shared data structures  All these features make the transition to modern hardware platforms dif   cult  It is now widely accepted that modern hardware  be it multicore  or many other developments such as    ash storage or the memory CPU gap  create problems for current database engine designs  For instance  locking has been shown to be a major deterrent for scalability with the number of cores  Johnson 2009a  and the interaction between concurrent queries when updates or whole table scans are involved can have a severe impact on overall performance  Unterbrunner 2009   As a result  a great deal of work proposed either ways to modify the engine or to completely redesign the architecture  Just to mention a few examples  there are proposals to replace existing engines with pure main memory scans  Unterbrunner 2009   to use dynamicprogramming optimizations to increase the degree of parallelism for query processing  Han 2009   to use helper cores to ef   ciently pre fetch data needed by working threads  Papadopoulos 2008   to modularize the engine into a sequence of stages  obtaining a set of self contained modules  which improve data locality and reduce cache problems  Harizopoulos 2005   or to remove locking contention from the storage engine  Johnson 2009a b   Commercially  the    rst engines that represent a radical departure from the established architecture are starting to appear in niche markets  This trend can be best seen in the several database appliances that have become available  e g   TwinFin of IBM Netezza  and SAP Business Datawarehouse Accelerator  see  Alonso 2011  for a short overview   1 2 Results Inspired by recent work in multikernel operating systems  Baumann 2009  Liu 2009  Nightingale 2009  Wentzlaff 2009   our approach deploys a database on a multicore machine as a collection of distributed replicas coordinated through a middleware layer that manages consistency  load balancing  and query routing  In other words  rather than redesigning the engine  we partition the multicore machine and allocate an unmodi   ed database engine to each partition  treating the whole as a distributed database  The resulting system  Multimed  is based on techniques used in LANs as part of computer clusters in the Ganymed system  Plattner 2004   adapted to run on multicore machines  Multimed uses a primary copy approach  the master database  running on a subset of the cores  The master database receives all the update load and asynchronously propagates the changes to satellite databases  The satellites store copies of the database and run on non overlapping subsets of the cores  These satellites are kept in sync with the master copy  with some latency  and are used to execute the read only load  queries   The system guarantees global consistency in the form of snapshot isolation  although alternative consistency guarantees are possible  Our experiments show that a minimally optimized version of Multimed exhibits both higher throughput with lower response time and more stable behavior as the number of cores increase than standalone versions of PostgreSQL and MySQL for standard benchmark loads  TPC W   1 3 Contribution The main contribution of Multimed is to show that a sharenothing design similar to that used in clusters works well in multicore machines  The big advantage of such an approach is that the database engine does not need to be modi   ed to run in a multicore machine  The parallelism offered by multicore is exploited through the combined performance of a collection of unmodi   ed engines rather than through the optimization of a single engine modi   ed to run on multiple cores  An interesting aspect of Multimed is that each engine is restricted in the number of cores and the amount of memory it can use  Yet  the combined performance of several engines is higher than that of a single engine using all the cores and all the available memory  Like any database  Multimed is not suitable for all possible use cases but it does support a wide range of useful scenarios  For TPC W  Multimed can support all update rates  from the browsing and shopping mix to the ordering mix  with only a slight loss of performance for the ordering mix  For business intelligence and data warehouse loads  Multimed can offer linear scalability by assigning more satellites to the analytical queries  Finally  in this paper we show a few simple optimizations to improve the performance of Multimed  For instance  partial replication is used to reduce the memory footprint of the whole system  Many additional optimizations are possible over the basic design  including optimizations that do not require modi   cation of the engine   e g   data placement strategies  and specialization of the satellites through the creation of indexes and data layouts tailored to given queries   The paper is organized as follows  In the next section we motivate Multimed by analyzing the behavior of PostgreSQL and MySQL when running on multicore machines  The architecture and design of Multimed are covered in section 3  while section 4 discusses in detail our experimental evaluation of Multimed  Section 6 discusses related work and section 7 concludes the paper  2  Motivation To explore the behavior of traditional architectures in more detail  we have performed extensive benchmarks over PostgreSQL and MySQL  open source databases that we can easily instrument and where we can map bottlenecks to concrete code sequences   Our analysis complements and con     rms the results of similar studies done on other database engines over a variety of multicore machines  Hardavellas 2007  Johnson 2009b  Papadopoulos 2008   The hardware con   guration and database settings used for running the following experiments are described in section 4 1  The values for L2 cache misses and context switches were measured using a runtime system pro   ler  OPro   le   2 1 Load interaction Conventional database engines assign threads to operations and optimize one query at a time  The execution plan for each query is built and optimized as if the query would run alone in the system  As a result  concurrent transactions can signi   cantly interfere with each other  This effect is minor in single CPU machines where real concurrency among threads is limited  In multicores  the larger number of hardware contexts leads to more transactions running in parallel which in turn ampli   es load interaction  0  200  400  600  800  1000  1200  1400  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  a  Normal Browsing mix  0  1000  2000  3000  4000  5000  6000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  b  Browsing without BestSellers  0  50  100  150  200  250  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  c  BestSellers query only  0  10  20  30  40  50  60  70 Browsing Browsing  without  BestSellers BestSellers  Only L2 Data Cache Miss Ratio     4 Cores   100 Clients 4 Cores   1000 Clients 48 Cores   100 Clients 48 Cores   1000 Clients  d  PostgreSQL  L2 data cache miss ratio  0  20  40  60  80  100 Browsing Browsing  without  BestSellers BestSellers  Only PostgreSQL slock cache miss percentage     4 Cores   100 Clients 4 Cores   1000 Clients 48 Cores   100 Clients 48 Cores   1000 Clients  e  PostgreSQL  s lock cache misses  0  50  100  150  200  250  300  350 PerfectOverUnderPerfectOverUnderPerfectOverThread  yields to scheduler Transaction Mutex RW Shared Row Latch Exclusive Row Latch             load  48 Threads             load  24 Threads             load  12 Threads  f  MySQL  Context switches Figure 1  Current databases on multicore We have investigated load interaction in both PostgreSQL and MySQL using the Browsing mix of the TPC W Benchmark  see below for details on the experimental setup   PostgreSQL   s behavior with varying number of clients and cores is shown for the Browsing mix in    gure 1 a   for all other queries in the mix except BestSellers in    gure 1 b   and for the BestSellers query only in    gure 1 c   For the complete mix     gure 1 a    we observe a clear performance degradation with the number of cores  We traced the problem to the BestSellers query  an analytical query that is performing scans and aggregation functions over the three biggest tables in the database  On one hand the query locks a large amount of resources and  while doing this  causes a large amount of context switches  On the other hand all the concurrent queries have to wait until the BestSellers query releases the locked resources  When this query is removed from the mix     gure 1 b   the throughput increases by almost    ve times and now it actually improves with the number of cores  When running the BestSellers query alone     gure 1 c    we see a low throughput due to the interference among concurrently running queries and  again  low performance as the number of cores increases  The interesting aspect of this experiment is that BestSellers is a query and  as such  is not doing any updates  The negative load interaction it causes arises from the competition for resources  which becomes worse as the larger number of cores allows us to start more queries concurrently  Similar effects have been observed in MySQL  albeit for loads involving full table scans  Unterbrunner 2009   Full table scans require a lot of memory bandwidth and slow down any other concurrent operation  providing another example of negative load interaction that becomes worse as the number of cores increases  2 2 Contention One of the reasons why loads interact with each other is contention  Contention in databases is caused mainly by concurrent access to locks and synchronization primitives  To analyze this effect in more detail  we have pro   led PostgreSQL while running the BestSellers query  The individual run time for this query  running alone in the system  is on average less than 80ms  indicating that there are no limitations in terms of indexing and data organization  Figure 1 d  shows the L2 data cache misses for the full Browsing mix  the Browsing mix without the BestSellers and the BestSellers query alone  The L2 data cache miss ratio was computed using the expression below based on measured values for L2 cache misses  L2 cache    lls and L2 requests  using the CPU performance counters   We have done individual measurements for each CPU core  but as there are no signi   cant differences between the cores  we used the averaged values of the measured metrics  L2DC M iss Ratio   100    L2C ache Misses  L2C ache Fills   L2Requests With more clients and cores  we see a high increase in cache misses for the workloads containing the BestSellers query  We have traced this behavior to the    s lock     spin lock  function  which is used in PostgreSQL to control access to the shared buffers data structures  held in shared memory   Every time a lock can not be acquired  a context switch takes place  forcing an update of the L2 cache  Figure 1 e  shows that the time spent on the    s lock    function increases with both clients and cores  only when the BestSellers query is involved  We would expect to see an increase with the number of clients but not with more cores  Removing again the BestSellers query from the mix  we observe that it is indeed the one that causes PostgreSQL to waste CPU cycles on the    s lock    function as the number of cores increases  Finally  looking at the    s lock    while running only the BestSellers query we see that it dictates the behavior of the entire mix  The conclusion from these experiments is that  as the number of cores and clients increase  the contention on the shared buffers signi   cantly degrades performance  more memory leads to more data under contention  more cores just increase the contention  This problem that has also been observed by Boyd Wickizer  2010   The performance of MySQL for the Browsing mix with different number of cores and clients is shown in    gure 5  MySQL   s InnoDB storage engine acts as a queuing system  it has a    xed number of threads that process client requests  storage engine threads   If more client requests arrive than available threads  MySQL will buffer them until the previous ones have been answered  In this way MySQL is not affected by the number of clients but it shows the same pathological behavior as PostgreSQL with the number of cores  more cores result in lower throughput and higher response times  While running this experiment  we monitored the times a thread had to yield to the OS due to waits for a lock or a latch  Figure 1 f  presents the number of thread yields per transaction for different loads on the system  Running one storage engine thread for each CPU core available to MySQL  we looked at three scenarios  underload  a total of 12 clients   perfect load  same number of clients as storage engine threads  and over load  200 concurrent clients   Running on 12 cores  we see very few thread yields per transaction taking place  This indicates that for this degree of multi programming MySQL has no intrinsic problems  Adding extra cores and placing enough load as to fully utilize the storage engine threads  perfect load and over load scenarios   we see that the number of thread yields per transaction signi   cantly increases  We also observe that the queuing effect in the system does not add extra thread yields  With increasing cores  the contention of acquiring a mutex or a latch increases exponentially  Of the possible causes for the OS thread yields  we observe less than half are caused by the latches that MySQL   s InnoDB storage engine uses for row level locking  The rest are caused by mutexes that MySQL uses throughout its entire code  This implies that there is not a single locking bottleneck  but rather a problem with locking across the entire code base  making it dif   cult to change the system so that it does not become worse with the number of cores  In the case of the BestSellers query  MySQL does not show the same performance degradation issues due to the differences in engine architectures  MySQL has scalability problems with an increasing number of hardware contexts due to the synchronization primitives and contention over shared data structures  2 3 Our approach Load interaction is an intrinsic feature of existing database engines that can only become worse with multicore  Similarly     xing all synchronization problems in existing engines is a daunting task that probably requires major changes to the underlying architecture  The basic insight of Multimed is that we can alleviate the problems of load interaction and contention by separating the load and using the available cores as a pool of distributed resources rather than as a single parallel machine  Unlike existing work that focuses on optimizing the access time to shared data structures  Hardavellas 2007  Johnson 2009b  or aims at a complete redesign of the engine  Harizopoulos 2005  Unterbrunner 2009   Multimed does not require code modi   cations on the database engine  Instead  we use replicated engines each one of them running on a non overlapping subset of the cores  3  The Multimed System Multimed is a platform for running replicated database engines on multicore machines  It is independent of the database engine used  its main component being a middleware layer that coordinates the execution of transactions across the replicated database engines  The main roles of Multimed are   i  mapping database engines to hardware resources   ii  scheduling and routing transactions over the replicated engines and  iii  communicating with the client applications  3 1 Architectural overview From the outside  Multimed follows the conventional clientserver architecture of database engines  Multimed   s client component is implemented as a JDBC Type 3 Driver  Internally  Multimed     gure 2  implements a master slave replication strategy but offers a single system image  i e   the clients see a single consistent system  The master holds a primary copy of the data and is responsible for executing all updates  Queries  the read only part of the load  run on the satellites  The satellites are kept up to date by asynchronously propagating WriteSets from the master  To provide a consistent view  queries can be scheduled to run on a satellite node only after that satellite has done all the updates executed by the master prior to the beginning of the query Figure 2  A possible deployment of Multimed The main components of Multimed are the Communication component  the Dispatcher and the Computational Nodes  The Communication component implements an asynchronous server that allows Multimed to process a high number of concurrent requests  Upon receiving a transaction  Multimed routes the transaction to one of its Computational Nodes  each of which coordinates a database engine  The routing decision is taken by the Dispatcher subsystem  With this architecture  Multimed achieves several goals  First  updates do not interfere with read operations as the updates are executed in the master and reads on the satellites  second  the read only load can be separated across replicas so as to minimize the interference of heavy queries with the rest of the workload  3 2 How Multimed works We now brie   y describe how Multimed implements replication  which is done by adapting techniques of middleware based replication  Plattner 2004  to run in a multicore machine  In a later section we explore the optimizations that are possible in this context and are not available in network based systems  3 2 1 Replication model Multimed uses lazy replication  Gray 1996  between its master and satellite nodes but guarantees a consistent view to the clients  The master node is responsible for keeping a durable copy of the database which is guaranteed to hold the latest version of the data  All the update transactions are executed on the master node as well as any operation requiring special features such as stored procedures  triggers  or user de   ned functions  The satellite nodes hold replicas of the database  These replicas might not be completely up to date at a given moment but they are continuously fed with all the changes done at the master  A satellite node may hold a full or a partial replica of the database  Doing full replication has the advantage of not requiring knowledge of the data allocation for query routing  On the downside  full replication can incur both higher costs in keeping the satellites up to date due to larger update volumes  and lower performance because of memory contention across the replicas  In the experimental section we include an evaluation of partial replication but all the discussions on the architecture of Multimed are done on the basis of full replication to simplify the explanation  Each time an update transaction is committed  the master commits the transaction locally  The master propagates changes as a list of rows that have been modi   ed  A satellite enqueues these update messages and applies them in the same order as they were executed at the master node  Multimed enforces snapshot isolation as a consistency criterion  see  Daudjee 2006  for a description of snapshot isolation and other consistency options such as session consistency in this type of system   In snapshot isolation queries are guaranteed to see all changes that have been committed at the time the transaction started  a form of multiversion concurrency control found today in database engines such as Oracle  SQLServer  or PostgreSQL  When a query enters the system  the Dispatcher needs to decide where it can run the transaction  i e   to which node it should bind it  which may involve some small delay until a copy has all necessary updates  and if multiple options are available  which one to choose  Note that the master node is always capable of running any query without any delay and can be used as a way to minimize latency if that is an issue for particular queries  Within each database  we rely on the snapshot isolation consistency of the underlying database engine  This means that an update transaction will not interfere with read transactions  namely the update transaction is applied on a different    snapshot    of the database and once it is committed  the shadow version of the data is applied on the active one  In this way  Multimed can schedule queries on replicas at the same time they are being updated  3 2 2 WriteSet extraction and propagation In order to capture the changes caused by an update transaction  Multimed uses row level insert  delete and update triggers in the master database on the union of the tables replicated in all the satellites  The triggers    re every time a row is modi   ed and the old and new versions are stored in the context of the transaction  All the changed rows  with their previous and current versions  represent the WriteSet of a transaction  In our system  this mechanism is implemented using SQL triggers and server side functions  This is the only mechanism speci   c to the underlying database but it is a standard feature in today   s engines input   Connection con  Server Count Number scn WriteSet ws     con getWriteSet    if ws getStatementCount     0 then synchronized lock object con commit    ws setSCN scn atomicIncrement     foreach satellite sat do sat enqueue ws   end else con commit    Algorithm 1  WriteSet Propagation When an update transaction is committed on the master node  the WriteSet is extracted  by invoking a server side function   parsed and passed on to each satellite for enqueuing in its update queue  following algorithm 1  A total order is enforced by Multimed over the commit order of the transactions on the master node  This total order needs to be enforced so that it can be respected on the satellite nodes as well  This might be a performance bottleneck for the system  but as we show in the experimental section  the overhead induced by WriteSet extraction and by enforcing a total order over the commits of updates is quite small  In practice  Multimed introduces a small latency in starting a query  while waiting for a suitable replica  but it can execute many more queries in parallel and  often  the execution of each query is faster once started  Thus  the result is a net gain in performance  3 3 Multimed   s components The main components of Multimed are the computational nodes  the dispatcher  the system model and the client communication interface  3 3 1 Computational nodes A Computational Node is an abstraction over a set of hardware  CPUs  memory  etc   and software  database engine and stored data  connection pools  queues  etc  resources  Physically  it is responsible for forwarding queries and updates to its database engine  Each Computational Node runs in its own thread  It is in charge of  i  executing queries   ii  executing updates and  iii  returning results to the clients  Each transaction is bound to a single Computational Node  can be the master  which has the capability of handling all the requests that arrive in the context of a transaction  Multimed has one master Computational Node and any number of satellite Computational Nodes  Upon arrival  queries are assigned a timestamp and dispatched to the    rst satellite available that has all the updates committed up to that timestamp  thereby enforcing snapshot isolation   Satellite nodes do not need to have any durability guarantees  i e   do not need to write changes to disk   In the case of failure of a satellite  no data is lost  as everything is durably committed by the master Computational Node  3 3 2 Dispatcher The Multimed Dispatcher binds transactions to a Computational Node  It routes update transactions to the master node  leaving the read transactions to the satellites  The differentiation of the two types of transactions can be done based on the transaction   s readOnly property  from the JDBC API   The Dispatcher balances load by choosing the most lightly loaded satellite from among those that are able to handle the transaction  The ability of a satellite to handle a transaction is given by the freshness of the data it holds and by the actual data present  in the case of partial replication   The load can be the number of active transactions  the CPU usage  average run time on this node  etc  When no capable satellite is found  the Dispatcher waits until it can bind to a satellite with the correct update level or it may choose to bind the transaction to the master node  3 3 3 System model The system model describes the con   guration of all Computational Nodes  It is used by the Dispatcher when processing client requests  The System Model currently de   nes a static partitioning of the underlying software and hardware resources  dynamic partitioning is left for future work as it might involve recon   guring the underlying database   It is used at start time to obtain a logical and physical description of all the Computational Nodes and of the required connection settings to the underlying databases  It also describes the replication scheme in use  specifying what data is replicated where  the tables that are replicated  and where transactions need to be run  3 3 4 Communication component The communication subsystem  on the server side  has been implemented based on Apache Mina 2 0  Apache Mina   The communication interface is implemented as an asynchronous server using Java NIO libraries  Upon arrival  each client request is passed on to a Computational Node thread for processing  We have implemented the server component of the system as a non blocking message processing system so as to be able to support more concurrent client connections than existing engines  This is important to take advantage of the potential scalability of Multimed as often the management of client connections is a bottleneck in database engines  see the results for PostgreSQL above   3 4 System optimizations Multimed can be con   gured in many different ways and accepts a wide range of optimizations  In this paper we describe a selection to illustrate how Multimed can take advantage of multicore systems in ways that are not possible in conventional engines  On the communication side  the messages received by the server component from the JDBC Type 3 Driver are small  under 100 bytes   By default  multiple messages willbe packed together before being sent  based on Nagle   s algorithm  Peterson 2000    increasing the response time of a request  We disabled this by setting the TCP NODELAY option on the Java sockets  reducing the RTT for messages by a factor of 10 at the cost of a higher number of packets on the network  On the server side  all connections from the Computational Nodes to the database engines are done through JDBC Type 4 Drivers  native protocols  to ensure the best performance  Using our own connection pool increases performance as no wait times are incurred for creating freeing a database connection  At the Dispatcher level  the binding between an external client connection and an internal database connection is kept for as long as possible  This binding changes only when the JDBC readOnly property of the connection is modi   ed  For the underlying satellite node database engines  we can perform database engine speci   c tweaks  For instance  for the PostgreSQL satellites  we turned off the synchronous commit of transactions and increased the time until these reach the disk  Consequently  the PostgreSQL speci   c options like fsync  full page writes and synchronous commit were set to off  the commit delay was set to its maximum limit of 100  000  s  and the wal writer delay was set to 10  000ms  Turning off the synchronous commit of the satellites does not affect the system  since they are not required for durability  Similar optimizations can be done with MySQL although in the experiments we only include the delay writes option  In the experimental section  we consider three optimization levels  C0 implements full data replication on disk for all satellites  This is the na    ve approach  where we expect performance gains from the reduced contention on the database   s synchronization primitives  but also higher disk contention  C1 implements full data replication in main memory for all satellites  thereby reducing the disk contention  C2 implements partial or full data replication in main memory for the satellites and transaction routing at the Dispatcher  This approach uses far less memory than C1  but requires a priori knowledge of the workload to partition the data adequately  satellites will be specialized for running only given queries   For the case of the 20GB database used in our experiments  a specialized replica containing just the tables needed to run the BestSellers query needs only 5 6GB thus allowing us to increase the number of in memory satellite nodes  For CPU bound use cases this approach allows us to easily scale to a large number of satellite nodes  and effectively push the bottleneck to the maximum disk I O that the master database can use  4  Evaluation In this section we compare Multimed with conventional database engines running on multicore  We measure the throughput and response time of each system while running on a different number of cores  clients  and different database sizes  We also characterize the overhead and applicability of Multimed under different workloads  Aiming at a fair comparison between a traditional DBMS and Multimed  we used the TPC W benchmark  which allows us to quantify the behavior under different update loads  4 1 Setup All the experiments were carried out on a four way AMD Opteron Processor 6174 with 48 cores  128GB of RAM and two 146GB 15k RPM Seagate R Savvio R disks in RAID1  Each AMD Magny Cours CPU consists of two dies  with 6 cores per die  Each core has a local L1  128KB  and L2 cache  512KB   Each die has a shared L3 cache  12MB   The dies within a CPU are connected with two HyperTransport  HT  links between each other  each one of them having two additional HT links  For the experiments with three and    ve satellites  each satellite was allocated entirely within a CPU  respectively within a die  to avoid competition for the cache  In the experiments with ten satellites  partial replication was used  making the databases smaller  In this case  each satellite was allocated on four cores for a total of 3 satellites per socket  Two of these satellites are entirely within a die and the third spawns two dies within the same CPU  Due to the small size of the replicas  the point we want to make with partial replication   we have not encountered cache competition problems when satellites share the L3 cache  The hard disks in our machine prevented us from exploring more write intensive loads  In practice  network attached storage should be used  thereby allowing Multimed to support workloads with more updates  Nevertheless  the features and behavior of Multimed can be well studied in this hardware platform  A faster disk would only change at which point the the master hits the I O bottleneck  improving the performance of Multimed even further  The operating system used is a 64 bit Ubuntu 10 04 LTS Server  running PostgreSQL 8 3 7  MySQL 5 1 and Sun Java SDK 1 6  4 2 Benchmark The workload used is the TPC W Benchmark over datasets of 2GB and 20GB  Each run consists of having the clients connect to the database and issue queries and updates  as per the speci   cations of the TPC W mix being run  Clients issue queries for a time period of 30 minutes  without think times  Each experiment runs on a fresh copy of the database  so that dataset evolution does not affect the measurements  For consistent results  the memory and threading parameters of PostgreSQL and MySQL are    xed to the same values for both the standalone and Multimed systems  The clients are emulated by means of 10 physical machines  This way more than 1000 clients can load the target system without incurring overheads due to contention on the client side  Clients are implemented in Java and are used to 0  1000  2000  3000  4000  5000  6000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Transactions second Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  1000  2000  3000  4000  5000  6000  7000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  b  Throughput for 2GB database  0  1000  2000  3000  4000  5000  6000  7000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 5  c  Throughput for 20GB database  0  200  400  600  800  1000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Response Time  msec  Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  5  10  15  20  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  e  Response time for 2GB database  0  5  10  15  20  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 5  f  Response time for 20GB database Figure 3  PostgreSQL  Standalone vs  Multimed  Browsing mix emit the workload as well as to measure the throughput and response time  The TPC W benchmark speci   es three workload mixes  TPC W Browsing  10  updates   TPC W Shopping  20  updates  and TPC W Ordering  50  updates   Out of these three  we focus on the Browsing and Shopping mixes  The Ordering mix is disk intensive and hits an I O bottleneck before any proper CPU usage is seen  The TPC W benchmark speci   es both an application and a database level  We implemented only the database level  as this is the point of interest for this work  Due to the lack of the application level  some features required for correctly implementing the benchmark had to be emulated at the database level  For example the shopping cart  which should reside in the web server   s session state  is present in our implementation as a table in the database  In order to limit the side effects of holding the shopping cart in the database  an upper bound is placed on the number of entries that it can hold  equal to the maximum number of concurrent clients  We have done extensive tests on Multimed  trying to    nd the optimal con   guration to use in the experiments  The number of cores on which the satellites and the master nodes are deployed can be adjusted  Also  the number of cores allocated for Multimed   s middleware code can be con   gured  In the experiments below we mention the number of satellites   S  and the optimization  C0 C2  that were used  4 3 PostgreSQL  Standalone vs  Multimed version This section compares the performance of PostgreSQL and Multimed running on top of PostgreSQL  4 3 1 Query intensive workload Figures 3 a  and 3 d  present the scalability of PostgreSQL compared to Multimed C1  in the case of the 2GB database  and 200 clients  The x axis shows the number of cores used by both Multimed and PostgreSQL  as well as the number of satellites coordinated by Multimed  Both the throughput     gure 3 a   and the response time     gure 3 d   show that the TPC W Browsing mix places a lot of pressure on standalone PostgreSQL  causing severe scalability problems with the number of cores  Multimed running on 4 cores  the master node on 4 cores  and each satellite on 4 cores scales up almost linearly to a total of 40 cores  equivalent of 8 satellites   The limit is reached when the disk I O bound is hit  all queries run extremely fast  leaving only update transactions in the system to run longer  and facing contention on the disk  The gap between the linear scalability line and Multimed   s performance is constant  being caused by the computational resources required by Multimed   s middleware  4 3 2 Increased update workload Figures 3 b  and 3 c  present the throughput of PostgreSQL  running on different number of cores  and of Multimed  running with different con   gurations   as the number of clients increases  Note that PostgreSQL has problems in 0  1000  2000  3000  4000  5000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Transactions second Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  400 clients  0  500  1000  1500  2000  2500  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  b  Throughput for 2GB database  0  500  1000  1500  2000  2500  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200  1400  1600 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Response Time  msec  Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  d  Scalability response time  2GB  400 clients  0  2  4  6  8  10  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  e  Response time for 2GB database  0  2  4  6  8  10  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 4  PostgreSQL  Standalone vs  Multimed  Shopping mix scaling with the number of clients issuing the workload  and its performance at 48 cores is lower than at 12  For both dataset sizes  Multimed  at all optimization levels  outperforms the standalone version of PostgreSQL  The C0 optimization level for Multimed shows higher error bars  as all satellites are going concurrently to disk  in order to persist updates  Switching to the C1 optimization level  we reduce the contention on disk  by using more main memory  We see an improvement of more than 1000 transactions per second between the na    ve C0 and optimized C1 versions of Multimed  Finally  switching to the less generic optimization level C2  Multimed accommodates more satellites in the available memory  and can take advantage of the available computational resources  until a disk I O limit is hit  Using the C2 optimization  the problem of load interaction is also solved by routing the    heavy     analytical  queries to different satellite nodes  of   oading the other nodes in the system  In all these experiments we have used static routing  Note the fact that Multimed retains a steady behavior with increasing number of concurrent clients  up to 1000   without exhibiting performance degradation  Looking at the corresponding response times  even under heavy load  Multimed   s response time is less than 1 second  indicating that Multimed is not only solving the problems of load interaction  but also the client handling limitations of PostgreSQL  For the Shopping mix  standalone PostgreSQL   s performance is slightly better than for the Browsing mix due to the reduced number of heavy queries  Figures 4 a  and 4 d  show that even in the case of the Shopping mix  PostgreSQL can not scale with the number of available cores  on the 2GB database  with 400 clients  Multimed scales up to 16 cores  2 satellites   at which point the disk becomes a bottleneck  Multimed   s performance stays    at with increasing cores  while that of PostgreSQL drops  Figures 4 b  and 4 c  show that PostgreSQL can not scale with the number of clients for this workload either  regardless of the database size  In the case of Multimed  for a small number of clients  all queries run very fast  leaving the updates to compete for the master node  Past 150 clients  the run time of queries increases and the contention on the master node is removed  allowing Multimed to better use the available satellites  We again observe that Multimed   s behavior is steady and predictable with increasing load  Using the C0 optimization level and for a low number of clients  Multimed performs worse than PostgreSQL  especially on the 20GB database  although it is more stable with the number of clients  With more updates in the system and with all of the satellites writing to disk  Multimed is blocked by I O  As in the previous case  the C1 optimization solves the problem  standard deviation is reduced and the throughput increases  The C2 optimization  at the same number of satellites  also gives the system a performance gain as fewer WriteSets need to be applied on the satellites  they run faster   Both in the case of a query intensive workload  Browsing mix  and in the case of increased update workload  Shopping 0  1000  2000  3000  4000  5000  6000 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Transactions second Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  500  1000  1500  2000  2500  3000  3500  4000  4500  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  b  Throughput for 2GB database  0  500  1000  1500  2000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Response time  msec  Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  e  Response time for 2GB database  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 5  MySQL  Standalone vs  Multimed  Browsing mix mix   PostgreSQL does not scale with the number of cores or with the number of clients  regardless of the database size  PostgreSQL   s inability to scale with the number of clients is due to the fact that for each new client a new process is spawned on the server  This might lead to the conclusion that the number of processes is far greater than what the operating system and the hardware can handle  This is disproved by Multimed  which can cope with 1000 clients in spite of the limitations of PostgreSQL  The problem in this case is not the large number of processes in the system  but rather the inability of a single PostgreSQL engine to handle high concurrency  Since Multimed splits the number of clients over a set of smaller sized satellites  it reduces the contention in each engine  resulting in a higher throughput 4 4 MySQL  Standalone vs  Multimed version In this section we compare standalone MySQL to Multimed running on top of MySQL computational nodes  For MySQL  we have done our experiments using its InnoDB storage engine  This engine is the most stable and used storage engine available for MySQL  However it has some peculiar characteristics   i  it acts as a queuing system  allowing just a    xed number of concurrent threads to operate over the data  storage engine threads    ii  it is slower than the PostgreSQL engine for disk operations  In all the results presented below  the number of cores available for MySQL is equal to the number of storage engine threads  Being a queuing system  MySQL will not show a degradation in throughput with the number of clients  but rather exhibits linear increase in response time  For this reason  the experiments for MySQL only go up to 400 clients  4 4 1 Query intensive workload Figures 5 a  and 5 d  present the ability of the standalone engine  and of Multimed running on top of it  to scale with the amount of computational resources  in the case of the 2GB database and 200 clients  The x axis  as before  indicates the total number of cores available for MySQL and Multimed  as well as the number of satellites coordinated by Multimed  Each satellite runs on 4 cores  In the case of the TPC W Browsing mix  we notice that MySQL does not scale with the number of cores  Figure 5 a  shows that MySQL performs best at 12 cores  Adding more cores increases contention and performance degrades  The same conclusion can be seen in the throughput and response time plots for both the 2GB and 20GB datasets     gures 5 b  and 5 c    that show the performance of MySQL  running on different number of cores  and of Multimed  running on different con   gurations  with increasing clients  Since the behavior is independent of the dataset  we conclude that the contention is not caused by a small dataset  but rather by the synchronization primitives  i e   mutexes  that are used by MySQL throughout its entire code  In contrast  Multimed scales with the number of cores  Figure 5 a  shows that on the 2GB dataset  Multimed scales up to 6 satellites  at which point the disk I O becomes the 0  1000  2000  3000  4000  5000  6000  7000 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Transactions second Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  500  1000  1500  2000  2500  3000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  b  Throughput for 2GB database  0  500  1000  1500  2000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Response time  msec  Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  e  Response time for 2GB database  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 6  MySQL  Standalone vs  Multimed  Shopping mix bottleneck in the system  and the throughput and response times are    at  The fact that Multimed on top of PostgreSQL scaled in the same test up to 8 satellites corroborates the fact the PostgreSQL   s storage engine is faster than MySQL   s InnoDB for this workload  The three con   gurations that we have run for Multimed show that by replicating data  Multimed can outperform standalone MySQL by a factor of 2  before it reaches the disk I O bound  The C0 con   guration of Multimed shows a behavior similar to standalone MySQL   s best run  Removing this contention on disk from Multimed  by switching to its C1 con   guration  increases performance  The C2 optimization does not yield better performance than C1  The system is already disk bound and load interaction does not in   uence MySQL for this workload  To improve performance here  a faster disk or lower I O latency would be needed  4 4 2 Increased update workload The scalability plot     gure 6 a    shows that MySQL performs best at 8 cores  With more cores performance degrades  con   rming that contention is the bottleneck  not disk I O  Multimed scales up to 16 cores  at which point the throughput    attens con   rming that the disk becomes the bottleneck  Figures 6 b  and 6 c  show that on larger datasets data contention decreases  allowing standalone MySQL to perform better  On the 2GB database  Multimed brings an improvement of 3x  In the case of the 20GB database  Multimed achieves a 1 5x improvement  5  Discussion Multimed adapts techniques that are widely used in database clusters  As a database replication solution  Multimed inherits many of the characteristics of replicated systems and database engines  In this section we discuss such aspects to further clarify the effectiveness and scope of Multimed  5 1 Overhead The main overhead introduced by Multimed over a stand alone database is latency  Transactions are intercepted by Multimed before being forwarded either to the master or to a satellite  In the case the transactions go to a satellite  there might be further delay while waiting for a satellite with the correct snapshot  Multimed works because  for a wide range of database loads  such an increase in latency is easily compensated by the reduction in contention between queries and the increase in the resources available for executing each query  Although satellite databases in Multimed have fewer resources  they also have less to do  For the appropriate workloads  Multimed is faster because it separates loads across databases so that each can answer fewer queries faster than a large database can answer all the queries 5 2 Loads supported There is no database engine that is optimal for all loads  Stonebraker 2008   Multimed is a replication based solution and  hence  it has a limitation in terms of how many updates can be performed as all the updates need to be done at the master  Although this may appear a severe limitation  it is not so in the context of database applications  As the experiments above show  Multimed provides substantial performance improvements for the TPC W browsing and shopping mixes  For the ordering mix  with a higher rate of updates  Multimed offers similar performance as the single database since the bottleneck in both cases is the disk  Multimed can be used to linearly scale read dominated loads such as those found in business intelligence applications and data warehousing  For instance  it is possible to show linear scale up of Multimed by simply assigning more satellites to complex analytical queries  As a general rule  the more queries and the more complex the queries  the better for Multimed  Workloads with high update rates and without complex read operations are less suitable for Multimed     and indeed any primary copy replication approach     because the master becomes the bottleneck  regardless of why it becomes the bottleneck  CPU  memory  or disk   In cluster based replication  this problem is typically solved by simply assigning a larger machine to the master  Multimed can likewise be con     gured to mitigate this bottleneck with a larger allocation of resources  cores  memory  to the master  5 3 Con   guration Tuning and con   guring databases is a notoriously dif   cult problem  Some commercial database engines are known to provide thousands of tuning knobs  In fact  a big part of the impetus behind the autonomic computing initiative of a few years ago was driven by the need to automatically tune databases  Similarly  tuning Multimed requires knowledge of database loads  knowledge of the engines used  and quite a bit of experimentation to    nd the right settings for each deployment  The advantage of Multimed over a stand alone database is that the number of global tuning knobs is less as each element of the system needs to be tailored to a speci   c load  The master can be tuned for writes  the satellites for reads  It is even possible to con   gure Multimed so that a satellite answers only speci   c queries  for instance  particularly expensive or long running ones  and then optimize the data placement  indexes  and con   guration of that satellite for that particular type of query  In terms of the interaction with the operating system and the underlying architecture  Multimed can be con   gured using simple rules  allocate contiguous cores to the same satellites  restrict the memory available to each satellite to that next to the corresponding cores  prevent satellites from interfering with each other when accessing system resources  e g   cascading updates instead of updating all satellites at the same time   etc  Such rules are architecture speci   c but rather intuitive  Note as well that Multimed is intended to run in a database server  These are typically powerful machines with plenty of memory  often fast networks and even several network cards  and SAN NAS storage rather than local disks  The more main memory is available  the faster the I O  and the more cores  the more possibilities to tune Multimed to the application at hand and the bigger performance gains to be obtained from Multimed  5 4 Optimizations The version of Multimed presented in the paper does not include any sophisticated optimizations since the goal was to show that the basic concept works  There are  however  many possible optimizations over the basic sys</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Join Processing for Flash SSDs  Remembering Past Lessons ### Jaeyoung Do Univ  of Wisconsin Madison jae cs wisc edu Jignesh M  Patel Univ  of Wisconsin Madison jignesh cs wisc edu ABSTRACT Flash solid state drives  SSDs  provide an attractive alternative to traditional magnetic hard disk drives  HDDs  for DBMS applications  Naturally there is substantial interest in redesigning critical database internals  such as join algorithms  for    ash SSDs  However  we must carefully consider the lessons that we have learnt from over three decades of designing and tuning algorithms for magnetic HDD based systems  so that we continue to reuse techniques that worked for magnetic HDDs and also work with    ash SSDs  The focus of this paper is on recalling some of these lessons in the context of ad hoc join algorithms  Based on an actual implementation of four common ad hoc join algorithms on both a magnetic HDD and a    ash SSD  we show that many of the    surprising    results from magnetic HDD based join methods also hold for    ash SSDs  These results include the superiority of block nested loops join over sort merge join and Grace hash join in many cases  and the bene   ts of blocked I Os  In addition  we    nd that simply looking at the I O costs when designing new    ash SSD join algorithms can be problematic  as the CPU cost is often a bigger component of the total join cost with SSDs  We hope that these results provide insights and better starting points for researchers designing new join algorithms for    ash SSDs ###  1  INTRODUCTION Flash solid state drives  SSDs  are actively being considered as storage alternatives to replace or dramatically reduce the central role of magnetic hard disk drives  HDDs  as the main choice for storing persistent data  Jim Gray   s prediction of    Flash is disk  disk is tape  and tape is dead     7  is coming close to reality in many applications  Flash SSDs  which are made by packaging  NAND     ash chips  o   er several advantages over magnetic HDDs including faster random reads and lower power consumption  Moreover  as    ash densities continue to double as predicted in  9   and prices continue to drop  the appeal of    ash SSDs for DBMSs increases  In fact  vendors such as Fusion IO and HP sell    ashPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Proceedings of the Fifth International Workshop on Data Management on New Hardware  DaMoN 2009  June 28  2009  Providence  Rhode Island Copyright 2009 ACM 978 1 60558 701 1     10 00  based devices as I O accelerators for many data intensive workloads  The appeal of    ash SSDs is also attracting interest in redesigning various aspects of DBMS internals for    ash SSDs  One such aspect that is becoming attractive as a research topic is join processing algorithms  as it is well   known that joins can be expensive and can play a critical role in determining the overall performance of the DBMS  While such e   orts are well motivated  we want to approach a redesign of database query processing algorithms by clearly recalling the lessons that the community has learnt from over three decades of research in query processing algorithms  The focus of this paper is on recalling some of the important lessons that we have learnt about e   cient join processing in magnetic HDDs  and determining if these lessons also apply to joins using    ash SSDs  In addition  if previous techniques for tuning and improving the join performance also work for    ash SSDs  then it also changes what are interesting starting point for comparing new SSD based join algorithms  In the case of join algorithms  a lot is known about how to optimize joins with magnetic HDDs to use the available bu   er memory e   ectively  and to account for the characteristics of the I O subsystem  Speci   cally  the paper by Haas et al   8  derives detailed formulae for bu   er allocation for various phases of common join algorithms such as block nested loops join  sort merge join  Grace hash join  and hybrid hash join  Their results show that the right bu   er pool allocation strategy can have a huge impact     upto 400  improvements in some cases  Furthermore  the relative performance of the join algorithms changes once you optimize the bu   er allocations     block nested loops join is much more versatile  and Grace hash join is often not very competitive  The dangers of forgetting these lessons could lead to an incorrect starting point for comparing new    ash SSD join algorithms  For example  the comparison of RARE join  16  with Grace hash join  10  to show the superiority of the RARE join algorithm on    ash SSDs is potentially not the right starting point   It is possible that the RARE join is superior to the best magnetic HDD based join method when run over    ash SSDs  but this question has not been answered conclusively   As we show in this paper  in fact even block nested loops join far outperforms Grace hash join in most cases  on both magnetic HDDs and    ash SSDs  Cautiously  we note that we have only tried one speci   c magnetic HDD and one speci   c    ash SSD  but even this very    rst test produced interesting results   As part of future work  we want to look at wider range of hardware for both    ash SSDs and magnetic HDDs  The focus of this paper in on investigating four popular ad hoc join algorithms  namely block nested loops join  sortmerge join  Grace hash join  and hybrid hash join  on both    ash SSDs and magnetic HDDs  We start with the best bu   er allocation methods that are proposed in  8  for these join algorithms  and    rst ask the question  What changes for these algorithms as we replace a magnetic HDD with a    ash SSD  Then  we study the e   ect of changing the bu   er pool sizes and the page sizes and examine the impact of these changes on these join algorithms  Our results show that many of the techniques that were invented for joins on magnetic HDDs continue to hold for    ash SSDs  As an example  blocked I O is useful on both magnetic HDDs and    ash SSDs  though for di   erent reasons  In the case of magnetic HDDs  the use of blocked I O amortizes the cost of disk seeks and rotational delays  On the other hand  the bene   t of blocked I O with    ash SSDs comes from amortizing the latency associated with the software layers of    ash SSDs  and generating fewer erase operations when writing data  The remainder of this paper is organized as follows  In Section 2  we brie   y introduce the characteristics of    ash SSDs  Then we introduce the four classic join algorithms with appropriate assumptions and bu   er allocation strategies in Section 3  In Section 4  we explain and discuss the experimental results  After reviewing related work in Section 5  we conclude in Section 6  2  CHARACTERISTICS OF FLASH SSD Flash SSDs are based on NAND    ash memory chips and use a controller to provide a persistent block device interface  A    ash chip stores information in an array of memory cells  A chip is divided into a number of    ash blocks  and each    ash block contains several    ash pages  Each memory cell is set to 1 by default  To change the value to 0  the entire block has to be erased by setting it to 1  followed by selectively programming the desired cells to 0  Read and write operations are performed at the granularity of a    ash page  On the other hand  the time consuming erase operations can only be done at the level of a    ash block  Considering the typical size of a    ash page  4 KB  and a    ash block  64    ash pages   the erase before write constraint can significantly degrade write performance  In addition  most    ash chips only support 10 5    10 6 erase operations per    ash block  Therefore  erase operations should be distributed across the    ash blocks to prolong the service life of    ash chips  These kinds of constraints are handled by a software layer known as    ash translation layer  FTL   The major role of the FTL is to provide address mappings between the logical block addresses  LBAs  and    ash pages  The FTL maintains two kinds of data structures  A direct map from LBAs to    ash pages  and an inverse map for rebuilding the direct map during recovery  While the inverse map is stored on    ash  the direct map is stored on    ash and at least partially in RAM to support fast lookups  If the necessary portion of the direct map is not in RAM  it must be swapped in from    ash as required  While    ash SSDs have no mechanically moving parts  data access still incurs some latency  due to overheads associated with the FTL logic  However  latencies of    ash SSDs are typically much smaller than those of magnetic HDDs  4   3  JOINS In this section  we introduce four classic ad hoc join algorithms that we consider in this paper  namely  block nested loops join  sort merge join  Grace hash join  and hybrid hash join  The two relations being joined are denoted as R and S  We use  R    S  and B to denote the sizes of the relations and the bu   er pool size in pages  respectively  We also assume that  R     S   Each join algorithm needs some extra space to build and maintain speci   c data structures such as a hash table or a tournament tree  In order to model these structures  we use a multiplicative fudge factor  denoted as F  Next we brie   y describe each join algorithm  We also outline the bu   er allocation strategy for each join algorithm  The I O costs for writing the    nal results are omitted in the discussion below  as this cost is identical for all join methods  For the bu   er allocation strategy we directly use the recommendations by Haas et al   8   for magnetic HDDs   which shows that optimizing bu   er allocations can dramatically improve the performance of join algorithms  by 400  in some cases   3 1 Block Nested Loops Join Block nested loops join    rst logically splits the smaller relation R into same size chunks  For each chunk of R that is read  a hash table is built to e   ciently    nd matching pairs of tuples  Then  all of S is scanned  and the hash table is probed with the tuples  To model the additional space required to build a hash table for a chunk of R we use the fudge factor F  so a chunk of size  C   pages uses F C   pages in memory to store a hash table on C  The bu   er pool is simply divided into two spaces  one space  Iouter  is for an input bu   er with a hash table for R chunks  and another one  Iinner  is for an input bu   er to scan S  Note that reading R in chunks of size Iouter F     C    guarantees su   cient memory to build a hash table in memory for that chunk  5   3 2 Sort Merge Join Sort merge join starts by producing sorted runs of each R and S  After R and S are sorted into runs on disk  sortmerge join reads the runs of both relations and merges joins them  We use the tournament sort  a k a  heap sort  in the    rst pass  which produces runs that on average are twice the size of the memory used for the initial sorting  11   We also assume B   p F S  so that the sort merge join  which uses a tournament tree  can be executed in two passes  17   In the    rst pass  the bu   er pool is divided into three parts  an input bu   er  an output bu   er  and working space  WS  to maintain the tournament tree  During the second pass  the bu   er pool is split across all the runs of R and S as evenly as possible  3 3 Grace Hash Join Grace hash join has two phases  In the    rst phase  it reads each relation  applies a hash function to the input tuples  and hashes tuples into buckets that are written to disk  In the second phase  the    rst bucket of R is loaded into the bu   er pool  and a hash table is built on it  Then  the corresponding bucket of S is read and used to probe the hash table  Remaining buckets of R and S are handled in the same way iteratively  We assume B   p F R  to allow for a two phase Grace hash join  17  Join Algorithm First Phase Pass Second Phase Pass Block Nested Loops Join Iinner           y S  y S  B y  S      y S  y  S      y   Dl Dx Iouter   B     Iinner Sort Merge Join I   O             2z   4   B z   8     I         B NR NS     z    Dl Ds   F   R   S   Dl  B WS   B     I     O Grace Hash Join k        R F       R 2F 2 4B R F 2B     WS             F  R  k     O       B k 1     I   B     WS     I   B     k    O Hybrid Hash Join I   O      1 1     B    WS             F  R    WS k     k       F  R     B   I  B   I   O     I   B     WS     WS   B     I     k    O Table 1  Bu   er allocations for join algorithms from Haas et al   8   Ds  Dl  and Dx denote average seek time  latency  and page transfer time for magnetic HDDs  respectively There are two sections in the bu   er pool during the    rst partitioning phase  one input bu   er and an output bu   er for each of the k buckets  We subdivide the output bu   er as evenly as possible based on the number of buckets  and then give the remaining pages  if any  to the input bu   er  In the second phase  a portion of the bu   er pool  WS     is used for the i th bucket of R and its hash table  and the remaining pages are chosen as input bu   er pages to read S  3 4 Hybrid Hash Join As in the Grace hash join  there are two phases in this algorithm  assuming M   p F R   In the    rst phase  the relation R is read and hashed into buckets  Since a portion of the bu   er pool is reserved for an in memory hash bucket for R  this bucket of R is not written to a storage device while the other buckets are  Furthermore  as S is read and hashed  tuples of S matching with the in memory R bucket can be joined immediately  and need not be written to disk  The second phase is the same as Grace hash join  During the    rst phase  the bu   er pool is divided into three parts  one for the input  one for the output of k buckets excluding the in memory bucket  and the working space  WS  for the in memory bucket  The bu   er allocation scheme for the second phase is the same as Grace hash join  3 5 Buffer Allocation Table 1 shows the optimal bu   er allocations for the four join algorithms  for each phase of these algorithms  Note that block nested loops join does not distinguish between the di   erent passes  In this paper  we use the same bu   er allocation method for both    ash SSDs and magnetic HDDs  While these allocations may not be optimal for    ash SSDs  our goal here is to start with the best allocation strategy for magnetic HDDs and explore what happens if we simply use the same settings when replacing a magnetic HDD with a    ash SSD  Comments Values Magnetic HDD cost 129 99    0 36   GB  Magnetic HDD average seek time 12 ms Magnetic HDD latency 5 56 ms Magnetic HDD transfer rate 34 MB s Flash SSD cost 230 99    3 85   GB  Flash SSD latency 0 35 ms Flash SSD read transfer rate 120 MB s Flash SSD write transfer rate 80 MB s Page size 2 KB     32 KB Bu   er pool size 100 MB     600 MB Fudge Factor 1 2 orders table size 5 GB customer table size 730 MB Table 2  Device characteristics and parameter values 4  EXPERIMENTS In this section  we compare the performance of the join algorithms using the optimal bu   er allocations when using a magnetic HDD and a    ash SSD for storing the input data sets  Each data point presented here is the average over three runs  4 1 Experimental Setup We implemented a single thread and light weight database engine that uses the SQLite3  1  page format for storing relational data in heap    les  Each heap    le is stored as a    le in the operating system  and the average page utilization is 80   Our engine has a bu   er pool that allows us to control the allocation of pages to the di   erent components of the join algorithms  The engine has no concurrency control or recovery mechanisms  Our experiments were performed on a Dual Core 3 2GHz Intel Pentium machine with 1 GB of RAM running Red Hat100 200 300 400 500 600 0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 Join Time  sec  Buffer Size  MB  CPU time IO time                         B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H  a  Magnetic HDD 100 200 300 400 500 600 0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 Join Time  sec  Buffer Size  MB  CPU time IO time                         B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H  b  Flash SSD Figure 1  Varying the size of the bu   er pool  8 KB page  blocked I O  Bu   er Pool Size 100 MB 200 MB 300 MB 400 MB 500 MB 600 MB Algorithms Join I O Join I O Join I O Join I O Join I O Join I O BNL 1 64X 2 86X 1 59X 2 62X 1 72X 3 04X 1 73X 2 87X 1 67X 2 79X 1 65X 2 61X SM 1 41X 1 81X 1 45X 2 05X 1 44X 2 06X 1 45X 2 08X 1 43X 2 04X 1 48X 2 20X GH 1 34X 1 54X 1 29X 1 59X 1 41X 1 62X 1 33X 1 62X 1 39x 1 77X 1 30X 1 55X HH 1 45X 1 77X 1 55X 1 90X 1 35X 1 51X 1 51X 1 89X 1 50x 1 78X 1 65X 2 09X Table 3  Speedups of total join times and I O times with    ash SSDs Enterprise 5  For the comparison  we used a 5400 RPM TOSHIBA 320 GB external HDD and a OCZ Core Series 60GB SATA II 2 5 inch    ash SSD  We used wall clock time as a measure of execution time  and calculated the I O time by subtracting the reported CPU time from the wall clock time  Since synchronous I Os were used for all tests  we assumed that there is no overlap between the I O and the computation  We also used direct I Os so that the database engine transfers data directly from to the bu   er pool bypassing the OS cache  so there is no prefetching and double bu   ering   With this setup all join numbers repeated here are    cold    numbers  4 2 Data Set and Join Query As our test query  we used a primary foreign key join between the TPC H  2  customer and the orders tables  generated with a scale factor of 30  The customer table contains 4 500 000 tuples  730 MB   and the orders table has 45 000 000  5 GB   Each tuple of both tables contains an unsigned 4 byte integer key  the customer key   and an average 130 and 90 bytes of padding for the customer and the orders tables respectively  The data for both tables were stored in random order in the corresponding heap    les  Characteristics of the magnetic HDD and the    ash SSD  and parameter values used in these experiments are shown in Table 2  4 3 Effect of Varying the Buffer Pool Size The e   ect of varying the bu   er pool size from 100 MB to 600 MB is shown in Figure 1  for both the magnetic HDD and the    ash SSD  We also used blocked I O to sequentially read and write multiple pages in each I O operation  The size of the I O block is calculated for each algorithm using the equations shown in Table 1  In Figure 1 error bars denote the minimum and the maximum measured I O times  across the three runs   Note that the error bars for the CPU times are omitted  as their variation is usually less than 1  of the total join time  Table 3 shows the speedup of the total join times and the I O times of the four join algorithms under di   erent bu   er pool sizes  The results in Table 3 show that replacing the magnetic HDD with the    ash SSD bene   ts all the join methods  The block nested loops join whose I O pattern is sequential reads shows the biggest performance improvement  with speedup factors between 1 59X to 1 73X   Interestingly  a case can be made that for sequential reads and writes  comparable or much higher speedups can be achieved with striped magnetic HDDs  for the same   cost  15    Other join algorithms also performed better on the    ash SSD compared to the magnetic HDD  with smaller speedup improvements than the block nested loops join  This is because the write transfer rate is slower than the read transfer rate on the    ash SSD  See Table 2   and unexpected erase operations might degrade write performance further Algorithms Sort Merge Join Grace Hash Join Hybrid Hash Join Bu   er Pool Size First Phase Second Phase First Phase Second Phase First Phase Second Phase 100 MB 1 52X 3 00X 1 34X 2 27X 1 57X 2 61X 200 MB 1 83X 2 81X 1 43X 2 19X 1 66X 3 09X 300 MB 1 86X 2 79X 1 47X 2 12X 1 34X 2 32X 400 MB 1 90X 2 63X 1 47X 2 13X 1 70X 2 91X 500 MB 1 81X 2 86X 1 59X 2 44X 1 63X 2 83X 600 MB 2 00X 2 89X 1 31X 2 68X 1 98X 2 84X Table 4  Speedups of I O times with    ash SSDs  broken down by the    rst and second phases 2 4 8 16 32 0 100 200 300 400 500 600 700 800 900 1000 1100 Join Time  sec  Page Size  KB  CPU time IO time                         B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H  a  Magnetic HDD 2 4 8 16 32 0 100 200 300 400 500 600 700 800 900 1000 1100 Join Time  sec  Page Size  KB  CPU time IO time                         B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H  b  Flash SSD Figure 2  Varying the page size  500 MB Bu   er Pool  blocked I O  As an example  di   erent I O speedups were achieved in the    rst and the second phases of the sort merge join as shown in Table 4  While the I O speedup of the second phase was between 2 63X and 3 0X due to faster random reads  the I O speedup in the    rst phase  that has sequential writes as the dominant I O pattern   was only between 1 52X and 2 0X  which reduced the overall speedup for sortmerge join  In the case of Grace hash join  all the phases were executed with lower I O speedups than those of the sort merge join  See Table 4   Note that the dominant I O pattern of Grace hash join is random writes in the    rst phase  followed by sequential reads in the second phase  While the I O speedup between 2 12X and 2 68X was observed for the second phase of Grace hash join  the I O speedup of its    rst phase was only between 1 31X and 1 59X due to expensive erase operations  This indicates that algorithms that stress random reads  and avoid random writes as much as possible are likely to see bigger improvements on    ash SSDs  over magnetic HDDs   While there is little variation in the I O times with the magnetic HDD  See the error bars in Figure 1 a  for the I O bars   we observed higher variations in the I O times with the    ash SSD  Figure 1 b    resulting from the varying write performance  Note that since random writes cause more erase operations than sequential writes  hash based joins show wider range of I O variations than sort merge join  On the other hand  there is little variation in the I O costs for block nested loops join regardless of the bu   er pool size  since it does not incur any writes  Another interesting observation that can be made here  Figure 1  is the relative I O performance between the sortmerge join and Grace hash join  Both have similar I O costs with the magnetic HDD  but sort merge join has lower I O costs with the    ash SSD  This is mainly due to the di   erent output patterns of both join methods  In the    rst phase of the joins  where both algorithms incur about 80  of the total I O cost  each writes intermediate results  sorted runs for sort merge join  and buckets for Grace hash join  to disk in di   erent ways  sort merge join incurs sequential writes as opposed to the random writes that are incurred by Grace hash join  While this di   erence in the output patterns has a substantial impact on the join performance with the    ash SSD because random write</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Flash in a DBMS  Where and How  Manos Athanassoulis y Anastasia ### Ailamaki y Shimin Chen     Phillip B  Gibbons     Radu Stoica y y Ecole Polytechnique Fed   erale de Lausanne        Intel Labs Pittsburgh Abstract Over the past decade  new solid state storage technologies  with    ash being the most mature one  have become increasingly popular  Such technologies store data durably  and can alleviate many handicaps of hard disk drives  HDDs   Nonetheless  they have very different characteristics compared to HDDs  making it challenging to integrate such technologies into data intensive systems  such as database management systems  DBMS   that rely heavily on underlying storage behaviors  In this paper  we ask the question  Where and how will    ash be exploited in a DBMS  We describe techniques for making effective use of    ash in three contexts   i  as a log device for transaction processing on memory resident data   ii  as the main data store for transaction processing  and  iii  as an update cache for HDD resident data warehouses ###  1 Introduction For the past 40 years  hard disk drives  HDDs  have been the building blocks of storage systems  The mechanics of HDDs rotating platters dictate their performance limitations  latencies dominated by mechanical delays  seeks and rotational latencies   throughputs for random accesses much lower than sequential accesses  interference between multiple concurrent workloads further degrading performance  25   etc  Moreover  while CPU performance and DRAM memory bandwidth have increased exponentially for decades  and larger and deeper cache hierarchies have been increasingly successful in hiding main memory latencies  HDD performance falls further and further behind  As illustrated in Table 1  HDDs    random access latency and bandwidth have improved by only 3 5X since 1980  their sequential bandwidth lags far behind their capacity growth  and the ratio of sequential to random access throughput has increased 19 fold  New storage technologies offer the promise of overcoming the performance limitations of HDDs  Flash  phase change memory  PCM  and memristor are three such technologies  7   with    ash being the most mature  Flash memory is becoming the de facto storage medium for increasingly more applications  It started as a storage solution for small consumer devices two decades ago and has evolved into high end storage for performance sensitive enterprise applications  20  21   The absence of mechanical parts implies    ash is not limited by any seek or rotational delays  does not suffer mechanical failure  and consumes less power than HDDs  As highlighted in Table 1     ash based solid state drives  SSDs     ll in the latency and bandwidth gap left by HDDs  SSDs also provide a low ratio between sequential and random access throughput 1   The time to scan the entire device sequentially is much lower than modern HDDs  close to what it was in older HDDs  Copyright 2010 IEEE  Personal use of this material is permitted  However  permission to reprint republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists  or to reuse any copyrighted component of this work in other works must be obtained from the IEEE  Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 1 Random access throughput for SSDs is computed using a mix of reads and writes  If only reads are performed then sequential and random access throughputs are roughly equal  1Table 1  Comparison of Hard Disk Drives  1980  2010  and Flash Drives  2010  Device Capacity Cost Cost MB Random Access Random Access Sequential Access Sequential BW   Device Scan   Year  GB          Latency  ms  Bandwidth  MB s  Bandwidth  MB s  Random BW  s  HDD 1980 0 1 20000 200 28 0 28 1 2 4 3 83 HDD 2010 1000 300 0 0003 8 0 98 80 81 6 12500 SSD 2010 100 2000 0 02 0 026 300 700 2 3 143 Sources  Online documents and presentations  13   vendor websites and other sources  18   Flash devices  however  come with certain limitations of their own  There is an asymmetry between random reads and random writes  3  6  23   with the latter incurring a performance hit due to the speci   cs of the    ash technology  2  6   Newer SSDs  16  19  mitigate this performance hit  but random writes still have a negative impact on the future performance of the device  Interestingly  sequential writes not only offer good performance but in many cases repair device performance after extensive random writes  26   Finally     ash cells wear out after 10K   100K writes to the cell  The differences between HDDs and SSDs are particularly important in Database Management Systems  DBMS  because their components  query processing  query optimization  query evaluation  have been tuned for decades with the HDD characteristics in mind  Speci   cally  random accesses are considered slow  sequential accesses are preferred  and capacity is cheap  Thus  achieving successful integration of    ash requires revisiting DBMS design  Currently     ash usage in DBMS follows two trends  resulting either in    ash only systems or in hybrid    ash HDD systems  Flash only systems bene   t greatly from    ash friendly join algorithms  29   indexes  1  8  23  24   and data layout  this paper   Hybrid systems use    ash judiciously to cache either hot or incoming data or for speci   c operations  9  14   In this paper we describe techniques for using    ash to optimize data management in three different settings  covering DRAM resident  SSD resident  and HDD resident data stores  Each technique represents an example of how    ash can be used within current systems to address the limitations of HDDs  First  we consider transaction processing on a memory resident data store  and show how to use    ash for transactional logging  dramatically improving transaction throughput  Section 2 1   Second  a straightforward way to use    ash is to replace all the HDDs with SSDs without changing the DBMS software  In this scenario    ash addresses the performance limitations of HDDs but poses new challenges because excessive random writes degrade performance and wear out the    ash storage prematurely  To overcome these challenges  we present a technique that leaves a DBMS unchanged and yet avoids random writes on    ash  Section 2 2   Third  we show how    ash can be used as a performance booster for data warehouses stored primarily on HDDs  Speci   cally  we describe techniques for buffering data updates on    ash such that  i  for performance  queries process the HDD resident data without interference from concurrent updates and  ii  for correctness  the query results are adjusted on the    y to take into account the    ash resident updates  Section 3   Finally  we conclude by highlighting techniques and open problems for other promising uses of    ash in data management  Section 4   2 Ef   cient Transaction Processing Using Flash Devices In this section we describe how    ash can be used effectively in the context of online transaction processing  OLTP   We show that the absence of mechanical parts makes    ash an ef   cient logging device and that using SSDs as a drop in replacement for HDDs greatly bene   ts from    ash friendly data layout  2 1 Using Flash for Transactional Logging Synchronous transactional logging  in which log records are forced to stable media before a transaction commits  is the central mechanism for ensuring data persistency and recoverability in database systems  As DRAM capacity doubles every two years  an OLTP database that was considered    large    ten years ago can now    t into main memory  For example  a database running the TPCC benchmark with 30 million users requires less than 100GB space  which can easily    t into the memory of a server  64   128GB of memory   In contrast  2Figure 1  FlashLogging architecture  exploiting an array of    ash devices and an archival HDD for faster logging and recovery  0 5000 10000 15000 20000 25000 30000 35000 disk ideal 2f 2f 1d 2f 2f 1d 2f 2f 1d na  ve 1p 2p 4p 8p 10p flash A flash B flash C ssd ssd flashlogging NOTPM Figure 2  FlashLogging TPCC performance  synchronous logging requires writing to stable media  and therefore is becoming increasingly important to OLTP performance  By instrumenting a MySQL InnoDB running TPCC  we    nd that synchronous logging generates small sequential I O writes  Such write patterns are ill suited for an HDD because its platters continue to rotate between writes and hence each write incurs nearly a full rotational delay to append the next log entry  Because    ash supports small sequential writes well  we propose FlashLogging  10  to exploit    ash for synchronous logging  FlashLogging Design  Figure 1 illustrates the FlashLogging design  First  we exploit multiple    ash devices for good logging and recovery performance  We    nd that the conventional striping organization in disk arrays results in sub optimal behavior  such as request splitting or skipping  for synchronous logging  Because our goal is to optimize sequential writes during normal operations and sequential reads during recovery  we instead propose an unconventional array organization that only enforces that the LSNs  log sequence numbers  on an individual device are non decreasing  This gives the maximal    exibility for log request scheduling  Second  we observe that write latencies suffer from high variance due to management operations  such as erasures  in    ash devices  We detect such outlier writes and re issue them to other ready    ash devices  thus hiding the long latencies of outliers  Third  the    ash array can be implemented either with multiple low end USB    ash drives  or as multiple partitions on a single high end SSD  Finally  our solution can exploit an HDD as a near zerodelay archival disk  During normal processing     ash resident log data is    ushed to the HDD once it reaches a prede   ned size  e g   32KB   Logging performance is improved because the HDD can also serve write requests when all the    ash drives are busy  Performance Evaluation  We replace the logging subsystem in MySQL InnoDB with FlashLogging  Figure 2 reports TPCC throughput in new order transactions per minute  NOTPM   comparing 14 con   gurations     Disk    represents logging on a 10k rpm HDD  while    ideal    enables the write cache in    disk     violating correctness  so that small synchronous writes achieve almost ideal latency  We evaluate three low end USB    ash drives  A  B  and C  from different vendors     2f    uses two identical    ash drives  and    2f 1d    is    2f    plus an archival HDD  We also employ a single high end SSD either directly with the original logging system     naive      or with FlashLogging while using multiple SSD partitions  1   10 partitions  as multiple virtual    ash drives  From Figure 2  we see that  i  FlashLogging achieves up to 5 7X improvements over traditional  HDD based  logging  and obtains up to 98 6  of the ideal performance   ii  the optional archival HDD brings signi   cant bene   ts   iii  while replacing the HDD with an SSD immediately improves TPCC throughput by 3X  FlashLogging further exploits the SSD   s inner parallelism to achieve an additional 1 4X improvement  and  iv  when compared to the high end SSD  multiple low end USB    ash drives achieve comparable or better performance at much lower price  2 2 Transparent Flash friendly Data Layout Using    ash as persistent storage medium for a DBMS often leads to excessive random writes  which impact    ash performance and its predictability  In Figure 3 a  we quantify the impact of continuous 4K random writes 3 0  50  100  150  200  250  300  350  0 20000 40000 60000 80000 100000 Time  s  Throughput  MiB  Average over 1s Moving average  0  100  200  300  400  500  0 1000 2000 3000 4000 5000 Time  s  Throughput  MiB  Average over 1s Moving average  a  Random writes  b  Random read write mix  50  50   using A P Figure 3  Experiments with a FusionIO SSD as  a  drop in replacement of HDDs and  b  using Append Pack  on FusionIO  16   a high end SSD  We    nd that the sustained throughout is about 16  of the advertised random throughput  At the same time  sequential writes can    x random read performance  26   Transforming Temporal Locality To Spatial Locality  Viewing SSDs as append logs helps us exploit the good sequential write bandwidth when writing  Consequently  temporal locality is transformed to spatial locality and thus sequential reads are transformed to random reads  This transformation helps overall performance  because unlike HDDs     ash offers virtually the same random and sequential read performance  Append Pack  A P  Design  The A P data layout  26  treats the entire    ash device as a log  The dirty pages that come out of the buffer pool to be stored persistently are appended in the    ash device and an index is kept in memory maintaining the mapping between the database page id and the location in the log  The A P layer exports to the overlying device a transparent block device abstraction  We implement a log structure data layout algorithm that buffers the dirty pages and writes them in blocks equal to the erase block size in order to optimize write performance  A P organizes  as well  a second log structured region of the device that is used for the packing part of the algorithm  During the operation of the system  some logical pages are appended more than once  i e   the pages are updated  having multiple versions in the log   In order to keep track of the most recent version of the data  we invalidate the old entry of the page in the in memory data structure  When the log is close to full with appended pages  we begin packing  that is  we move valid pages from the beginning of the log to the second log structure assuming these pages are not updated for a long period  cold pages   The premise is that any  write  hot page should be invalidated and all remaining pages are assumed to be cold  The cold pages are appended in a second log structure to ensure good sequential performance and  thus  fast packing of the device  Experimentation and Evaluation  We implement A P as a standalone dynamic linked library which can be used by any system  The A P library offers the typical pwrite and pread functions but manipulates writes as described above  We experimented with a PCIe Fusion ioDrive card  16   The    ash card offers 160GB capacity  and can sustain up to 700MB s read bandwidth and up to 350MB s sequential write bandwidth  Serving a TPCC like workload using the A P design  we maintain stable performance  in a read write mix  achieving the max that the device could offer  400MB s throughput as a result of combining reads and writes   as shown in Figure 3 b   When compared to the performance of a TPCC system on the same device but without A P  we achieved speedups up to 9X  26   3 Flash enabled Online Updates in Data Warehouses In this section  we investigate the use of    ash storage as a performance booster for data warehouses  DW  stored primarily on HDDs  because for the foreseeable future  HDDs will remain much cheaper but slower than SSDs  We focus on how to minimize the interference between updates and queries in DWs  While traditional DWs allow only of   ine updates at night  the need for 24x7 operations in global markets and the data freshness requirement of online and other quickly reacting businesses make concurrent online updates 4Run scan Run scan Run scan Merge updates Mem scan Table range scan pages M pages Main memory M Merge data updates  main data  Disks updates Incoming SSD  updates  oldest newest Materialized sorted runs  each with run index  a  MaSM algorithm using 2M memory  b  TPCH performance with online updates Figure 4  MaSM design and performance   M          SSD     increasingly desirable  Unfortunately  the conventional approach of performing updates in place can greatly disturb the disk friendly access patterns  mainly  sequential scans  of large analysis queries  thereby slowing down TPCH queries by 1 5   4 0X in a row store DW and by 1 2   4 0X in a column store DW  4   Recent work studied differential updates for addressing this challenge in column store DWs  17  27   The basic idea is  i  to cache incoming updates in an in memory buffer   </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>Privacy Aware Data Management in Information Networks ### Michael Hay Cornell University Ithaca  NY mhay cs cornell edu Kun Liu Yahoo  Labs Santa Clara  CA kun yahoo inc com Gerome Miklau U  of Massachusetts Amherst Amherst  MA miklau cs umass edu Jian Pei Simon Fraser University Burnaby  BC Canada jpei cs sfu ca Evimaria Terzi Boston University Boston  MA evimaria cs bu edu ABSTRACT The proliferation of information networks  as a means of sharing information  has raised privacy concerns for enterprises who manage such networks and for individual users that participate in such networks  For enterprises  the main challenge is to satisfy two competing goals  releasing network data for useful data analysis and also preserving the identities or sensitive relationships of the individuals participating in the network  Individual users  on the other hand  require personalized methods that increase their awareness of the visibility of their private information  This tutorial provides a systematic survey of the problems and state of the art methods related to both enterprise and personalized privacy in information networks  The tutorial discusses privacy threats  privacy attacks  and privacypreserving mechanisms tailored speci   cally to network data  Categories and Subject Descriptors H 2 0  DATABASE MANAGEMENT   Security  integrity  and protection General Terms Algorithms  Security Keywords Networks  Privacy  Anonymization  Di   erential Privacy ### 1  INTRODUCTION A network dataset is a graph representing a set of entities and the connections between them  Network data can describe a variety of domains  a social network might describe individuals connected by friendships  an information network might describe a set of articles connected by ciPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  tations  a communication network might describe Internet hosts related by tra   c    ows  The ability for enterprises to collect network data has increased rapidly  creating the possibility for a wide range of compelling data analysis tasks  studying disease transmission  measuring a publication   s in   uence  evaluating a network   s resiliency to faults and attacks  etc  While members of the enterprise may be able to perform these analyses  they often wish to enlist outside experts  In addition  the ability to disseminate network data  and or release the results of analyses  supports experimental repeatability and advances scienti   c investigation  Unfortunately  access to network data is extremely constrained because many networks contain sensitive information about their participants  The    rst topic in this tutorial focuses on privately managing enterprise network data  We investigate the problem of limiting the disclosure of sensitive information while publishing network data sets for useful analysis  or alternatively  releasing the results of network analyses  At the same time  individuals increasingly participate in online information networks  complex systems in which their personal information and interactions with others are recorded and displayed publicly  The second part of the tutorial identi   es the privacy risks to individuals due to these public networked interactions  We focus our discussion on measures for quantifying users    privacy risks as well as mechanisms for helping them identify appropriate privacy settings  2  TUTORIAL OUTLINE Our tutorial is organized as follows  2 1 Information network data The tutorial begins with a brief introduction to network data that includes social network data  e g   Facebook  LinkedIn   instant messenger networks  collaboration networks  etc  We review examples of large scale networks currently being collected and analyzed  We provide examples of key analysis tasks as well as the nature of the sensitive information contained in network data sets  2 2 Threats and attacks for network data Because network analysis can be performed in the absence of entity identi   ers  e g   name  social security number   a natural strategy for protecting sensitive information is to replace identifying attributes with synthetic identi   ers  We refer to this procedure as naive anonymization  This com  1201mon practice attempts to protect sensitive information by breaking the association between the real world identity and the sensitive data  We review a number of attacks on naively anonymized network data which can re identify nodes  disclose edges between nodes  or expose properties of nodes  e g   node features   These attacks include  matching attacks  which use external knowledge of node features  20  14  39  27   injection attacks  which alter the network prior to publication  1   and auxiliary network attacks  which use publicly available networks as an external information source  25   2 3 Publishing networks privately For enterprises that wish to publish their network data without revealing sensitive information  the above attacks demonstrate that simply removing identi   ers prior to publication fails to protect privacy  Thus  enterprises must consider more complex transformations of the data  An active area of research has focused on designing algorithms that transform network data so that it is safe for publication  These algorithms have two primary objectives  First  the transformations should protect privacy  which is typically demonstrated by proving that the transformed network resists certain attacks  Second  the utility of the data should be preserved by the transformation   i e   salient features of the network should be minimally distorted  While the privacy objective makes some distortion inevitable  most algorithms are designed to minimize distortion  measured in various ways  depending on the algorithm   In most cases  utility is not provably guaranteed but rather assessed empirically  for instance through a comparison of the transformed network to the original in terms of a measure of graph distance  or in terms of the di   erence in various network statistics  such as average shortest path lengths  clustering coef     cient and degree distribution  In addition to protecting privacy and preserving utility  algorithm runtime and scalability are also important considerations  One of the    rst algorithms proposed was that of Liu and Terzi  20   which transforms the network through edge insertions  Edges are added until nodes cannot be distinguished by their degree  speci   cally  for each node  there are at least k   1 other nodes that share its degree  This prevents an adversary with knowledge of node degree from re identifying a target node beyond a set of k candidates   The transformed network may also resist attacks from adversaries with richer auxiliary information  but the algorithm provides no formal guarantee   To preserve utility  the algorithm attempts to    nd the minimal set of edge insertions necessary to achieve the privacy objective  In recent years  many other algorithms have been proposed  cf  surveys  13  32  38    They can be organized based on two key design decisions  the kind of data transformation and the privacy objective  Algorithms transform the network using one of several kinds of alteration  directed alterations transform the network through addition and deletion of edges  6  20  27  39   network generalization summarizes the network data in terms of node groups  5  7  8  14   random alteration transforms the network stochastically via random edge additions  deletions  or rewirings  15  22  31  34   There is also work comparing di   erent alteration strategies  33   While a common privacy objective is preventing re identi   cation  5  6  14  20  27  39   other work seeks to prevent the disclosure of sensitive informa
</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>TrustedDB  A Trusted Hardware based Outsourced Database Engine ### Sumeet Bajaj Stony Brook Computer Science Stony Brook  New York  USA sbajaj cs stonybrook edu Radu Sion Stony Brook Computer Science Stony Brook  New York  USA sion cs stonybrook edu ABSTRACT TrustedDB  11  is an outsourced database prototype that allows clients to execute SQL queries with privacy and under regulatory compliance constraints without having to trust the service provider  TrustedDB achieves this by leveraging server hosted tamper proof trusted hardware in critical query processing stages  TrustedDB does not limit the query expressiveness of supported queries  And  despite the cost overhead and performance limitations of trusted hardware  the costs per query are orders of magnitude lower than any  existing or  potential future software only mechanisms  In this demo we will showcase TrustedDB in action and discuss its architecture ### 1  INTRODUCTION Virtually all major    cloud    providers today o   er a database service of some kind as part of their overall solution  Numerous startups also feature more targeted data management and or database platforms  Yet  signi   cant challenges lie in the path of large scale adoption  Such services often require their customers to inherently trust the provider with full access to the outsourced datasets  But numerous instances of illicit insider behavior or data leaks have left clients reluctant to place sensitive data under the control of a remote  third party provider  without practical assurances of privacy and con   dentiality     especially in business  healthcare and government  Most of the existing research e   orts have addressed such outsourcing security aspects by encrypting the data before outsourcing  Once encrypted however  inherent limitations in the types of primitive operations that can be performed on encrypted data lead to fundamental expressiveness and practicality constraints  Recent theoretical cryptography results provide hope by proving the existence of universal homomorphisms  i e   encryption mechanisms that allow computation of arbitrary functions without decrypting the inputs  6   Unfortunately actual instances of such mechanisms seem to be decades Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Articles from this volume were invited to present their results at The 37th International Conference on Very Large Data Bases  August 29th   September 3rd 2011  Seattle  Washington  Proceedings of the VLDB Endowment  Vol  4  No  12 Copyright 2011 VLDB Endowment 2150 8097 11 08      10 00  10 5 10 10 10 15 10 20 10 25 10 30 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10 10 11 10 12 Cost  picocents  Database size  items  Cryptography based  SELECT query  Cryptography based  JOIN query  SCPU  SELECT query  SCPU  JOIN query  Figure 1  The SCPU is 1 2 orders of magnitude cheaper than deploying cryptography  logarithmic   away from being practical  7   TrustedDB on the other hand utilizes secure  tamper resistant hardware such as the IBM 4764 5  3  4  cryptographic coprocessors deployed on the service provider   s side to implement a complete SQL database processing engine  The TrustedDB design provides strong data con   dentiality assurances  Moreover  it does not limit query expressiveness  2  THE CASE FOR TRUSTED HARDWARE A cost based empirical comparison of solutions for query processing using cryptography and trusted hardware  11   selected results in Figure 1  1   shows a 2  orders of magnitude cost advantage of using trusted hardware over cryptography based mechanisms  This is so because cryptographic overheads  for cryptography that allows some processing by the server  are extremely high even for simple operations  a fact rooted not in cipher implementation ine   ciencies but rather in fundamental cryptographic hardness assumptions and constructs  such as trapdoor functions     the cheapest we have so far being at least as expensive as modular multiplication  9    This is unlikely to change anytime soon  none of the current primitives have  in the past half century   Tamper resistant designs provide a secure execution environment for applications  thereby avoiding the need to use expensive cryptographic operations  However  they are signi   cantly constrained in both computational ability and memory capacity which makes implementing fully featured database solutions using secure coprocessors  SCPUs  very 1 1 US picocent   10    14 USD 1359Figure 2  TrustedDB architecture  challenging  TrustedDB overcomes these limitations by utilizing common unsecured server resources to the maximum extent possible  For example  TrustedDB enables the SCPU to transparently access external storage while preserving data con   dentiality with on the    y encryption  This eliminates the limitations on the size of databases that can be supported  Moreover  client queries are pre processed to identify sensitive components to be run inside the SCPU  Non sensitive operations are o    loaded to the untrusted host server  This greatly improves performance and reduces the cost of transactions  3  ARCHITECTURE TrustedDB is built around a set of core components  Figure 2  including a request handler  a processing agent and communication conduit  a query parser  a paging module  a query dispatch module  a cryptography library  and two database engines  While presenting a detailed architectural blueprint is not possible in this space  in the following we discuss some of the key elements and challenges faced in designing and building TrustedDB  3 1 Outline Challenges  The IBM 4764 001 SCPU presents signi     cant challenges in designing and deploying custom code to be run within its enclosure  For strong security  the underlying hardware code as well as the OS are embedded and no hooks are possible e g   to augment virtual memory and paging mechanisms  We were faced with the choice of having to provide virtual memory and paging in user land  speci   cally inside the query processor as well as all the support software  The embedded Linux OS is a Motorola PowerPC 405 port with fully stripped down libraries to the bare minimum required to support the IBM cryptography codebase and nothing else  This constituted a signi     cant hurdle  as cross compilation became a complex task of mixing native logic with custom ported functionality  The SCPU communicates with the outside world synchronously through    xed sized messages exchanged over the PCI X bus in exact sequences  Interfacing such a synchronous channel with the communication model of the query processors and associated paging components required the development of the TrustedDB Paging Module  The SCPU   s cryptographic hardware engine features a set of latencies that e   ectively crippled the ability to run for highly interactive mechanisms manipulating small amounts of data  e g   32 bit integers   To handle this speci   c case we ended up porting several cryptographic primitives to be run on the SCPU   s main processor instead  and thus eliminate the hardware latencies for small data items  Space constraints prevent the discussion of the numerous other encountered challenges  Overview  To remove SCPU related storage limitations  the outsourced data is stored at the host provider   s site  Query processing engines are run on both the server and in the SCPU  Attributes in the database are classi   ed as being either public or private  Private attributes are encrypted and can only be decrypted by the client or by the SCPU  Since the entire database resides outside the SCPU  its size is not bound by SCPU memory limitations  Pages that need to be accessed by the SCPU side query processing engine are pulled in on demand by the Paging Module  Query execution entails a set of stages   0  In the    rst stage a client de   nes </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>Differentially Private Data Cubes  Optimizing Noise Sources and Consistency ### Bolin Ding 1 Marianne Winslett 2 1 Jiawei Han 1 Zhenhui Li 1 1 Department of Computer Science  University of Illinois at Urbana Champaign  IL  USA 2 Advanced Digital Sciences Center  Singapore  bding3  winslett  hanj  zli28  uiuc edu ABSTRACT Data cubes play an essential role in data analysis and decision support  In a data cube  data from a fact table is aggregated on subsets of the table   s dimensions  forming a collection of smaller tables called cuboids  When the fact table includes sensitive data such as salary or diagnosis  publishing even a subset of its cuboids may compromise individuals    privacy  In this paper  we address this problem using differential privacy  DP   which provides provable privacy guarantees for individuals by adding noise to query answers  We choose an initial subset of cuboids to compute directly from the fact table  injecting DP noise as usual  and then compute the remaining cuboids from the initial set  Given a    xed privacy guarantee  we show that it is NP hard to choose the initial set of cuboids so that the maximal noise over all published cuboids is minimized  or so that the number of cuboids with noise below a given threshold  precise cuboids  is maximized  We provide an ef   cient procedure with running time polynomial in the number of cuboids to select the initial set of cuboids  such that the maximal noise in all published cuboids will be within a factor  ln  L    1  2 of the optimal  where  L  is the number of cuboids to be published  or the number of precise cuboids will be within a factor  1     1 e  of the optimal  We also show how to enforce consistency in the published cuboids while simultaneously improving their utility  reducing error   In an empirical evaluation on real and synthetic data  we report the amounts of error of different publishing algorithms  and show that our approaches outperform baselines signi   cantly  Categories and Subject Descriptors H 2 8  DATABASE MANAGEMENT   Database Applications    Data mining  H 2 7  DATABASE MANAGEMENT   Database Administration   Security  integrity  and protection General Terms Algorithms  Security  Theory Keywords OLAP  data cube  differential privacy  private data analysis Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00 ###  1  INTRODUCTION Data cubes play an essential role in multidimensional data analysis and fast OLAP operations  Often the underlying data is sensitive  and publishing all or part of the cube may endanger the privacy of individuals  For example  privacy concerns prevent Singapore   s Ministry of Health  MOH  from performing wide scale monitoring for adverse drug reactions among Singapore   s three main ethnic groups  none of which are typically included in pharmaceutical companies    drug trials  Privacy concerns also limit MOH   s published health summary tables to extremely coarse categories  reducing their utility for policy planning  Institutional Review Board  IRB  approval is now required for access to most high level summary tables from studies funded by the US National Institutes of Health  making it very hard to leverage results from past studies to plan new studies  In these and many other scenarios  society can greatly bene   t from the publication of detailed  high utility data cubes that also preserve individuals    privacy  The data cube of a fact table consists of cells and cuboids  A cell aggregates the rows in the fact table that match on certain dimensions  The fact table in Figure 1 a  has three dimensions  to be aggregated with count measure  As in Figures 1 b  1 d   a cuboid can be viewed as the projection of a fact table on a subset of dimensions  producing a set of cells with associated aggregate measures  With background knowledge  an adversary can infer sensitive information about an individual from a published cube  16  21  35   For example  in the data cube in Figure 1  if we know that Alice is aged 31 40 and is in the table  the count in cuboid  Age  Salary  tells us her salary is 50 200k  If we know Bob is in the table and is aged 21 30  we learn there is a 75  chance that his salary is 10  50k and a 25  chance it is 50 200k  If we also know that Carl  aged 21 30 and with salary 50 200k  is in the table  then the values of count in cuboid  Age  Salary  tell us Bob   s salary is 10 50k  Even publishing large actual aggregate counts is still not safe  if an adversary has enough background knowledge  For example  suppose there are 100 individuals in a fact table  Sex Age  Salary   and we publish two cells            10 50k  and            50 200k   both with count equal to 50  Suppose the adversary knows everyone   s salary except Bob   s  if 49 people have salary 10 50k and 50 have 50 200k  then s he can infer that Bob   s salary is 10 50k  We apply the notion of   differential privacy  10   DP or   DP for short in the rest of this paper  in data cube publishing  Compared to previous techniques for privacy preserving data publishing  see  2  15  for surveys   DP makes very conservative assumptions about the adversary   s background knowledge  The privacy guarantee it provides is independent of any background knowledge the adversary may have about the database  In particular  a publishing algorithm satisfying DP protects the privacy of any individual row in the database even if the adversary knows every other row  10  Sex Age Salary F 21 30 10 50k F 21 30 10 50k F 31 40 50 200k F 41 50 500k  M 21 30 10 50k M 21 30 50 200k M 31 40 50 200k M 60  500k   a  Fact Table T Sex Age Salary c         0 10k 0         10 50k 3         50 200k 3                      b  Cuboid  Salary  Sex Age Salary c F 21 30 0 10k 0 F 21 30 10 50k 2                          c  Cuboid  Sex Age  Salary  Sex Age Salary c     21 30 0 10k 0     21 30 10 50k 3     21 30 50 200k 1     21 30 200 500k 0     21 30 500k  0     31 40 0 10k 0     31 40 10 50k 0     31 40 50 200k 2     31 40 200 500k 0     31 40 500k  0                        d  Cuboid  Age  Salary  Figure 1  Fact Table and count Data Cube Informally  DP guarantees that the presence absence or speci   c value of any particular individual   s record has a negligible impact on the likelihood that a particular result is returned to a query  Thus an adversary cannot make meaningful inferences about any one individual   s record values  or even whether the record was present  One way to achieve DP is to add random noise to query results  10   The noise is carefully calibrated to the query   s sensitivity  which measures the total change of the query output under a small change of the input  As the variance of the noise increases  the privacy guarantee becomes stronger  but the utility of the result drops  To publish an   DP data cube over d dimensions  there are two baselines   i  We can compute the count measure for each cuboid from the fact table  and then add noise to each cell  As changing one row in the table affects 2 d cells  according to  12   each cell needs Laplace noise Lap 2 d      which destroys the utility of the cube unless d is small  The same idea is applied in  4  to publish a set of marginals of a contingency table   ii  If we only compute count for the base cuboid   Sex  Age  Salary  in Figure 1 c   from the fact table  Lap 1    suf   ces  We compute the other cuboids from the noisy base cuboid to ensure DP  However  noise in highlevel cuboids  such as  Salary   will be magni   ed signi   cantly  and utility will be low  This idea can be applied to universal histograms  but noise accumulation also makes them ineffective there  19   Another possible way is to treat each cell in a cuboid as a query  and apply methods in  23  to answer a workload of count queries while ensuring DP  But this approach is not practical in our context  as its running time space is at least quadratic in the number of cells  If we roll up measures across DP cuboids  the sums may not match the totals recorded in other cuboids  According to a Microsoft user study  1   users are likely to accept these kinds of small inconsistencies if they trust the original data and understand why the inconsistencies are present  However  if the users do not trust the original data  they may interpret the inconsistencies as evidence of bad data  So it is desirable to enforce a requirement for correct roll up totals across dimensions in the DP cuboids to be published  Consistency also boosts accuracy of DP data publishing in some cases  e g   in answering one dimensional range queries  19   Contributions  We study how to publish all or part of a data cube for a given fact table  while ensuring   differential privacy and limiting the variance of the noise added to the cube  We propose a general noise control framework in which a subset Lpre of cuboids is computed from the fact table  plus random noise  The remaining cuboids are computed directly from those in Lpre  which is the    source of noise     When Lpre is larger  each of its members requires more noise  but the cuboids computed from Lpre accumulate less noise  So a clever selection of Lpre can reduce the overall noise  We de   ne two publishing scenarios that    t the needs of the Ministry of Health  In the    rst scenario  MOH identi   es a set of cuboids which must be released  and the goal is to minimize the max noise in the cuboids  In the second scenario  MOH has a large body of cross tabulations that can be useful for urban planners and the medical community  A weighting function indicates the importance of releasing each cuboid  The question is  which of these cuboids can be released in a DP manner  while respecting a given noise variance bound for each cell  called precise cuboids    the goal is to maximize the sum of the weights of the released precise cuboids  We formalize these two optimization problems for the selection of Lpre  prove that they are NP hard  and give ef   cient algorithms with provable approximation guarantees  For the    rst problem  the max noise variance in all published cuboids will be within a factor  ln  L    1  2 of optimal  where  L  is the number of cuboids to be published  and for the second  the number weight of precise cuboids will be within a factor  1     1 e  of optimal  We also show how to enforce consistency over a DP data cube by computing a consistent measure from the noisy measure released by a DP algorithm  We minimize the L p distance between the consistent measure and the noisy measure  The revised data cube is still DP  as we do not revisit the fact table when enforcing consistency  The consistency enforcing techniques in  4  are similar to our L    version  but our L 1 version yields a much better theoretical bound on error than the L    version  We show that in the L 2 version  the consistent measure can be computed ef   ciently  and provide better utility than the original inconsistent noisy measure  Organization  Section 2 provides background for data cubes and DP  plus our noise control goals  Section 3 gives our noise control publishing framework and formalizes the optimization problems for noise control  Section 4 gives approximation algorithms for these problems  Section 5 shows how to enforce consistency across cuboids  Experimental results are reported in Section 6  followed by discussion and extension of our techniques in Section 7  and related work in Section 8  Proofs of all theorems are in the Appendix  2  BACKGROUND AND PROBLEM Data Cubes Consider a fact table T with d nominal dimensions A    A1  A2           Ad   We also use Ai to denote the set of all possible values for the i th dimension  and  Ai  to denote the number of distinct values  i e   cardinality   For a row r     T   r i  is r   s value for Ai  The data cube of T consists of cells and cuboids  A cell a takes the form  a 1   a 2            a d    where a i       Ai            denotes the i th dimension   s value for this cell  A cell is associated with certain aggregate measure of interest  In this paper  we    rst focus on the count measure c a  and discuss how to handle other measures in Section 7 3  c a  is the number of rows r in T that are aggregated in cell a  with the same values on non     dimensions of a   c a      r     T      1     i     d  r i    a i      a i           Cell a is an m dim cell if exactly m of a 1   a 2            a d  are not      An m dim cuboid C is speci   ed by m dimensions  C     Ai1   Ai2             Aim   The cuboid C consists of all m dim cells a such that    1     k     m  a ik      Aik   and C can be interpreted as the projection of T on the set of dimensions  C   The d dim cuboid is called the base cuboid and the cells in it are base cells  For two cuboids C1 and C2  if  C1       C2   denoted as C1   C2   then  measures of cells in  C1 can be computed from C2  C1 is said to be an ancestor of C2  and C2 is a descendant of C1  Let Lall denote the set of all cuboids  Clearly   Lall      forms a lattice EX A M P L  E 2 1  Fact table T in Table 1 has three dimensions  Sex    M  F   Age    0 10  11 20  21 30  31 40  41 50  51  60  60    and Salary    0 10k  10 50k  50 200k  200 500k  500k    Figure 2 a  shows the lattice of cuboids of T   As  Salary       Age  Salary   cuboid  Salary  can be computed from cuboid  Age  Salary   For example  to compute cell            10 50k   we aggregate cells       0 10  10 50k                  60   10 50k     Differential Privacy Differential privacy  DP for short in the rest of this paper  is based on the concept of neighbors  Two tables T1 and T2 are neighbors if they differ by at most one row  i e     T1     T2       T2     T1     1  1 Let nbrs T   be the neighbors of T   and TB be the set of all possible table instances with dimensions A1           Ad  Let F   TB     R n be a function that produces a vector of length n from a table instance  In our context  F computes the set of cuboids we select  De   nition 1   Differential Privacy  10   A randomized algorithm K is   differentially private if for any two neighboring tables T1 and T2 and any subset S of the output of K  Pr  K T1      S      exp       Pr  K T2      S    where the probability is taken over the randomness of K  Consider an individual   s concern about the privacy of her his record r  When the publisher speci   es a small    De   nition 1 ensures that the inclusion or exclusion of r in the fact table makes a negligible difference in the chances of K returning any particular answer  De   nition 2   Sensitivity  10   The L1 sensitivity of F is  S F     max    T1 T2   TB   T1   nbrs T2    F T1      F T2   1  where   x     y  1     1   i   n  xi     yi  is the L 1 distance between two n dimensional vectors x    x1           xn  and y    y1           yn   Let Lap     denote a sample Y taken from a zero mean Laplace distribution with probability density function f x    1 2   e    x       Here  E  Y     0 and variance Var  Y     2   2   We write  Lap      n to denote a vector of n independent random samples Lap      TH E  O R E  M 1        12     Let F be a query sequence of length n  The randomized algorithm that takes as input database T and output F   T     F T      Lap S F      n is   differentially private  Problem Description Given a d dimensional fact table T   we aim to publish a subset L of all T    s cuboids Lall with measure   c      a noisy version of the count measure c      using an algorithm K that ensures   differential privacy  In particular  for any cell a in a cuboid in L  we want to publish a noisy count measure   c a  using the DP algorithm K  Measuring Noise  We measure the utility of an algorithm by the variance of the noisy measure it publishes  As we apply Laplace mechanism in Theorem 1  we will show noisy measure   c     published by our algorithms is unbiased  i e   the expectation E    c a     c a   So for one cell  the variance Var    c a   is equal to the expected squared error  i e   Var    c a     E      c a      c a   2     and we use it to measure the noise error in   c a   Similarly  for any set P of cells  we use Var    a   P   c a    to measure the noise error  1 Some DP papers use a slightly different de   nition  T1 and T2 are neighbors if they have the same cardinality and T1 can be obtained from T2 by replacing one row    indistinguishable  12    Our algorithms also work with this de   nition  if we double the noise  C110 C101 C011 C111 C100 C010 C001 C000  Sex  Age  Salary   Sex  Age   Sex  Salary   Age  Salary   Age   Salary       Sex   a  Lattice Structure of Cuboids C110 C101 C011 C111 C100 C010 C001 C000 7 2 2 7 5 5 7 2 5 2 7 5  b  Variance Magni   cation Figure 2  Lattice of cuboids  Variance magni   cation of noise Noise Control Objectives  We aim to control the noise in the published cuboids in one of the following two ways   i   Bounding max noise  Minimize the maximal variance over all cells  maxa Var    c a    in the published cuboids   ii   Publishing as many as possible  Given a threshold   0  a cuboid C is said to be precise if Var    c a         0 for all cells a in C  Maximize the number of precise cuboids in all published ones  Enforcing Consistency  When the consistency is required  we show how to take noisy measure   c     as input and alter it into measure   c     to    t consistency constraints that the cuboids should sum up correctly  Our consistency enforcing algorithm takes only   c     as input  not touching the fact table T    and thus from the composition property of differential privacy  28   it is also   DP  The output consistent measure   c     should be as close to   c     and c     as possible  The consistency constraints will be formulated in Section 5  3  NOISE CONTROL FRAMEWORK We    rst present two straw man approaches  Kall and Kbase  followed by our noise optimizing approach Kpart  Straw Man Release Algorithms Kall and Kbase One option  Kall   is to compute the exact measure c a  for each cell a in each cuboid in L  and add noise to each cell independently  including empty cells  Formally  let Fall T   be the vector containing measure c a  for each cell a of each cuboid in L  Fall has sensitivity  L   De   nition 2   since a row in T contributes 1 to exactly one cell in each cuboid in L  Kall adds noise drawn from Lap  L     to each cell     c a    c a    Lap  L       and publishes the resulting vector  The noise variance in Kall is high  With 8 dimensions   L  can reach 2 8   making Var    c a     2    2 16    2   We will discuss the relationship between Kall and related work  4  36  in Section 8  TH E  O R E  M 2  Kall is   differentially private  For any cell a to be published  E    c a     c a  and Var    c a     2 L  2    2   Another option  Kbase  computes only the cells in the d dim cuboid  i e   the base cuboid  directly from T   and adds noise into them  Kbase computes the measures of the remaining cells to be released by aggregating the noisy measures of the base cells  The vector Fbase T   has each entry as the measure c a  of a cell in the base cuboid  and thus its sensitivity is 1  therefore  publishing   c a    c a    Lap 1    for each base cell preserves   differential privacy  When Kbase computes higher level cuboids from Fbase T    we  do not touch T   so   DP is preserved  However  the noise variance grows as we aggregate more base cells for cells in higher levels  TH E  O R E  M 3  Kbase is   differentially private  For any published cell a  E    c a     c a   If a is in a cuboid with dimensions  C   then   c a  has noise variance Var    c a     2   Aj     C   Aj     2  EX A M P L  E 3 1  For the fact table T in Figure 1 a   Figure 2 a  shows the lattice of cuboids under the relationship    Three dimensions Sex  Age  and Salary have cardinality 2  7  and 5  respectively  Each cuboid is labeled as Cx1x2x3   where xi   1 iff the i th dimension is in this cuboid  For example  C011 is the cuboid  Age  Salary   The label on an edge from C to C   in Figure 2 b  is the variance magni   cation ratio when cells in cuboid C are aggregated to compute the cells in C     For example  if C011 is computed from C111  the noise variance doubles  since the dimension Sex has 2 distinct values   each cell in C011 is the aggregation of 2 cells in C111  If C001 is computed from C111  the noise variance is magni     ed 2    7   14 times  since 14 cells in C111 form one in C001  Suppose we want to publish all the cuboids in Figure 2 a   Using Kall   we add Laplace noise Lap 8    to each cell  giving noise variance 2    64   2   128   2 for one cell  Using Kbase  we add Laplace noise Lap 1    to each cell in C111 and then aggregate its cells  Each cell in C100 is built from 7    5 cells in C111  with noise variance 35   2   2   70   2   A cell in C000 is built from 7   5   2 cells in C111  with noise variance 70    2   2   140   2   A better approach is to compute cuboids C111  C110  C101  and C100 from T   adding Laplace noise Lap 4    to each  the sensitivity is 4   We compute the other cuboids from these  Then the noise variance in C100 is 32   2 and in C000 is 2    32   2   64   2  aggregating 2 cells of C100   As shown in Example 3 1  the more cuboids we compute directly from the table T   the higher their sensitivity is  and so the more noise they require  but the less noise is accumulated when other cuboids are computed from them  Kall and Kbase represent the extremes of computing as many or as few cuboids as possible directly from T   There must be a strategy between Kall and Kbase that gives better bounds for the noise variance of released cuboids  A Better Algorithm Kpart To publish a set L     Lall of cuboids  Kpart chooses which cuboids  denoted as Lpre  to compute directly from the fact table T   in a manner that reduces the overall noise  Lpost   L     Lpre includes all the other cuboids in L  We do not require Lpre     L  1   Noise Sources  For each cell a of cuboids in Lpre  Kpart computes c a  from T and releases   c a    c a    Lap s     where  the sensitivity s    Lpre   Note that Lpre is selected by our algorithms in Section 4 1 s t  all cuboids in L can be computed from Lpre  2   Aggregation  For each cuboid C     Lpost  Kpart selects a descendant cuboid C     from Lpre s t  C       C  and computes   c a  for each cell a     C by aggregating the noisy measure of cells in C       We discuss how to pick C     as follows  The measure   c a  output by Kpart is an unbiased estimator of c a  for every cell a  i e   E    c a     c a   For a cell a in cuboid C       Lpre  Var    c a     2s 2    2  s    Lpre    Suppose cell a in C     Lpost is computed from C       Lpre by aggregating on dimensions  C          C     Ak1           Akq    the variance magni   cation is de   ned as mag C  C         1   i   q  Aki    So the noise variance is Var    c a     mag C C        2s 2    2    1  Let mag C  C    1  If C cannot be computed from C    C   C      let mag C  C            We should compute the cells in C     Lpost from the cuboid C         Lpre for which mag C  C       is minimal  i e   mag C  C          minC    Lpre mag C  C      Let noise C  Lpre     min C    Lpre mag C C        2s 2    2  2  be the smallest possible noise variance when computing C from a single cuboid in Lpre  For C       Lpre  noise C     Lpre    2s 2    2   TH E  O R E  M 4  Algorithm Kpart is   differentially private  For any cuboid cell a to be released  E    c a     c a   EX A M P L  E 3 2  Consider the data cube in Example 2 1 and release algorithm Kpart with Lpre    C111  C110  C101  C100   C000 can be computed from C100 with noise variance 2    32   2   since mag C000  C100     2  or from C110 with noise variance 14    32   2   448   2   since mag C000  C110    2    7   14  So Kpart computes C000 from C100  and noise C000  Lpre    64   2   Kpart shares some similarities to the matrix mechanism in  23   Kpart chooses cuboids Lpre to compute L  while  23  chooses a set of queries to answer a given workload of count queries  If we adapt the matrix mechanism for our problem by treating a cell as a count query  we need to manipulate matrices of sizes equal to the number of cells  e g   matrices with 10 6    10 6 entries for a moderate data cube with 10 6 cells   So  23  is not applicable in our context  Ef   cient Implementation of Kpart  Suppose Lpre is given  A naive way to compute DP cuboids in L is to    rst inject noise into each cuboid in Lpre  Then for each C     L  query its descendant C         Lpre  C   C        and aggregate cells in C     to compute cells in C  We can compute DP cuboids in L more ef   ciently with fewer cell queries  Suppose C   C      C       noise is injected into C         Lpre for ensuring DP  and C    is computed from C       Then computing C from C     is equivalent to computing it from C      with DP ensured  So after noise is injected into all cuboids in Lpre  DP cuboids in L can be computed in a level by level way  i e   computing idim cuboids from  i   1  dim cuboids  The total running time is O N d 2    where N     j   Aj     1  is the total number of cells  Problems of Optimizing Noise Source  Choosing Lpre We formalize two versions of the problem of choosing Lpre with different goals  given table T and set L of cuboids to be published  Problem 1    BO U N D MA X VA R I A N C E  Choose a set Lpre of cuboids s t  all cuboids in L can be computed from Lpre and the max noise noise Lpre    maxC   L noise C  Lpre  is minimized  Problem 2    PU B L  I  S H MO S T  Given threshold   0 and cuboid weighting w      choose Lpre s t    C   L  noise C Lpre      0 w C  is maximized  In other words  maximize the weight of the cuboids computed from Lpre with noise variance no more than   0  TH E  O R E  M 5  BO U N D MA X VA R I A N C E and PU B L  I  S H MO S T are both NP hard  where  L  is the input size  The proof uses a non trivial reduction from the VE RT E X COVER problem in degree 3 graphs  Details are in the appendix  Note that Kall and Kbase are special cases of Kpart by letting Lpre   L and Lpre    the base cuboid   respectively  We will introduce two algorithms that choose Lpre carefully so that Kpart is better than Kall and Kbase w r t  objectives in Problems 1 and 2  4  OPTIMIZING THE SOURCE OF NOISE A brute force approach for Problems 1 and 2   enumerating all possible choices of Lpre  all possible cuboid subsets    takes O 2 2 d   time  which is not practical even for d   5  Due to the hardness result in Theorem 5  we introduce approximation algorithms for these two problems  with running time polynomial in 2 d   4 1 Bounding the Maximum Variance We now present a  ln  L   1  2  approximation algorithm for the BO U N D MA X VA R I A N C E problem  with running time polynomialin  L  and 2 d   First  suppose we have an ef   cient algorithm FE AS I  B  L  E for subproblem FE A S I B I L I T Y L       s   for a    xed    and s  is there a set of s cuboids Lpre s t  for all C     L  noise C  Lpre          Let FE A S I B L E L       s  return Lpre if a solution exists  and    NO    otherwise  Then to solve Problem 1  we can    nd the minimum    such that for some s  FE A S I B L E L       s  returns a feasible solution  This    can be found using binary search instead of guessing all possible values  Algorithm 1 provides the details  1    L     0    R     2 L  2    2  or   R     2    4 d    2 if  L    2 d    2  while    L       R    1   2 do 3            L     R  2  4  if FE A S  I  B L E L       s    NO for all s   1             L  then   L         else   R         5  return            R and the solution found by FE A S  I  B L E L           s   FE A S  I  B L E L       s  6  Compute coverage cov C  for each cuboid based on    and s  7  R          COV          8  repeat the following two steps s times 9  Select a cuboid C  such that  cov C        COV   is maximized 10  R     R      C     COV      COV     cov C     11  if COV   L then return R as Lpre  12  else return NO  Algorithm 1  Algorithm for BO U N D MA X VA R I A N C E problem Theorem 5 says that BO U N D MA X VA R I A N C E is NP hard  so there cannot be an ef   cient exact algorithm for the subproblem FE A S I B I L I T Y  We provide a greedy algorithm  analogous to that for SE T COVER  that achieves the promised approximation guarantee  ln  L    1  2 for the BO U N D MA X VA R I A N C E problem  Using  2   we rewrite FE A S I B I L I T Y   s noise constraint as  noise C  Lpre             min C    Lpre mag C  C             2 2s2    3  For    xed        and s  cuboid C   covers cuboid C if C   C   and mag C  C             2 2s2   De   ne C      s coverage to be  cov C            s     C     L   C   C     mag C  C             2 2s2     4  For simplicity  we write cov C     for cov C             s  when        and s are    xed  The following lemma is from  3  and  4   LE  M M A 1  noise C  Lpre         if and only if there exists C       Lpre such that C     cov C              Lpre    By Lemma 1  FE A S I B I L I T Y L       s  reduces to    nding s cuboids that cover all cuboids in L  To solve this problem  we employ the greedy algorithm for the SE T COVER problem  in each of s iterations  we add to Lpre the cuboid that covers the maximum number of not yet covered members of L  The greedy algorithm can    nd at most  ln  L    1 s     cuboids to cover all cuboids in L if the minimum covering set has size at least s       This algorithm  denoted as FE A S I B L E L       s   appears in lines 6 12 of Algorithm 1  TH E  O R E  M 6  Algorithm 1    nds an  ln  L  1  2  approximation to Problem 1 in O min 3 d   2 d  L   L  log  L   time  Moreover  using the solution Lpre produced by Algorithm 1  Kpart is at least as good as Kall and Kbase   in terms of the objective in Problem 1  Algorithm 1   s running time is polynomial in 2 d and  L   and it provides a logarithmic approximation  Note that parameter d is not very large in most applications  but  L  can be as large as 2 d   EX A M P L  E 4 1  Suppose we want to publish all cuboids in Figure 2 a   L   Lall      When FE A S I B L E     is called with      80   2 and s   2  from  4   C   covers C iff C   C   and mag C  C         10  mag C  C     can be computed from Figure 2 b  by multiplying the edge weights on the path from C   to C  In the    rst iteration of lines 8 10  Algorithm 1 chooses C111 for cov C111     C111  C110  C010  C101  C011  covers the most cuboids in L  and puts C111 into R  In the second iteration  Algorithm 1 chooses C101 because cov C101     C101  C100  C001  C000  which covers three  the most  cuboids not covered by C111 yet  C111 and C101 cover all the cuboids  so FE A S I B L E    returns Lpre    C111  C101   The binary search in Algorithm 1 determines that 64   2 is the smallest value of    for which FE A S I B L E    nds a feasible Lpre for some s  In that iteration  for s   4  FE A S I B L E returns Lpre    C111  C110  C101  C100   There  we have cov C111     C111  C011   cov C110     C110  C010   cov C101     C101  C001   and cov C100     C100  C000   4 2 Publishing as Many as Possible We now present a  1     1 e  approximation algorithm for the PU B L  I  S H MO S T problem  with running time polynomial in  L  and 2 d   Given threshold   0  assuming the optimal solution has s cuboids  from Lemma 1  PU B L  I  S H MO S T is equivalent to    nding a set of s cuboids Lpre s t  the weighted sum of the cuboids in L that they cover  with        0 and the    xed s  is as high  as  possible   We will consider values up to  L  for s  and apply the greedy algorithm for the MA X I M U M COVERAGE problem for each choice of s  As outlined in Algorithm 2  initially R and C OV are empty  In each iteration  we    nd the cuboid for which the total weight of the not previously covered cuboids it covers in L is maximal  We put this cuboid into R and put the newly covered cuboids into C OV  After repeating this s times   R    s and C OV is the set of cuboids with noise variance no more than   0 if computed from R  At the end  pick the best R over all choices of s as Lpre  If some cuboids in L cannot be computed from Lpre but we desire to publish them  we can simply add one more cuboid  the base cuboid  into Lpre  1  for s   1  2           L  do Rs     GR E E DYCOVER L    0  s   2  return the best Rs as Lpre  GR E E DYCOVER L    0  s  3  Compute coverage cov C  for each cuboid based on   0 and s  4  R          COV          5  repeat the following steps s times 6  Select a cuboid C  such that  cov C        COV   is maximized  or   C   cov C      COV w C  is maximized  7  R     R      C     COV      COV     cov C     8  return R  Algorithm 2  Algorithm for PU B L  I  S H MO S T TH E  O R E  M 7  Algorithm 2    nds a  1     1 e  approximation to Problem 2 in O min 3 d   2 d  L  d L   time  Moreover  using the solution Lpre produced by Algorithm 2  Kpart is as least as good as Kall and Kbase  in terms of the objective in Problem 2  EX A M P L  E 4 2  Consider the effect of Algorithm 2 on Example 4 1  with   0   40   2 and all cuboids to be published with equal weights  For s   2  when we call GR E E DYCOVER L 40   2   2    C111  C101  is returned as R in line 8  Since no other value of s covers more cuboids   C111  C101  is    nally returned as Lpre  5  ENFORCING CONSISTENCY Suppose Lpre    C1  C2   where  C1     A1  A2  A3  and  C2     A1  A2  A4   Consider cuboid  C     A1  A2  to be published  C can be computed from either C1 or C2  Since we add noise to C1 and C2 independently  the two computations of C will probably yield different results  If we revise Kpart by letting C bethe weighted  based on variance  average of the two results  then C will not be an exact roll up of either C1 or C2  though it will be unbiased  Such inconsistency occurs in data cubes released by both Kall and Kpart  as long as Lpre contains more than one cuboid  In this section  we introduce systematic approaches to enforce consistency across released cuboids  The basic idea is as follows  Consider the noisy measures   c     of cells in each cuboid C     Lpre released by Kpart  where Lpre is selected by either Algorithm 1 or Algorithm 2  Recall that  in Kall   Lpre is the set of all cuboids to be published  We construct consistent measure   c     from   c     so that the consistency constraints are enforced and the distance between   c     and   c     is minimized  Since the algorithm takes only   c     as the input and does not touch the real count measure c     or the fact table T     DP is automatically preserved  Consistency Constraints and Distance Measures Clearly  there is no inconsistency if we compute measures of all cells from the base cells  i e   d dim cells   For a cell a  let Base a  be the set of all base cells  each of which aggregates a disjoint subset of rows that are contained in a  Formally  a       Base a  if and only if for any dimension Ai  a i         implies a i    a    i   So we enforce consistency constraints for measure   c     as follows    a    Base a    c a         c a       cells a   5  We seek the choice of   c     that satis   es consistency constraints in  5  and is as close as possible to   c      but can be computed without reexamining c      We use L p distance  p     1  to measure how close   c     and   c     are  Let Epre be the set of cells in cuboids belonging to Lpre  Treat   c     and   c     as vectors in R Epre   and  the L p distance between is     c           c      p     a   Epre     c a        c a   p   1 p   Finding   c     to minimize     c           c      p subject to  5  can be viewed as a least norm problem  From classic results from convex optimization  7   it can be solved ef   ciently  at least in theory  We will prove that the utility of optimal solutions for L 1   L 2   and L    distances satis   es certain statistical guarantees  More importantly  classic algorithms do not work in our context because the number of variables involved in  5  is equal to the number of cells  which is huge  We provide a practical and theoretically sound algorithm that minimizes the L 2 distance in time linear in the number of cells  5 1 Minimizing L    and L 1 Distance We    rst consider minimizing the L    distance  which is essentially minimizing the maximal difference between   c a  and   c a  for any cell in Epre  i e       c         c            maxa   Epre    c a      c a    Equivalently  we solve for   c     in the following linear program  minimize z  6  s t     c a        c a       z       cells a     Epre    a    Base a    c a         c a       cells a     Epre   4  considers a similar consistency enforcing scheme  but injects noise into all cuboids instead of the carefully selected subset of cuboids Lpre  We can bound the error of   c     as follows  TH E  O R E  M 8   Generalized Theorem 8 in  4   For   c     obtained by solving  6   with probability at least 1         where       Epre  e   2     a   Epre    c a      c a        Epre  Lpre    2 log  Epre        Epre  Lpre        A linear program can also be used to minimize the L 1 distance  As     c         c      1     a    c a      c a    by introducing an auxiliary variable za for each cell a with constraint    za       c a      c a      za  minimizing     c           c      1 while enforcing consistency in   c     is equivalent to the following linear program  minimize   a   Epre za  7  s t     c a        c a       za      cell a     Epre    a    Base a    c a         c a       cell a     Epre  TH E  O R E  M 9  For   c     obtained by solving  7   if       1  with probability at least 1         where           2e   2   1    Epre        2     a   Epre    c a      c a        Epre  Lpre        For a data cube where the cardinality of every dimension is two  Theorem 8 in  4  yields a bound similar to that of our Theorem 8  by discarding the integrality constraint  i e   counts are integers   Theorem 9 mirrors Theorem 8  but replaces the upper tail       Epre  e   with      2e   2   1    Epre    As  Epre  is usually large  linear program  7  and Theorem 9 give a much better bound on the average error in   c     than linear program  6  and Theorem 8  To enforce the integrality constraint for   c     obtained from linear program  6  or  7   we can simply round   c a  for each cell to the nearest integer  which will replace the error bound  Epre  Lpre       with  Epre  Lpre          Epre  in both Theorems 8 and 9  5 2 Minimizing L 2 Distance Now let   s focus on minimizing the L 2 distance  i e   the sum of squared differences between   c a  and   c a  for all cells  This problem has a very ef   cient solution with good statistical guarantees  minimize   a   Epre    c a        c a   2  8  s t    a    Base a    c a         c a       cell a     Lpre  Program  8  can be viewed as a least L 2  norm problem  Its unique solution   c      called the least square solution  can be found using linear algebra  7   The classical method needs to compute multiplication inversion of M    M matrices  where M     j  Aj   is the total number of base cells  Since M is typically larger than 10 6   the classical method is inef   cient in our context  Fortunately  we can derive the optimal solution   c     much more ef   ciently by utilizing the structure of data cubes  as follows  We de   ne Ancs a     to be the set of ancestor cells of cell a     a     Ancs a     if and only if a is in an ancestor of the cuboid containing a   and has the same value as a   on all non     dimensions  formally  a     Ancs a     if and only if for any dimension Ai  a i         implies a i    a    i   If a   is a base cell  a     Ancs a         a       Base a   For a cell a and a cuboid C  let a C  be a   s values on dimensions of  C   Suppose  C     Ai1            Aik    a C     a i1            a ik    For two cuboids C1 and C2  let C1     C2 be the cuboid with dimensions  C1       C2  and C1     C2 with dimensions  C1       C2   We provide the following two stage algorithm to compute the consistent measure   c     that is the optimal solution to program  8   1  Bottom up stage  We    rst compute obs a      for every cell a    in a cuboid C    that can be computed from Lpre  Let obs a          a    Base a       a   Epre   Ancs a      c a    9 All AllC BMax BMaxC PMost PMostC Base  50  100  200  400  800  1600  3200  6400  12800 0 25 0 5 1 1 5 2 Privacy Parameter     a  Max Cuboid Error  20  40  80  160  320  640  1280 0 25 0 5 1 1 5 2 Privacy Parameter     b  Average Cuboid Error Figure 3  Varying privacy parameter   A cuboid C     Lpre is maximal in Lpre if there is no C       Lpre s t  C     C     In all maximal cuboids C    in Lpre  obs a      can be computed as in Formula  9   In all the other cuboids that can be computed from Lpre  obs a      can be computed recursively as follows  suppose a        C    and C    can be computed from Lpre  there must be some cuboid C with dimensionality dim C        1 s t  either C     Lpre or C can be computed from Lpre  then  obs a          a  a   C a C     a     C     obs a    10  2  Top down stage  After obs a      is computed for every possible cell a      consider another quantity est a      for each cell a        C      est a          C   Lpre   b  b    C   C      b C   C     a     C   C     deg C     C       c b    11  where deg C      Ai   A    C   Ai  and A is the set of all dimensions and  Ai  is the cardinality of dimension Ai  Suppose obs       s are computed in the    rst stage as constants and   c       s are variables  Solving the equations est a         obs a       we can compute   c       s in a top down manner  from ancestors to descendants  as follows  ratio C          C  C   Lpre  C    C deg C     C        12  aux a          C  C   Lpre  C    C   b  b    C   C      b C   C     a     C   C     deg C     C       c b    13    c a        1 ratio C       obs a          aux a           14  The above approach can be also applied to Kall because Kall is a special case of Kpart obtained by setting Lpre   L  Note that this approach generalizes the consistency enforcing scheme in  19  from a tree like structure  hierarchy of intervals  to a lattice  The method in  19  cannot be directly applied here  Optimality  We can prove   c     obtained above is not only consistent but also an unbiased estimator of c      Also    c     is optimal in the sense that no other linear unbiased estimator of c     obtained from   c     has smaller variance  i e   smaller expected squared error  TH E  O R E  M 10   i  The above algorithm correctly computes a value for   c     that is consistent and solves the L 2 minimization problem  8    ii  The above algorithm requires O N d 2  d Lpre    time to compute   c     for all N cells   iii  For any cell of cuboids in Lpre    c a  is an unbiased estimator of c a   i e   E    c a     c a    iv  For any cell a    c a  has the smallest variance among any linear unbiased estimator  e g     c a   of c a  obtained from   c       v  For any set P of cells    a   P   c a  has the smallest variance among any linear unbiased estimator of   a   P c a  obtained from   c       0  100  200  300  400  500  600  0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 Error ratio    0        PMost Max Error PMostC Max Error PMost Avg Error PMostC Avg Error  120  140  160  180  200  220  240  260  0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 ratio    0        total   of cuboids   of precise cuboids Figure 4  Varying   0 in Algorithm 2  for PMost and PMostC  6  EXPERIMENTS We evaluate and compare seven proposed techniques on both real and synthetic datasets  The    rst two techniques are Kall and Kbase  which were de   ned at the beginning of Section 3  We denote them as All and Base  respectively  in this section  Another two are based on our generalized framework Kpart  One of these has the objective of bounding the max noise variance  Problem 1  and is denoted by BMax  Kpart   Algorithm 1   The other one has the objective of maximizing the number of cuboids with noise variance no more than a given variance threshold   0  Problem 2  and is denoted by PMost  Kpart   Algorithm 2   The last three techniques involve applying the methods in Section 5 2 to enforce consistency in the data cubes published by All  Lpre   L   BMax  and PMost  The resulting three techniques are denoted AllC  BMaxC  and PMostC  respectively  The LP based techniques in Section 5 1 are not practical for large tables  All seven algorithms are coded in C   and evaluated on an 8GB 64 bit 2 40GHz PC  Real dataset  We use the Adult dataset from the UCI Machine Learning Repository  http   archive ics uci edu ml   with census information  It contains 32 561 rows with 8 categorical dimensions  workclass  cardinality 9   education  16   marital status  7   occupation  15   relationship  6   race  5   sex  2   and salary  2   Synthetic dataset  To generate the fact table  we    rst    x the number of dimensions and the dimension cardinalities  Then we generate each row independently  with each of its column values drawn uniformly and independently from the domain of its dimension  Error measurement  We compute the error as the absolute difference between the real count measure computed directly from the fact table and the noisy measure released by one of the seven   DP algorithms  The cuboid error is the average error for all cells in this cuboid  We report both the max cuboid error and the average cuboid error among all published cuboids  We release L   all cuboids and evaluate all algorithms on the same set of published cuboids  This is the hardest test for all algorithms  as less noise will be needed if L omits some cuboids  Exp I  Varying privacy parameter    Using the Adult dataset  we vary privacy parameter   from 0 25 to 2  Smaller   implies more privacy and thus more noise  Recall Algorithm 2  for PMost  takes a variance threshold   0 in the input  here  we set   0   0 5       where      is the variance bound found by Algorithm 1  for BMax   Figure 3 uses a logarithmic scale to show the max average error in the released cuboids  As the inconsistent version of an approach always has at least as much error as the consistent version  the two versions  All AllC  BMax BMaxC  and PMost PMostC  are stacked together on a single bar in each histogram in Section 6  BMax and PMost always incur much less error than the two baselines Base and All  BMax is better than PMost for bounding the max error  while PMost is better than BMax for bounding the average error  Base performs the worst in terms of the max error  because only the base cuboid is computed from the table  and the noise is magni   ed signi   cantly when cuboids at higher levels are computed by aggregating cells in the base cuboid  All is the worstAll AllC BMax BMaxC PMost PMostC Base  20  40  80  160  320  640  1280  2560 0 1 2 3 4 5 6 7 8 m  Dimensionality of Cuboids Figure 5  Max cuboid error in different cuboids as dimensionality varies  when all cuboids must be released in terms of the average error  for the large amount of noise initially injected in each cuboid  Error decreases as   increases  As suggested by Theorem 10  iv   v   our consistency enforcing techniques tend to reduce error  since the variance of the consistent measure   c     is no larger than that of the inconsistent   c      So AllC BMaxC PMostC reduces error by 30  50   compared to All BMax PMost  while providing the same privacy guarantee  Exp II  Varying variance threshold   0  Note that performance of PMost and PMostC depends on the input threshold   0 in Problem 2  as   0 determines Lpre  Fix     1  Given the variance bound      found by Algorithm 1  we vary   0 from 0 1     to 0 9       Figure 4 shows how   0 affects Algorithm 2 on the Adult dataset  As   0 increases  we have more precise cuboids  i e   the released cuboids with noise variance no more than   0  When   0          all 2 8   256 cuboids are precise  and PMost PMostC is equivalent to BMax BMaxC  as the same Lpre is used  But the max average error of PMost and PMostC does not increase monotonically with   0  both   0        and   0   0 5     are local optimums for minimizing errors  In the remaining experiments  we set   0   0 5       Exp III  Noise in different cuboids  Figure 5 shows the max error in cuboids of different dimensionality on Adult  for     1  Recall in an m dim cuboid  a cell has non     values on exactly m dimensions  As m decreases  a cell in an m dim cuboid aggregates more base cells  So Base aggregates more noise from base cells as m drops  and performs the worst for m   3  BMaxC is the best when 0   m    5  and its max error changes little for 0   m    7  The consistency enforcing techniques  AllC BMaxC PMostC  are very effective for small m  reducing error by up to 70   Exp IV  Varying number of dimensions  We create two more dimensions  each of which has cardinality 4  on the Adult dataset  and generate their values randomly for each row  We consider the fact table with the    rst 6 to all the 10 dimensions  Fix     1  We report the max average error of the seven approaches in Figure 6  As the number of dimensions increases  BMaxC and PMostC are always the best two  and the error in both All and Base is much larger and increases faster than the error in others  Exp V  Varying cardinality of dimensions  We generate synthetic tables with 7 dimensions  each of which has the same cardinality  We vary the cardinality from 2 to 10  and generate a table with 10 8 rows randomly for each  We report the error of each approach in Figure 7  for     1  The performance of Base deteriorates quickly as the cardinality increases  because more cells in the base cuboid need to be aggregated  The performance of All does not change much  for its performance is mainly determined by the number of dimensions  which determines the total number of cuboids  Again  BMaxC and PMostC perform best in most cases  Exp VI  Ef   ciency  Consider a data cube with 5 10 dimensions on the Adult dataset  with two more synthetic dimensions as in ExpIV   The overall DP data cube publishing time is reported in Figure 8  which can be decomposed as follows  Recall Algorithms 1 2  80  160  320  640  1280  2560  5120  10240 6 7 8 9 10 Dimensionality of Fact Table  a  Max cuboid error  40  80  160  320  640  1280 6 7 8 9 10 Dimensionality of Fact Table  b  Average cuboid error Figure 6  Varying dimensionality of fact table  20  40  80  160  320  640  1280  2560  3840 2 4 6 8 10 Cardinality of Dimensions  a  Max Cuboid Error  20  40  80  160 2 4 6 8 10 Cardinality of Dimensions  b  Average Cuboid Error Figure 7  Varying cardinality of dimensions  7 dimensions  select Lpre for BMax BMaxC and PMost PMostC  respectively  The running time of Algorithms 1 2 is reported in Figure 9 a   For different choices of Lpre on the 8 dim table  the time needed for noise injection  Kpart  and consistency enforcement  the method in Section 5 2  in all cuboids is reported in Figure 9 b   From Figure 9 a   although the running time of Algorithms 1 2 is polynomial in 2 d   they are not the bottleneck  Their running time is always a very small portion of the overall DP data cube publishing  From Figure 9 b   it is shown that the consistency enforcement time increases linearly with  Lpre   as predicted by Theorem 10  ii   The time for noise injection decreases as  Lpre  increases  This is because when more cuboids are initially injected with noise  less aggregation of noise occurs later on  From Figure 8  using consistency enforcement  AllC is especially expensive  because  Lpre    2 d in AllC  BMaxC and PMostC  although using consistency enforcement  usually only need 3  10  time of AllC  for they use smaller Lpre   s  For all approaches  the publishing time increases exponentially with the dimensionality  mainly because the total number of cells increases exponentially   1  3  10  40  160  640  2560  10240 5  2x10 5   6  9x10 5   7  3x10 6   8  8x10 6   9  4x10 7  10  2x10 8   Dimensionality of Fact Table  and Number of Cells  Figure 8  DP data cube publishing time  in seconds  Summary  Both All and Base are sensitive to the dimensionality of the fact table  and Base is also sensitive to cardinalities of dimensions  All usually has a large average error  as a large amount of noise is injected into all cuboids  Base has a large max error  because noise is aggregated from the base cells  and that is why Base incurs small average errors in the cuboids close to the base cuboid  BMaxC and PMostC are the best most of the time  All BMax PMost run much faster than AllC BMaxC PMostC  But with our consistency enforcement  AllC BMaxC PMostC reduce error in All BMax PMost  respectively  by typically 30   70  or more  and ensure that the published data is consistent  The error of BMaxC and PMostC is usually only 5  30  of 0 001  0 01  0 1  1  5  5 6 7 8 9 10 Number of Dimensions Select L pre  for BMax  Alg  1  Select L pre  for PMost Alg  2   a  Time  in seconds   1  2  4  8  16  32  64  128  256  3 5 12 20 64 256  L pre   Noise injection Enforce consistency  b  Time  in seconds  Figure 9  Ef   ciency of different algorithms the error incurred by All  and 20  50  of the error of AllC  Note that the y axis in our    gures is always in logarithmic scale  Because of our consistency enforcing method  the error of AllC is sometimes comparable to BMax BMaxC and PMost PMostC  when the dimensionality of the fact table is low  However  AllC is very expensive because  Lpre  in AllC is equal to 2 d  recall Theorem 10 for the running time of our consistency enforcement   When there are more than    ve dimensions  AllC   s publishing time is 6 10 times larger than BMaxC   s    10 times for ten dimensions   and 10 40 times larger than PMostC   s    40 times for ten dimensions   7  DISCUSSION AND EXTENSIONS 7 1 Relative Error vs  Absolute Error The amount of noise Kpart adds to DP cuboids is independent of the number of rows and speci   c data in the fact table  Instead  the selection of Lpre and the amount of noise depend only on which cuboids are to be published  the number of dimensions  and their cardinalities  So our expected absolute error is    xed if the structure of the fact table is    xed  no matter how many rows there are  This feature of our approaches is also true in DP based frameworks for different publishing tasks  4  19  23  36   The implication is that the expected relative error cannot be bounded in general  Because  with the expected absolute error    xed  some cells may have very small values of the count measure  e g   1   while some have very large values  e g   10 3    That is also why we report absolute error in Section 6  On the other hand  the advantage is  for a particular cell  it has less relative error if it aggregates more rows  7 2 Further Reduction of Noise We can generalize our noise control framework Kpart by adding different amounts of noise to different cuboids in Lpre to further reduce noise  Suppose Lpre    C1           Cs  and noise Lap   i  is injected into each cell in Ci  By the composition property of DP  28   we claim that if  s i 1 1   i      then publishing Lpre is   DP  Finding the optimal Lpre and parameters    i  to achieve the goals in Problems 1 and 2 is hard  but Algorithms 1 and 2 can be modi   ed to provide approximate solutions  Here  privacy parameter   can be interpreted as the cost budget in WE I G H T E D SE T COVER  To enforce consistency and yield an optimality result similar to Theorem 10  we can solve a weighted version of the least squares problem in Section 5 2  Our experiments show this reduces absolute error in BMaxC PMostC by 10   We omit the details here  7 3 Extension to Other Measures Our techniques for the count measure can be extended to other two basic measures sum and avg  sum can be considered as a generalized count measure  where each row in the fact table is associated with a value instead of just 0 1  Compared to count  the sensitivity of a publishing function for sum is magni   ed    times  where    is the range of possible values for any individual fact table tuple  Thus our techniques for count can be applied to sum  with the noise variance magni   ed   2 times  To handle the average measure avg  we can compute two DP data cubes  partitioning the privacy budget across them   one with sum measure and one with count measure  from the same fact table  The avg measure of a cell can be computed from the two cubes  without violating DP  7 4 Handling Larger Data Cubes Our approaches introduced so far are applicable for mid size data cubes  e g   with     12 dimensions or     10 9 cells  Since the current framework needs to store all cells for consistency enforcement  it cannot handle data cubes with too many cells  For even larger data cubes  e g   with     20 dimensions and     2 20 cuboids   it is unnecessary to publish all cuboids at one time  as typical users are likely to query only a very small portion of them  Also  it is impossible to publish all cuboids while ensuring DP  as the huge amount of noise will make the result meaningless  So we outline an online version of our approach as follows  Initially  Lpre       and we have certain amount   of privacy budget  When a cuboid query C comes  if C      Lpre and C can be computed from some DP cuboid C         Lpre  there are two choices  a  compute C from C       with error in C     magni   ed in C  or b  compute real cuboid C using high dimensional OLAP techniques like  25   inject noise into C to obtain a DP cuboid  insert C into Lpre  and deduct a certain amount of privacy budget from    If C      Lpre but C cannot be computed from any DP cuboid C         Lpre  we have to follow b  above  If C     Lpre or C used to be queried  we can directly output the old DP cuboid C  After we run out of privacy budget    to ensure   DP  we cannot create new DP cuboids in Lpre any more and may be unable to answer new queries  How to distribute the privacy budget online so that more queries can be answered with less error is interesting future work  8  RELATED WORK Since   differential privacy  DP   12  was introduced  many techniques have been proposed for different data publishing and analysis tasks  refer to  10  11  for a comprehensive review   For example  the notion of DP has been applied to releasing query and click histograms from search logs  18  22   recommender systems  29   publishing commuting patterns  27   publishing results of machine learning  6  8  20   clustering  13  30   decision tree  14   mining frequent patterns  5   and aggregating distributed time series  31   For a single counting query  one method to ensure DP  12  has been shown to be optimal under a certain utility model  17   Recent works  36  19  23  on differentially private count queries over histograms are related to our work  Xiao et al   36  propose to use the Haar wavelet for range count queries  Hay et al   19  propose an approach based on a hierarchy of intervals  Li et al   23  propose a general framework which supports answering a given workload of count queries  and consider the problem of    nding optimal strategies for a workload  While  36  and  19  can be uni   ed in the framework in  23   the speci   c algorithms given in  36  19  are more ef   cient than the matrix multiplication used in  23   Xiao et al  extend their wavelet approach to nominal attributes and multidimensional count queries  Their extended approach can be applied in our problem to achieve the same noise bounds as our Kall  refer to Theorem 3 in  36  and Theorem 2 in this work   In general  our Kpart will add less noise  as shown in Theorems 6 7  Barak et al   4  show how to publish a set of marginals of a contingency table while ensuring DP  One of their two approaches is similar to Kall   add noise to all the marginals to be published  Their LP rounding method minimizes the L    distance while enforcing consistency and integrality and removing negative numbers in the publishing  Our Theorem 9 shows that minimizing the L 1 distanceyields a much better theoretical bound on error  The other approach in  4  is similar  but moves to the Fourier domain at    rst  Unlike our work  they do not optimize the publishing strategy  i e   the selection of Lpre   Moreover  the number of variables in the LP equals the number of cells  often   10 6 in our experiments   So LP based methods can only handle data cubes with small numbers of cells  Agrawal et al   3  study how to support OLAP queries while ensuring a limited form of    1    2  privacy  by randomizing each entry in the fact table with a constant probability  An OLAP query on the fact table can be answered from the perturbed table within roughly    dataset   4      1    2  privacy is in general not as strong as   differential privacy  Also  the error incurred by this method  3  depends on the dataset size  in our framework  the amount of noise to be added is data independent  only determined by the number of cuboids to be published and the structure of the fact table  Differential privacy provides much stronger privacy guarantees than other privacy concepts based on deterministic algorithms do  such as k anonymity  32  and its extension l diversity  26  and tcloseness  24    34  studies how to specify authorization and control inferences for OLAP queries in the data cube  9  CONCLUSIONS We have introduced a general noise control framework to release all or part of a data cube in an   differentially private way  To reduce the noise in the released cuboids  we choose certain cuboids Lpre to compute directly from the fact table  and add noise to make them differentially private  All the other cuboids are computed directly from the cuboids in Lpre  without adding additional noise or revisiting the fact table  We modeled the selection of Lpre as two optimization problems  proved them NP hard  and proposed approximation algorithms  To ensure consistency across different rollups of released cuboids  we proposed consistency enforcing techniques that have the side bene   t of reducing noise  10  ACKNOWLEDGMENTS We thank the reviewers for their insightful suggestions that immensely improved the paper  This work was supported in part by NSF IIS 09 05215  U S  Air Force Of   ce of Scienti   c Research MURI award FA9550 08 1 0265  the U S  Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053  NS CTA   A STAR SERC grant 1021580074  and a grant from Boeing  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the of   cial policies  either expressed or implied  of the Army Research Laboratory or the U S  Government  The U S  Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon  11  REFERENCES  1  www cs cmu edu  compthink mindswaps oct07 difpriv ppt  2007   2  N  R  Adam and J  C  Wortmann  Security control methods for statistical databases  A comparative study  ACM Comput  Surv   21 4  515   556  1989   3  R  Agrawal  R  Srikant  and D  Thomas  Privacy preserving OLAP  In SIGMOD  pages 251   262  2005   4  B  Barak  K  Chaudhuri  C  Dwork  S  Kale  F  McSherry  and K  Talwar  Privacy  accuracy  and consistency too  a holistic solution to contingency table release  In PODS  pages 273   282  2007   5  R  Bhaskar  S  Laxman  A  Smith  and A  Thakurta  Discovering frequent patterns in sensitive data  In KDD  pages 503   512  2010   6  A  Blum  K  Ligett  and A  Roth  A learning theory approach to non interactive database privacy  In STOC  pages 609   618  2008   7  S  Boyd and L  Vandenberghe  Convex Optimization  Cambridge Univ  Press  2004   8  K  Chaudhuri and C  Monteleoni  Privacy preserving logistic regression  In NIPS  pages 289   296  2008   9  D  P  Dubhashi and A  Panconesi  Concentration of Measure for the Analysis of Randomized Algorithms  Cambridge Univ  Press  2009   10  C  Dwork  Differential privacy  A survey of results  In TAMC  pages 1   19  2008   11  C  Dwork  The differential privacy frontier  extended abstract   In TCC  pages 496   502  2009   12  C  Dwork  F  McSherry  K  Nissim  and A  Smith  Calibrating noise to sensitivity in private data analysis  In TCC  pages 265   284  2006   13  D  Feldman  A  Fiat  H  Kaplan  and K  Nissim  Private coresets  In STOC  pages 361   370  2009   14  A  Friedman and A  Schuster  Data mining with differential privacy  In KDD  pages 493   502  2010   15  B  C  M  Fung  K  Wang  R  Chen  and P  S  Yu  Privacy preserving data publishing  A survey on recent developments  ACM Comput  Surv   42 4   2010   16  S  R  Ganta  S  P  Kasiviswanathan  and A  Smith  Composition attacks and auxiliary information in data privacy  In KDD  pages 265   273  2008   17  A  Ghosh  T  Roughgarden  and M  Sundararajan  Universally utility maximizing privacy mechanisms  In STOC  pages 351   360  2009   18  M  G  tz  A  Machanavajjhala  G  Wang  X  Xiao  and J  Gehrke  Publishing search logs   a comparative study of privacy guarantees  TKDE  2011   19  M  Hay  V  Rastogi  G  Miklau  and D  Suciu  Boosting the accuracy of differentially private queries through consistency  In PVLDB  pages 1021   1032  2010   20  S  P  Kasiviswanathan  H  K  Lee  K  Nissim  S  Raskhodnikova  and A  Smith  What can we learn privately  In FOCS  pages 531   540  2008   21  D  Kifer  Attacks on privacy and deFinetti   s theorem  In SIGMOD  pages 127   138  2009   22  A  Korolova  K  Kenthapadi  N  Mishra  and A  Ntoulas  Releasing search queries and clicks privately  In WWW  pages 171   180  2009   23  C  Li  M  Hay  V  Rastogi  G  Miklau  and A  McGregor  Optimizing histogram queries under differential privacy  In PODS  pages 123   134  2010   24  N  Li  T  Li  and S  Venkatasubramanian  t closeness  Privacy beyond k anonymity and l diversity  In ICDE  pages 106   115  2007   25  X  Li  J  Han  and H  Gonzalez  High dimensional OLAP  A minimal cubing approach  In VLDB  pages 528   539  2004   26  A  Machanavajjhala  J  Gehrke  D  Kifer  and M  Venkitasubramaniam  l diversity  Privacy beyond k anonymity  In ICDE  page 24  2006   27  A  Machanavajjhala  D  Kifer  J  M  Abowd  J  Gehrke  and L  Vilhuber  Privacy  Theory meets practice on the map  In ICDE  pages 277   286  2008   28  F  McSherry  Privacy integrated queries  an extensible platform for privacy preserving data analysis  In SIGMOD  pages 19   30  2009   29  F  McSherry and I  Mironov  Differentially private recommender systems  building privacy into the Net   ix prize contenders  In KDD  pages 627   636  2009   30  K  Nissim  S  Raskhodnikova  and A  Smith  Smooth sensitivity and sampling in private data analysis  In STOC  pages 75   84  2007   31  V  Rastogi and S  Nath  Differentially private aggregation of distributed time series with transformation and encryption  In SIGMOD  pages 735   746  2010   32  P  Samarati and L  Sweeney  Generalizing data to provide anonymity when disclosing information  abstract   In PODS  page 188  1998   33  S  D  Silvey  Statistical Inference  Chapman Hall  1975   34  L  Wang  S  Jajodia  and D  Wijesekera  Preserving privacy in on line analytical processing data cubes  In Secure Data Management in Decentralized Systems  pages 355   380  2007   35  R  C  W  Wong  A  W  C  Fu  K  Wang  and J  Pei  Minimality attack in privacy preserving data publishing  In VLDB  pages 543   554  2007   36  X  Xiao  G  Wang  and J  Gehrke  Differential privacy via wavelet transforms  In ICDE  pages 225   236  2010 APPENDIX  Proofs Proof of Theorem 2  The vector Fall T   has sensitivity S Fall     L   So from Theorem 1  Kall is   DP  As the noise is Lap  L      we have E    c a     c a  and Var    c a     2 L    2     Proof of Theorem 3  The sensitivity S Fbase    1  So from Theorem 1  publishing Fbase T    Lap 1     is   DP  For any other cell a not in the base cuboid  the computation of   c a  takes the released base cuboid as input  so from the composition property  28   is    DP  For a cell a in a cuboid C  it aggregates   Aj     C   Aj   cells  a     in the base cuboid  Then E    c a       a  E    c a          a  c a       c a   And since noise is generated independently  we have the variance Var    c a       a  Var    c a            Aj     C   Aj        2   2     Proof of Theorem 4  Consider measures  c a   for all cells in the s selected cuboids in Lpre  the sensitivity of publishing Lpre is s    Lpre   So by Theorem 1 and the composition property of DP  adding a noise Lap s    to each cell in Lpre and computing L from Lpre is   DP  E    c a     c a  is also from the linearity of expectation    Proof of Theorem 5  To complete the proof  we reduce the problem VE RT E X COVER I N  D E  G R E  E  3 G R A P H S to the BO U N D MA X VA R I A N C E problem  Then the hardness of the PU B L  I  S H MO S T problem follows  Following is an instance of the VE RT E X COVER problem in a degree 3 graph  where the degree of a vertex is bounded by 3   Given a degree 3  undirected  graph G V   E  where  V     n  decide if G has a vertex cover V       V with at most m    n  vertices  V       V is said to be a vertex cover of G iff for any edge uv     E  we have either u     V   or v     V     Abusing the notations a bit  let cov v  be the edges incident on a vertex v  then we want to decide if there is V       V such that    v   V   cov v    E and  V         m  We can assume the degree of any vertex in V is larger than 1  since a degree 1 vertex will be never chosen into a minimum vertex cover  And since there is at least one vertex with degree 3  otherwise G can be decomposed into cycles which are the trivial case for the VE RT E X COVER problem   we have  E    2n 2   n  Construct an instance of the BO U N D MA X VA R I A N C E problem from the above instance of VE RT E X COVER problem  accordingly   i  For each edge e     E  create a dimension Ae with cardinality  Ae    2  and a 1 dim cuboid  C 1 e      Ae    ii  Create 3n distinct dimensions B1           B3n  each of cardinality 2  For each of them  create a 1 dim cuboid  C 1 i      Bi    iii  For each vertex v     V   create a 3 dim cuboid  C 3 v      Ae1   Ae2   Ae3   for each edge ei incident on v  Note that a vertex may have less than 3 edges incident on it  in this case  we create one or two new distinct dimensions with cardinality 2  and include them in  C 3 v    So we create n 3 dim cuboids here   iv  Create another n 3 dim cuboids C 3 x   s  each cuboid  C 3 x     Bx1   Bx2   Bx3    where each Bxi    s are distinct dimensions with cardinality 2 created in  ii    v  For each 3 dim cuboid   C 3 v      Ae1   Ae2   Ae3   or  C 3 x     Bx1   Bx2   Bx3    create a new dimension Dv or Dx  respectively  with cardinality 4  and a 4 dim cuboid  C 4 v      Ae1   Ae2   Ae3   Dv  or  C 4 x     Bx1   Bx2   Bx3   Dx   respectively  So in total  we have 2n 4 dim cuboids created here  In this instance of the BO U N D MA X VA R I A N C E problem  we want to publish all the 1 dim  3 dim  and 4 dim cuboids in  i   v   denoted by L  and to decide if we can select a set of cuboids Lpre such that the max noise variance in releasing L from Lpre using Kpart is at most 8 3n   m  2    2  noise Lpre      8 3n   m  2    2    We prove the VE RT E X COVER instance is YES if and only if the BO U N D MA X VA R I A N C E instance is YES  to complete our proof  Due to the space limit  details are deferred to the full version    Remark  The main dif   culty in the reduction is the lattice structure of cuboids  And  how to prove disprove the NP hardness when the number of dimensions d is bounded  d   O log  L    and how to prove the hardness of approximation are interesting open questions  Proof of Lemma 1  The proof is from the de   nition  Let s    Lpre   If noise C  Lpre          from Equation  2   there is a C       Lpre s t  mag C  C             2 2s2   Thus from Equation  4   C     cov C             s   The converse direction is similar    Proof of Theorem 6  Approximation Ratio  Suppose the optimal solution is          i e   there are s     cuboids Lpre s t  for any C     L  noise C  Lpre               From Lemma 1  equivalently  there are s     cuboids C     1            C     s    s t   s     i 1 cov C     i               s         L  And  from Equation  4   for any cuboid C and      0  cov C     2               s         cov C               s        Suppose in line 4 of Algorithm 1  we have         2        and s     s       In the following part  we will prove when      ln  L    1  the algorithm FE A S I B L E L       s  can    nd s cuboids C   1           C   s  s t   s i 1 cov C   i           s    L  which implies that  with Lpre    C   1             C   s   maxC   L noise C  Lpre              2          So we can conclude that Algorithm 1    nds an  ln  L    1  2  approximation  Since for any cuboid C  cov C         s    cov C              s       for the selection of    and p above  we write both of them as cov C   Now we only need to prove  if there exists s     cuboids C     1            C     s    such that  s     i 1 cov C     i     L  the algorithm FE A S I B L E    nds s     s     cuboids C   1           C   s  in this order  s t   s i 1 cov C   i     L  We apply the analysis of the greedy SE T COVER algorithm here  Let li be the number of cuboids in L uncovered by  C   1 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>iReduct  Differential Privacy with Reduced Relative Errors ### Xiaokui Xiao       Gabriel Bender       Michael Hay       Johannes Gehrke         School of Computer Engineering Nanyang Technological University Singapore xkxiao ntu edu sg    Department of Computer Science Cornell University Ithaca  NY  USA  gbender mhay johannes  cs cornell edu ABSTRACT Prior work in differential privacy has produced techniques for answering aggregate queries over sensitive data in a privacypreserving way  These techniques achieve privacy by adding noise to the query answers  Their objective is typically to minimize absolute errors while satisfying differential privacy  Thus  query answers are injected with noise whose scale is independent of whether the answers are large or small  The noisy results for queries whose true answers are small therefore tend to be dominated by noise  which leads to inferior data utility  This paper introduces iReduct  a differentially private algorithm for computing answers with reduced relative errors  The basic idea of iReduct is to inject different amounts of noise to different query results  so that smaller  larger  values are more likely to be injected with less  more  noise  The algorithm is based on a novel resampling technique that employs correlated noise to improve data utility  Performance is evaluated on an instantiation of iReduct that generates marginals  i e   projections of multi dimensional histograms onto subsets of their attributes  Experiments on real data demonstrate the effectiveness of our solution  Categories and Subject Descriptors H 2 0  DATABASE MANAGEMENT   Security  integrity  and protection General Terms Algorithms  Security Keywords Privacy  Differential Privacy ### 1  INTRODUCTION Disseminating aggregate statistics of private data has much bene   t to the public  For example  census bureaus already publish aggregate information about census data to improve decisions about the allocation of funding  hospitals could release aggregate information about their patients for medical research  However  since Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  those datasets contain private information  it is important to ensure that the published statistics do not leak sensitive information about any individual who contributed data  Methods for protecting against such information leakage have been the subject of research for decades  The current state of the art paradigm for privacy preserving data publishing is differential privacy  Differential privacy requires that the aggregate statistics reported by a data publisher should be perturbed by a randomized algorithm G  so that the output of G remains roughly the same even if any single tuple in the input data is arbitrarily modi   ed  This ensures that given the output of G  an adversary will not be able to infer much about any single tuple in the input  and thus privacy is protected  In this paper  we consider the setting in which the goal is to publish answers to a batch set of queries  each of which maps the input dataset to a real number  4 8 16 17 21 23 32   The pioneering work by Dwork et al   8  showed that it is possible to achieve differential privacy by adding i i d  random noise to each query answer if the noise is sampled from a Laplace distribution where the scale of the noise is appropriately calibrated to the query set  In recent years  there has been much research on developing new differentially private methods with improved accuracy  4 16 17 21 32   Such work has generally focused on reducing the absolute error of the queries  and thus the amount of noise injected into a query is independent of the actual magnitude of the query answer  In many applications  however  the error that can be tolerated is relative to the magnitude of the query answer  larger answers can tolerate more noise  For example  suppose that a hospital performs queries q1 and q2 on a patient record database in order to obtain counts of two different but potentially overlapping medical conditions  Suppose the    rst condition is relatively rare  so that the exact answer for q1 is 10  while the second is common  so that the exact answer for q2 is 10000  In that case  the noisy answer to q1 might be dominated by noise  even though the result of q2 would be quite accurate  Other applications where relative errors are important include learning classi   cation models  12   selectivity estimation  13   and approximate query processing  29   Our Contributions  This paper introduces several techniques that automatically adjust the scale of the noise to reduce relative errors while still ensuring differential privacy  Our main contribution is the iReduct algorithm  Section 4   In a nutshell  iReduct initially obtains very rough estimates of the query answers and subsequently uses this information to iteratively re   ne its estimates with the goal of minimizing relative errors  Ordinarily  iterative resampling would incur a considerable privacy    cost    because each noisy answer to the same query leaks additional information  We avoid this cost by using an innovative resampling function we call NoiseDown  The key to NoiseDown is that it resamples conditioned onprevious samples  generating a sequence of correlated estimates of successively reduced noise magnitude  We prove that the privacy cost of the sequence is equivalent to the cost of sampling a single Laplace random variable having the same noise magnitude as the last estimate in the sequence produced by NoiseDown  To demonstrate the application of iReduct  we present an instantiation of iReduct for generating marginals  i e   the projections of a multi dimensional histogram onto various subsets of its attributes  Section 5   With extensive experiments on real data  Section 6   we show that iReduct has lower relative errors than existing solutions and other baseline techniques introduced in this paper  In addition  we demonstrate the practical utility of optimizing for relative errors  showing that it yields more accurate machine learning classi   ers  2  PRELIMINARIES 2 1 Problem Formulation Let T be a dataset  and Q    q1          qm  be a sequence of m queries on T   each of which maps T to a real number  We aim to publish the results of all queries in Q on T using an algorithm that satis   es   differential privacy  a notion of privacy de   ned based on the concept of neighboring datasets  DEFINITION 1  NE I G H B O R I N G DATAS ETS   Two datasets T1 and T2 are neighboring datasets if they have the same cardinality but differ in one tuple    DEFINITION 2    DIFFERENTIAL PR I VAC Y   9       A randomized algorithm G satis   es   differential privacy  if for any output O of G and any neighboring datasets T1 and T2  P r  G T1    O      e      P r  G T2    O      We measure the quality of the published query results by their relative errors  In particular  the relative error of a published numerical answer r     with respect to the original answer r is de   ned as err r          r         r  max r         1  where    is a user speci   ed constant  referred to as the sanity bound  used to mitigate the effect of excessively small query results  This de   nition of relative error follows from previous work  13 29   For ease of exposition  we assume that all queries in Q have the same sanity bound  but our techniques can be easily extended to the case when the sanity bound varies from query to query  Table 1 summarizes the notations that will be frequently used in this paper  2 2 Differential Privacy via Laplace Noise Dwork et al   9  show that   differential privacy can be achieved by adding i i d  noise to the result of each query in Q  Speci   cally  the noise is sampled from the Laplace distribution which has the following probability density function  p d f   P r      x    1 2   e    x             2  where    is the mean of the distribution  we assume      0 unless stated otherwise  and     referred to as the noise scale  is a parameter that controls the degree of privacy protection  A random variable that follows the Laplace distribution has a standard deviation of     2       and an expected absolute deviation of     ie  E                   Symbol Description q T  the result of a query q on a dataset T Q a given sequence of m queries  q1            qm     a sequence of m noise scales    1            m     the sanity bound for the queries in Q S Q  the sensitivity of Q GS Q      the generalized sensitivity of Q with respect to    x an arbitrary real number yi a random variable such that yi     x follows a Laplace distribution         min x  y     1  f the conditional probability density function of y   given x  y      and        as de   ned in Equation 6 Table 1  Frequently Used Notations The resulting privacy depends both on the scale of the noise and the sensitivity of Q  Informally  sensitivity measures the maximum change in query answers due to changing a single tuple in the database  DEFINITION 3   SE N S I T I V I T Y   9       The sensitivity of a sequence Q of queries is de   ned as S Q     max T1 T2  m i 1  qi T1      qi T2    3  where T1 and T2 are any neighboring datasets    Dwork et al  prove that adding Laplace noise of scale    leads to  S Q      differential privacy  PROPOSITION 1   PR I VAC Y FROM LA P L AC E NO I S E   9       Let Q be a sequence of queries and G be an algorithm that adds i i d  noise to the result of each query in Q  such that the noise follows a Laplace distribution of scale     Then  G satis   es  S Q      differential privacy    3  FIRST CUT SOLUTIONS Dwork et al    s method  described above  may have high relative errors because it adds noise of    xed scale to every query answer regardless of whether the answer is large or small  Thus  queries with small answers have much higher expected relative errors than queries with large answers  In this paper  we propose several techniques that calibrate the noise scale to reduce relative errors  This section describes two approaches  The    rst is a simple idea that achieves uniform expected relative errors but fails to satisfy differential privacy  while the second is a    rst attempt at a differentially private solution  Before presenting the strategies  we introduce an extension to Dwork et al    s method that adds unequal noise to query answers  adapted from  32    Let         1            m  be a sequence of positive real numbers such that qi will be answered with noise scale   i for i      1  m   We extend the notion of sensitivity to a sequence of queries Q and a corresponding sequence of noise scales     DEFINITION 4  GE N E R A L I Z E D SE N S I T I V I T Y   3 2       Let Q be a sequence of m queries  q1           qm   and    be a sequence of m positive constants    1             m   The generalized sensitivity of Q with respect to    is de   ned as GS Q        max T1 T2  m i 1   1   i     qi T1      qi T2        4  where T1 and T2 are any neighboring datasets   The following proposition states that adding Laplace noise with unequal scale leads to GS Q      differential privacy  PROPOSITION 2   PR I VAC Y FROM UN E QUA L NO I S E   3 2       Let Q    q1           qm  be a sequence of m queries and         1             m  be a sequence of m positive real numbers  Let G be an algorithm that adds independent noise to the result of each query qi in Q  i      1  m    such that the noise follows a Laplace distribution of scale   i  Then  G satis   es GS Q      differential privacy    For convenience  we use LaplaceNoise T    Q      to denote the above algorithm  On input T    Q      it returns a sequence of noisy answers Y    y1          ym   where yi   qi T       i and   i is a sample from a Laplace distribution of scale   i for i      1  m   3 1 Proportional Laplace Noise To reduce relative errors  a natural but ultimately    awed approach is to set the scale of the noise to be proportional to the actual query answer   Extremely small answers are problematic  so we can set the noise scale to be the maximum of qi T   and the sanity bound      We refer to this strategy as Proportional  The expected relative error for query qi is   i max qi T         and so setting   i   c max qi T        for some constant c ensures that all query answers have equal expected relative errors  thereby minimizing the worst case relative error  Unfortunately Proportional is    awed because it violates differential privacy  This is because the scale of the noise depends on the input  As a consequence  two neighboring datasets may have different noise scales for the same query  and hence  some outputs may become considerably more likely on one dataset than the other  The following example illustrates the privacy defect of Proportional  EX A M P L E 1  Let      1 and     1  Suppose that we have a census dataset that reports the ages of a population consisting of 5 individuals  T1    42  17  35  19  55   We have two queries  q1 asks for the number of teenagers  ages 13     19   and q2 asks for the number of people under the age of retirement  ages under 65   Since we have q1 T1    2 and q2 T2    5  the Proportional algorithm would set         1    2  such that   1   2c and   2   5c for some constant c  It can be veri   ed that c   7 10 achieves GS Q           Thus  we set   1   2c   1 4 and   2   5c   3 5  Now consider a neighboring dataset T2 in which one of the teenagers has been replaced with a 20 year old  T2    42  17  35  20  55   Compared to T1  the answer on T2 for q1 is smaller by 1 and q2 is unchanged  and accordingly Proportional sets the scales so that   1   c and   2   5c  It can be veri   ed that c   6 5 achieves GS Q          and so   1   1 2 and   2   6  Thus  the expected error of q1 is higher on T1 than on T2  If we consider the probability of an output that gives a highly inaccurate answer for the    rst query  such as y1   102 and y2   5  we can see it is much more likely on T1 than T2  P r Proportional outputs y1  y2 given T1  P r Proportional outputs y1  y2 given T2    exp     y1     q1 T1   1 4     exp     y2     q2 T1   3 5  exp     y1     q1 T2   1 2     exp     y2     q2 T2   6    exp    100 1 4   101 1 2    exp 535 42    exp     This demonstrates that Proportional violates differential privacy    Algorithm TwoPhase  T  Q       1   2  1  let m    Q  and qi be the i th  i      1  m   query in Q 2  initialize         1             m  such that   i   S Q   1 3  Y   LaplaceNoise T    Q      4        Rescale Y            2  5  if GS Q             2 6  Y     LaplaceNoise T    Q        7  for any i      1  m  8  yi        2 i    yi      2 i    y   i       2 i       2 i   9  return Y Figure 1  The TwoPhase Algorithm 3 2 Two Phase Noise Injection To overcome the drawback of Proportional  a natural idea is to ensure that the scale of the noise is decided in a differentially private manner  Towards this end  we may    rst apply LaplaceNoise to compute a noisy answer for each query in Q  and then use the noisy answers  instead of the exact answers  to determine the noise scale  This motivates the TwoPhase algorithm illustrated in Figure 1  TwoPhase takes as input    ve parameters  a dataset T   a sequence Q of queries  a sanity bound     and two positive real numbers  1 and  2  Its output is a sequence Y    y1          ym  such that yi is a noisy answer for each query qi  The algorithm is called TwoPhase because it interacts with the private data twice  In the    rst phase  Lines 1 3   it uses LaplaceNoise to compute noisy query answers  such that the scale of noise for all queries is equal to S Q   1  In the second phase  Lines 4 8   the noise scales are adjusted  based on the output of the    rst phase  and a second sequence of noisy answers is obtained  Intuitively  we would like to rescale the noise so that it is reduced for queries that had small noisy answers in the    rst phase  This could be done in a manner similar to Proportional where the noisy answer yi is used instead of the private answer qi T     i e   set      i proportional to max yi       However  it is possible to get even lower relative errors by exploiting the structure of the query sequence Q  For now  we will treat this as a    black box     referred to as Rescale on Line 4  and we will instantiate it when we consider speci   c applications in Section 5  Using the adjusted noise scales        TwoPhase regenerates a noisy result y   i for each query qi  Line 6   and then estimates the    nal answer as a weighted average between the two noisy answers for each query  Line 7   Speci   cally  the    nal answer for qi equals      2 i   yi     2 i   y   i       2 i      2 i    which is an estimate of qi T   with the minimum variance among all unbiased estimators of qi T   given yi and y   i  this can be proved using a Lagrange multiplier and the fact that yi and y   i have variance 2   2 i and 2    2 i   respectively   The following proposition states that TwoPhase ensures    differential privacy  PROPOSITION 3   PR I VAC Y  O F TwoPhase    TwoPhase ensures   differential privacy when its input parameters  1 and  2 satisfy  1    2          PROOF  TwoPhase only interacts with the private data T through two invocations of the LaplaceNoise mechanism  The    rst invocation satis   es  1 differential privacy  which follows from Proposition 1 and the fact that   i   S Q   1 for i      1  m   Before the second invocation  the algorithm checks that the generalized sensitivity is at most  2  therefore by Proposition 2 it satis   es  2 differential privacy  Finally  differentially private algorithms compose  the sequential application of algorithms  Gi   each satisfying  i differential privacy  yields     i  i  differential privacy  24   Therefore TwoPhase satis   es   1    2  differential privacy TwoPhase differs from Proportional in that the noise scale is decided based on noisy answers rather than the correct  but private   answers  While this is an improvement over Proportional from a privacy perspective  TwoPhase has two principal limitations in terms of utility  The    rst obvious issue is that errors in the    rst phase can lead to mis calibrated noise in the second phase  For example  if we have two queries q1 and q2 with q1 T     q2 T    the    rst phase may generate y1 and y2 such that y1   y2  In that case  the second phase of TwoPhase would incorrectly reduce the noise for q2  The second  more subtle issue is that given the requirement that  1    2       for a    xed    it is unclear how to set  1 and  2 so that the expected relative error is minimized  There is a tradeoff  If  1 is too small  the answers in the    rst phase will be inaccurate and the noise will be mis calibrated in the second phase  possibly leading to high relative errors for some queries  Although increasing  1 makes it more likely that the noise scale in the second phase will be appropriately calibrated  the answers in the second phase will be less accurate overall as the noise scale of all queries increases with decreasing  2  In general   1 and  2 must be chosen to strive a balance between the effectiveness of the    rst and second phases  which may be challenging without prior knowledge of the data distribution  In the next section  we will remedy this de   ciency with a method that does not require user inputs on the allocation of privacy budgets  4  ITERATIVE NOISE REDUCTION This section presents iReduct  iterative noise reduction   an improvement over the TwoPhase algorithm discussed in Section 3  The core of iReduct is an iterative process that adaptively adjusts the amount of noise injected into each query answer  iReduct begins by producing a noisy answer yi for each query qi     Q by adding Laplace noise of relatively large scale  In each subsequent iteration  iReduct    rst identi   es a set Q    of queries whose noisy answers are small and may therefore have high relative error  Next  iReduct resamples a noisy answer for each query in Q     reducing the noise scale of each query by a constant  This iterative process is repeated until iReduct cannot decrease the noise scale of any answer without violating differential privacy  Intuitively  iReduct optimizes the relative errors of the queries because it gives queries with smaller answers more opportunities for noise reduction  The aforementioned iterative process is built upon an algorithm called NoiseDown which takes as input a noisy result yi and outputs a new version of yi with reduced noise scale  We will introduce NoiseDown in Sections 4 1 and 4 2 below and present the details of iReduct in Section 4 3  4 1 Rationale of NoiseDown At a high level  given a query q and a dataset T   iReduct estimates q T   by iteratively invoking NoiseDown to generate noisy answers to q with reduced noise scales  The properties of the NoiseDown function ensure that an adversary who sees the entire sequence of noisy answers can infer no more about the dataset T than an adversary who sees only the last answer in the sequence  For ease of exposition  we focus on executing a single invocation of NoiseDown  Let Y  Y     be a Laplace random variable with mean value q T   and scale             such that            Intuitively  Y represents the noisy answer to q obtained in one iteration  and Y   corresponds to the noisy estimate generated in the next iteration  Given Y   the simplest approach to generating Y   is to add to the true answer q T   fresh Laplace noise that is independent of Y and has scale        This approach  however  incurs considerable privacy cost  For example  let q be a count query  such that for any two neighboring datasets T1 and T2  we have q T1      q T2          1  0  1   We will show that an algorithm that publishes independent samples Y   and Y in this scenario can satisfy   differential privacy only if       1       1      In other words  the privacy    cost    of publishing independent estimates of Y   and Y is 1       1      For any neighboring datasets T1 and T2  let q be a count query such that q T1    c and q T2    c   1 for some integer constant c  Then for any noisy answers y  y     c  we have P r  Y     y     Y   y   T   T1  P r  Y     y     Y   y   T   T2    P r  Y     y     Y   y   q T     c  P r  Y     y     Y   y   q T     c   1    1 2    exp      c     y               1 2   exp      c     y      1 2    exp      c   1     y              1 2   exp      c   1     y        exp   1        c   1     y          c     y         1      c   1     y       c     y       exp   1        1        In contrast  the privacy cost of publishing Y   alone is 1        That is  we pay an extra cost of 1    for sampling Y   even though the sample is discarded once we generate Y  1   Intuitively  the reason for this excess privacy cost is that both Y and Y   leak information about the state of the dataset T   Even though Y is less accurate than Y     an adversary who knows Y in addition to Y   has more information than an adversary that knows only Y     because the two samples are independent  The problem  then  is that the sampling process for Y   does not depend on the previously sampled value of Y   Therefore  instead of sampling Y   from a fresh Laplace distribution  we want to sample Y   from a distribution that depends on Y   Our aim is to do so in a way such that Y provides no new information about the dataset once the value of Y   is known  We can formalize this objective as follows  From an adversary   s perspective  the dataset T is unknown and can be modeled as a random variable  The adversary tries to infer the contents of the dataset by looking at Y   and Y   For any possible dataset T1  we want the following property to hold  P r   T   T1   Y   y  Y     y       P r T   T1   Y     y      5  To see how this restriction allows us to use the privacy budget more ef   ciently  let us again consider any two neighboring datasets T1 and T2  Observe that when Equation 5 is satis   ed  we can apply Bayes    rule  twice  to show that  P r Y     y     Y   y   T   Ti    P r T   Ti   Y     y     Y   y     P r Y     y     Y   y  P r T   Ti    P r T   Ti   Y     y        P r Y     y     Y   y  P r T   Ti    P r Y     y     T   Ti P r T   Ti  P r Y     y        P r Y     y     Y   y  P r T   Ti    P r Y     y     T   Ti     P r Y   y   Y     y     where the term P r Y   y   Y     y     does not depend on the value of T   1 Rather than discarding Y   one could combine Y and Y   to derive an estimate of q T    as done in the TwoPhase algorithm  see Line 8 in Figure 1   This does reduce the expected error  but it still has excess privacy cost compared to NoiseDown  as explained in Appendix A This allows us to derive an upper bound on the privacy cost of an algorithm that outputs Y followed by Y     Let us again consider a count query q such that q T1      q T2          1  0  1   Let Y   be a random variable that  i  follows a Laplace distribution with mean q T   and scale      but  ii  is generated in a way that depends on the observed value for Y   If these criteria are satis   ed and Equation 5 holds  it follows that  P r Y     y     Y   y   T   T1  P r Y     y     Y   y   T   T2    P r Y     y     T   T1     P r Y   y   Y     y     P r Y     y     T   T2     P r Y   y   Y     y       P r Y     y     T   T1  P r Y     y     T   T2      exp 1        The last step works because of our assumption that Y   follows a Laplace distribution with scale        The above inequality implies that obtaining correlated samples Y   and Y now incurs a total privacy cost of just 1        i e   no privacy budget is wasted on Y   In summary  if Y   follows a Laplace distribution but is sampled from a distribution that depends on Y   and if Equation 5 is satis     ed  then we can perform the desired resampling procedure without incurring any loss in the privacy budget  We now de   ne the conditional probability distribution of Y     DEFINITION 5  NO I S E DOWN DI S T R I BU T I O N   The Noise Down distribution is a conditional probability distribution on Y   given that Y   y  It is de   ned by the following conditional probability distribution function  p d f    Let              be    xed parameters  The conditional p d f  of Y   given that Y   y is de   ned as  f           y    Y   y              exp        y                exp        y                             y     y   6  where               y     y    1 4      1 cosh   1           1      2 cosh   1          exp        y   y              exp        y   y      1           exp        y   y    1           7  and cosh     denotes the hyperbolic cosine function  i e   cosh z     e z   e   z   2 for any z  Theorem 1 shows that this conditional distribution has the desired properties  namely   i  Y   follows a Laplace distribution and  ii  releasing Y in addition to Y   leaks no additional information  TH E O R E M 1   PROPERTIES OF NO I S E DOWN    Let Y be a random variable that follows a Laplace distribution with mean q T   and scale     Let Y   be a random variable drawn from a Noise Down distribution conditioned on the value of Y with parameters              such that      q T   and            Then  Y   follows a Laplace distribution with mean q T   and scale        Further  Y   and Y satisfy Equation 5 for any values of y     y  and T1  Theorem 1 provides the theoretical basis for the iterative resampling that is key to iReduct  The next section describes an algorithm for sampling from the Noise Down distribution  4 2 Implementing NoiseDown To describe an algorithm for sampling from the Noise Down distribution  Equation 6   it is suf   cient to    x Y   y for some real constant y and    x parameters         and        Let f   R      0  1  be 0 001 0 01 0 1 1    y   1 y y   1 y    f y     Figure 2  Illustration of f de   ned by f y       f           y     Y   y   i e   the conditional p d f  from Equation 6  We now describe an algorithm for sampling from the probability distribution de   ned by f  In the following derivation  we focus on the case where        y  we will describe later the modi   cations that are necessary to handle the case where       y  Let      min     y     1   By Equation 6  for any y           f y       e y                          y     y               exp               y               where               y     y    e y          1 4      1 cosh   1           1      2 cosh   1          exp     y          exp   1   y          exp     1   y           Note that the function    above is as de   ned in Equation 7 but takes a simpli   ed form given y          As     y      and      are all given  f y         exp y          y        holds  Similarly  it can be veri   ed that 1     y            y     1   f y         exp y           y            2     y        y   1         f y         exp    y             y         For example  Figure 2 illustrates f with the y axis in log scale  In summary  f conforms to an exponential distribution on each of the following intervals                     y     1   and  y   1         When the probability mass of f on each of the three intervals is known  it is straightforward to generate random variables that follow f on those intervals  Let   1               f y    dy       2     y   1    f y    dy     and   3          y 1 f y    dy     It can be shown that   1          cosh  1           cosh  1           exp     1       1                     2                cosh  1           1      8    2         cosh  1            e 1          e 1          1     e   1         1      4                   cosh  1           1        1     exp     1         1                 y   1        9    3           cosh  1           cosh  1           exp       y   1              y 1      2                cosh  1           1      10  For the remaining interval  y     1  y   1  on which f has a complex form  we resort to a standard importance sampling approach  Speci   cally  we    rst generate a random sample y   that is uniformly distributed in  y     1  y   1   After that  we toss a coin that comesAlgorithm NoiseDown      y             1  initialize a boolean variable invert   true 2  if      y 3  set             y      y  and invert   false 4       min     y     1  5  let   1    2    3  and    be as de   ned in Eqn  8  9  10  and 11 6  generate a random variable u uniformly distributed in  0  1  7  if u      0    1  8  generate a random variable y                    such that P r y     y         exp y           y        9  else if u        1    1     2  10  generate a random variable y            y     1  such that P r y     y         exp y           y           11  else if u      1       3  1  12  generate a random variable y        y   1        such that P r y     y         exp    y             y        13  else 14  while true 15  generate a random variable y   uniformly distributed in  y     1  y   1  16  generate a random variable u   uniformly distributed in  0  1  17  if u       f y        then break 18  if invert   true then return y     otherwise  return    y   Figure 3  The NoiseDown Algorithm up heads with a probability f y         where      1 2       cosh   1           exp       1      cosh   1           1    exp   y               max 0  y            1           11  If the coin shows a tail  we resample y   from a uniform distribution on  y     1  y   1   and we toss the coin again  This process is repeated until the coin shows a head  at which time we return y   as a sample from f  The following proposition proves the correctness of our sampling approach  PROPOSITION 4  NO I S E DOWN SA M P L I N G   Given        y  we have f y          for any y        y     1  y   1     So far  we have focused on the case where        y  For the case when       y  we    rst set             y      y  and then generate y   using the method described above  After that  we set y        y   before returning it as a sample from f  The correctness of this method follows from the following property of the Noise Down distribution f           y    Y   y   as de   ned Equation 6   f           y    Y   y    f                 y    Y      y  for any                y     y  As a summary  Figure 3 shows the pseudocode of the NoiseDown function that takes as input     y           and returns a sample from the distribution de   ned in Equation 6  4 3 The iReduct Algorithm We are now ready to present the iReduct algorithm  as illustrated in Figure 4  It takes as input a dataset T   a sequence Q of queries on T   a sanity bound     and three positive real numbers      max  and        iReduct starts by initializing a variable   i for each query qi     Q  i      1  m    setting   i     max  Lines 1 2   The userspeci   ed parameter   max is a large constant that corresponds to the greatest amount of Laplace noise that a user is willing to accept in any query answer returned by iReduct  For example  if Q is a sequence of count queries on T then a user may set   max to 10  of the number of tuples in the dataset T   Algorithm iReduct  T  Q           max         1  let m    Q  and qi be the i th  i      1  m   query in Q 2  initialize         1             m   such that   i     max 3  if GS Q          then return     4  Y   LaplaceNoise T    Q      5  let Q    Q 6  while Q         7  Q      PickQueries Q    Y           8  for each i      1  m  9  if qi     Q    then   i     i           10  if GS Q            11  for each i      1  m  12  if qi     Q    then yi   NoiseDown   qi T   yi    i            i   13  else 14  for each i      1  m  15  if qi     Q    then   i     i         16  Q    Q    Q    17  return Y Figure 4  The iReduct Algorithm As a second step  iReduct checks whether the conservative setting of the noise scale guarantees   differential privacy  Line 3   This is done by measuring the generalized sensitivity of the query sequence given noise scales     If the generalized sensitivity exceeds    iReduct returns an empty set to indicate that the results of Q cannot be released without adding excessive amounts of noise to the queries  Otherwise  iReduct generates a noisy result yi for each query qi     Q by applying Laplace noise of scale   i  Line 4   Given Y   iReduct iteratively applies NoiseDown to adjust the noise in yi so that noise magnitude is reduced for queries that appear to have high relative errors  Lines 5 16   In each iteration  iReduct    rst identi   es a set Q    of queries  Line 7  and then tries to decrease the noise scales of the queries in Q    by a constant        Lines 8 16   The selection of Q    is performed by an applicationspeci   c function that we refer to as PickQueries  An instantiation of PickQueries is given in Section 5 3  but in general  any algorithm can be applied  so long as the algorithm utilizes only the sanity bound     the noisy queries answers seen so far  and the queries    noise scales  and does not rely on the true answer qi T   of any query qi  This requirement ensures that the selection of Q    does not reveal any private information beyond what has been disclosed by the noisy results generated so far  For example  if we aim to minimize the maximum relative error of the query results  we may implement PickQueries as a function that returns the query qi that maximizes   i max yi       i e   the query whose noise scale is likely to be large with respect to its actual result  Once Q    is selected and the scales   i have been adjusted accordingly  iReduct checks whether the revised scales are permissible given the privacy budget  This is done by measuring the generalized sensitivity given the revised scales  Line 10   If the revised scales are permissible  iReduct applies NoiseDown to reduce the noise scale of each qi in Q    by        Lines 11 12   Otherwise  iReduct reverts the changes in    and removes the queries in Q    from its working set  Line 13 16   This iterative process is repeated until no more queries remain in the working set  which indicates that iReduct cannot    nd a subset of queries whose noise can be reduced without violating the privacy constraint  At this point  it outputs the noisy answers Y   TH E O R E M 2   PR I VAC Y  O F iReduct    iReduct ensures       differential privacy whenever its input parameter   satis   es              5  CASE STUDY  PRIVATE MARGINALS In this section  we present an instantiation of the TwoPhase and iReduct algorithms for generating privacy preserving marginals  Section 5 1 describes the problem and discusses existing solutions and Sections 5 2 and 5 3 describe the instantiations of TwoPhase and iReduct  5 1 Motivation and Existing Solution A marginal is a table of counts that summarize a dataset along some dimensions  Marginals are widely used by the U S  Census Bureau and other federal agencies to release statistics about the U S  population  More formally  a marginal M of a dataset T is a table of counts that corresponds to a subset A of the attributes in  T   If A contains k attributes A1  A2          Ak  then M comprises k i 1  Ai  counts  where  Ai  denotes the number of values in the domain of Ai  Each count in M pertains to a point  v1  v2           vk  in the k dimensional space A1  A2            Ak  and the count equals the number of tuples whose value on Ai is vi  i      1  k    For example  Table 2 illustrates a dataset T that contains three attributes  Age   Marital  Status  and Gender  Table 3 shows a marginal of T on  Status  Gender   In general  a marginal de   ned over a set of k attributes is referred to as a k dimensional marginal  While a dataset with k attributes can be equivalently represented as a single k dimensional marginal  such a marginal is likely very sparse  i e   all counts near zero   and thus will not be able to tolerate the random noise added for privacy  Instead  we therefore publish a setMof low dimensional marginals  each of which is a projection onto j dimensions for some small j   k  This is common practice at statistical agencies  and is consistent with prior work on differentially private marginal release  1   We can publish M in a   differentially private manner as long as we add Laplace noise of scale 2     M    to each count in every marginal  This is because changing a record affects only two counts in each marginal  each count would be offset by one   thus the sensitivity of the marginals is 2   M   i e   Laplace noise of scale 2     M    suf   ces for privacy  as shown in Proposition 1  However  adding an equal amount of noise to each marginal in M may lead to sub optimal results  For example  suppose that M contains two marginals M1 and M2  such that M1  M2  has a large  small  number of counts  all of which are small  large   If identical amount of noise is injected to M1 and M2  then the noisy counts in M1 would have much higher relative errors than the noisy counts in M2  Intuitively  a better solution is to apply smaller  larger  amount of noise to M1  M2   so as to balance the quality of M1 and M2 without degrading their overall privacy guarantee  We measure the utility of a set of noisy marginals in terms of minimizing overall error  as de   ned next  Each marginal Mi     M is represented as a sequence of queries  qi1           qi Mi    where qij denotes the j th query in Mi for i      1   M   and j      1   Mi    Let M    i denote a noisy version of marginal Mi and let yij denote to the noisy answer to qij for i      1   M   and j      1   Mi    DEFINITION 6  OV E R A L L ER RO R  O F MA R G I NA L S   The overall error of a set of noisy marginals  M    1           M     M    is de   ned as 1  M       M  i 1 1  M    i       M    i    j 1  yij     qij  T    max     qij  T      We next describe how we instantiate TwoPhase and iReduct with this utility goal in mind  The query sequence input to both algorithms is simply the concatenation of the Age Status Gender 23 Single M 25 Single F 35 Married F 37 Married F 85 Widowed F Table 2  A dataset T Gender Status M F Single 1 1 Married 0 2 Divorced 0 0 Widowed 0 1 Table 3  A marginal of T individual query sequences for each marginal  i e   Q    q11          q1 M1            q M 1           q M  M M       5 2 Instantiation of TwoPhase To instantiate TwoPhase  we must describe the implementation of the Rescale subroutine that was treated as a    black box    in Section 3 2  Let us    rst consider an ideal scenario where the    rst phase of TwoPhase returns the exact count of every query qij   In this case  we know that if we added Laplace noise of scale   i to every count in a marginal Mi  then each count qij  T   in Mi would have an expected relative error of   i max     qij  T      Recall that the expected absolute deviation of a Laplace variable equals its scale   The expected average relative error in Mi would therefore be   i  Mi        Mi  j 1 1 max     qij  T        Consequently  to minimize the expected overall error of the marginals  it suf   ces to ensure that   Mi     i  Mi       Mi  j 1 1  max     qij  T      is minimized  subject to the privacy constraint that the marginals should ensure   differential privacy  i e     M  i 1 2   i        Using a Lagrange multiplier  it can be shown that   i should be proportional to    Mi      Mi  j 1 1  max     qij  T      We refer to this optimal strategy for deciding noise scale as the Oracle method  Oracle is similar to the Proportional algorithm described in Section 3 1  but sets its   i values to minimize average rather than worst case relative error  Rescale sets the noise scale similarly to how it is set by Oracle except that it uses the noisy counts to approximate the exact counts  To be precise    i is proportional to    Mi     Mi  j 1 max     yij    where yij denotes the noisy answer produced by the    rst phase of TwoPhase  5 3 Instantiation of iReduct Before we can use iReduct to publish marginals  we need to instantiate two of its subroutines   i  a method for computing the generalized sensitivity of a set of weighted marginal queries and  ii  an implementation of the PickQueries black box  discussed in Section 4 3  that chooses a set Q    of marginal queries for noise reduction  In practice  the sensitivity of a set of marginals depends only on the smallest noise scale in each marginal  and so we gain the best tradeoff between low sensitivity and high utility by always selecting the same noise scale for every count in a given marginal  We maintain the invariant by  i  initially setting all marginal counts to have the same noise scale   max and  ii  having PickQueries always pass to NoiseDown the queries corresponding to a single marginal M    i in which the noise scale for each cell in M    i is reduced by the same constant        As the counts in the same marginal always have identical noise scale  we can easily derive the generalized sensitivity of the marginal queries as follows  Assume that  in the beginning of a certain iteration  the counts in M    i  i      1   M    have noise scale   i Age Gender Marital Status State Birth Place Race Education Occupation Class of Worker Brazil 101 2 4 26 29 5 5 512 4 US 92 2 4 51 52 14 5 477 4 Table 4  Sizes of Attribute Domains It can be veri   ed that the generalized sensitivity of the marginals is g       i    1  M   2   i  12  Consider that we apply NoiseDown on a noisy marginal M    j  j      1   M     The noise scale of the queries in M    j would become   j            in which case the generalized sensitivity would become  g     2   j               i    1  M     i   j 2   i    13  Recall that  in each iteration of iReduct  we aim to ensure that the generalized sensitivity of the queries does not exceed    Therefore  we can apply NoiseDown on a marginal M    j only if g          We now discuss our implementation of the PickQueries function  Notice that when we apply NoiseDown to a noisy marginal M    j  j      1   M     the expected errors of the estimated counts for that marginal decrease  due to the decrease in the noise scale of M    j   but the privacy guarantee degrades  Ideally  running NoiseDown on the selected marginal should lead to a large drop in the overall error and a small increase in the privacy overhead  To identify good candidates  we adopt the following methods to quantify the changes in the overall error and privacy overhead that are incurred by invoking NoiseDown on a marginal M    j   First  recall that when we employ NoiseDown to reduce the noise scale   j of M    j by a constant        the generalized sensitivity of the noisy marginals increases from g   to g    where g   and g   are as de   ned in Equations 12 and 13  In light of this  we quantify the cost of privacy entailed by applying NoiseDown on M    j as g       g     2   j               2   j    14  Second  for each noisy count y in M    j   we estimate its relative error as   j  max y       where    is the sanity bound  In other words  we use y to approximate the true count  and we estimate the absolute error of y as the expected absolute deviation   j of the Laplace noise added in y  Accordingly  the average relative error of M    j is estimated as   j      y   M    j 1  max y       Following the same rationale  we estimate the average relative error of M    j after the application of NoiseDown as    j                 y   M    j 1  max y       Naturally  the decrease in the overall error of the marginals  resulted from invoking NoiseDown on M    j   is quanti   ed as 1  M         j      y   M    j 1 max y             j                 y   M    j 1 max y                 M       y   M    j 1 max y       15  Given Equations 14 and 15  in each iteration of iReduct  we heuristically choose the marginal M    j that maximizes          M       y   M    j 1 max y          2   j               2   j     That is  we select the marginal that maximizes the ratio between the estimated decrease in overall error and the estimated increase in privacy cost  0 0 02 0 04 0 06 0 08 0 1 0 12 0 14 0 0 1 0 2 0 3 0 4 0 5 0 6    1       overall error TwoPhase 0 0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 0 1 0 2 0 3 0 4 0 5 0 6    1       overall error TwoPhase  a  Brazil  b  USA Figure 5  Overall Error vs   1    1D Marginals  6  EXPERIMENTS We evaluate the accuracy of the proposed algorithms on three tasks  estimating all one way marginals  Section 6 3   estimating all two way marginals  Section 6 4   and learning a Naive Bayes classi   er  Section 6 5   6 1 Experimental Settings We use two datasets 2 that are composed of census records collected from Brazil and the US respectively  Each dataset contains nine attributes  whose domain sizes are as illustrated in Table 4  The Brazil dataset consists of nearly 10 million tuples  while the US dataset has around 14 million records  We use iReduct to generate noisy marginals from each dataset  and we compare the performance of iReduct against four alternate methods  The    rst method is the Oracle algorithm presented in Section 5 2  which utilizes the exact counts in the marginals to decide the noise scale of each marginal in a manner that minimizes the expected overall error  Although Oracle does not conform to    differential privacy  it provides a lower bound on the error incurred by any member of a large class of   differential privacy algorithms  The second and third methods are the TwoPhase and iResamp algorithms presented in Section 3 and Appendix A respectively  The    nal technique  henceforth referred to as Dwork  is Dwork et al    s method  Section 2 2   which adds an equal amount of noise to each marginal  We measure the quality of noisy marginals by their overall errors  as de   ned in Section 5 1  In every experiment  we run each algorithm 10 times  and we report the mean of the overall errors incurred by the algorithm  Among the input parameters of iReduct  we    x   max    T   10 and          T   10 6   where  T   denotes the number of tuples in the dataset  That is  iReduct starts by adding Laplace noise of scale  T   10 to each marginal and  in each iteration  it reduces the noise scale of a selected marginal by  T   10 6   All of our experiments are run on a computer with a 2 66GHz CPU and 24GB memory  6 2 Calibrating TwoPhase As was discussed in Section 3 2  the performance of the TwoPhase algorithm depends on how the    xed privacy budget is allocated across its two phases  This means that before we can use 2 Both datasets are available online as part of the Integrated Public Use Microdata Series  25  iReduct Oracle TwoPhase iResamp Dwork 0 1 1 0 2 0 4 0 6 0 8 1       10  2   overall error 0 1 1 0 2 0 4 0 6 0 8 1       10  2   overall error  a  Brazil  b  USA Figure 6  Overall Error vs     1D Marginals  iReduct Oracle TwoPhase iResamp Dwork 0 0 05 0 1 0 15 0 2 0 25 0 2 0 4 0 6 0 8 1       T     10  4   overall error 0 0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 08 0 09 0 2 0 4 0 6 0 8 1       T     10  4   overall error  a  Brazil  b  USA Figure 7  Overall Error vs      1D Marginals  the TwoPhase algorithm  we must decide how to set the values of  1 and  2 to optimize the quality of the noisy marginals  To determine the appropriate setting  we    x     0 01 and measure the overall error of TwoPhase when  1 varies and  2          1  Figure 5 illustrates the overall error on the set of one dimensional marginals as a function of  1    As  1   increases  the overall error of TwoPhase decreases until it reaches a minimum and then increases monotonically  When  1 is too small  the    rst step of TwoPhase is unable to generate accurate estimates of the marginal counts  This renders the second step of TwoPhase less effective  since it relies on the estimates from the    rst step to determine the noise scale for each marginal  On the other hand  making  1 too large causes  2 to becomes small and forces the second step to inject large amounts of noise into all of the marginal counts  The utility of the noisy marginals is optimized only when  1   strikes a good balance between the effectiveness of the    rst and second steps  As shown in Figure 5  the overall error of TwoPhase hits a sweet spot when  1      0 06   0 08    We set  1   0 07  in the experiments that follow  6 3 Results on 1D Marginals We compare the overall error incurred when each of the    ve algorithms described above is used to produce differentially private estimates of the one dimensional marginals of the Brazil and USA census data  We vary both   and     Figure 6 shows the overall error of each algorithm as a function of    with       xed to 10   4     T    iReduct yields error values that are almost equal to those of Oracle  which indicates that its performance is close to optimal  TwoPhase performs worse than iReduct in all cases  but it still consistently outperforms the other methods  The overall error of iResamp is comparable to that of Dwork  Figure 7 illustrates the overall error of each method as a function of the sanity bound    when     0 01  The relative performance of each method is the same as in Figure 6  The overall errors of all algorithms decrease as    increases  since a larger    leads to lower relative errors for marginal counts smaller than     see Equation 1   iReduct Oracle TwoPhase iResamp Dwork 0 0 2 0 4 0 6 0 8 1 1 2 1 4 1 6 0 2 0 4 0 6 0 8 1       10  2   overall error 0 0 2 0 4 0 6 0 8 1 1 2 0 2 0 4 0 6 0 8 1       10  2   overall error  a  Brazil  b  USA Figure 8  Overall Error vs     2D Marginals  iReduct Oracle TwoPhase iResamp Dwork 0 0 2 0 4 0 6 0 8 1 1 2 0 2 0 4 0 6 0 8 1       T     10  4   overall error 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 2 0 4 0 6 0 8 1       T     10  4   overall error  a  Brazil  b  USA Figure 9  Overall Error vs      2D Marginals  In the aforementioned experiments  every method but iReduct needs only a few linear scans of the marginal counts to generate its output  and therefore incurs negligible computation cost  In contrast  the inner loop of iReduct is executed O   max        times  and the iReduct takes around 5 seconds to output a set of marginal counts  The relative high computational overhead of iReduct is justi   ed by the fact that it provides improved data utility over the other methods  6 4 Results on 2D Marginals In the second sets of experiments  we consider the task of generating all two dimensional marginals of the datasets  For the input parameters of TwoPhase  we set  1     0 025 based on an experiment similar to that described in Section 6 2  Figure 8 shows the overall error of each method when      10   4     T   and   varies  The overall error of iReduct is almost the same as that of Oracle  TwoPhase is outperformed by iReduct  but its overall error is consistently lower than that of iResamp or Dwork  Interestingly  the performance gaps among iReduct  TwoPhase  and Dwork are not as substantial as the case for one dimensional marginals  The reason is that a large fraction of the two dimensional marginals are sparse  As a consequence  iReduct and TwoPhase apply roughly equal amounts of noise to those marginals in order to reduce overall error  The noise scale of the marginals is therefore relatively uniform  the allocation of noise scale selected by iReduct or TwoPhase does not differ too much from the allocation used by Dwork  This explains why the improvement of iReduct and TwoPhase over Dwork is less signi   cant  Figure 9 illustrates the overall error of each algorithm as a function of     with     0 1  The relative performance of each technique remains the same as in Figure 8  Regarding computation cost  iReduct requires around 15 minutes to generate each set of two dimensional marginals  This running time is considerably longer than for one dimensional marginals because iReduct must handle more marginal counts in the twodimensional case iReduct Oracle TwoPhase iResamp Dwork 0 1 2 3 4 5 6 0 1 0 2 0 4 0 7 1       10  2   overall error 0 0 5 1 1 5 2 2 5 3 3 5 0 1 0 2 0 4 0 7 1       10  2   overall error  a  Brazil  b  USA Figure 10  Overall Error vs     Marginals for Classi   er  6 5 Results on Naive Bayes Classi   er Our last set of experiments demonstrates that relative error is an important metric for real world analytic tasks  In these experiments  we consider the task of constructing a Naive Bayes classi   er from each dataset  We use Education as the class variable and the remaining attributes as feature variables  The construction of such a classi   er requires 9 marginals  a one dimensional marginal on Education and 8 two dimensional marginals  each of which contains Education along with another attribute  We apply each algorithm to generate noisy versions of the 9 marginals  and we measure the accuracy of the classi   er built from the noisy data 3   For robustness of measurements  we adopt 10 fold crossvalidation in evaluating the accuracy of classi   cation  That is  we    rst randomly divide the dataset into 10 equal size subsets  Next  we take the union of 9 subsets as the training set and use the remaining subset for validation  This process is repeated 10 times in total  using each subset for validation exactly once  We report  i  the average overall error of the 10 sets of noisy marginals generated from the 10 training datasets and  ii  the average accuracy of the classi   ers built from the 10 sets of noisy marginals  Figure 10 illustrates the overall error incurred by each algorithm when      10   4     T   and   varies   The input parameter  1 of TwoPhase is set to 0 03 based on an experiment similar to that described in Section 6 2   Again  iReduct and Oracle entail the smallest errors in all cases  followed by TwoPhase  In addition  iResamp consistently incurs higher error than Dwork does  Figure 11 shows the accuracy of the classi   ers built upon the output of each algorithm  The dashed line in the    gure illustrates the accuracy of a classi   er constructed from a set of marginals without any injected noise  Observe that methods that incur lower relative errors lead to more accurate classi   ers  7  RELATED WORK There is a plethora of techniques  1   5  5  9   12  14  15  17   20  22  26   28  32  for enforcing   differential privacy in the publication of various types of data  such as relational tables  9 17 21 32   search logs  15 20   data mining results  12 23   and so on  None of these techniques optimizes the relative errors of the published data  instead  they optimize either  i  the variance of the noisy results or  ii  some application speci   c metric such as the accuracy of classi   cation  12   Below  we will discuss several pieces of work that are closely related to  but different from  ours  Barak et al   1  devise a technique for publishing marginals of a given dataset  Their objective  however  is not to improve the accuracy of the released marginal counts  Instead  their technique is de  3 Following previous work  6   we postprocess each noisy marginal cell y by setting y   max y   1  1  before constructing the classi   er from the marginals  iReduct Two Phase iResamp Dwor</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web"/>
        <doc>Facet Discovery for Structured Web Search  A Query log Mining Approach ### Jeffrey Pound   University of Waterloo Waterloo  Canada jpound cs uwaterloo ca Stelios Paparizos Microsoft Research Mountain View  CA  USA steliosp microsoft com Panayiotis Tsaparas Microsoft Research Mountain View  CA  USA panats microsoft com ABSTRACT In recent years  there has been a strong trend of incorporating results from structured data sources into keyword based web search systems such as Bing or Amazon  When presenting structured data  facets are a powerful tool for navigating  re   ning  and grouping the results  For a given structured data source  a fundamental problem in supporting faceted search is    nding an ordered selection of attributes and values that will populate the facets  This creates two sets of challenges  First  because of the limited screen real estate  it is important that the top facets best match the anticipated user intent  Second  the huge scale of available data to such engines demands an automated unsupervised solution  In this paper  we model the user faceted search behavior using the intersection of web query logs with existing structured data  Since web queries are formulated as free text queries  a challenge in our approach is the inherent ambiguity in mapping keywords to the different possible attributes of a given entity type  We present an automated solution that elicits user preferences on attributes and values  employing different disambiguation techniques ranging from simple keyword matching  to more sophisticated probabilistic models  We demonstrate experimentally the scalability of our solution by running it on over a thousand categories of diverse entity types and measure the facet quality with a real user study  Categories and Subject Descriptors  H 2 8  Database Management   Data Mining General Terms  Algorithms  Experimentation  Human Factors  Keywords  Facets  Faceted search  Structured web search ###  1  INTRODUCTION In recent years  search engines like Google or Bing have evolved to include in their results information from structured data sources along with text documents  Furthermore  there are specialized search verticals like Amazon that rely on an even greater variety of structured data sources for their results  Structured data have the ad   Work done while at Microsoft Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  vantage of having rich meta data  allowing for an improved search experience compared to just keyword search  Faceted search is a common interaction paradigm that utilizes structured information to provide a user friendly alternative to keyword search  In this paradigm  facets are used as summarized data axes that allow the users to pivot on the information shown and create a different or more selective result set  If we abstract the structured data as typed entities grouped together into logical tables  one per entity type  A table facet corresponds to a table attribute and underlying attribute values  Faceted search is enabled via a facet system which exposes an ordered list of attributes and an ordered list of attribute values  For example  for a table of digital cameras  a facet could be a popular attribute such as brand and prominent values such as    Canon        Nikon     and    Sony     Other facets could be megapixel resolution  color  size and corresponding values per attribute  Exposing them into a facet system  allows the user to quickly zoom into the appropriate camera subset by selecting characteristics of interest  Similar examples exist in other domains with structured data  such as automobiles or movies  It is clear that the user experience for structured web search can bene   t from facets  However for an engine to support facets effectively it needs to address two groups of challenges   a  Given the limited screen real estate and the large number of possible facets to consider  we need to select the top k most important facets  where k is usually a small number  Facet importance in this case can be measured by the utility of a facet towards a user   s anticipated action like a pivot or re   nement  Since an entity type can have over one hundred attributes  the challenge becomes    nding the few most important attributes  and attribute values with the maximum anticipated utility   b  There is a huge number of structured data sources currently available to engines like Google or Amazon  If we abstract the data as de normalized entity type tables  there are thousands of such tables to consider  So a solution that    nds facets from these tables must be fully automated to scale appropriately  The best method to learn the utility of attributes for users would be to consume user produced signals  Such an ideal signal would be produced if all possible attributes for all possible entity type tables are shown and then the system observes and records which ones are being selected by an extensive user study  Unfortunately  this is not feasible due to the scale of data and the number of required users for statistical signi   cance  Instead  common practice  8  10  has been to show attributes selected manually by experts with values populated from sampling the results  It is debatable how well a domain expert can match the average user intent  but regardless  the scale of structured data available to web engines makes it impossible to go this route with consistently good quality across all tables  As a result  existing approaches  3  22  rely on automatically learningfacets from the structured data  However  such approaches do not capture the true user intent since they focus on the structured data without incorporating user signals  In this work we use web query logs intersected with the structured data to model the facet utility  Our intuition is that query logs act as a proxy for the exploration process of the users when navigating through a structured domain  We assume that users issue queries to re   ne or alter their results into an appropriate consideration set in a similar manner they would have used facets if there were available  The attributes that appear frequently in the query logs are the ones that users would use to narrow down their search  and thus they are appropriate for facets  The values of these attributes that appear frequently in the queries are good values to populate the facets  This formulation has the advantage that it can lead to a scalable automated solution that captures the real user needs better than the opinion of an expert  or the just the statistics of the structured data  Additionally  it adapts to the needs and preferences of the users as they change over time  It is worth mentioning that trying to match the user intent for attribute importance has been studied in the past  see  7  21   with the application domain being either attribute ranking or product visibility  Although the high level principle is similar  the underlying models and corresponding solutions are different from those for discovering the facet utility  Using the web query logs to    nd frequently queried values and attributes would be easy if the attribute value pairs were clearly speci   ed in the queries as in SQL statements  However  although the data is structured  web queries are free text keywords  Finding the frequent attributes poses two hard challenges due to the inherent ambiguity of translating the keywords to attribute value pairs   a  Mapping the query to the appropriate structured source     e g   map the keyword    apple    to either mp3 players  computers  or fruit  and  b  identifying and disambiguating the occurrence of attributes in the query amongst attributes of the same entity type     e g   for the table televisions  disambiguate the keyword    30inch    between the diagonal screen size  height or width of a television  or for the table watches  map the keyword    gold    to the color or material of a watch  In this paper  we assume that the    rst problem  understanding if a query targets a source of structured data and which source in particular  has been addressed from existing literature  such as  13  19  24   We use the existing work to    nd the appropriate target data source  We focus on the second problem  disambiguating between attributes within an entity type domain  Using appropriate disambiguation  we learn the most suitable facet attributes and values  Furthermore  we also discuss how data statistics can be used to help with the disambiguation process by being tolerant to data noise and imperfections  and also which data statistics indicate good facets  Our contributions are summarized as follows   i  We formulate the problem of query log based facet attribute and value selection  presented in Section 2   ii  We introduce different attribute value identi   cation and disambiguation techniques in Section 3  which we extend for computing facet values  including context dependent ones  in Section 4   iii  We explore the effect of appropriate data statistics in facet selection  presented in Section 5  And  iv   we perform a detailed experimental study on a large scale data set with a thorough evaluation on the quality of the results from real users  presented in Section 6  where we discuss the qualitative difference between our techniques as well as show our best approach outperforming state of the industry systems  We conclude with related work in Section 7 and    nal thoughts in Section 8  2  FACET SELECTION In this section we formulate our problem  and provide the necessary de   nitions for the remainder of the paper  2 1 Problem Formulation We assume that structured data are organized as a collection of tables T   fT1  T2          T  g 1   A table T is a set of related entities de   ned over a set of attributes  T A   fT A1  T A2          T A g  We use T A V to denote the domain of the attribute T A  We omit the pre   x T and use A  whenever the table is unambiguous  Given a table T we de   ne a facet system F   hF1       Fki for table T as an ordered list of k facets  where k is is the facet budget  A facet Fi   fAi hAi v1       Ai vmi ig consists of an attribute Ai and an ordered list of mi values  where mi is the value budget for attribute Ai  For the following we use AF  or F A  to denote the set of attributes of F  and A VF to denote the set of values for attribute A 2 AF in facet system F  We say that a facet system supports an attribute A if A 2 AF   and we de   ne the attribute support function as follows  ASUP F  A      1  if F supports A 0  otherwise  1  We also say that a facet attribute Af 2 AF supports value v if v 2 Af  VF   and we de   ne the attribute value support function as follows  VSUP Af   v      1  if Af supports v 0  otherwise  2  A facet system is exposed through the user interface  and it enables the user to navigate through the data in the table T by selecting values for the different attributes  The design and analysis of faceted user interfaces is an active research area in the HCI community  10   In any faceted search system  the facet selection is paramount to the success of the system  The order of facets is also important since it determines how the facets will be displayed  with more important facets being displayed more prominently  The goal is to maximize the user engagement and satisfaction  and the i th facet corresponds to the i th best attribute for this task  A similar reasoning applies also to the ordering of the facet values within a facet  However  for a selected facet attribute there is usually a greater    exibility in the presentation of values  drop down menus  slider bars  text boxes  making the value selection and ordering less critical  We thus consider the facet attribute ordering problem and the facet attribute value ordering problem separately  with the former being more important than the latter  In this work we are interested in constructing a facet system using web search queries  We assume that we have a query log Q   fq1       qng  consisting of queries posed to a generic web search engine  For each query qi we also have a weight wi denoting the importance of the query  e g   the number of times that the query appears in the query log  Although queries are posed to the search engine  it is common that users make queries that are targeted to structured data  For example  the query    canon powershot    can be interpreted as targeting a table with structured data about digital cameras  For a given table T let QT   Q denote the subset of queries that target table T  We will drop the subscript T whenever the table is unambiguous  We assume that each query q 2 QT speci   es a set of attributevalue pairs AVq   fAVig of the form AVi    Ai  Ai vj    For example  the query    canon powershot    speci   es the value    canon    for attribute brand  and the value    powershot    for attribute product line  We can thus map the query q     canon powershot    to the set Aq    brand    canon      product line    powershot       We use Aq 1 The organization of data into tables is purely conceptual and orthogonal to the underlying storage layer  the data can be physically stored in XML    les  relational tables  retrieved from remote web services  etc  Our assumption is that a mapping between the storage layer and the    schema    of table collection T has been de   ned  a  Queries to discrete attribute value pairs  b  Queries to attribute value pairs through tagged tokens Figure 1  Query to Attribute Value Pairs Mappings to denote the set of attributes speci   ed in the query  Figure 1 a  depicts graphically an example  Note that we only assume that such a mapping exists  obtaining it from the web log is a challenging problem as we elaborate below  We now de   ne the attribute utility of a facet system F  making use of the attribute support function de   ned in Equation 1  DEFINITION 1  ATTRIBUTE UTILITY   Given a facet system F  and a query q  we de   ne the attribute utility of F for q as Ua F  q    X A2Aq ASUP F  A  the number of attributes in Aq supported by F  The attribute utility of F for query log Q is de   ned as Ua F  Q    X q2Q wqUa F  q  where wq denotes the weight of query q  We can similarly de   ne the attribute value utility of a facet attribute Af   making use of the attribute value support function de     ned in Equation 2  DEFINITION 2  ATTRIBUTE VALUE UTILITY   Given a facet attribute Af and a query q  we de   ne the attribute value utility of Af for q as Uv Af   q    X  A v 2AVq A Af VSUP Af   v  the number of values in AVq supported by Af   The attribute value utility of Af for query log Q is de   ned as Uv Af   Q    X q2Q wqUv Af   q  where wq denotes the weight of query q  As we have already argued  queries act as a proxy for the exploration process of the users over the structured data  Users include in their queries attributes over which they want to dissect the table  A good faceted system should anticipate the information need of the user and select attributes  and values  that maximize the utility to the users  We thus have the following de   nition of the query based facet selection problems  PROBLEM 1  QUERY BASED FACET ATTRIBUTE SELECTION   Given a table T  and a query log QT over table T  create an ordering L T A  of the attributes T A such that for any k   1  a facet system F with attribute set F A   Lk T A      the top k attributes according to ordering L     has maximum attribute utility Ua F  QT   over all facet systems with attribute set of size k  PROBLEM 2  QUERY BASED FACET VALUE SELECTION   Given a table T  an attribute A 2 T A  and a query log QT over table T  create an ordering L A V  of the values in A V such that for any m   1  a facet system F that contains A with attribute value set A VF   Lm A V      the top m attributes according to ordering L     has maximum attribute value utility Uv A  QT   over all facet systems that contain A with attribute value set of size m  If we are given the mapping AVq from queries to attribute value pairs  as in Figure 1 a  then the facet selection problems we de     ned above are easy to solve  This is the case for example with structured query logs  e g  SQL   where the user speci   es explicitly the attributes and values they are interested in  Then  it is straightforward to see that the optimal solution for Problem 1 is to order the attributes according to their popularity in the query log  The popularity FQ Ai  of attribute Ai over query log Q is computed as FQ Ai    X q2Q X  Ai v 2AVq wq  3  The popularity FQ Ai  is thus the number of attribute value pairs in Q that contain attribute Ai weighted by the weight of the query in which they appear  Similarly  the optimal solution for Problem 2 is to order the values within attribute Ai according to their popularity in the query log  The popularity of a value vj of attribute Ai is computed as FQ Ai vj     X q2Q X  Ai vj  2AVq wq  4  the number of attribute value pairs in the query log  where Ai is the attribute and vj is the value  Unfortunately  in the case of web query logs this mapping is not given to us and we need to discover it from the data  We elaborate on this challenge in the following section  2 2 Mapping Web Queries to Structured Data In order to extract attribute value pairs from the queries in the unstructured web query log  we need to address two issues   a  for every table T  identify the queries in Q that target table T  i e   determine the set QT   and  b  identify and disambiguate the attribute occurrences in the queries  As previously discussed  we use existing work  13  24  to solve  a  and instead focus on the latter problem  Given a query q 2 QT we now need to identify the occurrences of the attributes in T A in the query q  We assume a tagging process TAGGER that performs this task  De   ne a token to be a string consisting of one or more contiguous words  and let Lq denote the set of all tokens in q  The TAGGER produces all tokens t 2 Lq  and matches them with attributes in T A  producing attribute tokenFigure 2  Fraction of tokens mapped to varying num of attributes  pairs  AT    A  t   For a categorical attribute Ac 2 T A  the TAGGER outputs an attribute token pair AT    Ac  t   if the token t appears in Ac V  Approximate  or synonymous matching is also possible as an extension of the same process  although discussing it in detail is outside the scope of this paper  For a numerical attribute An with unit u  e g   inches  Kg  or x zoom   the TAGGER produces an attribute token pair AT    An  t  for every token t that consists of a number followed by a unit u  For example  the tagger output for the query    5x nikon    over the digital cameras table is f digital zoom     5x       optical zoom     5x       brand     nikon    g  For a query q we use AT q   f A  t g   T A   Lq to denote the set of attribute token pairs output by the TAGGER   Note that attribute token pairs are different from attribute value pairs  since a token may not be a value in the table T  This is the case with synonyms and approximate matches  as well as with numeric attributes  For example  in the query    30 inch lcd TV     the token    TV    can match the data value    Television     and the token    30 inch    can be mapped to diagonal screen size even though we may only have televisions with 27 and 32 inch diagonals  Given the query log Q the TAGGER outputs a set of attributetoken pairs AT Q  It would appear that our job is now done  since we can compute the popularity of attributes and values as before  simply by interchanging tokens with values  and attribute value pairs with attribute token pairs  However  this is not the case due to ambiguity in the token interpretation  The domains of attributes in a table often overlap  and as a result a token t can map to more than one attribute  For example  in the query    30 inch lcd TV     the token    30 inch    is potentially mapped to the diagonal  width  height  and depth attributes  since all these attributes are numeric with the same unit  similarly in the query    camera with 3x zoom        3x    can be either optical zoom or digital zoom  Such confusion is not limited to just numeric attributes  For example in the query    gold watch     the token    gold    can specify either the material  or the color attribute  similarly for    leather dress shoes    the token    leather    can map to either the top shoe material or the shoe sole  There are many such examples that appear in web queries  We computed the number of attributes of the same table that a token matches on average  over all queries  As we can see from Figure 2  approximately half the tokens are confused over at least two or more attributes  So ambiguity is a real problem that needs to be addressed  Estimating attribute popularity in the presence of ambiguity becomes problematic  Our data no longer looks like Figure 1 a   but instead like Figure 1 b   Computing the attribute popularity over attribute token pairs using Equation 3 will lead to misleading results  TV height will incorrectly be deemed equally important to TV diagonal  and camera digital zoom the same as optical zoom  In order to obtain correct estimates for the true popularity of an attribute  we need to disambiguate the ambiguous tokens  Let t denote a token in a query q  We say that token t is ambiguous for a table T  if there are at least two attribute token pairs  Ai t   involving t   We use At  to denote the set of candidate attributes to which the token t is mapped  Our goal is to disambiguate between the attributes in At   We do this by estimating the probability of an attribute given the ambiguous token  We thus have the following problem de   nition  PROBLEM 3  ATTRIBUTE DISAMBIGUATION   Given a table T  a query log Q over T  and an ambiguous token t  2 LQ that maps to the set of attributes At    T A  compute the disambiguation probability distribution  denoted P Ajt    over the set of candidate attributes A 2 At   Given P Ajt   we now have a measure of likelihood for the mapping of token t to attribute A  We can use this probability to perform either hard or soft disambiguation  In hard disambiguation  we map token t to the most likely attribute At    arg maxA2At  P Ajt    In hard disambiguation our data will take the form of Figure 1 a   and we can apply directly Equation 3 to estimate the attribute popularity  In soft disambiguation  we modify Equation 3 such that each occurrence of attribute A with the ambiguous token t is weighted by the probability P Ajt    We thus have F  Q Ai    X q2Q X  Ai t 2AT q wqP Aijt   5  We have P Ajt    1 if token t maps unambiguously to attribute A  3  ATTRIBUTE DISAMBIGUATION In this section we describe four different approaches for attribute disambiguation  The    rst three rely on the principle that the best solution is the one that better explains the attribute token pairs we observe  If we assume that the attributes in the table are generating queries and tokens  we identify the attribute that is most likely to have generated the tokens we are observing  The three different algorithms differ in the granularity at which they try to explain the data  becoming progressively more complex  The last approach relies on user feedback from a click log to disambiguate between different interpretations of a token  3 1 Token level Disambiguation The    rst algorithm we consider treats each token independently and tries to    nd the attribute that is most likely to have generated this speci   c token  Let t denote an ambiguous token and let At  the set of candidate attributes for t   We want to estimate P Ajt   for all A 2 At   Using Bayes rule we have P Ajt     P t jA P A  P Ai2At  P t jAi P Ai  We will estimate the right hand side using the data in T  Assuming that all attributes are equally likely in the table  i e   P A  is the same for all attributes in the table   then P Ajt   is proportional to PT  t jA   the probability that the token t is generated from the distribution of A in the table  We have that PT  t jAi    jT A t  j jAj where T A t   denotes the set of entries in the table where the attribute A takes the value t   and jAj is the number of table entries with some value for attribute A  Numeric attributes are similarly described using histograms to deal with the continuity of numbers  Therefore  according to this algorithm  the most likely attribute is the one that is most likely to have generated token t   From an information theoretic point of view  this is the attribute who   s value distribution can encode token t with the fewest number of bits  i e   the one that reduces the uncertainty the most for t  3 2 Cluster level Disambiguation The token level disambiguation approach considers the probability of each token independently mapping to a candidate attribute  A natural extension to this model is to consider the ambiguity among a cluster of tokens confused with the same set of attributes  In this cluster based approach  we aggregate over all tokens in the querylog to    nd clusters of ambiguous attribute token pairs  We then resolve which attributes in the cluster are better candidates to model the distribution of confused tokens over the full cluster  Consider the bipartite graph formed by the set of attribute token pairs in AT Q  e g   see Figure 3  a    Each time a token is mapped to a set of attributes  it supports the ambiguity of these attributes  For example  if the token    8x    is mapped to both digital zoom and optical zoom  it supports the ambiguity of these two attributes  If the token    2x    is mapped to digital zoom  optical zoom  and model it supports the ambiguity of all three of these attributes  By aggregating over all tokens in the query log  we can    nd the groups of attributes most commonly confused  In particular  if we consider the set of all tokens that form a bi clique with a set of attributes  then these attributes are all pairwise ambiguous over the same set of tokens  and the support for the ambiguity of this cluster is proportional to the number of tokens in the bi clique  It is these strongly supported bi cliques that we aim to    nd  To    nd clusters of ambiguous attributes  we    rst construct an attribute ambiguity graph per table  For a table T  consider a graph G    V  E  w  where there exists a vertex va 2 V for every attribute a 2 T  A  We create an edge e   hvA1   vA2 i between two vertices vA1 and vA2 if some token in the query log is mapped to both A1 and A2 over all interpretations of queries  The weight function w e  assigns a weight to the edge e equal to the number of tokens confused between  the attributes denoted by  vA1 and vA2   We then enumerate all cliques in the attribute ambiguity graph  with the support being the minimum edge weight in the clique  For example  in Figure 3  b   the support of the 2 clique containing digital zoom and optical zoom is ten  while the support of the 3  clique containing digital zoom  optical zoom  and model is two  After identifying the clusters of ambiguous attributes  we want to compute how likely each attribute is to model the set of confused tokens  We do this by computing the KL divergence  KullbackLeibler divergence  between the value distribution of the attribute and the set of confused tokens  Let C    AC  LC  denote an attribute token cluster  We compute the disambiguation probability of an attribute A 2 AC given the cluster C as follows  P AjC    KL AjjC  P Ai2C KL AijjC  where the KL divergence between two distributions P and Q is de   ned as follows  KL PjjQ    X x p x  log p x  q x  For every ambiguous attribute token pair  A t   2 LC  we set the disambiguation probability P Ajt     P AjC   The intuition in this kind of disambiguation is that the attribute which forms a better model for the query token distribution is more likely to be the correct attribute users had in mind when formulating these queries  For example  if a collection of tokens around measurements are confused among television height and diagonal  we would expect to    nd the distribution of values in the diagonal attribute to be a better model for the set of confused query tokens  As an example  consider Figure 3  There are eight tokens with the same connectivity of the token labeled    8x     two tokens with the same connectivity as    2x     and    ve tokens with the same connectivity Figure 3   a  a bipartite token attribute graph and  b  the resulting attribute ambiguity graph  as    Pix     The ambiguous attribute clusters correspond to the 2  clique  Digital Zoom  Optical Zoom  with support 10  the 2 clique  Product Name  Model  with support    ve  and lastly the 3 clique  Digital Zoom  Optical Zoom  Model  with a support of two  The 3 clique will be used to disambiguate among tokens confused with all three attributes  while the sub clique will be used to disambiguate between tokens confused among only those two attributes  We may also    nd the remaining two 2 cliques  Digital Zoom  Model  and  Optical Zoom  Model  however in our implementation we are not concerned with these as there are no tokens that confuse Model with one of the other attributes and not the third  i e   all tokens confused with Model and Optical Zoom are also confused with Digital Zoom  and all tokens confused with Model and Digital Zoom are also confused with Optical Zoom   3 3 Query log level Disambiguation We now present an approach that considers the tokens in the full query log  and estimates the probability P Ajt   for ambiguous tokens t   such that the likelihood of the full observed query log is maximized  Let Q be a query log over table T  Let LQ denote the set of all tokens generated by the TAGGER that appear in the attribute token pairs AT Q  Each token t is assigned a weight wt   X q2Q X  Ai t 2AT q wq  the total weight of queries in which token t appears  For simplicity we can consider wt to be the frequency of the token t in AT Q  We assume that the tokens in LQ were generated according to a generative model  which generates tokens as follows  an attribute Ai is selected with probability P Ai   and then a token t is drawn from the data distribution PT  tjAi   Thus token t is generated with a probability given by the following  P t    X Ai2T A PT  tjAi P Ai  The probability of observing the full set of tokens LQ is then P LQ    Y t2Q P t  wt The generative model we have described has   parameters  i   P Ai   one for each attribute Ai in the table T  Parameter  i is the probability that attribute Ai is activated for the generation of a given token  and we have that P  i 1  i   1  Let   denote the vector of parameters  1           We perform Maximum LikelihoodEstimation of    and we look for the value of   that maximizes the probability that we observe the tokens in LQ  Using log likelihood notation we want to minimize the following  L LQ     log P LQ     X t2Q wt log P t  We use an iterative EM algorithm for    nding the Maximum Likelihood Estimate for    The algorithm initializes the values of the parameters  i to an arbitrary distribution  in our implementation uniform  Then it alternates between the following two steps  In the E step given an estimation for the parameters    for each attribute token pair  Ai  tj   in LQ we compute the posterior probability of the attribute Ai given token tj P Aijtj     P P tj jAi  i A 2AT  A P tj jA      6  In the M step  we can now estimate new values for the parameters in   as follows   i   X  Ai t 2AT Q P Aijt P t    wt W X  Ai t 2AT Q P Aijt   7  where W   P t2LQ wt is the total weight of all tokens  We repeat these two steps until the algorithm converges  Given that the optimization function is convex we know that it will reach a global maximum  After the algorithm has converged  for an ambiguous token t we can now compute P Ajt   using Equation 6  The Maximum Likelihood Estimation of the probabilities P Ai     nds a set of values that best explains the full query log we observe  Intuitively what this means is that when observing a token t  even though attribute A1 may have the highest probability P tjA1   the token t may be attributed to another attribute A2 since this is overall more likely to occur in the log  For example  consider a query log with queries over the televisions table  and let Linch denote the set of all tokens of the form number followed by the unit    inch     If the overall distribution of these tokens agrees better with the attribute diagonal than height then for a token like    30 inch    in Linch the attribute diagonal will have higher probability P Ajt  even though the value is more likely in the height attribute  This agrees well with our intuition that    30 inch    is more likely to refer to the diagonal rather than the height  since it is typical for users to query for diagonal when querying for televisions  The iterative algorithm captures nicely this intuition  3 4 Clicks based Disambiguation The ambiguity problem is essentially an intent issue  Given an ambiguous token  and multiple interpretations of the token in the data table we have no indication which interpretation was intended by the user  In an online environment  a strong indication of intent is the user click behavior  A query click log QC   f q1  e1        qn  en g over a table T is a set of query entity pairs  where q is a query posed in a commercial search engine  and e is an entity in table T that was shown to the user as a result of the query and the user chose to click on it to obtain further information about it  An entity e is an entry in table T  We represent it as a set of attribute value pairs e   f Ai  Ai vj  g for every attribute Ai in table T  Given a query click pair  q  e   we make the assumption that the user who posed the query intended to access the attributes of clicked entity e  Since we now have an indication for the intent of the user  the disambiguation problem becomes signi   cantly easier to tackle  Given an ambiguous token t that maps to attributes At    fAi1        Aik g we disambiguate by selecting from entity e the attribute A   2 At  that has value A    v that best matches token t   The notion of a match depends on the type of token that we consider  In the case of categorical tokens we require that A    v   t   In such cases we have that P A   jt     1  since we assume complete certainty in our mapping  If there is more than one attribute that match token t then we assume that t maps to all of them  and we assign equal probability to all  It is worth noting that  although possible  this event is not very probable  In our experiments  we did not observe any case where multiple categorical attributes match on the same token for the same clicked entity  In the case of numerical tokens  we map token t  to the attribute A   that has the closest value  i e   it minimizes the difference jt  A    vj  we compute the disambiguation probability as P A   jt     exp jt  A    vj  P A2At  exp jt  A   vj  favoring the attributes with smallest distance  Clicks have been used extensively in web search for eliciting the preferences of users  14  19   It is well known that they contain a very strong signal for the intent of the user  but they also come with different caveats  such as presentation bias  users click only on what they are shown   ranking bias  users tend to click on the top positions   and noise  We discuss some of these issues in the experimental evaluation  see Section 6   4  ATTRIBUTE VALUE SELECTION Equally important to    nding important attributes  is the problem of    nding important values to populate the facets  We apply the same philosophy for values as we do for attributes  popular values in a query log will be strong facet values that are important to users  In the following section  we discuss two types of facet value selection  category dependent facet value selection  and dynamic context dependent facet value selection  4 1 Table Dependent Value Selection To compute the value score  we consider Equation 4 and modify it such that each occurrence of attribute A and value v with the ambiguous token t  is weighted by the probability P A vjt    We then have the popularity of a value as the following  F  Q Ai vj     X q2Q X  Ai t  2AT q wqP Ai vj jt    8  In the case that value vj does not map to the token t   the probability will be zero  Also  for all attributes Ai of the same table we have P Ai vj jt    P Aijt  since the token to value mapping is part of the attribute based disambiguation  Hence each of the attribute disambiguation methods from Section 3 are directly applicable  For numeric attributes  we can compute weights for buckets of values via the usage of a histogram  This method allows for each table attribute to display a set of values based on user popularity  For example  for the golf clubs table and brand attribute  the top 3 facet values would be    Nike        Ping     and    TaylorMade     4 2 Context Dependent Value Selection We described how to use query log popularity to    nd good facet values given the table and an attribute  However  we still have the problem that certain combination of values across attributes will return empty results  For example  consider a faceted search engine which includes brand and golf club type among its facets for the golf clubs table  There exist some specialized brands that only produce very few types of golf clubs  i e     Odyssey    or    Scotty Cameron    only produce    putters     So now when a user selects    Odyssey     then golf club type values such as    drivers    or    wedges    are not good facet       choices since selecting them would produce empty results  Showing    putters    would be a much better choice  Also consider the    ip side of this example that exposes another interesting problem  When the    rst user action is to select    putters    as golf club type  then popular choices such as    Nike    or    TaylorMade    for brand will produce valid results  albeit not necessarily the most desirable ones  Golf a   cionados may prefer    Odyssey    or    Scotty Cameron    putters  Although it is not that dramatic to put the generally popular    Nike    on top  it is arguably a better user experience to be able to show    Odyssey    given the    putters    selection  The example output from our implementation can be seen in Figure 4  In summary  it is preferable for a facet system to take into consideration the existing user context  in the form of pre selected attribute value pairs from a facet selection or a user query  via  20  24    and dynamically adjust the shown facet values  One way to achieve this task would be to consider the data and pre compute the correlation between all possible value combinations across attributes  This information is then stored for online processing and use the context to    lter the possible choices  However  such computation is very expensive for the scale of data sets available in a web search engine  Even after this expensive computation  supporting the retrieval of such information for real time search is a non negligible investment that will have an overall impact in resource usage and result response time  Besides the computational cost  it is also not clear whether data correlation frequency corresponds well with the user intent  To limit the computational cost while still trying to best satisfy the user  we turn again to query logs  We compute co occurrence for value pairs across different attributes that appear frequently together in the same query  We then validate the co occurring values against the database to ensure a non zero entity count in the results  By going to the query logs we signi   cantly reduce the space of possible data value pairs to consider when compared to all attributes and all values of a table  As a result  we effectively address the issue of empty results without signi   cantly affecting the response time  resource usage and off line computation cost  More importantly  we further optimize the user experience by dynamically placing the most relevant facet values at the top of the list for a given selection  To compute the co occurrence  we de   ne the conditional score of a value for a given attribute by the probability that the value represents the attribute in a query that co occurs with a given attribute value pair  Let Ac vc denote conditional value vc bound to attribute Ac  A v denote value v bound to attribute A  and t be a token  We want to estimate P A vjt  A  c vc   the probability of value v bound to attribute A given the observed token and conditional attribute value binding  Since tokens are tagged independent of one another  this simpli   es to P A vjt   which we can compute using Equation 8  The difference is in which queries are used from the query log when computing the popularity  Given a query log Q and attribute value pair AV    Ac  Ac vc   let QAV denote the subset of queries fq j q 2 Q    Ac  Ac vc  2 AVqg  We then estimate the popularity of a value over all queries satisfying the given attribute value condition  F  QAV  A v    X q2QAV X  A v 2AVq wqP A vjt   The last consideration for context dependent facet values is the case where there are multiple selection conditions speci   ed in the query  This may occur either as multiple recognized attribute values in a keyword query  or as multiple facet selections  Supporting n way conditionals with exact information is impractical  The datadriven approach suffers the combinatorial explosion of an already large data set  and the query log driven approach suffers from the Input Table  Golf Clubs Query     golf putters    Facet Values Nike Odyssey Ping Ping TaylorMade Scotty Cameron Figure 4  Example top 3 facet values for golf club brand  sparseness of queries  After    ltering by multiple selection conditions  the number of queries from which to compute popularity frequencies becomes too small  For this scenario  we adopt a simple approach of intersecting value lists and aggregating popularity counts by addition  This will    nd values relevant to multiple selections  and score them proportionally to how relevant they are to their respective conditional selections  Handling an empty intersection is more of a user interface design decision  One could eliminate the facet entirely  or default to the category dependent values  5  FACETS AND DATA STATISTICS The query logs can reveal which attributes in the table are popular among the users  However  just because an attribute is popular does not necessarily mean that it is good to be used as a facet  In this section we discuss how to use some signals from the data to discover which attributes make better facets  Information Content  Recall that the goal of facets is to enable the users to quickly zoom into a smaller subset of products that are of interest to them  Therefore  in order for an attribute A to be a good candidate for a facet  it should have high information content  that is  the value selection v for the attribute A should give information about the tuples of interest to the user  This property is naturally captured in the entropy of the attribute A  For an attribute A de   ned over the domain A V  the entropy of A is de   ned as H A     X v2V PA v  log PA v  where PA v  is the fraction of entries in the table where attribute A takes value v  over the number of entries in the table where the attribute A takes any value  i e   is non null   Attributes with low entropy do not make good facets  For such attributes the distribution of values is highly skewed  and the knowledge of the value gives very little information for the subset of entities of interest to the user  This is clear in the extreme example when the entropy is zero  meaning that all entities take the same value  In this case  the attribute is useless for navigation  since it gives no information about which entities the user is interested in  For example  the attribute color in the televisions table conveys no useful information as a facet if all televisions are    black     An alternative interpretation of H A  is that it captures the decrease in the uncertainty for the preference of the user for the entities  rows  in table T  When no facet attribute has been selected  any entity e 2 E is equally likely to be of interest to the user  therefore  if N is the number of entities in the table  each entity has probability P e    1 N  The uncertainty is captured in the entropy of the random variable E  which in this case is maximum  H E    log N  Now assume that the user is allowed to use attribute A to zoom in on a smaller subset of entries  The uncertainty for E decreased given the knowledge of the value in A is measured as H E   H EjA   where H EjA  is the conditional entropy of E given A  It is well known that H EjA    H E  A   H A   Since each entity is distinct from the rest  and assuming that A takes a value for all entities  we have that H E  A    H E    log N  Therefore  H EjA    H E   H A   Thus the entropy H A  captures how much our uncertainty for the entities that the user is interested in will    decrease when we have the knowledge for attribute A  If H A    0  then the knowledge of A gives no extra information for E and thus it is redundant  It follows naturally  that attributes with low entropy should not be used as facets  Sparsity  Real world data is often noisy with missing or incomplete information  As a result there are often cases where some attributes are only sparsely populated  If such an attribute is selected for faceted navigation  then selecting any value will immediately discard most of the entities since they do not have a values for the sparse attribute  This could be ok if this corresponded to a rare feature  in which case missing information corresponds to negative information for the existence of the feature  However  it is often the case that sparsity is due to noise in the data collection  in which case  the entities that are eliminated are valid for the selected facet  yet will never be visible to users  Furthermore  noisy attributes are often likely to contain incorrect values  confusing the tagging process of the queries and the corresponding probabilities  Therefore  we exclude from the candidate attributes the ones that are very sparse  The sparsity of attribute A is de   ned as R A    jT A j jTj where T A  is the set of entries in which the attribute has a non null value  and jTj is the total number of entries in the table  Similarly to entropy  sparse attributes are usually not good facet candidates  6  EXPERIMENTAL EVALUATION We abstract the structured data as tables  each containing entities of the same type  For our experiments we have 1164 such tables that we crawled using the MSN shopping XML API 2   The tables correspond to products used to answer shopping queries and are similar to the data used by sites like Amazon or Google Product Search  We consider each category of products to be a table of entities of the same type  The available range covers entities from electronics like digital cameras or televisions to golf clubs and soft goods like shoes and shirts  In total  there were around 10 million structured distinct product entities occupying approximately 300GB on disk when stored in our database  Besides the structured data  we also use a web query log and a vertical click log  The web query log contains queries posed on a major search engine on a period of    ve months from July to November 2009  The web queries are approximately 31 million distinct queries all in the English language  As query weight we use the aggregated impressions count     each time a query is asked it increments its impression count  We limit ourselves to queries with at least 10 impressions to reduce noise in the queries  The total aggregate impression weight of the log is approximately 4 2 billion impressions  The average query length is 3 42 words and 22 04 characters  The click log is available via toolbar usage logs collected from users that have explicitly opted in to install the toolbar browser add on of a major search engine  It contains queries and clicks on Amazon products over a period of one year from March 2009 to February 2010  Since our structured data set is in the shopping domain  we were able to map the Amazon click log to our product entities using unique identi   er references like UPC and EIN codes  The format of the log is query  entity id and number of clicks as the query weight  The total number of distinct queries is in the order of 2 2 million with average length of 3 67 and 24 1 characters  It is worth noting that the click log is smaller although the period used 2 See http   shopping msn com xml v1 getresults aspx text digital cameras for for a table of digital cameras and http   shopping msn com xml v1 getspecs aspx   itemid 25236859 for an example of a camera entity with its attributes  is longer  This is attributed to two reasons  there are more queries on a search engine than on Amazon  and the toolbar  as an add on  means that it can only capture a fraction of the click activity since it is opt in only and many users do not install such add ons  However both logs provide very valuable information  For our experiments  we implemented our work as part of  23  and exploit the query annotator described in  24  to map queries to tables in our collection and provide the token to attribute value mapping  We run the annotator on our web query log and kept only the interpretation to tables that are above the threshold of     1  Since a query can map to more than one table  we took all the plausible tables and normalized their probabilities to fractions summing to 1  Then we used each fraction to map the query to the table while appropriately adjusting the query weight  Thus we produced a subset of the query log for each table  with each query having a weight that is the corresponding fraction of its impressions multiplied by the normalized table probability  We tested the various techniques we proposed in the paper and we present the results below  The naming scheme is as follows  DataProb is the probabilistic disambiguation from Section 3 1  ClusterProb the cluster based approach from Section 3 2  Iterative is the approach described in Section 3 3  and Clicks utilizes the click log as described in Section 3 4  In addition to these techniques  we use a simpler one  shown as NoProb  that computes the score using for each query mapped to a table T  all the plausible token attribute pairs from the table T  while assigning to all of them equally the query weight multiplied by the normalized table weight produced by the annotator  NoProb is not blind counting because it does consider the query weight and annotation probability  from  24   that we used in assigning the queries to tables  Thus we consider NoProb as a state of art baseline  The main difference with our proposed disambiguation techniques is that it does not use an explicit probabilistic disambiguation on the possible token attribute mappings within each table  6 1 Relevance Judgments To measure the effectiveness of our results we conducted an extensive study using Amazon Mechanical Turk  Assessing the entire dataset of all possible tables and all possible attributes would have been prohibitively expensive  Instead we chose a diverse subset of our tables covering a variety of different areas representing as much as possible the entire data spectrum  The tables used in our evaluation were televisions  microwave ovens  cell phones  golf clubs  mp3 players  shoes  laptop computers  printers  watches  video games  engagement rings  digital cameras  skin cleansers  and camera bags and cases  We ran all our approaches on the full set of tables and produced a set of facet attributes for each table and each algorithm  Then we took the results for the evaluation tables and created a single pool of attributes for judging by merging the results of all algorithms  We posted Mechanical Turk HITs  Human Intelligence Tasks  to obtain user judged relevance for the quality of attributes for search  Users were asked to judge how important a given attribute was for searching within a given category  with the example of faceted search explained  on a three point scale  The scale items were labeled    highly important        somewhat important     and    not important     Producing high quality judgments in a crowd sourced environment like Mechanical Turk is a challenging problem on its own  We employed a number of methods to ensure quality judgments  First  we created HITs of ten judgments from our result set and we added an extra honey pot judgment used for spam detection  The additional honey pot judgment was drawn from a pool of judgments we performed manually  and for which we felt there was a clear    highlyjudgements mturk 1 2 3 manual judgments 1 6 15 3 2 6 34 4 3 2 17 50 Figure 5  Confusion matrix between mturk and manual judgments  important    or    not important    answer  The deviation of workers from our gold standard tasks was often a clear indication of spam  However  to ensure fair judgment  we manually inspected the results of any candidate spammer for consistency  The average completion time of a worker was also used as an aid in detecting unreliable workers  Lastly  each task was repeated by nine unique workers  This allowed us to    nd majority decisions for roughly two thirds of all attribute judgments  For the remaining third without a majority  we took the average score  The result is a single valuation of the importance of each attribute  Using these graded judgments we computed the normalized discounted cumulative gain  NDCG  for the ranked output of each approach  Given a list of results L  DCG was computed using linear gain and a logarithmic rank discount as follows  DCG L    Xk i 1 rel Li  log2 i   1  Where rel Li  denotes the judged relevance of result Li  Relevance scores where assigned from zero to two for the three judgment levels  Let I denote the ideal result ordering  the list of all possible results ordered by judged relevance   then NDCG is de   ned as follows  NDCG L    DCG L  DCG I  We report on various values of k  the number of attributes returned  If a system returned less than k attributes it was penalized with a relevance score of 0 for each missing value  While we were con   dent in our quality assurance techniques  we further validated the collection for signi   cant outliers  by manually judging    ve categories for which we have personal knowledge of the domain through shopping experience  The table in Figure 5 shows the confusion matrix for the judgments  Across the top are the Mechanical Turk judgments  and on the left are the manual judgments  Entry  i  j  is the number of times the manual judgment was i  and the Mechanical Turk was j  We round averaged Mechanical Turk valuations to the nearest integer for this computation  We can see from this that the Mechanical Turk workers often favor the    safe    middle valuation  The effect of averaging judgements that do not reach a majority may also push valuations to the middle  The manual judgments tend to distribute more judgments to the    highly important    and    not important    valuations  Overall  we see exact agreement on 66  of judgments  with opposing 1 vs 3 valuations on only 4   The Mechanical Turk valuations are therefore somewhat more coarse grained than our careful manual evaluations  but are still similar in overall trend  We have also run all of our experiments over the manual judgments and found the results to be equivalent in terms of trend and rank order of the systems  We omit these graphs for brevity of presentation  6 2 Attribute Pre selection with Data Filters We start with an experiment that measures the effect of data statistics on the produced facets such as selectivity and entropy as described in Section 5  The results are summarized in Figure 6  Figure 6  Attribute pre selection using data    lters  The    rst step was to take all attributes for each table 3 and perform a run on our two simpler algorithms  shown as NoProb Full and DataProb Full in Figure 6  Surprisingly  DataProb Full does not improve much over NoProb Full  In fact it actually performs worse at higher values of k  Upon further investigation  this is understandable due to how certain attributes affect the computed probabilities  For example  there is an attribute called video input type for digital cameras where the value is almost always    digital camera     The probability P tjA  for the token    digital camera    was very high  affecting the disambiguation process whenever it was recognized in a query  As a result the DataProb Full got skewed in counting the proper attribute importance incorrectly  There were other such attributes with similar characteristics that produced a skewed behavior  We observed that such attributes had in common certain data statistics such as low value entropy and low attribute sparsity  We did a second run where we removed attributes that appear in less than 10   R A    0 1  of the entities and have entropy less than 0 1  H A    0 1   Using this data set we run again the same two algorithms  shown as NoProb and DataProb in Figure 6  The pre selection step reduced noise in the data signi   cantly and also removed attributes that have little information content  low entropy  and are not appropriate for facets  As a result the same algorithm with the pre selected data set perform much better  Now the actual data probabilities used for intra table attribute disambiguation are more meaningful and DataProb performs better than NoProb  Entropy and sparsity can be seen as continuous discount values  We saw in practice that using them as parameterized    lters is easier and produces good results  The speci   c values we used capture the problems with our particular data set and might not be optimal for all data sets  But the point we wish to make is that one has to consider such data    lters to pre select good attribute candidates in the mining algorithms  The full attribute set triggers bad results even on the more complex algorithms  It is not shown here for presentation simplicity  The remainder of the experimental section uses the same pre selected set of attributes for all our techniques  6 3 Explicit Disambiguation Using the above mentioned preselected attribute set we ran all of our disambiguation algorithms  The results are shown in Figure 7  As we discussed in Section 6 2  DataProb has clear advantages over NoProb  However DataProb has the problem of treating attributes independently  For example  in the query    30 inch television     DataProb will incorrectly determine the height attribute to be most probable over diagonal screen size  The confusion happens because televisions in that range generally have diagonals of 32 or 27 inches  whereas large 50 inch televisions tend to have a height around 30 3 We exclude metadata and boolean attributes with values like    yes no        true false   Figure 7  Token attribute disambiguation comparison inches  Since there are many 50 inch televisions in the data  height is assigned a higher probability for the token    30 inch     While ClusterProb appeared to be an intuitive solution to this issue  we see in practice that it performs no better than DataProb  ClusterProb works well for categorical attributes  but it actually does even worse on numerical ones for a couple of reasons  First  since user queries do not always re   ect the exact values of numeric attributes it tries to map the continuous domain into discrete buckets  That discretization process makes the issue of the query token    30 inch    matching the diagonal data values 27 or 32 inch even worse than DataProb  Because the actual data values vary from the query tokens  the values of the user intended attribute do not form a good model for the query tokens  Furthermore  KL divergence is only well de   ned for distributions that have non zero probability over the entire value domain  meaning even depth with actual small numeric values is assigned a small probability for large values like 50 inches  thus further enhancing the confusion of numeric attributes  Iterative tries to detect correlations between data and queries and it exploits the overall bias of users towards the correct attributes  Intuitively  the reason it disambiguates the best is due to how users enter queries  Although the system can be confused with multiple interpretations  users actually know which attributes they are looking for when they enter their values  When examined across the full query log  the user behavior tends to match the intended attribute  For example  although token    30 inch    can be consider closer to a TV height than diagonal screen size for many modern televisions  users query much more frequently for diagonal screen size including other values like    50inch    or    55inch     This creates a bias for inch tokens towards the attribute diagonal  which Iterative applies to the    30 inch    token  This causes diagonal size to be preferred over height even for that particular value  One surprise in the disambiguation methods is that Clicks do more or less similarly to DataProb  At    rst  one would think that clicks offer the perfect disambiguation method  A user click on a particular entity id allows us to select only the attributes that closely match that entity id disambiguating attributes almost perfectly  However  clicks have problems in the way they were produced that make them less than optimal for learning good facets  First  a user can only click on results shown to them  so clicks incorporate the engine bias and do not represent the true popularity of how many times all possible queries are asked  Hence some attributes are underrepresented in a click log due to the engine bias of what results were shown  Second  there are far fewer queries with clicked results than generally asked  meaning many categories have very few queries  This means the mined facets do not have much support and are somewhat erratic  Finally  the queries that tend to trigger clicks are very speci   c queries looking for one or few products  e g     canon Figure 8  Comparing with Amazon and Bing  eos 50d digital camera     More general category based queries tend to have few clicks and commonly followed by a re   nement since they return a large result set that users cannot easily consider  e g     12 megapixel digital camera     This creates a skew of the importance learned towards attributes that act as unique keys  However these attributes are not well suited for facets  If a user already knows the unique key they are looking for  like camera model   they can simply select it from the results and do not need a facet for it  Instead such attributes are better suited for entity snippets  that is summarized views that help users differentiate amongst entities  6 4 Commercial Faceted Search Commercial web engines are already supporting faceted search  Although we do not know the technical details of industrial implementations  we felt the best way to test the real world effectiveness of our solution is by comparing against state of the industry engines  Since our data is shopping based  we considered Amazon as the world   s most popular shopping engine  Furthermore  since our data comes from the public MSN Shopping API  we considered Bing Shopping  redirects from MSN Shopping  as a shopping engine that is using the same data set as our experiments  Both Amazon and Bing Shopping show facets  To test them  we crawled the attributes shown on Amazon and Bing for our test categories and submitted them in the same pool of attributes that we labeled with mturk judges  To remove any bias  we dropped any af   liation information on the web site  Furthermore  we did not consider the handful of generic shopping attributes shown on both Amazon and Bing  like price or shipping options  Although very valuable facets there is no need for an automated technique to discover them  as such attributes can easily be added to all categories  Instead we limited our comparison to only category speci   c attributes from each solution  The results are summarized in Figure 8  Our best solution performs better than Amazon and signi   cantly better than Bing Shopping  For many categories Amazon shows very few attributes and Bing shows even less  As a result  both Amazon and Bing drop dramatically as the size k increases in our experiments  It is important to note that Amazon does show up to 12 category dependent facets in some cases  e g   watches  so screen real estate is not a limiting factor  For small values of k  k   3  we still do better but the difference  particularly with Amazon  is smaller  We are not familiar with the details of their facet selection approach and so their techniques cannot be contrasted to ours  However  since they are a popular site with lots of query traf   c and domain knowledge  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web"/>
        <doc>Schema As You Go  On Probabilistic Tagging and Querying of Wide ### Tables Meiyu Lu     Divyakant Agrawal        School of Computing National University of Singapore  lumeiyu  dbt  atung  comp nus edu sg Bing Tian Dai     Anthony K H  Tung        Department of Computer Science University of California  Santa Barbara agrawal cs ucsb edu ABSTRACT The emergence of Web 2 0 has resulted in a huge amount of heterogeneous data that are contributed by a large number of users  engendering new challenges for data management and query processing  Given that the data are uni   ed from various sources and accessed by numerous users  providing users with a uni   ed mediated schema as data integration is insu   cient  On one hand  a deterministic mediated schema restricts users    freedom to express queries in their preferred vocabulary  on the other hand  it is not realistic for users to remember the numerous attribute names that arise from integrating various data sources  As such  a user oriented data management and query interface is required  In this paper  we propose an out of the box approach that separates users    actions from database operations  This separating layer deals with the challenges from a semantic perspective  It interprets the semantics of each data value through tags that are provided by users  and then inserts the value into the database together with these tags  When querying the database  this layer also serves as a platform for retrieving data by interpreting the semantics of the queried tags from the users  Experiments are conducted to illustrate both the e   ectiveness and e   ciency of our approach  Categories and Subject Descriptors H 2 8  Database Management   Database applications General Terms Management  Performance Keywords Probabilistic Tagging  Wide Table  Top k Query Processing ### 1  INTRODUCTION The rapid growth of Web 2 0 technologies and social networks has provided us with new opportunities for sharing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  data and collaborating with others  Within a community  users may like to contribute their own data  share the data with friends  and meanwhile explore the shared data at the community scale  E   ectively and e   ciently managing and exploring these data at a web scale level is an interesting and important problem  In this paper  we focus on the management of shared structured tables  e g  Google fusion table  16   which is an important type of data in Web 2 0  Such data have some inherent properties that make their management challenging  Heterogeneity  Data provided by di   erent users may be widely heterogeneous  containing di   erent attribute names that describe semantically similar values or same attribute name with di   erent semantics  For example  in Figure 1 a   user A chooses attribute make to describe the car manufacturer in his her private data  while user B uses manufacturer to represent the same meaning  Similarly  the semantics of car in  b  is model but in  c  it contains both make and model information  Numerosity  As all users can publish their data into the community as shown in Figure 1 a    b  and  c   the number of distinct attributes will grow drastically as more users participate  This results in a wide table  7  as Figure 1 d  which is essentially a structured table with a large number of attributes and many empty cells  Given such a wide table  most existing data integration approaches  23  create a global mediated schema that usually contains a huge number of attributes  Such huge mediated schema is di   cult to query as users cannot remember all the attributes  Therefore  a more    exible and user oriented query interface is required  This interface should allow users to choose di   erent attribute names to represent the same semantics in di   erent queries  and have the power to express their con   dence over the attribute semantic heterogeneity in posed queries  Personalized Attributes  Since attributes are contributed by users  there could be many personalized attributes  which cannot be captured by existing ontology or taxonomy  such as DBpedia 1 and WordNet 2   Examples of such personalized attributes include myear in Figure 1 c   which represents the production year of a car  and eadd  the abbreviation for email address  Due to this  existing techniques that rely on ontology for schema mapping  26  30  will not be suitable any more  A better system should be able to infer more popular 1 DBpedia  http   dbpedia org About 2WordNet  http   wordnet princeton educar myear BMW 525i 2005 Toyota Camry 2009 make Ford BMW model car Fiesta 318Ci manufacturer myear                   Ford Honda Focus 1 6A Accord                         BMW 525i Toyota Camry       2005 2009       User A User B User C Q1 Q4 Q2 User E User F User G User H Q3  a   b   c  make model Ford Fiesta BMW 318Ci manufacturer car Ford Focus 1 6A Honda Accord TID T1 T2 T4 T5 T6 T3  d  Figure 1  The Wide Table Tag1 Similarity make 0 95 make 0 5 manufacturer 0 52 model 0 65 Tag2 manufacturer car car car Figure 2  Tag Similarity attributes for such personalized ones  so that these data can be explored by the other users asides from its owner  Due to the inherent semantic heterogeneity of the attributes in user contributed data  we will no longer refer to them as    attribute     Instead  we borrow the term tag from the multimedia domain  1  2   and view these attribute names as just a way to annotate the values that are provided by users 3   In this paper  we describe a query system which enables users to e   ectively query a large quantity of user contributed data  Our system provides the following functionalities      Users can contribute and share data by inserting them into a single wide table together with their tags  as shown in Figure 1 d       Our system is able to automatically discover the semantic relationships between users    tags  which is handled by our tag inference approach in Section 3  Figure 2 illustrates the semantic similarities between each pair of tags produced by our approach for the example data in Figure 1 4       Users can use their own preferred tags to query the database  and the system then dynamically determines the semantics of these tags at query time  Combining the data from Figure 1 d  with the tag similarities in Figure 2  we can easily obtain the virtual probabilistic tagged data  as shown in Figure 3  where each value is associated with a list of htag  probabilityi pairs  The probability indicates the semantic association strength between the value and tag pair  The generation of such semantic association probabilities is called probabilistic tagging  and its formal de   nition will be given in Section 2  We next present an example to illustrate several characteristics of our query interface via a set of queries  3 In this case  users might even choose to annotate the same value with multiple tags  4 As our similarity measure is symmetric  we only show unidirectional tag similarity in Figure 2  Example 1  Figure 1 shows a scenario where four users  A  B  C and D contribute and share their datasets  The wide table is used to store the shared data  User E  F  G and H then submit four di   erent queries which are Q1  Q2  Q3 and Q4  respectively  The queries are shown in Table 1 in an extended SQL format  The delimiter         in the query is for users to explicitly specify their con   dence over tag semantics  Its formal de   nition is presented in Section 2  Here we can interpret make  0 9 as a set of tags in Figure 2 which are similar to make with con   dence no less than 90   The table name CAR refers to the probabilistic tagging table in Figure 3  Values in the result tuples have the same order as the tags in the SELECT clause  From the query result in Table 1  we observe that i  different users can choose di   erent tags to express the same semantics  such as Q1 and Q2  ii  explicit con   dence speci   cation gives users control over the search scope  For instance  with lower con   dence Q3 retrieves two more tuples than Q1  iii  schema for users    query is dynamically determined at query time  which is user oriented rather than prede   ned  Take Q4 as an example  where a user would like to retrieve values that are associated with car and model from CAR  From the data perspective  for T3 and T4  tag car in the query should be aligned with car in the source table  But from the users    perspective  intuitively the best schema alignment for T3 and T4 should be  car  model   This is what our system retrieves given Q4  To further see the bene   t of our approach  Figure 4 shows the result of Q4 over tuples T3 and T4 using our approach and the one proposed by Dong and Halevy in  12  25  5   Both records returned by our approach are correct and ranked in the right order  However  the top two ranked tuples retrieved by probabilistic integration are not correct although their score is much higher than the correct ones  The result di   ers because we are dynamically determining the semantics for user posed tags at query time  The reasons for such di   erences will be explained in Section 2 2  Besides the functionalities discussed above  our work also makes the following contributions      We proposed the concept of probabilistic tagging to represent the associations between values and tags      We presented an e   ective distance function to measure the semantic distance between a pair of tags  Based 5 Please refer to Appendix A for the detailed score computation for probabilistic integration Table 1  Example Queries QID Query Result Q1 SELECT make 0 9 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda  Q2 SELECT manufacturer 0 85 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda  Q3 SELECT make 0 5 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda   T5 BMW 525i  T6 Toyota Camry  Q4 SELECT car  model FROM CAR  T1 Ford Fiesta  T2 BMW 318Ci   T3 Ford Focus 1 6A  T4 Honda  Accord  TID Value Probabilistic Tagging T1 Ford  make 1 0   manufacturer 0 95   car 0 5  T1 Fiesta  model 1 0   car 0 65  T2 BMW  make 1 0   manufacturer 0 95   car 0 5  T2 318Ci  model 1 0   car 0 65  T3 Ford  manufacturer 1 0   make 0 95   car 0 52  T3 Focus 1 6A  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T4 Honda  manufacturer 1 0   make 0 95   car 0 52  T4 Accord  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T5 BMW  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T5 2005  myear 1 0  T6 Toyota  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T6 2009  myear 1 0  Figure 3  Probabilistic Tagging TID car T4 T3 Ford Honda model Focus 1 6A Accord SELECT car  model FROM CAR Q4 rank 1 2 TID car T4 T3 Focus 1 6A Accord model Focus 1 6A Accord rank 1 2 T4 T3 Ford Honda Focus 1 6A Accord 3 4 Data Integration Result Our Result Query QID Figure 4  Query Result Comparison on this  we are able to automatically infer semantically similar tags for each value      We proposed an e   cient dynamic instantiation approach to associate the queried tags and data values during query processing      A complete and extensive experimental study is conducted to validate our approach  2  PROBLEM DEFINITION 2 1 Probabilistic Tagging As discussed earlier  one of our main objectives is to provide a    exible interface  which allows users to query the underlying database with any tag they like  The very    rst challenge is to discover which tags are similar in semantics  However  this is insu   cient since some tags are more similar to each other in semantics while others are not  Taking T1 in Figure 3 as an example  it is intuitive that the semantics of    Ford    is better re   ected by manufacturer than car since the semantics of manufacturer is quite speci   c  while car is a relatively more general term  Therefore  the second issue is that the semantic association strength between a value and tag should be well quanti   ed  as shown in Figure 3  In this paper  for a given value v in the wide table  we interpret its semantic association with a tag t as the likelihood belief that v is tagged by t  Definition 1  Probabilistic Tagging   Let T be a set of tags  for a value v in Wide Table and a tag t     T   probabilistic tagging refers to the process of associating a probability   v t for v and t  such that value v is tagged by tag t with probability   v t  The probability   v t is called the associated probability or association probability  If the semantic association between value v and tag t is stronger  then their associated probability   v t will be higher and vice versa  As shown by T1 in Figure 3  for the value    Ford     its associated probability with its original tag make is 1 0 while its associated probability with inferred tags manufacturer and car are 0 95 and 0 5  showing di   erence in the strength of semantic association  We note here that unlike previous works  12  25  17  in data integration  the probabilities for all tags being associated with a value v do not sum up to 1  This is because there is no assumption of mutual exclusion between tags at this stage of data processing  We avoid doing so to prevent a label bias problem in which a value v will have much lower associated probability with each tag if it can be semantically described by many di   erent tags  For example  a value associated with 5 or more tags will have much lower probability than value with 2 or less tags  Instead  we choose to introduce the assumption of mutual exclusion during query time where the semantic meaning of the tags are clearer  2 2 Query Semantics After discovering and quantifying the tag semantic relationships during probabilistic tagging  the resulting probabilistic table is shown as in Figure 3  In this table  each value is associated with a htag  probabilityi pair list  Query over such probabilistic table is much more challenging than with a prede   ned schema  We will now look into the query relevant issues over a probabilistically tagged table  2 2 1 Extended SQL Query In this paper we adopted a SQL like query syntax to support our tag based queries  Speci   cally  we extended SQL query to allow users to express their semantic con   dence for each tag using the delimiter          We show some examples in Table 1  Below is a more complex extended SQL query  SELECT car  year 0 9  price 0 3 FROM CAR WHERE make 0 5      Ford    AND price     10 000 AND price     20 000When a user provides a tag with high con   dence  it means that he is very sure about the semantics of the tag  and does not want our system to do much tag inference  In contrast  if the user is unsure of the semantics and he likes to use more tags that are similar to the queried tag  then he can provide a lower con   dence threshold  Definition 2  Tag Expansion   Given a tag tq and its semantic con   dence    in query q  tq    refers to the expanded tag set Te    tei  i   1          n   where for each tag tei     Te  Semsim tq  tei           We call Te as the expanded tag set for tq  In the above de   nition  Semsim tq  tei   is the semantic similarity between tag tq and tei   which is formally de   ned by Equation 6 in Section 3  When processing query q  tq will be expanded to a tag set Te  a set of tags that are semantically similar to tq based on a threshold     All values that are originally associated with any of the tags in Te are valid for this tag expansion condition  and should be taken into consideration when answering the query  If no semantic con   dence is explicitly speci   ed for tq  we will expand it to all our inferred tags  Take the queries in Table 1 as example  make 0 5 will be expanded to all the tags that are similar to make in semantics inferred by our system  that is  manufacturer car   but make 0 9 will only be expanded to  manufacturer   2 2 2 Query Answering Before explaining the semantics of query answering  we    rst look at the intuitions behind our query answering techniques  Intuitively  syntactically equivalent tags in user posed queries should be instantiated with the same value  no matter how many places it appears in the query  For example  although price occurs three times in the example query in Section 2 2 1  once in the SELECT clause and twice in the WHERE clause  it is obvious that all the occurrences of price refer to exactly the same semantics  i e  price of a car  Thus  in the resulting record  all these three instances of price should be instantiated with the same value  Based on this  we proposed the following Consolidation Rule for our query answering  Rule 1  Consolidation Rule   Let set Tq    ti i   1          n  be the set of tags in query q  for two tags ti  tj     Tq  if ti   tj   then ti and tj should be instantiated with the same value from the underlying record  Consider the query in Section 2 2 1  although we know tag car and make are similar in semantics from Figure 2  in this query they actually represent di   erent attributes because the user speci   es them as di   erent  In other words  car and make are mutually exclusive from each other in this query  and should be instantiated with di   erent values in the resultant record  We proposed our Mutual Exclusion Rule as below  Rule 2  Mutual Exclusion Rule   Let Tq    ti i   1          n  be the set of tags in query q  for two tags ti  tj     Tq  if ti  6 tj   then ti and tj should be instantiated with di   erent values from the underlying record  We have discussed the impact of our mutual exclusion rule using Q4 earlier  in Example 1   There for T3  we argued that car in the query should be aligned with manufacturer in the source table  Here we look at the query in Section 2 2 1  where a user would like to retrieve the value associated with car from a record whose value    Ford    is associated with make  for simplicity  we skipped the other constraints   Now for T3  it is more reasonable for car in the query to be aligned with car in the source table  Comparing these two queries  we can see that with our mutual exclusion rule  the same tag  car  can be aligned with di   erent semantics in di   erent queries even for the same record  T3   As explained earlier  applying the mutual exclusion rule during query answering time allows us to avoid the label bias problem  Moreover  our mutual exclusion rule cannot be trivially introduced in  12  25  17  since they enforced the fact that two attributes tags are either semantically equivalent  associated with the same value  or not in each possible world  As such  the problem illustrated by Figure 4 in Example 1 is inherent for  12  25  17   For a given tuple record  we aim to    nd its possible best alignment to the queried tags  As the semantics of queried tags are determined dynamically on the    y  our query answering is referred to as Dynamic Instantiation  Definition 3  Dynamic Instantiation   Let set Tq    ti i   1          n  be the set of tags in query q  and Vr    vj  j   1          m  be the set of values from a record r  we refer to a mapping f   Tq     Vr as the dynamic instantiation of Vr with respect to Tq  Based on f  the instantiated score Score r  for r with respect to q is de   ned as the multiplication of associated probabilities for edges in f  i e  Score r    Qn i 1   f ti  ti   In addition  all the following conditions must be satis   ed in f  1  f satis   es all the value constraints in the WHERE clause  2  f satis   es both Consolidation Rule and Mutual Exclusion Rule  3  The associated probability for each edge in f  i e    f ti  ti   must satisfy its corresponding tag expansion  4  Among the mappings in which all the above three conditions hold  dynamic instantiation refers to f whose score is maximized  i e  f   arg maxf Score r   The    rst condition states that if tag t has value constraints in the WHERE clause  then its mapped value f t  should satisfy all the value constraints over t in query q  The second condition is for us to better capture the users    intention as discussed earlier  The third condition is used to meet the tag semantic speci   cations  Finally  the last condition is to    nd the best possible instantiation between tags set Tq and values set Vr  i e  the one with maximum instantiation score Score r   Our instantiated score is de   ned as the multiplication of associated probabilities in the instantiation f  that is because in our case the associations between tag and value pairs are independent  and based on our mutual exclusion rule  all the edges falling outside of f are invalid  Due to the semantic uncertainty of tags  answering a query q can result in a large number of records with score larger than 0  In view of this  we mainly focus on top k query processing to    nd the best k answers based on our query answering semantics  Definition 4  Top k Query Answering   Given a query q  a user speci   ed positive integer k  and a set ofrecords R    ri i   1           R    the problem of top k query answering is to retrieve a record set RS  such that RS     R   RS  is maximized with the condition  RS      k     r     RS and    r         RS  which is the complementary set of RS  Score r      Score r        From the above de   nition  it is possible that less than k results are retrieved  We however feel that it is a worthy cost to pay for freeing users from a    xed mediated schema and providing relaxation over the tag speci   cations  3  TAG INFERENCE We next describe our tag inference process which is essential for discovering the semantic relationships between tags  and computing the associated probabilities between tag and value pairs  We use T    ti i   1          n  to denote all the tags in our database  and V    vj  j   1          m  to denote all the values  For a tag t     T   we use Vt     V to represent the set of values that fall under the tag t in the wide table  E   ectively discovering the semantic similarities between user contributed tags however is challenging  Although several approaches have been proposed in the literature  23  26  30   none of them is suitable for our case  First  string similarity over tag names could not convey all cases  26  23   With such approaches  model and model year will be treated as similar but make and manufacturer will be treated as dissimilar  Second  ontology and taxonomy based methods  26  30  are not applicable  as there are many personalized tags like myear and eadd  which cannot be captured by any existing ontology and taxonomy  Finally  similarity function based on values overlap  23  works only for categorical values  not for string and numerical values  For example  given two set of emails that are contributed by different users using two di   erent tags  it is unlikely that there is much overlap between the two set of emails  Our approach to discovering the semantic similarities in this paper can generally be described in two steps  First  soft clustering  28  is applied to the values in the wide table to separate them probabilistically into K clusters  i e  each value can belong to each of the K clusters with different probabilities  Since each tag is associated with a set of values with di   erent probabilities  we can thus indirectly compute the probability of these tags being associated with the K clusters  In the second step  we measure the distance between tags by comparing the probability distribution of their associated membership to each of the K clusters using KL divergence  8  9   KL divergence is a function for measuring the relative entropy between two distributions  Speci   cally  if the probabilistic cluster membership distributions for two tags are p and q respectively  their relative entropy is KL pkq    P i 1  K pi log pi qi   where pi is the probability of the    rst tag being associated to cluster i and qi is the probability of the second tag being associated to cluster i  3 1 Tag Semantic Distance Discovery To perform the soft clustering  our approach    rst extracts a set of features F from the underlying data V   Depending on the type of values in t  di   erent approaches are adopted to generate the features for t  If values in t are string values  we extract a set of frequent q grams called sigrams as its features  If values in t are numerical values  bins of the distribution histogram over Vt are extracted as its features  After extracting the features F for all values V   soft clustering  28  8  is performed over F  In soft clustering  the membership of a value in a cluster is probabilistic  Suppose there are K clusters C    c1          cK   for a value v  its membership probabilities in each cluster form a vector     v    hp1          pKi  where PK i 1 pi   1  and pi is the probability that v belongs to cluster ci  We call     v  the cluster probability distribution for value v over C  Based on  8   our soft clustering based tag inference is robust when K 100  We set K to be 200 in this paper  Since a tag t can be associated with di   erent values which have di   erent cluster probability distributions on the K clusters  we can indirectly compute a cluster probability distribution for a tag t  denoted as     t   by taking the average cluster probability distributions of all its associated values Vt      t    X vt   Vt 1  Vt         vt  To measure the semantic relationship between two tags  t1 and t2  one important factor that needs to be considered    rst is the semantic scope of each of these tags  Intuitively  if the semantics of a tag t is more general  the values associated with it will be more diverse  For instance  both make and model information of a car can be tagged by car  while only the make information can be tagged by make  Thus  the semantic scope Scope t  for tag t can be captured by the average KL divergence between     vt  and     t   Scope t    X vt   Vt 1  Vt     KL     vt       t    1  Another important factor we need to consider is the distance between tag t1 and the values of tag t2  If t1 is similar to t2  then     vt1   should be close to     t2   For clarity  we de   ne the distance between the value set Vt1 of tag t1 and another tag t2 as DV  Vt1   t2   DV  Vt1   t2    X vt1    Vt1 1  Vt1      KL     vt1        t2    2  Based on the above discussion  if two tags t1 and t2 are similar in semantics  they should i  have similar semantic scopes  and ii  close to the value set of each other  Now we consider one extreme case  the distance between two semantically equivalent tags t1 and t1  We can easily    nd that DV  Vt1   t1    Scope t1   From this we can see that if tag t1 is more similar to t2  the ratio of DV  Vt1   t2  over Scope t1  will be closer to 1  and vise versa  Thus  the dissimilarity between two tags t1 and t2 is de   ned by such ratios  Dis t1  t2    DV  Vt1   t2  Scope t1    DV  Vt2   t1  Scope t2   3  However  it is costly to compute such tag dissimilarity for each tag pairs based on Equation 3 since we need to enumerate all values in Vt1 and compute their KL divergence to t2 to obtain DV  Vt1   t2  and vice versa  Assuming there are n tags in T   and each tag t being associated with m values  the complexity for computing all tag pair dissimilarity using Equation 3 is O C 2 n    m   nm   Fortunately  we are able to use the following proposition to reduce the computational complexity Proposition 1  Bregman Information Equality   Let X be a random variable that takes values in X    xi  n i 1     R d following the probability measure          xi   n i 1  given a Bregman divergence    6 and another point y  E   d   X   y     E   d   X          d       y  where      E   X    Proof  E   d   X   y     Pn i 1    xi d   xi  y    Pn i 1    xi     xi         y      hxi     y       y i    Pn i 1    xi     xi                 hxi                 i  hxi               i             y    hxi           y       y i    Pn i 1    xi     xi                 hxi                 i    Pn i 1    xi                y      h       y       y i    E   d   X          d       y  As Pn i 1    xi  xi           0  and both Pn i 1    xi hxi                  i   Pn i 1    xi hxi              y i   0  that is how the second last equation comes from  Intuitively  our proposition simply states that the expected KL divergence between a random variable X following the probability distribution    and a point y is equivalent to the sum of the KL divergence between X and its means    plus the KL divergence between    and y  By this proposition and  3   we can show that the distance between the set of values associated with tag t1  i e  Vt1   and the tag t2 can in fact be computed simply by using Scope t1  and the KLdivergence between     t1  and     t2   DV  Vt1   t2    X vt1    Vt1 1  Vt1      KL    vt1       t2     X vt1    Vt1 1  Vt1      KL    vt1       t1    KL    t1 k   t2     Scope t1    KL    t1 k   t2    4  By applying Equation 4 over Equation 3  we can now de     ne our semantic distance between t1 and t2 as  D t1  t2    KL     t1 k    t2   Scope t1    KL     t2 k    t1   Scope t2   5  D t1  t2  is symmetric  however  it does not satisfy triangle inequality  This coincides with the intuition that two tags cannot be inferred as similar via a third tag  For example  we cannot say model and make are similar although both are semantically similar to car  Note that unlike Equation 3  the computation of the new distance function only involves the semantic scope  and pairwise KL divergence between tags  The computational complexity for all tag pair distances is thus reduced from O C 2 n    m   nm  to O C 2 n   nm   3 2 Tag Inference With the tag distance de   ned above  now we give our semantic similarity between two tags by taking the power of the negative distance  i e  SemSim t1  t2    e        D t1 t2   6  6 Note that KL divergence is a special instance of Bregman divergence  32  and thus the derivation here immediately applies to KL divergence  Table 2  TagTable for make TID LID Value Probability 1 1 Ford 1 0 2 1 BMW 1 0 3 1 Ford 0 95 4 1 Honda 0 95 5 1 BMW 525i 0 5 6 1 Toyota Camry 0 5 where    is a parameter which controls the power of inference  When    is set to be 0  every value will be associated with every tag  when    is set to be in   nity  there will be no inference among tags  In the experiments  we set    at 0 5  An example tag similarity table is shown in Figure 2  Suppose value v is originally associated with tag t in user contributed data  then the associated probability of v with respect to other tags ti     T is de   ned as follows    v ti     v t    Semsim t  ti    Semsim t  ti   7  associated probability   v t   1 as v is originally associated with t  After applying the above tag inference equation  each value in the database will be associated with a set of semantically similar tags with proper probabilities  One example probabilistic tagged data is shown in Figure 3  4  TOP k QUERY PROCESSING Next we will look at our top k query processing approaches  4 1 Data Organization To facilitate top k query processing  we partition the probabilistic tagged table according to tags  For a speci   c tag t  all its associated values are stored into one relational table  and this table is named by the tag name  i e  t  We call such a table TagTable  Table 2 shows the TagTable for make  Each TagTable has four attributes  TID  tuple id   LID  location o   set in the source tuple   Value  data value  and Probability  associated probability   Note that the majority of    is approximately 0 and only values with associated probability above a cuto    threshold are stored  On the other hand  value v may be associated with multiple tags  so there is a copy of v in each of its associated TagTable  For each TagTable  a B    tree index is built over the attributes hV alue  P robabilityi  which will be used for the sorted list retrieval in top k query processing  4 2 Dynamic Instantiation Given a query q and record r  let Tq be the set of tags in q  and Vr be the set of values from r  We can build a weighted bipartite graph G    U  V  E  as follows  Let U   Vr  V   Tq  and for each vr     Vr  each tq     Tq  if vr satis   es the value constraints over tq and   vr tq meets the tag expansion condition  we add an edge ehvr  tqi to E  with weight ln   vr tq   For simplicity  we denote this bipartite graph as G    Vr  Tq  ln   Vr Tq    After taking logarithm over the associated probabilities   Vr Tq   it is easy for us to apply Hungarian algorithm  18  on G    Vr  Tq  ln   Vr Tq   to    nd the best one to one matching between Vr and Tq  where the total sum of cost weight is maximized  We denote the maximum score found by Hungarian algorithm for record r as ScoreH r   Based on De     nition 3  we can see that the mapping found by Hungarianprius price mileage make   model Toyota    10 000   Prius    20 000 year 2006 16204 12198 18500 toyota 1 0 0 9 0 9 0 6 0 7 0 9 0 8 0 3 0 3 Figure 5  Dynamic Instantiation Illustration algorithm is exactly the dynamic instantiation  The    rst and third statements in De   nition 3 are met by our edge insertion constraints discussed above  one to one mapping guarantees the second statement  and the last statement is ful   lled as the solution found by Hungarian algorithm is optimal  From the above discussion  it can be easily proven that Score r    e ScoreH r  Take the following extended SQL query as an example  The values associated with tags year  price and mileage are to be retrieved  with constraints issued over tags make  model and price  SELECT year 0 9  price 0 3  mileage FROM CAR WHERE make 0 8      Toyota    AND model      Prius    AND price     10 000 AND price     20 000 The corresponding bipartite graph is shown in Figure 5  For clarity  edge weights in the    gure are original associated probabilities without logarithm  As illustrated by Figure 5  if mileage is instantiated with 16204 and price is instantiated with 18500 as the darker lines indicate  it gives the best instantiation as the multiplication of the associated probabilities is maximized  4 3 Top k Query Answering Next we will discuss the retrieval of top k answers  For ease of illustration  we refer to tags in the SELECT clause as QTags  and tags in the WHERE clause as VTags  Tag set in query q is denoted as Tq  4 3 1 Sorted List Retrieval For each queried tag tq     Tq  we aim to retrieve a list of valid values from its TagTable  sorted over associated probability from high to low  For a tag that appears in both VTags and QTags  we merge its constraints from SELECT and WHERE clauses  and retrieve one sorted list  We    rst study the situation where a user speci   ed semantic con   dence    is issued over tq  i e  tq     Based on De   nition 2  tq will be expanded to a tag set Te    tei  Semsim tq  tei            Next we will look at the associated probabilities with tq for valid and invalid values  respectively  Valid case  Given a valid value v  suppose it is originally associated with tag tei     Te  The associated probability between v and tq is   v tq   Semsim tq  tei           This means that for each valid value  its associated probability in TagTable tq is not less than     Invalid case  Given an invalid value v       we know that its original tag falls outside of Te  Assume its original tag is tv      From Definition 2  we know that Semsim tq  tv            Therefore  the associated probability between v     and tq  i e    v     tq   Semsim tq  tv            In other words  for each invalid value  its associated probability in TagTable tq is less than     In conclusion  associated probability with valid value is not less than     while with invalid value is  Hence  the sorted list for tq    can be retrieved by the following SQL statement issued over the TagTable for tq  SELECT   FROM tq WHERE Probability        ORDER BY Probability DESC  We then look at the complex situation where there is value constraint over tq besides the tag expansion  i e  tq    op vq  Here op is a comparison operator and vq is a speci   c value  Its sorted list can be resolved by adding one WHERE condition    Value op vq    into the above SQL statement  For the simple cases where there is no tag expansion or value constraint over tq  their sorted lists can be easily retrieved by removing the corresponding WHERE condition from the SQL statement  Bene   ting from the B    tree index built over attributes hV alue  P robabilityi for each TagTable  most sorted lists can be done by an index only scan  After retrieving the sorted list for each tag tq  Threshold Algorithm  TA   13  can be applied to fetch the top k results  using Hungarian algorithm to    nd the best instantiation  We proposed two TA based approaches for top k query processing  Eager and Lazy  4 3 2 Eager Top k Query Answering In eager query answering  for each tag tq in query q  its sorted list is retrieved and maintained  TA  13  is then performed over all the retrieved sorted lists  The main idea is shown in Algorithm 1  A priority queue RS is maintained to store the best k records that have been processed so far  All the scanned records are inserted into Sr to avoid accessing the same record more times  Each time  we scan the top unseen records from all the sorted lists  and pop the record r with the highest logarithmic probability  MP  from its sorted list  line 14   If there are more than one sorted lists whose top probability is equal to MP  our algorithm randomly chooses one from them  The popped record r is then retrieved and Hungarian algorithm is applied to get its score ScoreH r   line 15   Finally  priority queue RS is updated accordingly  line 16 19   and r is inserted into the scanned record set Sr  line 20   For all the unseen records  their scores are upper bounded by the summation of the top probabilities from each sorted list  i e  UBS  This is because our sorted list is ranked over probability  so the probability for each unseen record in SLtq is not greater than SLtq  top prob  Algorithm 1 can be terminated when the k th score in the result set RS is not less than the upper bound score UBS  line 23   or any sorted list is exhausted  line 21   Eager query processing searches through the sorted lists for all queried tags  including all QTags and VTags  However  this is not e   cient enough  Generally  the sorted lists for QTags are much longer than those for VTags  because VTags have value constraints in the WHERE clause  while most QTags do not  especially for those that only exist in QTags  Due to the low selectivity of QTags  their sorted lists usuallyAlgorithm 1  Eager top k Query Processing Input   A query q in extended SQL A positive integer k Output  Result set RS 1 Tq     tags in query q  2 RS             top k result set 3 Sr             scanned records 4 foreach tag tq     Tq do 5 SLtq     sorted list of tq  6 Let SLtq  top be the most top unseen item in SLtq   7 Let SLtq  top prob be the probability of item SLtq  top  8 while true do 9 UBS     0     upper bound score for unseen records 10 MP                maximum in all SLtq  top prob 11 foreach tag tq     Tq do 12 UBS     UBS   SLtq  top prob  13 MP     max MP  SLtq  top prob   14 Pop record r from SLt where SLt top prob   MP  15 Let score   ScoreH r      computed by Hungarian 16 if  RS    k or score   the k th score in RS then 17 if  RS    k then 18 pop one item from RS  19 insert  score  r  into RS  20 insert record r into Sr  21 if SLt has reached its end then 22 break     while loop is terminated 23 if  RS    k and UBS     the k th score in RS then 24 break     while loop is terminated 25 return RS  contain lots of invalid items  Based on this  we proposed another approach  namely lazy top k query answering  which only focuses on the sorted lists of VTags  4 3 3 Lazy Top k Query Answering Unlike the eager approach  our lazy approach only retrieves and maintains sorted lists for VTags in query q  For a record  any values that are not related to Vtags are only accessed when computing its Hungarian score  line 15   For queried tags falling outside of Vtags  we assume their associated probabilities to be 1  the largest associated probability  when computing the upper bound score  UBS   Compared to the eager approach  our lazy approach does not need to probe the long sorted lists which include many invalid records  making it more I O e   cient  In both eager and lazy approaches  the score for each unseen record is upper bounded by the summation of all the top item scores  Hence  the correctness of our algorithms can be guaranteed  5  EXPERIMENTAL STUDIES We now present the experiments to validate the performance of our system  We have two main goals in this section  1  to examine the e   ectiveness of our tag inference approach  2  to show that our top k query processing is capable of retrieving top k results with high precision and low cost  5 1 Experimental Setup We implement our system on top of MySQL  The tag inference and query answering algorithms are written in C    The system is set up on a PC with Intel R  Core TM 2 Duo CPU at speed 2 33GHz and 3 5GB memory  Through all the experiments  we randomly select 2  samples for soft clustering  1  Car Datasets  CAR Domain   The    rst datasets we deal with are datasets in the car domain  We insert 101 di   erent tables from di   erent sources into a wide table to simulate the scenario where di   erent users share their data  Among the 101 tables  one is obtained from Google Base  and has 24 thousand tuples and 15 columns  while the others are downloaded from Many Eyes 7   These tables have di   erent sizes  with number of tuples ranging from dozens to thousands  and the number of columns ranging from 5 to 31  In total  there are 94 854 rows  284 distinct tags and 850 215 distinct values after combining the data into one wide table  2  Directory Datasets  DIR Domain   We extract three directory datasets from three websites that provide directory services 8   The directory data sets consist of information about companies  such as company name  address  telephone  fax  email and so on  In total  there are 139 022 tuples  31 distinct tags and 495 908 distinct values  5 2 Tag Inference Validation We verify the e   ectiveness of our tag inference approach    rst on real datasets  and compare it with the matching results from Open II  We then further validate it over the semi real Google Base car datasets  5 2 1 Validation over Real Datasets To evaluate our tag inference approach  we    rst compare it with Open II  26  based on the golden standard  Open II is an open source data integration toolkit that has implemented several schema matching methods 9   In this experiment  for Open II  we choose all the matching methods it implemented to produce its matching result  These matchers include    Name Similarity Matcher        Documentation Matcher       Mapping Matcher        Exact Matcher       Quick Matcher    and    WordNet Matcher     The golden standard is identi   ed by manually scanning through all the columns in each domain  In car domain  totally there are 160 semantically equivalent tag pairs in the golden standard  Both the output format of Open II and our system are a list of tag pairs  each associated with a similarity score  For car domain  Figure 6 shows the precision for the top k similar tag pairs in Open II and our approach  with k varying from 20 to 200  The result clearly demonstrates that our approach achieves better precision than Open II when we are looking at the same number of top k similar tag pairs  The reason is that we de     ne the tag semantic similarity based on soft clustering and 7 http   manyeyes alphaworks ibm com manyeyes  8 They are thegreenbook com  streetdirectory com and yellowpages com sg respectively  9 Note that Open II is a schema level mapper while our approach is an instance based approach  23   As such  Open II takes only one second for all its attribute mappings while our approach takes on average 2 5 hours to perform high dimensional clustering  These operations are however preprocessing which do not a   ect the query answering e   ciency of our approach  0  0 2  0 4  0 6  0 8  1  20 40 60 80 100 120 140 160 180 200 Precision Top k similar tag pairs OpenII Ptagging Figure 6  Comparison with Open II KL divergence  rather than directly over attributes  Hence  we are able to discover similar tags that have di   erent syntaxes  such as make and manufacturer  while they cannot be found by Open II  At the same time  Open II makes many wrong predictions on tag pairs which are similar in names but di   erent in semantics  Examples include fuel type and body type  model and model year etc  Figure 6 also indicates that our precision decreases when k increases  This is because there are some semantically di   erent tags whose value distributions are so similar that our approach cannot tell them apart  such tags include price and mileage in car domain  telephone and fax in directory domain  For such tag pairs  it might not even be possible for human being to distinguish them if no extra information is provided  The result in directory domain is similar  due to limited space  we do not list it here  5 2 2 Validation over Semi real Datasets We further validate our tag inference by arti   cially introducing semantically similar tags  The large Google Base  which contains 15 di   erent columns and about 2 million records  is pre processed as follows 10   For each attribute  we create 9 di   erent attributes from the original one  with shared pre   x  For example  we create another 9 attributes make a  make b         make i from attribute make  These attribute names are then used to simulate the scenario where di   erent tags with same semantics come from heterogeneous data sources  In addition  with the observation that there are many small datasets but few large ones  we take this into account in our data generation  The 10 semantically similar tags are divided into four groups with di   erent probabilities  The    rst group contains 4 attributes  make  make a  make b and make c  each with probability 5   the second group contains 3 attributes  make d  make e and make f  each with probability 10   the third group has only two attributes  make g and make h  each having probability 15   the last group  with the sole attribute make i  is assigned with probability 20   We then assign every value to a tag according to this probability distribution and with an associated probability of 1  In the experiment  we consider each attribute as a tag  and run our soft clustering 3 times with di   erent initializations on a 2  sample     40 thousand data values   Figure 7 shows the pairwise similarity between tags  Each row and column represents a tag  The similarity between the row and column tag at a given pixel is related to the darkness 10We used the entire Google Base data in data generation  but only 10  samples in tag inference to avoid dominance  body type color condition doors drivetrain engine fuel type hull material make mileage model price transmission vehicle type year Figure 7  Tag Similarity Bitmap of the pixel  The darker the square is  more similar the tag pairs are  Note that the tags are arranged in the same order in both row and column orientations  and tags generated from the same original tags are grouped together  From Figure 7 we can see that the squares on the diagonal are much darker than the other squares  This coincides with the fact that tags generated from the same attribute are much more similar with each other than the other tags  However  the only outlier is the fourth last attribute price and the sixth last attribute mileage  Their diagonal intersections only give a moderate grey scale  This is because mileage and price share very similar value distributions  as explained earlier  5 3 Top k Query Processing Validation We evaluate the performance of our top k query processing over both car domain datasets and directory domain datasets  For car domain data  we vary the number of distinct tags in a query from 1 to 5  and denote them as T 1 to T 5  For each T i  i      1          5   we choose 5 di   erent queries with approximately equal number of tags in SELECT clause and WHERE clause  When selecting queries  we randomly vary the selectivity for predicates in the WHERE clause  The process is similar for directory domain data  except that we vary the distinct number of tags from 2 to 4  as directory domain has less number of tags  To get the precision and recall for query result  we build the golden standard as follows  We manually check the extended SQL query to determine its semantics  then translate it to a set of traditional SQL queries over each source data set  and    nally merge the results retrieved by SQL queries  For instance  if tags in  make  manufacturer  g make  are semantically equivalent to each other  then an extended SQL query containing make will be broadcasted to each data source that includes any equivalent tag of make  On average  a tagbased query can be translated to 4 to 6 SQL queries  Based on golden standard  we de   ne precision and recall as follows  let P be the set of answers retrieved by the query processor  and GS be the set of answers in golden standard  then P recision    P   GS   P     and Recall    P   GS   GS    5 3 1 Effectiveness Figure 8 a  shows the precision over car datasets  with top k  varying from 20  to 100   For each k   it also illustrates the precision for each T i  with i  i e   the number of distinct tags  ranging from 1 to 5  Note that our top k  query is a little di   erent from the traditional top k query  In top k query  the number of returned answers is k  How 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Precision Top k    of Tag 1   of Tag 2   of Tag 3   of Tag 4   of Tag 5  a  Precision in CAR domain 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Recall Top k    of Tag 1   of Tag 2   of Tag 3   of Tag 4   of Tag 5  b  Recall in CAR domain 0 0 2 0 4 0 6 0 8 1  20 40 60 80 100 Precision Top k  OpenII Ptagging  c  Precision Comparison 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Precision Top k    of Tag 2   of Tag 3   of Tag 4  d  Precision in DIR domain 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Recall Top k    of Tag 2   of Tag 3   of Tag 4  e  Recall in DIR domain  0  0 2  0 4  0 6  0 8  1  20 40 60 80 100 Recall Top k  OpenII Ptagging  f  Recall Comparison Figure 8  Precision and Recall by Varying k  ever  in top k  query it is  GS      k   The reason we choose top k  query is that we would like to see how recall changes with di   erent number of distinct tags  As golden standard size for di   erent queries may di   er a lot  setting the same absolute k for all queries will make recall bias the performance towards queries with small answer set  Top k  query avoids this problem since no matter how much the golden standard di   ers  recall for top k  query is upper bounded by k   Figure 8 a  shows that our approach achieves high precision in all the cases  and is not sensitive to k   It also indicates that for T 1  precision increases slightly with larger k  while T 2 has a slight decrement  The reason is that our tag inference approach is unable to distinguish tag pairs with similar value distributions  e g  price and mileage   Such false positives are ranked higher in T 1 than in T 2  However  this problem can be eliminated by our dynamic instantiation when price and mileage are queried together  as our query processor always attempts to    nd the best instantiation  Figure 8 b  shows the corresponding recall for car domain datasets  We can see that when k  increases  there is a steady increase for recall  As mentioned before  recall in k  case is upper bounded by k   However  since we missed some true similar tag pairs in tag inference  we cannot grantee 100  recall  Such missed tags are those which are semantically similar but have heterogeneous values  For example  it is di   cult to associate year which has format    yyyy    with model year which has format    yy     This problem will become severer if there are more tags in query  because the possibility of including missed tags also increases  as shown in case T 5  However  for queries containing less distinct tags  our approach is still able to achieve good recall  Consider the    rst 4 cases T 1 to T 4  at the point where k  equals to 100   our approach can achieve 0 7 recall in average  Together with the high precision we provide  our proposed approach is quite e   ective  We also get precision and recall on the directory data  shown in Figure 8 d  and Figure 8 e  respectively  As there are less number of tags than the car dataset  we vary its distinct number of tags from 2 to 4  We observe similar behavior as over car datasets  Since the number of tags in the directory datasets is less  and most are well formatted  its recall is much better than the car datasets  Finally  we compare our work with Open II for the query processing  Comparisons over precision and recall are shown in Figure 8 c  and Figure 8 f   respectively  In particular  we only list the results for T 3 case over car datasets  The reason is that Open II makes many wrong inferences  as such  precision in most cases is really low  Here we choose its best case  i e  T 3  for comparison  It is indicated that our approach provides better precision and recall than Open II  Speci   cally  among the 5 queries in T 3  Open II produces 0  precision for two of them because they contain wrong matched tags  and 100  precision for the rest three  In average  Open II has 60  precision across all k  cases  With less correct records returned  Open II has lower recall than ours  5 3 2 Ef   ciency We validate the e   ciency of our top k query processing over T 3 on car datasets  For the directory data sets and the other queries  the overall trend is similar  We    rst compare the running time of our approaches with the translated SQL queries  as shown in Figure 9  Note that when k  is set to 100   our approaches retrieve almost the same number of tuples as the translated SQLs  As illustrated  the running time of the translated SQL queries is quite stable with respect to di   erent top k   because they always retrieve the golden standard  i e  all the true answers   no matter how k  changes  In contrast  the running time of our approaches  i e   lazy retrieval and eager retrieval  increases linearly when top k  increases  However  eager retrieval takes longer time than lazy retrieval by almost 1 5 times since eager retrieval maintains and probes the sorted lists for QTags  but most tuples in them are invalid  The running time that lazy retrieval takes is almost k  of translated SQLs  which clearly shows the e   ciency of our lazy retrieval approach  We next look at how the number of random and sequential 0  0 04  0 08  0 12  0 16  0 2  20 40 60 80 100 Running Time sec Top k  SQL Eager Retrieval Lazy Retrieval Figure 9  Running Time by Varying k  accesses vary as we vary k  Figure 10 a  shows the number of random accesses for eager and lazy retrieval when varying k from 20 to 120  The corresponding graphs for sequential I Os are in Figure 10 b   Note that in both    gures  we adopt top k rather than top k   as the number of tuples returned by top k  varies proportionally to query   s answer set  Consistent with earlier observation that eager retrieval takes almost 1 5 times longer than lazy retrieval  the same conclusion holds for both the number of sequential accesses and random accesses  6  RELATED WORK Query Relaxation In the recent decade  there are several e   orts to provide users a more friendly query interface that requires minimal or no user knowledge of the database schema  Such e   ort includes the extensive studies about keyword searches over RDBMS  19  31   However  keyword query con   nes users from issuing structural constraints  and it is not applicable for range queries  Moreover  it does not allow users to express semantic con   dence over tags  In  15  21   it was illustrated that tags on multimedia objects are prevalent over Web 2 0 application  1  2   to facilitate searching over multimedia objects  Our use of tags here serves the same purpose  Database Relaxation  There is also some e   ort to relax the strict schema de   ned in RDBMS  including researches on malleable schemas  11  33   Bigtable  6   Wide table  7   WebTable  5   and RDF tables  22   These approaches provide some    exibility for users to interact with the schema without the strict constraints of a RDBMS  However  most of them only provide a    exible interface to store data  but the columns or attributes in their system play the same role as in RDBMS  While  33  did look at issues involving query relaxation  they only do so for string attributes and have to generate many alternate relaxed queries in order to retrieve relevant data  Users are also not given the control over which attributes to relax  Probabilistic Database  There also exist a l</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>It was easy  when apples and blackberries were only fruits Surender ### Reddy Yerva  Zolt an Mikl os  and Karl Aberer EPFL IC LSIR Lausanne  Switzerland fsurenderreddy yerva  zoltan miklos  karl abererg epfl ch Abstract  Ambiguities in company names are omnipresent  This is not accidental  companies deliberately chose ambiguous brand names  as part of their marketing and branding strategy  This procedure leads to new challenges  when it comes to  nding information about the company on the Web  This paper is concerned with the task of classifying Twitter messages  whether they are related to a given company  for example  we classify a set of twitter messages containing a keyword apple  whether a message is related to the company Apple Inc  Our technique is essen  tially an SVM classi er  which uses a simple representation of relevant and irrelevant information in the form of keywords  grouped in speci c  pro les   We developed a simple technique to construct such classi ers for previously unseen companies  where no training set is available  by training the meta features of the classi er with the help of a general test set  Our techniques show high accuracy  gures over the WePS 3 dataset ### 1 Introduction Twitter 1 is a popular service where users can share short messages  a k a  tweets  on any subject  Twitter is currently one of the most popular sites of the Web  as of February 2010  Twitter users send 50 million messages per day 2   As users are sharing information on what matters to them  analyzing twitter messages can reveal important social phenomena  indeed there are number of recent works  for example in  11   exploring such information  Clearly  twitter messages are also a rich source for companies  to study the opinions about their products  To perform sentiment analysis or obtain reputation related information  one needs  rst to identify the messages which are related to a given company  This is a challenging task on its own as company or product names are often homonyms  This is not accidental  companies deliberately choose such names as part of their branding and marketing strategy  For example  the company Apple Inc  shares its name with the fruit apple  which again could have a number of  gurative meanings depending on the context  for example   knowledge   Biblical story of Adam  Eve and the serpent  or New York  the Big Apple   1 http   twitter com 2 http   www telegraph co uk technology twitter 7297541  Twitter users send 50 million tweets per day htmlIn this paper  we focus on how to relate tweets to a company  in the context of the WePS 3 challenge  where we are given a set of companies and for each com  pany a set of tweets  which might or might not be related to the company  i e  the tweets contain the company name  as a keyword   Constructing such a classi er is a challenging task  as tweet messages are very short  maximum 140 charac  ters   thus they contain very little information  and additionally  tweet messages use a speci c language  often with incorrect grammar and speci c abbreviations  which are hard to interpret by a computer  To overcome this problem  we con  structed pro les for each company  which contain more rich information  For each company  in fact  we constructed several pro les  some of them automati  cally  some of them manually  The pro les are essentially sets of keywords  which are related to the company in some way  We also created pro les  which explic  itly contains unrelated keywords  Our technique is essentially an SVM classi er  which uses this simple representation of relevant and irrelevant information in the  pro les   We developed a simple technique to construct such classi ers for previously unseen companies  where no training set is available  by training the meta features of the classi er with the help of a general test set  available in WePS 3  Our techniques show high accuracy  gures over the WePS 3 dataset  The rest of the paper is organized as follows  Section 2 gives a more precise problem de nition  Section 3 presents our techniques  while Section 4 gives more details on the classi cation techniques we used  Section 5 gives details on the experimental evaluation of our methods  Section 6 summarizes related work and  nally Section 7 concludes the paper  2 Problem Statement In this section we formulate the problem and our computational framework more formally  The task is concerned to classify a set of Twitter messages    fT1          Tng  whether they are related to a given company C  We assume that each message Ti 2  contains the company name as a sub string  We say that the message Ti is related to the company C  related Ti   C   if and only if the Twitter message refers to the company  It can be that a message refers both to the company and also to some other meaning of the company name  or to some other company with the same name   but whenever the message Ti refers to company C we try to classify as TRUE otherwise as FALSE  The task has some other inputs  such as the URL of the company url C   the language of the webpage  as well as the correct classi cation for a small number of messages  for some of the companies   3 Information representation The tweet messages and company names alone contain very little information to realize the classi cation task with good accuracy  To overcome this problem  we created pro les for the companies  several pro les for each company  These set of pro les can be seen as a model for the company  In this section  we discuss  how we represent tweet messages and companies and we also discuss how we obtained these pro les  In the the classi cation task we eventually compare a tweet against the pro les representing the company  see Section 4   3 1 Tweet Representation We represented a tweet as a bag of words  unigrams and bigrams   We do not access the tweet messages directly in our classi cation algorithm  but apply a preprocessing step  rst  which removes all the stop words  emoticons  and twitter speci c stop words  such as  for example  RT  username   We store a stemmed 3 version of keywords  unigrams and bigrams   i e  Ti   setfwrdjg  3 2 Company Representation We represent each company as a collection of pro les  formally Ek   fP k 1   P k 2           P k n g  Each pro le is a set of weighted keywords i e  P k i   fwrdj   wtjg  with wtj   0 for positive evidence and wtj   0 for negative evidence  For the tweets classi cation task  we eventually compare the tweet with the entity  i e  company  pro le  For better classi cation results  the entity pro le should have a good overlap with the tweets  Unfortunately  we do not know the tweet messages in advance  so we tried to create such pro les from alternative sources  independently of the tweet messages  The entity pro le should not be too general  because it would result many false positives in the classi cation and also not too narrow  because then we could miss potential relevant tweets  We generated most of our pro les automatically  i e  if one would like to construct a classi er for a previously unseen company  one can automatically generate the pro les  Further  small  manually constructed pro les could further improve the accuracy of the classi cation  as we explain in Section 5  In the following we give an overview of the pro les we used  and their con  struction  Homepage Pro le For each company name  the company homepage URL was provided in the WePS 3 data  To construct the homepage pro le  we crawled all the relevant links up to a depth of level d  2   starting from the given homepage URL  We extracted all the keywords present on the relevant pages  then we removed all the stopwords   nally we stored in the pro le the stemmed version of these keywords  From this construction pro  cess one would expect that homepage pro le should capture all the impor  tant keywords related to the company  However  since the construction is 3 Porter stemmer from python based natural language toolkit available at http   www nltk organ automated process  it was not always possible to capture good quality representation of the company  for various reasons  the company Webpages use java scripts   ash  some company pages contain irrelevant links  there are non standard homepages etc  Metadata Pro le HTML standards provides few meta tags 4   which enables a webpage to list set of keywords that one could associate with the webpage  We collect all such meta keywords in this pro le whenever they are present  If these meta keywords are present in the HTML code  they have high quality  the meta keywords are highly relevant for the company  On the negative side  only a fraction of webpages have this information available  Category Pro le The category  to which the company belongs  is a good source of relevant information of the company entity  The general terms associated with the category would be a rich representation of the entity  One usually fails to  nd this kind of keywords in the homepage pro le  We make use of wordnet  a network of words  to  nd all the terms linked to the category keywords  This kind of pro le helps us assign keywords like  software install  update  virus  version  hardware  program  bugs etc to a software company  GoogleSet CommonKnowledge Pro le GoogleSet is a good source of ob  taining  common knowledge  about the company  We make use of Google  Sets 5 to get words closely related to the company name  This helps us identify companies similar to the company under consideration  we get to know the products  competitor names etc  This kind of information is very useful  es  pecially for twitter streams  as many tweets compare companies with others  With this kind of pro le  we could for example associate Mozilla  Firefox  Internet Explorer  Safari keywords to Opera Browser entity  UserFeedback Positive Pro le The user himself enters the keywords which he feels are relevant to the company  that we store in the manually con  structed UserFeedback pro le  In case of companies where sample ground truth is available  we can infer the keywords from the tweets  in the training set  belonging to the company  UserFeedback Negative Pro le The knowledge of the common entities with which the current company entity could be confused  would be a rich source of information  using which one could classify tweets e ciently  The common knowledge that  apple  keyword related to  Apple Inc  company could be interpreted possibly as the fruit  or the New York city etc  This particular pro le helps us to collect all the keywords associated with other entities with similar keyword  An automated way of collecting this information would be very helpful  but it is di cult  For now we make use of few sources as an initial step to collect this information  The user himself provides us with this information  Second  the wiki disambiguation pages 6 contains this information  at least for some entities  Finally this information could be 4 http     www w3schools com html html meta asp 5 http   labs google com sets 6 http   en wikipedia org wiki Apple  disambiguation  page contains apple entitiesgathered in a dynamic way i e   using the keywords in all the tweets  that do not belong to the company  This information could also be obtained if we have training set for a particular company with tweets that do not belong to the company entity  Table 1 shows how an  Apple Inc  7 company entity is represented using di erent pro les  Table 1  Apple Inc Company Pro les Pro le Type Keywords WebPage iphone  ipod  mac  safari  ios  iphoto  iwork  leopard  forum  items  em  ployees  itunes  credit  portable  secure  unix  auditing  forums  mar  keters  browse  dominicana  music  recommend  preview  type  tell  no  tif  phone  purchase  manuals  updates   fa  8GB  16GB  32GB       HTML Metatag femptyg Category opera  code  brainchild  movie  telecom  cruncher  trade  cathode ray  paper  freight  keyboard  dbm  merchandise  disk  language  micropro  cessor  move  web  monitor  diskett  show   gure  instrument  board  lade  digit  good  shipment  food  cpu  moving picture   uid  con  sign  contraband  electronic  volume  peripherals  crt  resolve  yield  server  micro  magazine  dreck  byproduct  spiritualist  telecommunica  tions  manage  commodity   ick  vehicle  set  creation  procedure  con  sequence  second  design  result  mobile  home  processor  spin o   wan  der  analog  transmission  cargo  expert  record  database  tube  pay  load  state  estimate  intersect  internet  print  factory  contrast  out  come  machine  deliver  e ect  job  output  release  turnout  convert  river       GoogleSet itunes  intel  belkin  512mb  sony  hp  canon  powerpc  mac  apple  iphone  ati  microsoft  ibm       User Positive ipad  imac  iphone  ipod  itouch  itv  iad  itunes  keynote  safari  leop  ard  tiger  iwork  android  droid  phone  app  appstore  mac  macintosh User Negative fruit  tree  eat  bite  juice  pineapple  strawberry  drink 4 Classi cation Task In machine learning literature  the learning tasks could be broadly classi ed as supervised and unsupervised learning  The problem scenario for the WePS 3 task  classi cation of tweets with respect to a company entity can be seen as a problem where one needs a machine learning technique between supervised and unsupervised learning  since we have no training set for the actual classi cation task  but a test training set is provided for a separate set of companies  Here we brie y discuss the di erent classes of machine learning techniques  and outline our classi cation method  7 http   www apple comSupervised Learning for Classi cation Task Supervised learning is a ma  chine learning technique for deducing a function from training data  The training data consist of pairs of input objects  typically vectors   and desired outputs  The output of the function can predict a class label of the input object  called classi cation   The task of the supervised learner is to predict the value of the function for any valid input object after having seen a number of training exam  ples  i e  pairs of input and target output   To achieve this  the learner has to generalize from the presented data to unseen situations in a  reasonable  way  An example of supervised learning in our current setting is  given a training set of tweets for a particular company XYZ company   with example of tweets belonging to and not belonging to the company  one learns a classi er for this particular company XYZ company   Using this classi er the new unseen tweets related to this company XYZ company  can be classi ed as belonging or not belonging to that company  Unsupervised Learning In machine learning  unsupervised learning is a class of problems in which one seeks to determine how the data are organized  Many methods employed here are based on data mining methods used to preprocess data  It is distinguished from supervised learning in that the learner is given only unlabeled examples  In broad sense  the task of classifying tweets of an unknown company  without seeing any relevant examples can fall into this category  Generic Learning For the current scenario  WePS 3   challenge 2   we are pro  vided with training sets corresponding to few companies  C T R    Finally we have to classify test sets corresponding to new companies C T est    with C T R T C T est   0  This particular scenario can be seen as in between supervised and unsuper  vised learning  It is unsupervised as we are not given any labeled tweets cor  responding to the test set  At the same time it is also related to supervised learning as we have access to few training sets  with labeled tweets correspond  ing to the companies  This kind of generic learning needs the classi er to identify the generic features from the general training set  based on which one can make accurate classi cation of tweets corresponding to the unseen companies  The classi ers based on the features of the tweet decides if it belongs to a company or not  In the following section 4 1  we discuss the features which our classi ers take as input  After the features are introduced  we propose di erent ways of developing a generic classi er in section 4 2 4 1 Features Extraction We de ne a feature extraction function  which compares a tweet Ti to the com  pany entity representation Ek and outputs a vector of features  F n Ti   Ek    f metaf eatures z      G1          Gm   F1          Fn    z   tweetspecif ic   heuristics z      U1          Uzg  Here the Gi are generic meta features  which are entirely based on the quality of the entity pro les and do not depend on Tweet message Ti   One could use di erent ways of quantifying the quality of the pro les    Boolean  In this work we make use of boolean metrics to represent if a pro le is empty or has su cient keywords    Other possibility is that a human can inspect the pro les and assign a metric of x 2  0 1  based on the perceived quality  One could think of exploring an automated way of assigning this number  The Fi features are tweet speci c features  i e  they quantify how close a tweet overlaps with the entity pro les  We use a comparison function to compare the tweet message Ti   which is a bag of words  with j th pro le P k j   which is also a bag of weighted keywords  to get the F th j feature  In this work we make use of a simple comparison function  which compares two bags of words looking for exact overlap of keywords  and for all such keywords the sum of their weights quantify how close the tweet message is to the entity pro le  Formally with Ti   Setfw t 1   w t 2           w t k g and P k j   Setfw p 1   wt1  w p 2   wt2          w p m   wtmg  we compute the Fj feature using the simple comparison function as  Fj   CmpF n Ti   P k j     X q wtq  where q such that w p q 2 Setfw t 1   w t 2           w t kg   Setfw p 1   w p 2           w p mg  1  The above comparison function is simple and easy to realize  but it may miss out some semantically equivalent words  One could make use of cosine similarity  or semantic similarity based comparison functions  The Ui features encapsulate some user based rules  for example  presence of the company URL domain in the tweet URL list  is a big enough evidence to classify the tweet as belonging to the company  4 2 Generic Classi er The classi er is a function which takes the feature vector as input and classi es the tweet as fT RUE  F ALSEg  with TRUE label if the tweet is related to the company and as FALSE otherwise  We are provided with training data corre  sponding to a set of companies  C T R    Based on the training data we have the task of training a generic classi er  which should be used to classify the tweets corresponding to a new set of companies  C T est    We present here two possible ways of designing this generic classi er  Ensemble of Naive Bayes Classi ers  We adapt the Naive Bayes Classi er model for this task  For each company in the training set C T R    based on the company tweets we  nd the conditional distribution of values over features for two classes i e  a class of tweets which are related to the company and another class of tweets which are not related to the company  With these conditionalprobabilities  shown in equations 2 3  and by applying Bayes theorem  we can classify an unseen tweet whether it is related to the company or not  Let us denote the probability distribution of features of the tweets that are related to a given company with P f1  f2          fn j C    2  and the probability distribution of features of the tweets that are not related to the company with P f1  f2          fn j C    3  Then  for an unseen tweet t  using the features extraction function we com  pute the features values  f1  f2          fn   The posterior probabilities of whether the tweet is related to the company or not  are calculated as in equations  4  5   P C j t    P C    P t j C  P t    P C    P f1  f2          fn j C  P f1  f2          fn   4  P C j t    P C    P t j C  P t    P C    P f1  f2          fn j C  P f1  f2          fn   5  Depending on whether P C j t  is greater than P C j t  or not  the naive Bayes classi er decides whether the tweet t is related to the given company or not  respectively  Corresponding to each company ci 2 C T R   we train a naive Bayes clas  si er 12   15   NBCi   for which the input features are tweet speci c features F1          Fn and heuristics based features U1          Uz  as discussed in the section 4 1  Along with training a naive Bayes classi er  we also assign an accuracy measure for this classi er and keep a note of meta features G1          Gm of this classi er  The generic classi er makes use of ensemble function which either chooses the best classi er or combines the decision of classi ers from this set  to classify an unseen tweet corresponding to a new company i e  ci 2 C T est   The ensemble function would make use of the meta features and accuracy measures to pick up the right classi er or the right combination of classi ers  We refer to  9   21  for details about the design of such ensemble functions  SVM Classi er  Alternatively one could train a single classi er based on all the features  meta features  tweet speci c features and heuristics features  This single classi er can be seen as using an ensemble function implicitly in either picking an apt classi er or aptly combing the classi er decisions  In the current work  we train an SVM Classi er  10   16  as a generic classi er  which makes use of all features  meta features  tweet speci c features and heuristics based features  in its classi cation task 5 Experiments and Evaluation Our experimental setup was the following  We are given a general training set  which consists tweets related to about 50 companies  we denote this set as C T R    For each company c 2 C T R we are provided around 400 tweets with their cor  responding ground truth  i e  if the tweet is related to the company or not  For each company  we are provided with the following meta information  URL  Language  Category  We have trained a generic classi er based on this train  ing set  The test set for this task consisted tweets of around 50 new compa  nies  We denote this set of companies as C T est   There was no overlap with the training set  C T R T C T est   0  For each company c 2 C T est there are about 400 Tweets  which are to be classi ed  We classi ed them with our trained generic classi er  as explained in Section 4  The WePS 3 dataset is available at http   nlp uned es weps weps 3 data  The task is of classifying the tweets into two classes  one class which repre  sents the tweets related to the company  positive class  and second class repre  sents tweets that are not related to the company  negative class   For evaluation of the task  the tweets can be grouped into four categories  true positives  T P   true negatives  T N   false positives  F P  and false negatives  F N   The true positives are the tweets that belong to positive class and in fact belong to the company and the other tweets which are wrongly put in this class are false posi  tives  Similarly for the negative class we have true negatives which are correctly put into this class and the wrong ones of this class are false negatives  We use the following metrics to study the performance of our classi cation process  Accuracy   T P T N T P F P T N F N P recsion       T P T P F P   Recall     T P T P F N   F  Measure     2 P recsion    Recall   P recsion  Recall  P recsion    T N T N F N   Recall    T N T N F P   F  Measure    2 P recsion   Recall  P recsion Recall In Table 2 we show the average values of the di erent performance metrics  along with the corresponding variances  Table 2  Performance of Classi er which makes use of all pro les Metric  Mean Value Variance Accuracy 0 83 0 02 Precision  positive class  0 71 0 07 Recall  positive class  0 74 0 13 F Measure  positive class  0 63 0 1 Precision  negative class  0 84 0 07 Recall  negative class  0 52 0 17 F Measure  negative class  0 56 0 15 The results show high accuracy  gures for our classi er  The precision and recall values corresponding to positive class can be further increased by re ning         the pro les corresponding to positive evidence  for example by using more sources to accumulate more relevant keywords and by using e cient quality metrics for rejecting irrelevant keywords  In spite of using very few sources for populating the negative pro le of a company  we are still able to have high precision and decent recall values for </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>Eliminating the Redundancy in Blocking based Entity Resolution Methods ### George Papadakis       Ekaterini Ioannou    Claudia Nieder  e    Themis Palpanas z   and Wolfgang Nejdl     National Technical University of Athens  Greece gpapadis mail ntua gr   Technical University of Crete  Greece ioannou softnet tuc gr   L3S Research Center  Germany  surname  L3S de z University of Trento  Italy themis disi unitn eu ABSTRACT Entity resolution is the task of identifying entities that refer to the same real world object  It has important applications in the context of digital libraries  such as citation matching and author disambiguation  Blocking is an established methodology for e ciently addressing this problem  it clusters similar entities together  and compares solely entities inside each cluster  In order to e ectively deal with the current large  noisy and heterogeneous data collections  novel blocking methods that rely on redundancy have been introduced  they associate each entity with multiple blocks in order to increase recall  thus increasing the computational cost  as well  In this paper  we introduce novel techniques that remove the super   uous comparisons from any redundancy based blocking method  They improve the time e ciency of the latter without any impact on the end result  We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity  For applications with space limitations  we also present an alternative  lightweight solution that operates at the abstract level of blocks in order to discard a signi   cant part of the redundant comparisons  We evaluate our techniques on two large  real world data sets and verify the signi   cant improvements they convey when integrated into existing blocking methods  Categories and Subject Descriptors H 3 3  Information Search and Retrieval   Information    ltering General Terms Algorithms  Experimentation  Performance Keywords Data Cleaning  Entity Resolution  Redundancy based Blocking ### 1  INTRODUCTION Nowadays  the growing availability of semi structured and structured data in the Web of Data opens new opportunities for digital libraries  These data collections can clearly pro   t from a variety of digital library principles and technologies  such as the systematic Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  JCDL   11  June 13   17  2011  Ottawa  Ontario  Canada  Copyright 2011 ACM 978 1 4503 0744 4 11 06     10 00  and uniform description of data by metadata  metadata harvesting services and technologies for federated search  Furthermore  they can be exploited to create new types of services by combining them with traditional types of library content  The integration of related data in meaningful ways relies on the detection of data records  from di erent collections  that refer to the same object  e g   author  The process of identifying  among a set of entities  those referring to the same real world object is called Entity Resolution  ER   There are two main applications of this process in digital libraries  citation matching for identifying references that describe the same publication  and author disambiguation for identifying author pro     les that pertain to the same person  9  10  25  27   The latter consists of detecting   within a collection of bibliographical records   the correct coupling between author names and persons  by resolving the mixed citation problem  same name   di erent persons  and the split citation problem  same person   di erent names   14   At its core  ER constitutes a quadratic problem  as each entity has to be compared with all others  To enhance its e ciency  blocking methods are typically employed  6  11  15   they extract from every entity pro   le  or record  a Blocking Key Value  BKV  that encapsulates its most distinguishing information and de   ne blocks on the equality  or similarity  of BKVs  Thus  each block corresponds to a speci   c instance of the BKV and contains all entities associated with that value  However  in order to select the most reliable and distinguishing attributes of the given entity pro   les  traditional blocking methods rely on a prede   ned entity schema  This renders them inapplicable for the Web of Data  due to the special characteristics of the latter  it involves individual collections that are highly heterogeneous  stemming from a rich diversity of sources  which evolve autonomously  following an unprecedented growth rate  especially the user generated data of the Social Web  and the data created by sensors   More speci   cally  the following challenges are present in these settings  Loose schema binding  The schemata describing entities may range from locally de   ned attributes to pure tag style annotations  and data often have no strict binding to the employed schemata  Noisy  missing  and inconsistent values  They are introduced in the data due to extraction errors  sources of low quality  and use of alternative descriptions  As a result  entity pro   les may contain de   cient  or even false information  Extreme levels of heterogeneity  This is caused by the fact that data stem from a variety of distributed  self organized  collaborative sources  Actually  heterogeneity pertains not only to schemata describing the same entity types  but also to pro   les describing the same entity  For instance  GoogleBase 1 encompasses 100  000 distinct schemata corresponding to 10  000 entity types  16   High growth rates in terms of volume and fast evolution  This is caused partly due to automatic generation and partly due to the high involvement of users  they typically add new content  and update incorrect  outdated  or simply irrelevant information  These inherent characteristics of heterogeneous information spaces break the fundamental assumptions of traditional blocking techniques  Novel blocking schemes  that do not require a prede     ned schema  have been introduced to e ectively deal with these challenges  They all rely on redundancy  associating each entity with multiple blocks  3  20  21  28   In this way  they minimize the likelihood that two duplicate entities have no block in common  and achieve high levels of e ectiveness  i e   detected duplicate entities   This comes  though  at the cost of time e ciency  the resulting blocks are overlapping  and the same pairs of entities may be compared multiple times  Therefore  the main challenge for improving the e ciency of redundancy based blocking methods is to eliminate the super   uous comparisons they entail  without a ecting their accuracy  In this paper  we address the above problem through an abstraction of the redundancy based blocking techniques  blocks are associated with an index indicating their position in the processing list and entities are associated with a list of the indices of the blocks that contain it  Thus  we can identify whether a pair of entities contained in the current block has already been compared in another block simply by comparing their least common block index with the index of the current block  In this way  we achieve the optimal solution to the problem  since we e ciently propagate all executed comparisons  without explicitly storing them  The above approach has low computational cost  but results in quadratic space complexity  In order to remedy this drawback  we introduce a method that approximates the optimal solution by gracefully trading space for computational cost  It comprises of a series of block manipulation techniques  which discard those blocks that exclusively entail super   uous comparisons  i e   they are entirely contained in another block   and merge pairs of highly overlapping blocks  giving birth to blocks that entail less comparisons  These functionalities are facilitated by mapping the blocks to a Cartesian space and contrasting their spatial representations  without the need to examine their contents analytically  In summary  the contributions of this paper are the following   1  We formulate the problem of purging redundant comparisons from a blocking technique and explain how its solution can be facilitated through the abstraction of blocks  i e   enumerating and mapping them to the Cartesian space    2  We describe Comparisons Propagation  an optimal solution to this problem  which e ciently propagates all executed comparisons based on the enumeration of blocks  This method is suitable for applications that can a ord high space complexity   3  We further propose a solution that partially discards redundant comparisons  trading space requirements for time complexity  It consists of a series of methods that remove blocks involving exclusively redundant comparisons and merge highly overlapping ones   4  Finally  we thoroughly evaluate our methods on two large  realworld data sets  demonstrating the great bene   ts they convey to the e ciency of existing blocking methods  The rest of the paper is structured as follows  Section 2 summarizes previous work and Section 3 de   nes the basic notions of our algorithms  Section 4 introduces our approach to determining the processing order of blocks and mapping them to the Cartesian 1 See http   www google com base  Space  and Section 5 presents our approach to propagating comparisons and manipulating blocks  Experimental evaluation is presented in Section 6  while Section 7 concludes the paper  2  RELATED WORK A variety of methods for solving the ER problem has been presented in literature  They range from string similarity metrics  2   to similarity methods using transformations  19  26  and relationships between data  5  12   A comprehensive overview of the existing work in this domain can be found in  4  6  13   The approximate methods of data blocking typically associate each record  i e   entity  with a Blocking Key Value  BKV   They de   ne blocks on the equality  or similarity  of BKVs and compare solely the entities that are contained in the same block  6   For instance  the Sorted Neighborhood approach  11  orders records according to their BKV and slides a window of    xed size over them  comparing the records it contains  The StringMap method  15  maps the BKV of each record to a multi dimensional Euclidean space  and employs suitable data structures for e ciently identifying pairs of similar records  The q grams blocking approach  8  builds overlapping clusters of records that share at least one qgram  i e   sub string of length q  of their BKV  Canopy clustering  17  employs a cheap string similarity metric for building high dimensional overlapping blocks  whereas the Su x Arrays approach  3  considers the su xes of the BKV instead   28  explores another aspect of these blocking approaches  arguing that more duplicates can be detected and more pair wise comparisons can be saved through the iterative distribution of identi   ed matches to subsequently  re  processed blocks  The performance of blocking methods typically depends on the    ne tuning of a wealth of application  and data speci   c parameters  3  29   To automate the parameter setting procedure  several methods that model it as a machine learning problem have been proposed in the literature  For instance   18  de   nes it as learning disjunctive sets of conjunctions that consist of an attribute  used for blocking  and a method  used for comparing the corresponding values   Similarly   1  considers disjunctions of blocking predicates  i e   conjunctions of attributes and methods  along with predicates combined in disjunctive normal form  DNF   On the other hand   29  introduces a method for adaptively and dynamically setting the size of the sliding window of the Sorted Neighborhood approach  Attribute agnostic blocking methods were recently introduced to make blocking applicable to voluminous  heterogeneous data collections  such as the Web of Data  These methods do not need a prede   ned schema for grouping entities into blocks  as they completely disregard attribute names  In this way  they are able to handle thousands of attribute names without requiring the    ne tuning of numerous parameters  Instead  they tokenize  on all special characters  the attribute values of each entity pro   le  and create an individual block for each token  that is  every block corresponds to a speci   c token and contains all entities having this token in their pro   le  21   Blocks of such low level granularity guarantee high e ectiveness due to the high redundancy they convey  each entity is associated with multiple blocks  which are  thus  overlapping  Hence  the likelihood of a missed match  i e   a pair of duplicates that has no block in common  is low  This redundancy based approach is a common practice among blocking techniques for noisy  but homogeneous data  as well  3  17  20  28   To the best of our knowledge  this is the    rst work on formally de   ning and dealing with the problem of eliminating redundant comparisons of blocking methods for ER 3  PROBLEM DEFINITION To formally describe the problem we are tackling in this paper  we adopt the de   nitions introduced in  21  for modeling entity pro     les and entity collections  As such  an entity pro   le p is a tuple hid  Api  where Ap is a set of attributes ai   and id 2 ID is a global identi   er for the pro   le  Each attribute ai 2 Ap is a tuple hni   vii  consisting of an attribute name ni and an attribute value vi   Each attribute value can also be an identi   er  which allows for modeling relationships between entities  An entity collection E is a tuple hAE  VE  IDE  PEi  where AE is the set of attribute names appearing in it  VE is the set of values used in it  IDE   ID is the set of global identi   ers contained in it  and PE  IDE is the set of entity pro   les that it comprises  We de   ne a blocking scheme as follows  Definition 1  A blocking scheme bs for an entity collection E is de   ned by a transformation function f t   E 7  T and a set of constraint functions f i cond   T   T 7  ftrue  f alseg  The transformation function ft derives the appropriate blocking representation from the complete entity pro   le  or parts of it   The constraint function f i cond is a transitive and symmetric function that encapsulates the condition that has to be satis   ed by two entities  if they are to be placed in the same block bi   Apparently  any blocking method can de   ne and use its own blocking scheme that follows the above de   nition  For example  the schemes described in Section 2 consist of a transformation function that extracts the BKV from an entity pro   le and a set of constraint functions that de   ne blocks on the equality  or similarity  of the BKVs  Once a blocking scheme is applied on an entity collection  a set of blocks is derived  whose instances are formally de   ned as follows  Definition 2  Given an entity collection E and a blocking scheme bs for E  a block bi 2 B is the maximal subset  with a minimum cardinality of 2  that is de   ned by the transformation function f t and one of the constraint functions f i cond of bs  bi   E   8p1  p2 2 E   f i cond  f t p1   f t p2     true   p1  p2 2 bi   The ER process on top of a blocking method consists of iterating over its set of blocks B in order to compare the entities contained in each one of them  We use mi j to denote a match between two pro   les pi and pj that have been identi   ed as matching pi   pj  i e   describing the same real world object   The output  therefore  of a blocking method is a set of matches  which we denote as M  To address the aforementioned characteristics of heterogeneous information spaces  redundancy bearing blocking methods have been recently introduced  3  20  21  28   They associate each entity with multiple  overlapping blocks  This practice minimizes the likelihood that two duplicate entities have no block in common  thus resulting in higher e ectiveness for the ER process  Ef     ciency  on the other hand  is signi   cantly downgraded  due to the redundant comparisons between pairs of entities that appear in many blocks  Apparently  the higher the redundancy conveyed by a blocking method  the lower the e ciency of the ER process  In this paper  we focus on developing methods that enhance the e ciency of redundancy based blocking methods  without a ecting their e ectiveness  To this end  our techniques aim at eliminating the super   uous comparisons of redundancy bearing blocking methods in order to save considerable computational e ort  In this way  they can operate on top of any blocking method  without altering its e ectiveness  producing an output that is equivalent to the original one  The following de   nition introduces the concept of semantically equivalent blocking sets  Definition 3  A blocking set B 0 is semantically equivalent to blocking set B  if the set of matches resulting from blocking set B 0 are equal to the set of matches resulting from blocking set B  i e   MB0 MB   Based on the above de   nition  we now formally state the problem we are addressing in this paper  Problem 1  Given a set of blocks B that are derived from a redundancy bearing blocking technique     nd the semantically equivalent blocking set B 0 that involves no redundant pair wise comparisons  4  BLOCK SCHEDULING AND MAPPING As stated above  our goal is to propose generic methods for enhancing the e ciency of any redundancy bearing blocking technique  such as the ones discussed in Section 2   Therefore  the methods we describe make no assumptions on the mechanism or functionality of the underlying blocking method  Instead  they treat blocks at an abstract level  considering solely the identi   ers of the entities they contain  i e   each block is represented as a set of entity ids   We distinguish between two types of blocks according to the lineage of their entities  The    rst type of blocks is called unilateral  since it contains entities of the same lineage  i e   stemming from the same entity collection  This type of blocks arises when integrating one dirty collection  i e   a collection that contains duplicate entities  either with a clean  duplicate free collection  i e   DirtyClean   or with another dirty collection  i e   Dirty Dirty   Both cases are equivalent with resolving a single  dirty entity collection  where each entity pro   le could match to any other  24   More formally  the blocks of this kind are de   ned as follows  Definition 4  A unilateral block is a block containing entity ids from a single entity collection E  thus being of the form bi   fid1  id2          idng  where idi 2 ID  The second type of blocks is called bilateral  and arises when integrating two individually clean entity collections  E1 and E2  that are overlapping  Clean Clean   24   The goal is  therefore  to identify matches only between E1 and E2  thus requiring that each block contains entities from both input collections  More formally  this kind of blocks is de   ned as follows  Definition 5  A bilateral block is a block containing entity ids from two entity collections  E1 and E2  It follows the form bi  j   ffidi 1  idi 2         idi ng fidj 1  idj 2         idj mgg  where idi k 2 ID1 and idj l 2 ID2  The subsets bi   fidi 1  idi 2         idi ng and bj   fidj 1  idj 2          idj mg are called the inner blocks of bi  j   4 1 Block Scheduling Specifying the processing order of blocks is important for the e ectiveness of ER techniques  This order forms the basis for block enumeration  which associates each block with an integer that represents its position in the processing list  This practice    nds application in various techniques  such as the propagation of comparisons  see Section 5 1   The processing order is also an integral part of lossy e ciency techniques  like Block Pruning in  21   these are methods that sacri   ce  to some extent  the e ectiveness of a blocking method in order to enhance its e ciency  They do so by discarding comparisons according to some criteria  even if they involve non redundant comparisons among matching entities  Scheduling techniques typically associate each element of the given set of blocks B with a numerical value and sort B in ascending or descending order of this value  Their computational cost a  0 1 2 3 4 5 b1 b b2 3  b  0 1 2 3 4 5 1 2 3 4 b1 1 b2 2 b3 3 Figure 1  Illustration of block mapping  is O jBj   log jBj   which scales even for large sets of blocks  In each case  the most suitable approach for determining the processing order of blocks depends heavily on the application at hand  For the needs of the methods we introduce in Section 5  we de   ne a di erent scheduling method for each kind of block  In particular  unilateral blocks are ordered in ascending order of cardinality  the more entities a block bi contains  the higher its position in the list  Bilateral blocks  on the other hand  are ordered in ascending order of their utility  21   ubi  j   1 max jbi j jb j j    where jbi j and jbj j are the cardinalities of the inner blocks of the bilateral block bi  j   Bilateral blocks of equal utility are ordered in ascending order of the cardinality of their smallest inner block  4 2 Block Mapping We now introduce our approach for Block Mapping  The gist of this technique is that it allows us to e ciently check whether two blocks have overlapping content  i e   they share some entities   without exhaustively comparing them  The mapping is performed by transforming blocks into the Cartesian space  for unilateral blocks this corresponds to Cartesian coordinates in one dimension  i e   lines   and for bilateral blocks to coordinates in two dimensions  i e   rectangles   Thus  Block Mapping is performed by assigning each entity to a point on the corresponding axis  Example 1  Figure 1 a  illustrates the mapping of the unilateral blocks b1   fid2  id3  id4g  b2   fid0  id1  id4g  and b3   fid0  id1  id3  id4g on the X axis  Their entities are assigned to coordinates as follows  C  hid0  3i  hid1  4i  hid2  0i  hid3  1i  hid4  2i   Figure 1 b  illustrates the mapping of the bilateral blocks b1 1   ffid1 0  id1 2g  fid2 0  id2 1gg  b2 2   ffid1 0  id1 3g  fid2 1  id2 3gg and b3 3   ffid1 0  id1 3  id1 4g  fid2 1  id2 2  id2 4gg to the XY axes  where id1 i 2 E1 and id2 i 2 E2  The entities of E1 are transformed to points on the X axis as follows  CX  hid1 0  3i hid1 1  4i  hid1 2 1i  hid1 3  5i  hid1 4  2i   whereas the entities of E2 are mapped to points on the Y axis as follows  CY  hid2 0  3i hid2 1  1i hid2 2 0i  hid2 3  2i  hid2 4  4i   The di erence between the size b sp i of the spatial representation of a block bi and its actual size b as i is called spatial deviation spi of block bi   More formally  it is de   ned as follows  spi   b sp i  b as i   Algorithm 1  Mapping Blocks to the Cartesian Space  Input  B fbig a set of unilateral blocks Output  C fhidi   jig a mapping of entity ids to coordinates 1 B 0   blockScheduling B   2 C       3 lastIndex   0  4 foreach bi 2 B 0 do 5 E   sortInAscendingOrderOfFrequency bi  entities     6 foreach e 2 E do 7 if   C containsKey e id   then 8 C   C    he id  lastIndexi   9 lastIndex    10 return C  where b sp i is the length  area  of the spatial representation of a unilateral  bilateral  block bi   and b as i is the actual length  area  of the unilateral  bilateral  block bi   In the case of unilateral blocks  we have b as i   jbi j  1  while for a bilateral block bi  j it is equal to b as i  j    jbi j  1     jbj j  1   For example  b2 2 has an actual area of   2 1   2 1   1  whereas its spatial representation has an area of   5 3   2 1   2  that is  sp2 2   1  The value of spi is always positive  but in the ideal case it should be equal to 0  This requirement can be easily satis   ed for nonoverlapping blocks  by associating the entities of each block with contiguous coordinates  In the case of overlapping blocks  though  the spatial transformation leads to a positive deviation  since it cannot be done independently for each block  assigning a coordinate to an entity idi in the context of a block bi can be contiguous with the rest of entities in bi   but not necessarily with the other entities that share blocks with idi   Example 2  Consider the entities in Figure 1 a   The way the depicted blocks are mapped is the optimal one  since the entities of every block are contiguous  Imagine  though  that we place an additional entity to each block  id5 to b1  id6 to b2 and id7 to b3  In this case  there is no way of mapping the new blocks to the X axis  so that the entities of each block are contiguous  The above discussion gives rise to the following optimization problem  Problem 2  Given a set of blocks B  transform its elements to the Cartesian space  so that their aggregated spatial deviation P bi2B  b sp i  b as i   is minimized  In our methods  we require that emphasis is placed on minimizing the spatial deviation of large blocks  we elaborate on the reasons in Section 5 2 2   That is  the larger a block is  the lower its spatial deviation should be  More formally  this optimization problem can be de   ned as the minimization of the following quantity  X bi2B bi  size      b sp i  b as i     1  where bi  size   is the size of block bi   For a unilateral block  it is equal to its cardinality  i e   number of entities it contains   while for a bilateral block bi  j it is equal to the sum of cardinalities of its inner blocks  bi  j  size     jbi j   jbj j  We solve this optimization problem using a scalable method  applicable to voluminous data collections  Algorithm 1 outlines this method for the case of unilateral blocks  for bilateral ones  the algorithm is applied twice  independently for each axis  considering in each iteration solely the corresponding entity collection  In essence  the algorithm assigns coordinates from the interval  0  jEj  1  to the entity pro   les of the given collection E        After Block Scheduling  it starts assigning the entities of the last  usually largest  block to contiguous coordinates  thus minimizing the spatial deviation of this block  To ensure the minimal spatial deviation for the rest of the blocks  as well  the pro   les are ordered and mapped in ascending order of their frequency  i e   the number of blocks associated with each entity   the least frequent of the not yet mapped entities takes the    rst available coordinate  the second least frequent takes the next coordinate etc  Two entities that share many blocks are more likely to be contiguous in this way  The algorithm is then repeated for the remaining blocks  traversing their ordered list from bottom to top  The algorithm has a linear space complexity  O jBj   jEj   and time complexity of O jBj   log jBj   jE   log jEj   due to the sorting of blocks and entities  Example 3  The result of applying this algorithm is illustrated in Figure 1 a   The pro   les of b1 are mapped to the X axis as follows  id2 has frequency 0 and goes to the    rst available coordinate  i e   0   id3 with frequency 1 goes to next available coordinate  i e   1    and     nally  id4 with frequency 2 goes to point 2  5  APPROACH In this section  we present the methods we developed for reducing the redundancy of blocking methods  Problem 1   based on the Block Scheduling and Mapping techniques we introduced above  The optimal solution to this problem  i e   the one that discards all redundant comparisons  is presented in Section 5 1  Its e ectiveness  though  comes at the cost of high space complexity  caused by the data structure it employs  As an alternative  we present in Section 5 2 an approximate solution  that removes the high space requirements  5 1 Comparisons Propagation Block Scheduling determines the processing order of blocks  and enables their enumeration  that is  each block is assigned to an index indicating its position in the processing list  Based on this enumeration  the propagation of comparisons is made feasible through a common data structure  namely a hash table  in particular  its keys are the ids of the entities of a given collection E  and its values are lists of the indices of the blocks that contain the corresponding entities  The elements of these lists are sorted in ascending order  from the lowest block index to the highest  This data structure can be used in the context of a blocking method in the following way  to compare a pair of entities  the Least Common Block Index Condition should be satis   ed  That is  the lowest common block index of these entities should be equal to the index of the current block  indicating in this way that this is the    rst block in the processing list that contains both of them  Otherwise  if the former index is lower than the latter  the entities have already been compared in another block  and the comparison is redundant  In this way  each pair of entities is compared just once  and Comparisons Propagation provides the optimal solution to Problem 1  Theorem 1  Optimality of Comparisons Propagation   Given a set of blocks B  Comparisons Propagation produces the semantically equivalent set of blocks B 0 that entails no redundant pair wise comparisons  Proof  Let us assume that the set of blocks produced by Comparisons Propagation entails redundant comparisons  This means that there is a blocking set B 00 that is semantically equal to B and involves no redundant comparisons  Hence  there must be at least one pair of entities that is compared twice in B 0 and just once in B 00   The Least Common Block Index Condition is  therefore  satis   ed in two blocks of B 0   which is a contradiction  Thus  B 00   B 0   Algorithm 2  Propagating Comparisons  Input  B a set of blocks Output  B 00 the semantically equivalent set of blocks with no redundant comparisons 1 B   blockScheduling B   2 B 0   blockEnumeration B   3 B 00       4 entityIndex   indexBlocksOnEntityIds B 0    5 foreach bi 2 B 0 do 6 E   bi  entities    7 for i   1 to E size do 8 BEi   entityIndex associatedBlocks E i    9 for j   i   1 to E size do 10 BEj   entityIndex associatedBlocks E j    11 if  bi  index   leastCommonBlockIndex BEi  BEj    then 12 bi   newBlock E i    E j    13 B 00   B 00 S bi   14 return B 00   Algorithm 2 outlines the way Comparisons Propagation operates on a set of blocks B in order to produce its semantically equivalent set of blocks B 0 that is free of redundant comparisons  In essence  B 0 consists of blocks with minimum cardinality  since each nonredundant comparison results in a new block that contains the corresponding pair of entities  This may result in a very large number of blocks  and storing them poses a serious challenge  Processing them on the    y  though  is an e cient alternative  Comparisons Propagation can be integrated in the execution of any blocking method  without a ecting its time complexity  The reason is that the computation of the least common block index is linear to the number of blocks associated with the corresponding pair of entities  due to the ordering of indices in the values of the hash table   Its space complexity  though  is equal to O jBj   jEj   in the worst case  i e   each entity is placed in all blocks   where jBj is the total number of blocks  and jEj is the cardinality of the given entity collection  for Clean Clean ER  this cardinality is equal to jE1j   jE2j   In practice  however  space complexity depends on the level of redundancy introduced by the underlying blocking method  In fact  it is equal to O jEj   BPE      where BPE   is an estimate of redundancy  denoting the average number of blocks per entity  5 2 Block Manipulation Block Manipulation consists of a series of techniques that operate on two levels     rst  they investigate the given set of blocks in order to discard those elements that contain purely redundant comparisons  In this way  they reduce not only the number of comparisons  but also the number of blocks that will be processed in the next level  Second  they aim at identifying pro   table block merges  that is  pairs of highly overlapping blocks  which  when combined  result in a block with fewer comparisons  The combined result of these two levels approximates the optimal solution of Comparisons Propagation at a lower space complexity  The individual strategies of Block Manipulation are analytically presented in the following paragraphs  in the order they should be executed  5 2 1 Block Cleaning Cleaning a set of blocks B is the process of purging the duplicate elements from it  These are blocks that contain exactly the same entities with another block  regardless of the constraint function de   ning each of them  i e   independently of the information that is associated with them   We call such blocks identical  and  depending on their lineage  we formally de   ne them as follows Algorithm 3  Mining a clean set of blocks  Input  B a clean set of highly similar blocks Output  B 00 the semantically equivalent  mined set of blocks 1 B 0   blockScheduling B   2 DominatedB       3 for i   1 to B 0  size do 4 for j   B 0  size to i   1 do 5 if  B 0  i  size     B 0  j  size     then 6 break  7 if  areaConditionHolds B 0  i   B 0  j   then 8 if  isDominated B 0  i   B 0  j   then 9 DominatedB   DominatedB   B 0  i   10 break  11 B 00   B 0   DominatedBlocks  12 return B 00   Definition 6  Given a set of unilateral blocks B  a block bi 2 B is unilaterally identical with another block bj 2 B  denoted by bi   bj   if both blocks contain the same entities  regardless of their constraint functions  f i cond and f j cond   bi   bj   bi   bj   bj   bi   Definition 7  Given a set of bilateral blocks B  a block bi  j 2 B is bilaterally identical with another block bk l 2 B  denoted by bi j   bk l   if their corresponding inner blocks are unilaterally identical  bi  j   bk l   bi   bk   bj   bl   In this context  the process of Block Cleaning can be formally de   ned as follows  Problem 3  Block Cleaning   Given a set of blocks B  reduce B to its semantically equivalent subset B 0   B that contains no identical blocks  We call B 0 a clean set of blocks  The solution to this problem can be easily implemented by associating each block with a hash signature  its value is equal to the sum of the coordinates assigned to its entities by Block Mapping  Identical blocks necessarily have the same signature  but not vice versa  signature equality can also lead to false positives  For this reason  the size as well as the elements of two blocks with the same signature are analytically compared to make sure that they are indeed identical  In practice  this functionality is e ciently offered by default by most programming languages  Both its time and space complexity are linear to the size of the input set of blocks  i e   O jBj    as it involves traversing its elements just once  5 2 2 Block Mining Given a clean set of blocks  the process of mining it consists of identifying the blocks that are subsets of at least one other block in the set  that is  blocks whose entities are all contained in some other block  independently of the corresponding constraint functions  This situation is called a relation of dominance  where the latter is the dominant block  and the former the dominated one  This is more formally de   ned as follows  Definition 8  Given a clean set of unilateral blocks B  a block bi 2 B is unilaterally dominated by another block bj 2 B  denoted by bi   bj   if bi is a proper subset of bj   regardless of the constraint functions f i cond and f j cond   bi   bj   jbi j   jbj j    idi 2 bi   idi   bj   Definition 9  Given a clean set of bilateral blocks B  a block bi  j 2 B is bilaterally dominated by another block bk l 2 B  denoted by bi j   bk l   if at least one inner block of bi  j is unilaterally dominated by the corresponding inner block of bk l and the other is either unilaterally identical or unilaterally dominated  bi  j   bk l    jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j   jbj j   jbl j   In this context  the problem of Block Mining can be formally de   ned as follows  Problem 4  Block Mining   Given a clean set of block B  reduce B to its semantically equivalent subset B 0   B that contains no dominated blocks  We call B 0 a mined set of blocks  Apparently  this constitutes another quadratic problem  since the elements of every block have to be compared with those of all others  However  the abstract representation of blocks we are proposing leads to a series of necessary conditions that have to be satis   ed by a pair of blocks  if one of them is dominated  The bene   t is that these conditions can be checked in a fast and easy way  without the need to analytically compare the elements of the blocks  The conditions are also complementary  with their conjunction forming a composite mining method that e ectively restricts the required number of comparisons  In the following  we describe them in more detail   i  Size Condition  SC   In a clean set of blocks B  there cannot be a relation of dominance among a pair of equally sized blocks  Instead  the dominant block has to be larger in size than the dominated one  Therefore  to check whether a block is dominated  we need to compare it solely with blocks of larger size   ii  Area Condition  AC   Block mapping adds an additional condition for a relation of dominance  the spatial representation of the dominated block has to be fully contained in the representation of the dominant one  that is  the line  area  of the former lies entirely inside the line  area  mapped to the latter  More formally  the AC for a unilateral block bi to be dominated by a block bj is expressed as follows  bi   bj    min bj  entityCoods    min bi  entityCoods   max bi  entityCoods    max bj  entityCoods    where bk  entityCoods is the set of coordinates assigned to the entities of block bk   Similarly  the AC for a bilateral block bi  j to be dominated by a block bk l takes the following form  bi  j   bk l    min bk  entityCoods    min bi  entityCoods   max bi  entityCoods    max bk  entityCoods     min bl  entityCoods    min bj  entityCoods   max bj  entityCoods    max bl  entityCoods    AC is illustrated in Figure 1 a   b1 is smaller than b3  but cannot be dominated by it  since the line of b1 is not entirely covered by the line of b3  On the other hand  b2 satis   es the AC with respect to b3 but not with b1  As mentioned in Section 4 2  the priority of Algorithm 1 is to ensure that large blocks end up with low spatial deviation  The reason is that AC is intended to be used in conjunction with SC  In this way  the latter condition saves comparisons among equally sized blocks  even if their spatial representations are intersecting  while the former saves unnecessary comparisons that involve large blocks  Without AC  each block is inevitably compared with all smaller blocks  even if they share no entities   iii  Entity Condition  EC   Another  straightforward way of considering the content of blocks before comparing them is to considermerely those pairs of blocks that have at least one entity in common  This can be achieved by using the same data structure that is employed in Comparisons Propagation  a hash table that contains for each entity a list of the blocks that are associated with it  However  this solution has the same  quadratic  space complexity with Comparisons Propagation  thus violating the requirement for an alternative solution with minimal space requirements  Thus  we replace it with a near duplicates detection method  Locality Sensitive Hashing  LSH   7  constitutes an established method for hashing items of a high dimensional space in such a way that similar items  i e   near duplicates  are assigned to the same hash value with a high probability p1  In addition  dissimilar items are associated with the same hash values with a very low probability p2  In our case  blocks are the items that are represented in the high dimensional space of E  or E1 and E2  through Block Mapping  Thus  LSH can be employed to group highly similar blocks in buckets  so that it su ces it compare blocks contained in the same bucket  The details of the con   guration of LSH we employ are presented in Section 6  Algorithm 3 outlines the steps of our Block Mining algorithm  In short  it encompasses two nested loops  with SC and AC integrated in the inner one  Note that the input consists of the blocks contained in the same bucket of LSH  It is worth noting that the nested loop starts from the bottom of the ordered list of blocks and traverses it to the top  In this way  smaller blocks are    rst compared with the largest ones  The reason is that the larger the di erence in the size of the two blocks  the higher the likelihood that the larger block contains all the elements of the smaller one  thus  dominating it   SC is encapsulated in lines 5 and 6  while AC in line 7  Note that SC terminates the inner loop as soon as an equally sized block is encountered  because Block Scheduling ensures that the next blocks are of equal or smaller size  5 2 3 Block Merging An e ective way of discarding super   uous comparisons is to identify blocks that are highly overlapping  These are blocks that share so many entities that  if merged  they would result in fewer comparisons than the sum of the comparisons needed for each one  Merging such blocks eliminates their redundant comparisons  thus enhancing e ciency without any impact on e ectiveness  Indeed  pairs of entities that are common among the original blocks  i e   the source of redundancy  are considered only once in the new blocks  while the pairs of duplicate entities are maintained without any change  This situation is illustrated in the following example  Example 4  Consider the following unilateral blocks  b1   fid1  id2  id3  id4  id5g  b2   fid1  id2  id4  id5  id6g and b3   fid1  id3  id4  id5  id7  id8g  Individually  these blocks involve 35 comparisons  in total  However  b2 and b3 contain most of the comparisons in b1  they share four entities with b1   Merging b1 with b2 leads to the new block b 0 2   fid1  id2  id3  id4  id5  id6g  We now need 30 comparisons in total  between b 0 2 and b3   In addition  merging b 0 2 with b3 in a single block containing all 8 entities further reduces the total number of comparisons to 28  i e   20  less than the initial number of comparisons   More formally  the merge of two unilateral bilateral blocks is de   ned as follows  Definition 10  Given a set of unilateral blocks B  the unilateral merge of a block bi with a block bj is a new unilateral block bmi j that contains the union of the entities of bi and bj  bmi  j   bi   bj   Definition 11  Given a set of bilateral blocks B  the bilateral merge of a block bi  j with a block bk l is a new bilateral block Algorithm 4  Merging a mined set of blocks  Input  B a clean  mined set of blocks Output  B 00 the semantically equivalent  merged set of blocks 1 B 0   blockMapping B   2 FromMergesB       3 MergedB       4 OpenB       5 ProcessedB       6 for i   0 to spatialBlocks maxDimension do 7 NewOpenB   getBlocksStartingAtIndex i  B 0    8 NewOpenB   NewOpenB S FromMergesB  9 NewOpenB   NewOpenB n MergedB  10 NewOpenB   NewOpenB n OpenB  11 NewOpenB 0   blockScheduling NewOpenB   12 OpenB   blockScheduling OpenB S NewOpenB   13 FromMergesB       14 foreach bi 2 NewOpenB 0 do 15 mostS imilarBlock   getMostSimilarBlock bi   OpenB   16 if mostS imilarBlock   null then 17 newBlock   mergeBlocks bi   mostS imilarBlock   18 newBlock mapToCartesianSpace    FromMergesB add newBlock   19 MergedB add bi   20 MergedB add mostS imilarBlock   21 OpenB remove bi   22 OpenB remove mostS imilarBlock   23 OpenB addAll NewOpenB   24 EndingBlocks   getBlocksEndingAtIndex i  OpenB   25 OpenB removeAll EndingBlocks   26 ProcessedB addAll EndingBlocks   27 return ProcessedB  bmi k mj l   whose inner blocks constitute the unilateral merge of the corresponding inner blocks of bi  j and bk l  bmi k  mj l   fbi   bk   bj   blg  Typically  each block shares comparisons with many other blocks of the input set  However  these pairs of blocks di er in their degree of overlap  i e   the number of comparisons they share   In fact  the higher the Jaccard coe cient 2 of two blocks  the more comparisons their merge saves  Thus  to maximize the e ect of Block Merging  each block should be merged with the block that has the highest proportion of common entities with it  We call this block maximum Jaccard block  We can now formally de   ne the problem of Block Merging as follows  Problem 5  Block Merging   Given a mined set of blocks B  identify for each block bi its maximum Jaccard block bj   and merge these bi bj pairs  so as to produce a semantically equivalent set B 0 with a smaller number of redundant comparisons  We call B 0 a merged set of blocks  To address this problem  we introduce Algorithm 4  At its core lies the idea that each block should have overlapping spatial representations with its maximum Jaccard block  In other words  there is no point in estimating the Jaccard similarity between two blocks with disjoint spatial representations  Based on this principle  our algorithm works for unilateral blocks as follows  two main lists are maintained  the Open and the Processed ones  lines 4 and 5  respectively   The former contains the blocks that are available for comparison  whereas the latter encompasses the blocks that do not 2 The Jaccard similarity coe cient J A  B  of two sets A and B is equal to  J A  B    jA Bj jA Bj   a  0 1 2 3 4 5 b1 b b2 3 1 2 5 3 4 6  b  4 b3 3 5 5 4 6 1 2 3 b1 1 b2 2 2 3 4 0 1 2 3 4 5 1 Figure 2  Illustration of the block merge algorithm  need to be considered any more  Starting from the beginning of the X axis  the algorithm traverses the X axis by one point in every iteration  line 6   blocks  whose spatial representation starts from the current point  line 7   are compared with the blocks in the Open list  line 14   Then  at the completion of the iteration  they are added in it  together with the merged blocks  lines 23 and 8  respectively   Blocks  whose spatial representation ends at the current point  are placed in the Processed list  lines 24 26   The reason is that they do not overlap with any of the subsequently examined blocks  In the case of bilateral blocks  the only di erence in the execution of the algorithm is that it traverses both axes simultaneously  The following example illustrates the functionality of this algorithm  Example 5  Figure 2 a  is an illustration of the Block Merging algorithm for unilateral blocks that are mapped to the X axis  The main idea is that the grey area expands by one unit after each step  All blocks lying partly within it are in the OpenB list  while all blocks that lie entirely within it are placed in the ProcessedB list  At the highlighted Step 4  b2 and b3 lie in the former list  while b1 is placed in the latter  The execution of the algorithm is as follows  at Step 1  only b1 is in OpenB  while  at Step 2  b3 is also added and compared with b1  At Step 3  b1 is removed from OpenB  Then  b2 is placed in OpenB and compared with all other blocks  The remaining blocks  b2 and b3  are placed in ProcessedB at the end of Step 5  Figure 2 b  is an illustration of the block merge algorithm for bilateral blocks that are mapped to the XY space  Again  the grey area expands at each step by one unit  in both dimensions this time  All blocks lying partly within it are placed in the OpenB list  while all blocks that lie entirely inside its borders are moved to the ProcessedB list  In the depicted case  step 5   b1 1 lies in the ProcessedBlocks list  while b2 2 and b3 3 are in the OpenBlocks one  As mentioned in De   nition 5  the input to Block Merging is a mined set of blocks  that is  the input set contains neither identical nor dominated blocks  since they do not contribute non redundant comparisons  The computational cost of Algorithm 4 is thus significantly reduced  without a ecting its output  However  during the execution of Algorithm 4  blocks that belong to one of these categories can be produced  and should be discarded on the    y  Indeed  merges that lead to a block identical to another block of the input set are immediately removed  lines 9 10 Algorithm 4   Regarding new relations of dominance  merges can be involved in them only as dominant blocks  Otherwise  the original blocks that produce them would have already been dominated  More formally  bmi  j   bk    bi   bj    bk    bi   bk     bj   bk   Thus  only a block of the input set can be dominated by the merge of two other blocks  Apparently  it is not e cient to apply the Block Mining method after each iteration of Block Merging  Dominated blocks are  therefore  removed after the completion of the merging process  by comparing the original  non merged blocks with the set of merges  6  EVALUATION In this section we present a series of experiments that investigate the higher e ciency conveyed by our techniques  when incorporated into existing  redundancy bearing blocking methods  Our techniques were fully implemented in Java 1 6  and the experiments were performed on a server with Intel Xeon 3 0GHz  Metrics  Given that our techniques aim at eliminating redundant comparisons  we evaluate them on the basis of an established metric for measuring the e ciency of blocking methods  namely the Reduction Ratio  RR   1  3  18   It expresses the reduction in the number of pair wise comparisons required by a method with respect to the baseline one  Thus  it is de   ned as follows  RR   1  mc bc  where mc stands for the number of comparisons entailed by our technique  and bc expresses the number of comparisons entailed by the baseline  in our case  this is the original blocking method   RR takes values in the interval  0  1   for mc   bc   with higher values denoting higher e ciency  Recall  named Pair Completeness in the context of blocking  21   is not reported  since our techniques do not a ect the duplicates identi   ed by a blocking method   their goal is exclusively to detect and avoid the super   uous and repeated comparisons  Data Sets  To evaluate the impact of our techniques on existing blocking methods  we employ two real world data sets  one for the Clean Clean  and one for the Dirty Dirty and Dirty Clean cases of ER  They are DBPedia and the BTC09 data set  respectively  which are described in more detail in the following paragraphs  DBPedia Infobox Dataset  DBPedia   This data collection consists of two di erent versions of the DBPedia Infobox Data Set 3   They have been collected by extracting all name value pairs from the infoboxes of the articles in Wikipedia   s English version  at speci   c points in time  Although it may seem simple to resolve two versions of the same data set  this is not the case  More speci   cally  the older version  DBPedia1  is a snapshot of Wikipedia Infoboxes in October 2007  whereas the latest one  DBPedia2  dates from October 2009  During the two years that intervene between these two versions  Wikipedia Infoboxes were so heavily modi   ed that there is only a small overlap between their pro   les  even for duplicate entities  just 25  of all name value pairs are shared among the entities common in both versions  As matches  we consider those entities that have exactly the same URL in both versions  The attribute agnostic blocking method introduced in  21  was applied on this data set to produce the set of bilateral blocks we employ in our experiments  The blocks we consider are those resulting from the Block Purging step  21   involving 3 98 10 10 comparisons and PC   99 89   The technical characteristics of DBPedia are presented in Table 1  3 See http   wiki DBPedia org Datasets  DBPedia DBPedia1 Entities 1  190  734 DBPedia1 Name Value Pairs 17  453  516 DBPedia2 Entities 2  164  058 DBPedia2 Name Value Pairs 36  653  387 Bilateral Blocks 1  210  262 Comparisons 3 98   10 10 Av  Comparisons Per Block 32  893 Av  Blocks Per Entity 15 38 BTC09 Entities 1 82   10 8 Name Value Pairs 1 15   10 9 Unilateral Blocks 8 04   10 7 Comparisons 4 05   10 9 Av  Comparisons Per Block 50 37 Av  Blocks Per Entity 2 61 Table 1  Technical characteristics for both data sets  Billion Triple Challenge 2009  BTC09   This data set constitutes a publicly available 4   large snapshot of the Semantic Web  aggregating RDF statements from a variety of sources  In total  it comprises 182 million distinct entities  described by 1 15 billion triples  The ground truth set of duplicate entities is derived from the explicit as well as implicit equivalence relationships  i e   the owl sameAs statements and the Inverse Functional Properties  respectively   BTC09 was employed as a test bed for the attribute agnostic blocking method presented in  22   which is the source of our experimental set of unilateral blocks  well  Similar to DBPedia  the blocks we employ in our experiments are those stemming from the Block Purging method of  22   They entail 4 05   10 9 comparisons  exhibiting a PC very close to 90   A more detailed overview of this data set is presented in Table 1  6 1 Block Mapping In this section  we compare Algorithm 1 with the Random Mapping  RM  of blocks to the Cartesian space  As mentioned in Section 4 2  the higher the performance of a mapping method  the lower the sum of Formula 1 should be  Table 2 shows the outcome for both algorithms and for both data sets  In each case  we considered 100 iterations with RM in order to get a safe estimation of its performance  The sums in Table 2 is equal to the average value  It is remarkable that standard deviation is equal with  4 16 10 16 and  3 97   10 12 for DBPedia and BTC09  respectively  thus being 4 orders of magnitude lower than the mean value in both cases  This indicates that the performance of RM is relatively stable  We can see that Algorithm 1 substantially outperforms RM in both cases  for bilateral blocks  it improves RM by 30 34   whereas for unilateral blocks the enhancement is 25 28   The reason for the slightly lower improvement in the second case is twofold  First  redundancy is much higher in DBPedia than in BTC09  as documented in Table 1  in the former data set  an entity is associated with more than 15 blocks  in comparison with the less than 3 blocks for the latter  Thus  mapping an entity to a suboptimal  random  coordinate a ects the spatial deviation of more blocks in DBPedia than in BTC09  Second  suboptimal mappings have a larger impact in the two dimensional space than in the unidimensional one  Consider for example Figure 1  Assigning entity id4 to point 5 of the X axis instead of 4  increases the spatial deviation of b2 by 1  However  assigning entity id1 4 to point 5 of the Y axis instead of 4 increments the spatial deviation of b3 3 by 4  For these 4 See http   vmlion25 deri ie  Method DBPedia Sum BTC09 Sum Algorithm 1 0 81   10 20 1 33   10 16 Random Mapping 1 16   10 20 1 78   10 16 Table 2  Comparison of the Block Mapping technique with respect to the sum of Formula 1 for both data sets  Method DBPedia Comp  BTC09Comp  Cartesian Product 5 11   10 11 3 23   10 19 LSH   Algorithm 3 1 27   10 8 4 52   10 9 Table 3  Performance of the Block Mining algorithm  DBPedia BTC09 Method Comp  RR Comp  RR Input Set 3 98   10 10   4 01   10 9   Comp  Prop  2 59   10 9 93 49  3 08   10 9 23 19  Table 4  E ect of Comparisons Propagation on the e ciency of both data sets  reasons  the performance of RM is slightly closer to Algorithm 1 for unilateral blocks  6 2 Block Mining We now present the performance of our Block Mining algorithm  i e   LSH and Algorithm 3  with respect to the number of block comparisons it requires in order to examine the blocks inside each bucket  In general  to determine the structure of LSH  two parameters have to be speci   ed  L and k  the former denotes the total number of hash tables that will be employed  while the latter speci   es the number of hash functions that are merged to compose the hash signature for each entity  for a particular hash table  In our implementation  we followed  23  and used exclusive or for e   ciently merging the k hash functions into the composite signature  In more detail  L was set to 10  and k to 12  while the probabilities p1 and p2  see Section 5 2 2  were set to 0 9 and 0 1  respectively  Table 3 shows the performance of our method in comparison with the naive method of examining all possible pairs of blocks  The aim is not to examine the optimal con   guration of LSH  Rather  the main conclusion to be drawn from these numbers is that it is a scalable approach  whose performance depends on the level of redundancy of the underlying blocking method  the higher the redundancy  the more similar the blocks are between them  and the larger the corresponding buckets get  this results in more comparisons and lower e ciency  This explains why the performanc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>Eliminating the Redundancy in Blocking based Entity Resolution Methods ### George Papadakis       Ekaterini Ioannou    Claudia Nieder  e    Themis Palpanas z   and Wolfgang Nejdl     National Technical University of Athens  Greece gpapadis mail ntua gr   Technical University of Crete  Greece ioannou softnet tuc gr   L3S Research Center  Germany  surname  L3S de z University of Trento  Italy themis disi unitn eu ABSTRACT Entity resolution is the task of identifying entities that refer to the same real world object  It has important applications in the context of digital libraries  such as citation matching and author disambiguation  Blocking is an established methodology for e ciently addressing this problem  it clusters similar entities together  and compares solely entities inside each cluster  In order to e ectively deal with the current large  noisy and heterogeneous data collections  novel blocking methods that rely on redundancy have been introduced  they associate each entity with multiple blocks in order to increase recall  thus increasing the computational cost  as well  In this paper  we introduce novel techniques that remove the super   uous comparisons from any redundancy based blocking method  They improve the time e ciency of the latter without any impact on the end result  We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity  For applications with space limitations  we also present an alternative  lightweight solution that operates at the abstract level of blocks in order to discard a signi   cant part of the redundant comparisons  We evaluate our techniques on two large  real world data sets and verify the signi   cant improvements they convey when integrated into existing blocking methods  Categories and Subject Descriptors H 3 3  Information Search and Retrieval   Information    ltering General Terms Algorithms  Experimentation  Performance Keywords Data Cleaning  Entity Resolution  Redundancy based Blocking 1  INTRODUCTION Nowadays  the growing availability of semi structured and structured data in the Web of Data opens new opportunities for digital libraries  These data collections can clearly pro   t from a variety of digital library principles and technologies  such as the systematic ### Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  JCDL   11  June 13   17  2011  Ottawa  Ontario  Canada  Copyright 2011 ACM 978 1 4503 0744 4 11 06     10 00  and uniform description of data by metadata  metadata harvesting services and technologies for federated search  Furthermore  they can be exploited to create new types of services by combining them with traditional types of library content  The integration of related data in meaningful ways relies on the detection of data records  from di erent collections  that refer to the same object  e g   author  The process of identifying  among a set of entities  those referring to the same real world object is called Entity Resolution  ER   There are two main applications of this process in digital libraries  citation matching for identifying references that describe the same publication  and author disambiguation for identifying author pro     les that pertain to the same person  9  10  25  27   The latter consists of detecting   within a collection of bibliographical records   the correct coupling between author names and persons  by resolving the mixed citation problem  same name   di erent persons  and the split citation problem  same person   di erent names   14   At its core  ER constitutes a quadratic problem  as each entity has to be compared with all others  To enhance its e ciency  blocking methods are typically employed  6  11  15   they extract from every entity pro   le  or record  a Blocking Key Value  BKV  that encapsulates its most distinguishing information and de   ne blocks on the equality  or similarity  of BKVs  Thus  each block corresponds to a speci   c instance of the BKV and contains all entities associated with that value  However  in order to select the most reliable and distinguishing attributes of the given entity pro   les  traditional blocking methods rely on a prede   ned entity schema  This renders them inapplicable for the Web of Data  due to the special characteristics of the latter  it involves individual collections that are highly heterogeneous  stemming from a rich diversity of sources  which evolve autonomously  following an unprecedented growth rate  especially the user generated data of the Social Web  and the data created by sensors   More speci   cally  the following challenges are present in these settings  Loose schema binding  The schemata describing entities may range from locally de   ned attributes to pure tag style annotations  and data often have no strict binding to the employed schemata  Noisy  missing  and inconsistent values  They are introduced in the data due to extraction errors  sources of low quality  and use of alternative descriptions  As a result  entity pro   les may contain de   cient  or even false information  Extreme levels of heterogeneity  This is caused by the fact that data stem from a variety of distributed  self organized  collaborative sources  Actually  heterogeneity pertains not only to schemata describing the same entity types  but also to pro   les describing the same entity  For instance  GoogleBase 1 encompasses 100  000 distinct schemata corresponding to 10  000 entity types  16   High growth rates in terms of volume and fast evolution  This is caused partly due to automatic generation and partly due to the high involvement of users  they typically add new content  and update incorrect  outdated  or simply irrelevant information  These inherent characteristics of heterogeneous information spaces break the fundamental assumptions of traditional blocking techniques  Novel blocking schemes  that do not require a prede     ned schema  have been introduced to e ectively deal with these challenges  They all rely on redundancy  associating each entity with multiple blocks  3  20  21  28   In this way  they minimize the likelihood that two duplicate entities have no block in common  and achieve high levels of e ectiveness  i e   detected duplicate entities   This comes  though  at the cost of time e ciency  the resulting blocks are overlapping  and the same pairs of entities may be compared multiple times  Therefore  the main challenge for improving the e ciency of redundancy based blocking methods is to eliminate the super   uous comparisons they entail  without a ecting their accuracy  In this paper  we address the above problem through an abstraction of the redundancy based blocking techniques  blocks are associated with an index indicating their position in the processing list and entities are associated with a list of the indices of the blocks that contain it  Thus  we can identify whether a pair of entities contained in the current block has already been compared in another block simply by comparing their least common block index with the index of the current block  In this way  we achieve the optimal solution to the problem  since we e ciently propagate all executed comparisons  without explicitly storing them  The above approach has low computational cost  but results in quadratic space complexity  In order to remedy this drawback  we introduce a method that approximates the optimal solution by gracefully trading space for computational cost  It comprises of a series of block manipulation techniques  which discard those blocks that exclusively entail super   uous comparisons  i e   they are entirely contained in another block   and merge pairs of highly overlapping blocks  giving birth to blocks that entail less comparisons  These functionalities are facilitated by mapping the blocks to a Cartesian space and contrasting their spatial representations  without the need to examine their contents analytically  In summary  the contributions of this paper are the following   1  We formulate the problem of purging redundant comparisons from a blocking technique and explain how its solution can be facilitated through the abstraction of blocks  i e   enumerating and mapping them to the Cartesian space    2  We describe Comparisons Propagation  an optimal solution to this problem  which e ciently propagates all executed comparisons based on the enumeration of blocks  This method is suitable for applications that can a ord high space complexity   3  We further propose a solution that partially discards redundant comparisons  trading space requirements for time complexity  It consists of a series of methods that remove blocks involving exclusively redundant comparisons and merge highly overlapping ones   4  Finally  we thoroughly evaluate our methods on two large  realworld data sets  demonstrating the great bene   ts they convey to the e ciency of existing blocking methods  The rest of the paper is structured as follows  Section 2 summarizes previous work and Section 3 de   nes the basic notions of our algorithms  Section 4 introduces our approach to determining the processing order of blocks and mapping them to the Cartesian 1 See http   www google com base  Space  and Section 5 presents our approach to propagating comparisons and manipulating blocks  Experimental evaluation is presented in Section 6  while Section 7 concludes the paper  2  RELATED WORK A variety of methods for solving the ER problem has been presented in literature  They range from string similarity metrics  2   to similarity methods using transformations  19  26  and relationships between data  5  12   A comprehensive overview of the existing work in this domain can be found in  4  6  13   The approximate methods of data blocking typically associate each record  i e   entity  with a Blocking Key Value  BKV   They de   ne blocks on the equality  or similarity  of BKVs and compare solely the entities that are contained in the same block  6   For instance  the Sorted Neighborhood approach  11  orders records according to their BKV and slides a window of    xed size over them  comparing the records it contains  The StringMap method  15  maps the BKV of each record to a multi dimensional Euclidean space  and employs suitable data structures for e ciently identifying pairs of similar records  The q grams blocking approach  8  builds overlapping clusters of records that share at least one qgram  i e   sub string of length q  of their BKV  Canopy clustering  17  employs a cheap string similarity metric for building high dimensional overlapping blocks  whereas the Su x Arrays approach  3  considers the su xes of the BKV instead   28  explores another aspect of these blocking approaches  arguing that more duplicates can be detected and more pair wise comparisons can be saved through the iterative distribution of identi   ed matches to subsequently  re  processed blocks  The performance of blocking methods typically depends on the    ne tuning of a wealth of application  and data speci   c parameters  3  29   To automate the parameter setting procedure  several methods that model it as a machine learning problem have been proposed in the literature  For instance   18  de   nes it as learning disjunctive sets of conjunctions that consist of an attribute  used for blocking  and a method  used for comparing the corresponding values   Similarly   1  considers disjunctions of blocking predicates  i e   conjunctions of attributes and methods  along with predicates combined in disjunctive normal form  DNF   On the other hand   29  introduces a method for adaptively and dynamically setting the size of the sliding window of the Sorted Neighborhood approach  Attribute agnostic blocking methods were recently introduced to make blocking applicable to voluminous  heterogeneous data collections  such as the Web of Data  These methods do not need a prede   ned schema for grouping entities into blocks  as they completely disregard attribute names  In this way  they are able to handle thousands of attribute names without requiring the    ne tuning of numerous parameters  Instead  they tokenize  on all special characters  the attribute values of each entity pro   le  and create an individual block for each token  that is  every block corresponds to a speci   c token and contains all entities having this token in their pro   le  21   Blocks of such low level granularity guarantee high e ectiveness due to the high redundancy they convey  each entity is associated with multiple blocks  which are  thus  overlapping  Hence  the likelihood of a missed match  i e   a pair of duplicates that has no block in common  is low  This redundancy based approach is a common practice among blocking techniques for noisy  but homogeneous data  as well  3  17  20  28   To the best of our knowledge  this is the    rst work on formally de   ning and dealing with the problem of eliminating redundant comparisons of blocking methods for ER 3  PROBLEM DEFINITION To formally describe the problem we are tackling in this paper  we adopt the de   nitions introduced in  21  for modeling entity pro     les and entity collections  As such  an entity pro   le p is a tuple hid  Api  where Ap is a set of attributes ai   and id 2 ID is a global identi   er for the pro   le  Each attribute ai 2 Ap is a tuple hni   vii  consisting of an attribute name ni and an attribute value vi   Each attribute value can also be an identi   er  which allows for modeling relationships between entities  An entity collection E is a tuple hAE  VE  IDE  PEi  where AE is the set of attribute names appearing in it  VE is the set of values used in it  IDE   ID is the set of global identi   ers contained in it  and PE  IDE is the set of entity pro   les that it comprises  We de   ne a blocking scheme as follows  Definition 1  A blocking scheme bs for an entity collection E is de   ned by a transformation function f t   E 7  T and a set of constraint functions f i cond   T   T 7  ftrue  f alseg  The transformation function ft derives the appropriate blocking representation from the complete entity pro   le  or parts of it   The constraint function f i cond is a transitive and symmetric function that encapsulates the condition that has to be satis   ed by two entities  if they are to be placed in the same block bi   Apparently  any blocking method can de   ne and use its own blocking scheme that follows the above de   nition  For example  the schemes described in Section 2 consist of a transformation function that extracts the BKV from an entity pro   le and a set of constraint functions that de   ne blocks on the equality  or similarity  of the BKVs  Once a blocking scheme is applied on an entity collection  a set of blocks is derived  whose instances are formally de   ned as follows  Definition 2  Given an entity collection E and a blocking scheme bs for E  a block bi 2 B is the maximal subset  with a minimum cardinality of 2  that is de   ned by the transformation function f t and one of the constraint functions f i cond of bs  bi   E   8p1  p2 2 E   f i cond  f t p1   f t p2     true   p1  p2 2 bi   The ER process on top of a blocking method consists of iterating over its set of blocks B in order to compare the entities contained in each one of them  We use mi j to denote a match between two pro   les pi and pj that have been identi   ed as matching pi   pj  i e   describing the same real world object   The output  therefore  of a blocking method is a set of matches  which we denote as M  To address the aforementioned characteristics of heterogeneous information spaces  redundancy bearing blocking methods have been recently introduced  3  20  21  28   They associate each entity with multiple  overlapping blocks  This practice minimizes the likelihood that two duplicate entities have no block in common  thus resulting in higher e ectiveness for the ER process  Ef     ciency  on the other hand  is signi   cantly downgraded  due to the redundant comparisons between pairs of entities that appear in many blocks  Apparently  the higher the redundancy conveyed by a blocking method  the lower the e ciency of the ER process  In this paper  we focus on developing methods that enhance the e ciency of redundancy based blocking methods  without a ecting their e ectiveness  To this end  our techniques aim at eliminating the super   uous comparisons of redundancy bearing blocking methods in order to save considerable computational e ort  In this way  they can operate on top of any blocking method  without altering its e ectiveness  producing an output that is equivalent to the original one  The following de   nition introduces the concept of semantically equivalent blocking sets  Definition 3  A blocking set B 0 is semantically equivalent to blocking set B  if the set of matches resulting from blocking set B 0 are equal to the set of matches resulting from blocking set B  i e   MB0 MB   Based on the above de   nition  we now formally state the problem we are addressing in this paper  Problem 1  Given a set of blocks B that are derived from a redundancy bearing blocking technique     nd the semantically equivalent blocking set B 0 that involves no redundant pair wise comparisons  4  BLOCK SCHEDULING AND MAPPING As stated above  our goal is to propose generic methods for enhancing the e ciency of any redundancy bearing blocking technique  such as the ones discussed in Section 2   Therefore  the methods we describe make no assumptions on the mechanism or functionality of the underlying blocking method  Instead  they treat blocks at an abstract level  considering solely the identi   ers of the entities they contain  i e   each block is represented as a set of entity ids   We distinguish between two types of blocks according to the lineage of their entities  The    rst type of blocks is called unilateral  since it contains entities of the same lineage  i e   stemming from the same entity collection  This type of blocks arises when integrating one dirty collection  i e   a collection that contains duplicate entities  either with a clean  duplicate free collection  i e   DirtyClean   or with another dirty collection  i e   Dirty Dirty   Both cases are equivalent with resolving a single  dirty entity collection  where each entity pro   le could match to any other  24   More formally  the blocks of this kind are de   ned as follows  Definition 4  A unilateral block is a block containing entity ids from a single entity collection E  thus being of the form bi   fid1  id2          idng  where idi 2 ID  The second type of blocks is called bilateral  and arises when integrating two individually clean entity collections  E1 and E2  that are overlapping  Clean Clean   24   The goal is  therefore  to identify matches only between E1 and E2  thus requiring that each block contains entities from both input collections  More formally  this kind of blocks is de   ned as follows  Definition 5  A bilateral block is a block containing entity ids from two entity collections  E1 and E2  It follows the form bi  j   ffidi 1  idi 2         idi ng fidj 1  idj 2         idj mgg  where idi k 2 ID1 and idj l 2 ID2  The subsets bi   fidi 1  idi 2         idi ng and bj   fidj 1  idj 2          idj mg are called the inner blocks of bi  j   4 1 Block Scheduling Specifying the processing order of blocks is important for the e ectiveness of ER techniques  This order forms the basis for block enumeration  which associates each block with an integer that represents its position in the processing list  This practice    nds application in various techniques  such as the propagation of comparisons  see Section 5 1   The processing order is also an integral part of lossy e ciency techniques  like Block Pruning in  21   these are methods that sacri   ce  to some extent  the e ectiveness of a blocking method in order to enhance its e ciency  They do so by discarding comparisons according to some criteria  even if they involve non redundant comparisons among matching entities  Scheduling techniques typically associate each element of the given set of blocks B with a numerical value and sort B in ascending or descending order of this value  Their computational cost a  0 1 2 3 4 5 b1 b b2 3  b  0 1 2 3 4 5 1 2 3 4 b1 1 b2 2 b3 3 Figure 1  Illustration of block mapping  is O jBj   log jBj   which scales even for large sets of blocks  In each case  the most suitable approach for determining the processing order of blocks depends heavily on the application at hand  For the needs of the methods we introduce in Section 5  we de   ne a di erent scheduling method for each kind of block  In particular  unilateral blocks are ordered in ascending order of cardinality  the more entities a block bi contains  the higher its position in the list  Bilateral blocks  on the other hand  are ordered in ascending order of their utility  21   ubi  j   1 max jbi j jb j j    where jbi j and jbj j are the cardinalities of the inner blocks of the bilateral block bi  j   Bilateral blocks of equal utility are ordered in ascending order of the cardinality of their smallest inner block  4 2 Block Mapping We now introduce our approach for Block Mapping  The gist of this technique is that it allows us to e ciently check whether two blocks have overlapping content  i e   they share some entities   without exhaustively comparing them  The mapping is performed by transforming blocks into the Cartesian space  for unilateral blocks this corresponds to Cartesian coordinates in one dimension  i e   lines   and for bilateral blocks to coordinates in two dimensions  i e   rectangles   Thus  Block Mapping is performed by assigning each entity to a point on the corresponding axis  Example 1  Figure 1 a  illustrates the mapping of the unilateral blocks b1   fid2  id3  id4g  b2   fid0  id1  id4g  and b3   fid0  id1  id3  id4g on the X axis  Their entities are assigned to coordinates as follows  C  hid0  3i  hid1  4i  hid2  0i  hid3  1i  hid4  2i   Figure 1 b  illustrates the mapping of the bilateral blocks b1 1   ffid1 0  id1 2g  fid2 0  id2 1gg  b2 2   ffid1 0  id1 3g  fid2 1  id2 3gg and b3 3   ffid1 0  id1 3  id1 4g  fid2 1  id2 2  id2 4gg to the XY axes  where id1 i 2 E1 and id2 i 2 E2  The entities of E1 are transformed to points on the X axis as follows  CX  hid1 0  3i hid1 1  4i  hid1 2 1i  hid1 3  5i  hid1 4  2i   whereas the entities of E2 are mapped to points on the Y axis as follows  CY  hid2 0  3i hid2 1  1i hid2 2 0i  hid2 3  2i  hid2 4  4i   The di erence between the size b sp i of the spatial representation of a block bi and its actual size b as i is called spatial deviation spi of block bi   More formally  it is de   ned as follows  spi   b sp i  b as i   Algorithm 1  Mapping Blocks to the Cartesian Space  Input  B fbig a set of unilateral blocks Output  C fhidi   jig a mapping of entity ids to coordinates 1 B 0   blockScheduling B   2 C       3 lastIndex   0  4 foreach bi 2 B 0 do 5 E   sortInAscendingOrderOfFrequency bi  entities     6 foreach e 2 E do 7 if   C containsKey e id   then 8 C   C    he id  lastIndexi   9 lastIndex    10 return C  where b sp i is the length  area  of the spatial representation of a unilateral  bilateral  block bi   and b as i is the actual length  area  of the unilateral  bilateral  block bi   In the case of unilateral blocks  we have b as i   jbi j  1  while for a bilateral block bi  j it is equal to b as i  j    jbi j  1     jbj j  1   For example  b2 2 has an actual area of   2 1   2 1   1  whereas its spatial representation has an area of   5 3   2 1   2  that is  sp2 2   1  The value of spi is always positive  but in the ideal case it should be equal to 0  This requirement can be easily satis   ed for nonoverlapping blocks  by associating the entities of each block with contiguous coordinates  In the case of overlapping blocks  though  the spatial transformation leads to a positive deviation  since it cannot be done independently for each block  assigning a coordinate to an entity idi in the context of a block bi can be contiguous with the rest of entities in bi   but not necessarily with the other entities that share blocks with idi   Example 2  Consider the entities in Figure 1 a   The way the depicted blocks are mapped is the optimal one  since the entities of every block are contiguous  Imagine  though  that we place an additional entity to each block  id5 to b1  id6 to b2 and id7 to b3  In this case  there is no way of mapping the new blocks to the X axis  so that the entities of each block are contiguous  The above discussion gives rise to the following optimization problem  Problem 2  Given a set of blocks B  transform its elements to the Cartesian space  so that their aggregated spatial deviation P bi2B  b sp i  b as i   is minimized  In our methods  we require that emphasis is placed on minimizing the spatial deviation of large blocks  we elaborate on the reasons in Section 5 2 2   That is  the larger a block is  the lower its spatial deviation should be  More formally  this optimization problem can be de   ned as the minimization of the following quantity  X bi2B bi  size      b sp i  b as i     1  where bi  size   is the size of block bi   For a unilateral block  it is equal to its cardinality  i e   number of entities it contains   while for a bilateral block bi  j it is equal to the sum of cardinalities of its inner blocks  bi  j  size     jbi j   jbj j  We solve this optimization problem using a scalable method  applicable to voluminous data collections  Algorithm 1 outlines this method for the case of unilateral blocks  for bilateral ones  the algorithm is applied twice  independently for each axis  considering in each iteration solely the corresponding entity collection  In essence  the algorithm assigns coordinates from the interval  0  jEj  1  to the entity pro   les of the given collection E        After Block Scheduling  it starts assigning the entities of the last  usually largest  block to contiguous coordinates  thus minimizing the spatial deviation of this block  To ensure the minimal spatial deviation for the rest of the blocks  as well  the pro   les are ordered and mapped in ascending order of their frequency  i e   the number of blocks associated with each entity   the least frequent of the not yet mapped entities takes the    rst available coordinate  the second least frequent takes the next coordinate etc  Two entities that share many blocks are more likely to be contiguous in this way  The algorithm is then repeated for the remaining blocks  traversing their ordered list from bottom to top  The algorithm has a linear space complexity  O jBj   jEj   and time complexity of O jBj   log jBj   jE   log jEj   due to the sorting of blocks and entities  Example 3  The result of applying this algorithm is illustrated in Figure 1 a   The pro   les of b1 are mapped to the X axis as follows  id2 has frequency 0 and goes to the    rst available coordinate  i e   0   id3 with frequency 1 goes to next available coordinate  i e   1    and     nally  id4 with frequency 2 goes to point 2  5  APPROACH In this section  we present the methods we developed for reducing the redundancy of blocking methods  Problem 1   based on the Block Scheduling and Mapping techniques we introduced above  The optimal solution to this problem  i e   the one that discards all redundant comparisons  is presented in Section 5 1  Its e ectiveness  though  comes at the cost of high space complexity  caused by the data structure it employs  As an alternative  we present in Section 5 2 an approximate solution  that removes the high space requirements  5 1 Comparisons Propagation Block Scheduling determines the processing order of blocks  and enables their enumeration  that is  each block is assigned to an index indicating its position in the processing list  Based on this enumeration  the propagation of comparisons is made feasible through a common data structure  namely a hash table  in particular  its keys are the ids of the entities of a given collection E  and its values are lists of the indices of the blocks that contain the corresponding entities  The elements of these lists are sorted in ascending order  from the lowest block index to the highest  This data structure can be used in the context of a blocking method in the following way  to compare a pair of entities  the Least Common Block Index Condition should be satis   ed  That is  the lowest common block index of these entities should be equal to the index of the current block  indicating in this way that this is the    rst block in the processing list that contains both of them  Otherwise  if the former index is lower than the latter  the entities have already been compared in another block  and the comparison is redundant  In this way  each pair of entities is compared just once  and Comparisons Propagation provides the optimal solution to Problem 1  Theorem 1  Optimality of Comparisons Propagation   Given a set of blocks B  Comparisons Propagation produces the semantically equivalent set of blocks B 0 that entails no redundant pair wise comparisons  Proof  Let us assume that the set of blocks produced by Comparisons Propagation entails redundant comparisons  This means that there is a blocking set B 00 that is semantically equal to B and involves no redundant comparisons  Hence  there must be at least one pair of entities that is compared twice in B 0 and just once in B 00   The Least Common Block Index Condition is  therefore  satis   ed in two blocks of B 0   which is a contradiction  Thus  B 00   B 0   Algorithm 2  Propagating Comparisons  Input  B a set of blocks Output  B 00 the semantically equivalent set of blocks with no redundant comparisons 1 B   blockScheduling B   2 B 0   blockEnumeration B   3 B 00       4 entityIndex   indexBlocksOnEntityIds B 0    5 foreach bi 2 B 0 do 6 E   bi  entities    7 for i   1 to E size do 8 BEi   entityIndex associatedBlocks E i    9 for j   i   1 to E size do 10 BEj   entityIndex associatedBlocks E j    11 if  bi  index   leastCommonBlockIndex BEi  BEj    then 12 bi   newBlock E i    E j    13 B 00   B 00 S bi   14 return B 00   Algorithm 2 outlines the way Comparisons Propagation operates on a set of blocks B in order to produce its semantically equivalent set of blocks B 0 that is free of redundant comparisons  In essence  B 0 consists of blocks with minimum cardinality  since each nonredundant comparison results in a new block that contains the corresponding pair of entities  This may result in a very large number of blocks  and storing them poses a serious challenge  Processing them on the    y  though  is an e cient alternative  Comparisons Propagation can be integrated in the execution of any blocking method  without a ecting its time complexity  The reason is that the computation of the least common block index is linear to the number of blocks associated with the corresponding pair of entities  due to the ordering of indices in the values of the hash table   Its space complexity  though  is equal to O jBj   jEj   in the worst case  i e   each entity is placed in all blocks   where jBj is the total number of blocks  and jEj is the cardinality of the given entity collection  for Clean Clean ER  this cardinality is equal to jE1j   jE2j   In practice  however  space complexity depends on the level of redundancy introduced by the underlying blocking method  In fact  it is equal to O jEj   BPE      where BPE   is an estimate of redundancy  denoting the average number of blocks per entity  5 2 Block Manipulation Block Manipulation consists of a series of techniques that operate on two levels     rst  they investigate the given set of blocks in order to discard those elements that contain purely redundant comparisons  In this way  they reduce not only the number of comparisons  but also the number of blocks that will be processed in the next level  Second  they aim at identifying pro   table block merges  that is  pairs of highly overlapping blocks  which  when combined  result in a block with fewer comparisons  The combined result of these two levels approximates the optimal solution of Comparisons Propagation at a lower space complexity  The individual strategies of Block Manipulation are analytically presented in the following paragraphs  in the order they should be executed  5 2 1 Block Cleaning Cleaning a set of blocks B is the process of purging the duplicate elements from it  These are blocks that contain exactly the same entities with another block  regardless of the constraint function de   ning each of them  i e   independently of the information that is associated with them   We call such blocks identical  and  depending on their lineage  we formally de   ne them as follows Algorithm 3  Mining a clean set of blocks  Input  B a clean set of highly similar blocks Output  B 00 the semantically equivalent  mined set of blocks 1 B 0   blockScheduling B   2 DominatedB       3 for i   1 to B 0  size do 4 for j   B 0  size to i   1 do 5 if  B 0  i  size     B 0  j  size     then 6 break  7 if  areaConditionHolds B 0  i   B 0  j   then 8 if  isDominated B 0  i   B 0  j   then 9 DominatedB   DominatedB   B 0  i   10 break  11 B 00   B 0   DominatedBlocks  12 return B 00   Definition 6  Given a set of unilateral blocks B  a block bi 2 B is unilaterally identical with another block bj 2 B  denoted by bi   bj   if both blocks contain the same entities  regardless of their constraint functions  f i cond and f j cond   bi   bj   bi   bj   bj   bi   Definition 7  Given a set of bilateral blocks B  a block bi  j 2 B is bilaterally identical with another block bk l 2 B  denoted by bi j   bk l   if their corresponding inner blocks are unilaterally identical  bi  j   bk l   bi   bk   bj   bl   In this context  the process of Block Cleaning can be formally de   ned as follows  Problem 3  Block Cleaning   Given a set of blocks B  reduce B to its semantically equivalent subset B 0   B that contains no identical blocks  We call B 0 a clean set of blocks  The solution to this problem can be easily implemented by associating each block with a hash signature  its value is equal to the sum of the coordinates assigned to its entities by Block Mapping  Identical blocks necessarily have the same signature  but not vice versa  signature equality can also lead to false positives  For this reason  the size as well as the elements of two blocks with the same signature are analytically compared to make sure that they are indeed identical  In practice  this functionality is e ciently offered by default by most programming languages  Both its time and space complexity are linear to the size of the input set of blocks  i e   O jBj    as it involves traversing its elements just once  5 2 2 Block Mining Given a clean set of blocks  the process of mining it consists of identifying the blocks that are subsets of at least one other block in the set  that is  blocks whose entities are all contained in some other block  independently of the corresponding constraint functions  This situation is called a relation of dominance  where the latter is the dominant block  and the former the dominated one  This is more formally de   ned as follows  Definition 8  Given a clean set of unilateral blocks B  a block bi 2 B is unilaterally dominated by another block bj 2 B  denoted by bi   bj   if bi is a proper subset of bj   regardless of the constraint functions f i cond and f j cond   bi   bj   jbi j   jbj j    idi 2 bi   idi   bj   Definition 9  Given a clean set of bilateral blocks B  a block bi  j 2 B is bilaterally dominated by another block bk l 2 B  denoted by bi j   bk l   if at least one inner block of bi  j is unilaterally dominated by the corresponding inner block of bk l and the other is either unilaterally identical or unilaterally dominated  bi  j   bk l    jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j   jbj j   jbl j   In this context  the problem of Block Mining can be formally de   ned as follows  Problem 4  Block Mining   Given a clean set of block B  reduce B to its semantically equivalent subset B 0   B that contains no dominated blocks  We call B 0 a mined set of blocks  Apparently  this constitutes another quadratic problem  since the elements of every block have to be compared with those of all others  However  the abstract representation of blocks we are proposing leads to a series of necessary conditions that have to be satis   ed by a pair of blocks  if one of them is dominated  The bene   t is that these conditions can be checked in a fast and easy way  without the need to analytically compare the elements of the blocks  The conditions are also complementary  with their conjunction forming a composite mining method that e ectively restricts the required number of comparisons  In the following  we describe them in more detail   i  Size Condition  SC   In a clean set of blocks B  there cannot be a relation of dominance among a pair of equally sized blocks  Instead  the dominant block has to be larger in size than the dominated one  Therefore  to check whether a block is dominated  we need to compare it solely with blocks of larger size   ii  Area Condition  AC   Block mapping adds an additional condition for a relation of dominance  the spatial representation of the dominated block has to be fully contained in the representation of the dominant one  that is  the line  area  of the former lies entirely inside the line  area  mapped to the latter  More formally  the AC for a unilateral block bi to be dominated by a block bj is expressed as follows  bi   bj    min bj  entityCoods    min bi  entityCoods   max bi  entityCoods    max bj  entityCoods    where bk  entityCoods is the set of coordinates assigned to the entities of block bk   Similarly  the AC for a bilateral block bi  j to be dominated by a block bk l takes the following form  bi  j   bk l    min bk  entityCoods    min bi  entityCoods   max bi  entityCoods    max bk  entityCoods     min bl  entityCoods    min bj  entityCoods   max bj  entityCoods    max bl  entityCoods    AC is illustrated in Figure 1 a   b1 is smaller than b3  but cannot be dominated by it  since the line of b1 is not entirely covered by the line of b3  On the other hand  b2 satis   es the AC with respect to b3 but not with b1  As mentioned in Section 4 2  the priority of Algorithm 1 is to ensure that large blocks end up with low spatial deviation  The reason is that AC is intended to be used in conjunction with SC  In this way  the latter condition saves comparisons among equally sized blocks  even if their spatial representations are intersecting  while the former saves unnecessary comparisons that involve large blocks  Without AC  each block is inevitably compared with all smaller blocks  even if they share no entities   iii  Entity Condition  EC   Another  straightforward way of considering the content of blocks before comparing them is to considermerely those pairs of blocks that have at least one entity in common  This can be achieved by using the same data structure that is employed in Comparisons Propagation  a hash table that contains for each entity a list of the blocks that are associated with it  However  this solution has the same  quadratic  space complexity with Comparisons Propagation  thus violating the requirement for an alternative solution with minimal space requirements  Thus  we replace it with a near duplicates detection method  Locality Sensitive Hashing  LSH   7  constitutes an established method for hashing items of a high dimensional space in such a way that similar items  i e   near duplicates  are assigned to the same hash value with a high probability p1  In addition  dissimilar items are associated with the same hash values with a very low probability p2  In our case  blocks are the items that are represented in the high dimensional space of E  or E1 and E2  through Block Mapping  Thus  LSH can be employed to group highly similar blocks in buckets  so that it su ces it compare blocks contained in the same bucket  The details of the con   guration of LSH we employ are presented in Section 6  Algorithm 3 outlines the steps of our Block Mining algorithm  In short  it encompasses two nested loops  with SC and AC integrated in the inner one  Note that the input consists of the blocks contained in the same bucket of LSH  It is worth noting that the nested loop starts from the bottom of the ordered list of blocks and traverses it to the top  In this way  smaller blocks are    rst compared with the largest ones  The reason is that the larger the di erence in the size of the two blocks  the higher the likelihood that the larger block contains all the elements of the smaller one  thus  dominating it   SC is encapsulated in lines 5 and 6  while AC in line 7  Note that SC terminates the inner loop as soon as an equally sized block is encountered  because Block Scheduling ensures that the next blocks are of equal or smaller size  5 2 3 Block Merging An e ective way of discarding super   uous comparisons is to identify blocks that are highly overlapping  These are blocks that share so many entities that  if merged  they would result in fewer comparisons than the sum of the comparisons needed for each one  Merging such blocks eliminates their redundant comparisons  thus enhancing e ciency without any impact on e ectiveness  Indeed  pairs of entities that are common among the original blocks  i e   the source of redundancy  are considered only once in the new blocks  while the pairs of duplicate entities are maintained without any change  This situation is illustrated in the following example  Example 4  Consider the following unilateral blocks  b1   fid1  id2  id3  id4  id5g  b2   fid1  id2  id4  id5  id6g and b3   fid1  id3  id4  id5  id7  id8g  Individually  these blocks involve 35 comparisons  in total  However  b2 and b3 contain most of the comparisons in b1  they share four entities with b1   Merging b1 with b2 leads to the new block b 0 2   fid1  id2  id3  id4  id5  id6g  We now need 30 comparisons in total  between b 0 2 and b3   In addition  merging b 0 2 with b3 in a single block containing all 8 entities further reduces the total number of comparisons to 28  i e   20  less than the initial number of comparisons   More formally  the merge of two unilateral bilateral blocks is de   ned as follows  Definition 10  Given a set of unilateral blocks B  the unilateral merge of a block bi with a block bj is a new unilateral block bmi j that contains the union of the entities of bi and bj  bmi  j   bi   bj   Definition 11  Given a set of bilateral blocks B  the bilateral merge of a block bi  j with a block bk l is a new bilateral block Algorithm 4  Merging a mined set of blocks  Input  B a clean  mined set of blocks Output  B 00 the semantically equivalent  merged set of blocks 1 B 0   blockMapping B   2 FromMergesB       3 MergedB       4 OpenB       5 ProcessedB       6 for i   0 to spatialBlocks maxDimension do 7 NewOpenB   getBlocksStartingAtIndex i  B 0    8 NewOpenB   NewOpenB S FromMergesB  9 NewOpenB   NewOpenB n MergedB  10 NewOpenB   NewOpenB n OpenB  11 NewOpenB 0   blockScheduling NewOpenB   12 OpenB   blockScheduling OpenB S NewOpenB   13 FromMergesB       14 foreach bi 2 NewOpenB 0 do 15 mostS imilarBlock   getMostSimilarBlock bi   OpenB   16 if mostS imilarBlock   null then 17 newBlock   mergeBlocks bi   mostS imilarBlock   18 newBlock mapToCartesianSpace    FromMergesB add newBlock   19 MergedB add bi   20 MergedB add mostS imilarBlock   21 OpenB remove bi   22 OpenB remove mostS imilarBlock   23 OpenB addAll NewOpenB   24 EndingBlocks   getBlocksEndingAtIndex i  OpenB   25 OpenB removeAll EndingBlocks   26 ProcessedB addAll EndingBlocks   27 return ProcessedB  bmi k mj l   whose inner blocks constitute the unilateral merge of the corresponding inner blocks of bi  j and bk l  bmi k  mj l   fbi   bk   bj   blg  Typically  each block shares comparisons with many other blocks of the input set  However  these pairs of blocks di er in their degree of overlap  i e   the number of comparisons they share   In fact  the higher the Jaccard coe cient 2 of two blocks  the more comparisons their merge saves  Thus  to maximize the e ect of Block Merging  each block should be merged with the block that has the highest proportion of common entities with it  We call this block maximum Jaccard block  We can now formally de   ne the problem of Block Merging as follows  Problem 5  Block Merging   Given a mined set of blocks B  identify for each block bi its maximum Jaccard block bj   and merge these bi bj pairs  so as to produce a semantically equivalent set B 0 with a smaller number of redundant comparisons  We call B 0 a merged set of blocks  To address this problem  we introduce Algorithm 4  At its core lies the idea that each block should have overlapping spatial representations with its maximum Jaccard block  In other words  there is no point in estimating the Jaccard similarity between two blocks with disjoint spatial representations  Based on this principle  our algorithm works for unilateral blocks as follows  two main lists are maintained  the Open and the Processed ones  lines 4 and 5  respectively   The former contains the blocks that are available for comparison  whereas the latter encompasses the blocks that do not 2 The Jaccard similarity coe cient J A  B  of two sets A and B is equal to  J A  B    jA Bj jA Bj   a  0 1 2 3 4 5 b1 b b2 3 1 2 5 3 4 6  b  4 b3 3 5 5 4 6 1 2 3 b1 1 b2 2 2 3 4 0 1 2 3 4 5 1 Figure 2  Illustration of the block merge algorithm  need to be considered any more  Starting from the beginning of the X axis  the algorithm traverses the X axis by one point in every iteration  line 6   blocks  whose spatial representation starts from the current point  line 7   are compared with the blocks in the Open list  line 14   Then  at the completion of the iteration  they are added in it  together with the merged blocks  lines 23 and 8  respectively   Blocks  whose spatial representation ends at the current point  are placed in the Processed list  lines 24 26   The reason is that they do not overlap with any of the subsequently examined blocks  In the case of bilateral blocks  the only di erence in the execution of the algorithm is that it traverses both axes simultaneously  The following example illustrates the functionality of this algorithm  Example 5  Figure 2 a  is an illustration of the Block Merging algorithm for unilateral blocks that are mapped to the X axis  The main idea is that the grey area expands by one unit after each step  All blocks lying partly within it are in the OpenB list  while all blocks that lie entirely within it are placed in the ProcessedB list  At the highlighted Step 4  b2 and b3 lie in the former list  while b1 is placed in the latter  The execution of the algorithm is as follows  at Step 1  only b1 is in OpenB  while  at Step 2  b3 is also added and compared with b1  At Step 3  b1 is removed from OpenB  Then  b2 is placed in OpenB and compared with all other blocks  The remaining blocks  b2 and b3  are placed in ProcessedB at the end of Step 5  Figure 2 b  is an illustration of the block merge algorithm for bilateral blocks that are mapped to the XY space  Again  the grey area expands at each step by one unit  in both dimensions this time  All blocks lying partly within it are placed in the OpenB list  while all blocks that lie entirely inside its borders are moved to the ProcessedB list  In the depicted case  step 5   b1 1 lies in the ProcessedBlocks list  while b2 2 and b3 3 are in the OpenBlocks one  As mentioned in De   nition 5  the input to Block Merging is a mined set of blocks  that is  the input set contains neither identical nor dominated blocks  since they do not contribute non redundant comparisons  The computational cost of Algorithm 4 is thus significantly reduced  without a ecting its output  However  during the execution of Algorithm 4  blocks that belong to one of these categories can be produced  and should be discarded on the    y  Indeed  merges that lead to a block identical to another block of the input set are immediately removed  lines 9 10 Algorithm 4   Regarding new relations of dominance  merges can be involved in them only as dominant blocks  Otherwise  the original blocks that produce them would have already been dominated  More formally  bmi  j   bk    bi   bj    bk    bi   bk     bj   bk   Thus  only a block of the input set can be dominated by the merge of two other blocks  Apparently  it is not e cient to apply the Block Mining method after each iteration of Block Merging  Dominated blocks are  therefore  removed after the completion of the merging process  by comparing the original  non merged blocks with the set of merges  6  EVALUATION In this section we present a series of experiments that investigate the higher e ciency conveyed by our techniques  when incorporated into existing  redundancy bearing blocking methods  Our techniques were fully implemented in Java 1 6  and the experiments were performed on a server with Intel Xeon 3 0GHz  Metrics  Given that our techniques aim at eliminating redundant comparisons  we evaluate them on the basis of an established metric for measuring the e ciency of blocking methods  namely the Reduction Ratio  RR   1  3  18   It expresses the reduction in the number of pair wise comparisons required by a method with respect to the baseline one  Thus  it is de   ned as follows  RR   1  mc bc  where mc stands for the number of comparisons entailed by our technique  and bc expresses the number of comparisons entailed by the baseline  in our case  this is the original blocking method   RR takes values in the interval  0  1   for mc   bc   with higher values denoting higher e ciency  Recall  named Pair Completeness in the context of blocking  21   is not reported  since our techniques do not a ect the duplicates identi   ed by a blocking method   their goal is exclusively to detect and avoid the super   uous and repeated comparisons  Data Sets  To evaluate the impact of our techniques on existing blocking methods  we employ two real world data sets  one for the Clean Clean  and one for the Dirty Dirty and Dirty Clean cases of ER  They are DBPedia and the BTC09 data set  respectively  which are described in more detail in the following paragraphs  DBPedia Infobox Dataset  DBPedia   This data collection consists of two di erent versions of the DBPedia Infobox Data Set 3   They have been collected by extracting all name value pairs from the infoboxes of the articles in Wikipedia   s English version  at speci   c points in time  Although it may seem simple to resolve two versions of the same data set  this is not the case  More speci   cally  the older version  DBPedia1  is a snapshot of Wikipedia Infoboxes in October 2007  whereas the latest one  DBPedia2  dates from October 2009  During the two years that intervene between these two versions  Wikipedia Infoboxes were so heavily modi   ed that there is only a small overlap between their pro   les  even for duplicate entities  just 25  of all name value pairs are shared among the entities common in both versions  As matches  we consider those entities that have exactly the same URL in both versions  The attribute agnostic blocking method introduced in  21  was applied on this data set to produce the set of bilateral blocks we employ in our experiments  The blocks we consider are those resulting from the Block Purging step  21   involving 3 98 10 10 comparisons and PC   99 89   The technical characteristics of DBPedia are presented in Table 1  3 See http   wiki DBPedia org Datasets  DBPedia DBPedia1 Entities 1  190  734 DBPedia1 Name Value Pairs 17  453  516 DBPedia2 Entities 2  164  058 DBPedia2 Name Value Pairs 36  653  387 Bilateral Blocks 1  210  262 Comparisons 3 98   10 10 Av  Comparisons Per Block 32  893 Av  Blocks Per Entity 15 38 BTC09 Entities 1 82   10 8 Name Value Pairs 1 15   10 9 Unilateral Blocks 8 04   10 7 Comparisons 4 05   10 9 Av  Comparisons Per Block 50 37 Av  Blocks Per Entity 2 61 Table 1  Technical characteristics for both data sets  Billion Triple Challenge 2009  BTC09   This data set constitutes a publicly available 4   large snapshot of the Semantic Web  aggregating RDF statements from a variety of sources  In total  it comprises 182 million distinct entities  described by 1 15 billion triples  The ground truth set of duplicate entities is derived from the explicit as well as implicit equivalence relationships  i e   the owl sameAs statements and the Inverse Functional Properties  respectively   BTC09 was employed as a test bed for the attribute agnostic blocking method presented in  22   which is the source of our experimental set of unilateral blocks  well  Similar to DBPedia  the blocks we employ in our experiments are those stemming from the Block Purging method of  22   They entail 4 05   10 9 comparisons  exhibiting a PC very close to 90   A more detailed overview of this data set is presented in Table 1  6 1 Block Mapping In this section  we compare Algorithm 1 with the Random Mapping  RM  of blocks to the Cartesian space  As mentioned in Section 4 2  the higher the performance of a mapping method  the lower the sum of Formula 1 should be  Table 2 shows the outcome for both algorithms and for both data sets  In each case  we considered 100 iterations with RM in order to get a safe estimation of its performance  The sums in Table 2 is equal to the average value  It is remarkable that standard deviation is equal with  4 16 10 16 and  3 97   10 12 for DBPedia and BTC09  respectively  thus being 4 orders of magnitude lower than the mean value in both cases  This indicates that the performance of RM is relatively stable  We can see that Algorithm 1 substantially outperforms RM in both cases  for bilateral blocks  it improves RM by 30 34   whereas for unilateral blocks the enhancement is 25 28   The reason for the slightly lower improvement in the second case is twofold  First  redundancy is much higher in DBPedia than in BTC09  as documented in Table 1  in the former data set  an entity is associated with more than 15 blocks  in comparison with the less than 3 blocks for the latter  Thus  mapping an entity to a suboptimal  random  coordinate a ects the spatial deviation of more blocks in DBPedia than in BTC09  Second  suboptimal mappings have a larger impact in the two dimensional space than in the unidimensional one  Consider for example Figure 1  Assigning entity id4 to point 5 of the X axis instead of 4  increases the spatial deviation of b2 by 1  However  assigning entity id1 4 to point 5 of the Y axis instead of 4 increments the spatial deviation of b3 3 by 4  For these 4 See http   vmlion25 deri ie  Method DBPedia Sum BTC09 Sum Algorithm 1 0 81   10 20 1 33   10 16 Random Mapping 1 16   10 20 1 78   10 16 Table 2  Comparison of the Block Mapping technique with respect to the sum of Formula 1 for both data sets  Method DBPedia Comp  BTC09Comp  Cartesian Product 5 11   10 11 3 23   10 19 LSH   Algorithm 3 1 27   10 8 4 52   10 9 Table 3  Performance of the Block Mining algorithm  DBPedia BTC09 Method Comp  RR Comp  RR Input Set 3 98   10 10   4 01   10 9   Comp  Prop  2 59   10 9 93 49  3 08   10 9 23 19  Table 4  E ect of Comparisons Propagation on the e ciency of both data sets  reasons  the performance of RM is slightly closer to Algorithm 1 for unilateral blocks  6 2 Block Mining We now present the performance of our Block Mining algorithm  i e   LSH and Algorithm 3  with respect to the number of block comparisons it requires in order to examine the blocks inside each bucket  In general  to determine the structure of LSH  two parameters have to be speci   ed  L and k  the former denotes the total number of hash tables that will be employed  while the latter speci   es the number of hash functions that are merged to compose the hash signature for each entity  for a particular hash table  In our implementation  we followed  23  and used exclusive or for e   ciently merging the k hash functions into the composite signature  In more detail  L was set to 10  and k to 12  while the probabilities p1 and p2  see Section 5 2 2  were set to 0 9 and 0 1  respectively  Table 3 shows the performance of our method in comparison with the naive method of examining all possible pairs of blocks  The aim is not to examine the optimal con   guration of LSH  Rather  the main conclusion to be drawn from these numbers is that it is a scalable approach  whose performance depends on the level of redundancy of the underlying blocking method  the higher the redundancy  the more similar the blocks are between them  and the larger the corresponding buckets get  this results in more comparisons and lower e ciency  This explains why the performanc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>On k skip Shortest Paths ### Yufei Tao     Cheng Sheng     Jian Pei         Department of Computer Science and Engineering     School of Computing Science Chinese University of Hong Kong Simon Fraser University New Territories  Hong Kong Burnaby  BC Canada  taoyf  csheng  cse cuhk edu hk jpei cs sfu ca ABSTRACT Given two vertices s  t in a graph  let P be the shortest path  SP  from s to t  and P   a subset of the vertices in P  P   is a k skip shortest path from s to t  if it includes at least a vertex out of every k consecutive vertices in P  In general  P   succinctly describes P by sampling the vertices in P with a rate of at least 1 k  This makes P   a natural substitute in scenarios where reporting every single vertex of P is unnecessary or even undesired  This paper studies k skip SP computation in the context of spatial network databases  SNDB   Our technique has two properties crucial for real time query processing in SNDB  First  our solution is able to answer k skip queries signi   cantly faster than    nding the original SPs in their entirety  Second  the previous objective is achieved with a structure that occupies less space than storing the underlying road network  The proposed algorithms are the outcome of a careful theoretical analysis that reveals valuable insight into the characteristics of the k skip SP problem  Their ef   ciency has been con   rmed by extensive experiments with real data  Categories and Subject Descriptors H3 3  Information search and retrieval   Search process General Terms Algorithms Keywords k skip  shortest path  road network 1 ###  INTRODUCTION Finding shortest paths  SP  in a graph is a classic problem in computer science  Formally  let G    V   E  be a graph where V is a set of vertices and E a set of edges  Each edge carries a non negative weight  De   ne the length of a path P  represented as  P   to be the total weight of the edges in P  Given two vertices s  t     V   a SP query returns the path from s to t with the minimum length  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  The SP problem has received considerable attention from the research community in the past few years  see the recent work  4  24  27  and the references therein   due to its profound importance in Spatial Network Databases  SNDB   An SNDB manages geometric entities positioned in an underlying road network  and supports ef   cient data retrieval with predicates on network distances  and optionally also on spatial properties  a representative system is Google Maps   A standard modeling of a road network is a graph where each vertex corresponds to a junction and each edge represents a road segment  An edge   s weight is often set to the length of the corresponding road segment or the average time for a vehicle to pass through the segment  Traditionally  a SP query retrieves every vertex on the shortest path P  which is sometimes unnecessarily detailed in practice  Consider  for example  that a person leaves home for a retreat destination  Typically  the SP would    rst wind through her his neighborhood R1  continue onto a set of highways R2  and eventually enter the neighborhood R3 of the destination  The region  in which    nedetailed directions are imperative  is R3  In R1 and R2  it is often suf   cient to guide the user at a coarse level  in a manner similar to putting sign posts along the way  for example  by naming some streets to be passed  and the highways to be taken in succession  In fact  even the computation of turn by turn driving directions does not always demand all the vertices on P  This is because P may contain vertices at which no turning is needed  To illustrate  assume that P stays on the Main Street in an urban area for one kilometer  during which the street intersects another street every 100 meters  Each of those 10 intersections is a vertex in P  but only the    rst and last of them are enough to generate the instructions for turning into and away from the Main Street  respectively  The situation is similar if P involves a long highway  in which the vertices between the points where P enters and leaves the highway respectively can be ignored  In this paper  we are interested in computing a subset  say P     of the vertices in P  Instead of an arbitrary subset  however  we demand that P     be k skip shortest path  namely  it should contain at least one vertex out of every k consecutive vertices in P  Figure 1 shows an example with k   4  where P consists of all the  black and white  vertices while P   only the black ones  Note that there can be more than one black vertex in every 4 consecutive vertices in P  but it is impossible to have none  P   succinctly describes P by sampling its vertices with a rate of at least 1 k  Such a sample set can replace P in answering queries like  what are the highways  alternatively  neighborhoods or cities  that P needs to go through  P   is equally useful in discovering the gas stations  similarly  attractions or hotels  along P  becauses t Figure 1  The black vertices constitute a 4 skip shortest path it is often suf   cient to    nd the stations close to the vertices in P     as opposed to all the vertices of P  This is the idea of  18  in approaching the continuous nearest neighbor problem  Moreover  P   is also adequate for estimating various statistics about P  as  in the query     nd the percentage of dessert coverage in the route from Los Angeles to Las Vegas  Finally  P   is exactly what is needed to plot the original P in a digital map with a decreased resolution  For example  at Google Maps  only a subset of the vertices on a path need to be displayed according to the current zooming level  For traditional mapping purposes  P   has two notable advantages over P  First  it is  much  smaller in size  and hence  requires less bandwidth to transmit  This is a precious feature in mobile computing that will especially be appreciated by users charged by the amount of Internet usage  Second  using a clever algorithm  P   can be faster to compute because  intuitively  fewer vertices need to be retrieved than P  Such an ef   ciency gain provides valuable opportunities for in car navigation devices and routing websites such as Google Maps to support a great variety of on route services in shorter time  The concept of k skip SP comes with a zoom in operation  Given consecutive vertices u  v on P     the operation    nds the missing vertices on P from u to v  As there are at most k     1 missing vertices  for small k a zoom in incurs negligible time  because it only needs to compute a very short path  This adds a nice pay as you go feature in applying k skip SP  First  a driver can request the least zoom in   s to complete the part of the route outside her his knowledge  This allows her him to pay for the most useful information only  Second  an algorithm preparing turn by turn directions can zoom in only when necessary  i e   a turning may exist between two adjacent vertices P      thus saving the cost of locating the skipped vertices  Contributions  Somewhat surprisingly  the notion of k skip SP  or in general the idea of sampling  the vertices in  a SP  has not been mentioned previously to the best of our knowledge  let alone any algorithm dealing with the problem of k skip SP computation  We    ll the gap with a systematic study of this problem  In particular  our objectives are twofold  1  Resolve a k skip query signi   cantly faster than calculating the original SP in entirety  2  Achieve the previous purpose with a data structure that occupies less space than storing the input graph G    V   E  itself  This permits the structure to reside in memory even for the largest road network  as is crucial for real time query processing in SNDB  The    rst contribution of this paper is to formally establish the fact that  only a small subset V   of the vertices in G need to be considered to attack the k skip problem  In other words  the other vertices in V  V   are never needed to form any k skip SP  Referring to V   as a k skip cover  we show that there always exists a V   with size roughly proportional to  V   k  This theoretical    nding is of independent interest because it is not limited to road networks  but actually holds for general graphs  Our second contribution is a full set of algorithms required to process k skip SP queries within a stringent time limit  Speci     cally  these algorithms settle three sub problems   i     nd a small V   in time linear to the complexity of G   ii  construct from V   a space economical structure  and  iii  answer a k skip query by accessing only a small portion of the structure  We thoroughly evaluated these algorithms with extensive experiments on real data  including the massive road network of the US which has about 24 million vertices and 58 million edges  Our results prove that the proposed technique very well satisfy both requirements mentioned earlier in all settings  Roadmap  The next section reviews the literature of SP computation  Section 3 formally de   nes the target problem  and gives an overview of our solutions  Section 4 elaborates the theoretical results on k skip covers  and the algorithms for constructing the proposed structure  Section 5 clari   es how to answer a k skip query and perform a zoom in ef   ciently  Section 6 contains the experimental results  while Section 7 concludes the paper with directions for future work  2  RELATED WORK There is an extremely rich literature on the SP problem  In Section 2 1  we clarify the details of the reach algorithm  which is the state of the art for SP computation in road networks  Section 2 2 surveys the other recent progress in the database and theory communities  2 1 Dijkstra and reach To facilitate discussion  given two vertices u  v in the input graph G    V   E   we denote by S P u  v  the SP from u to v  The length of S P u  v   i e    S P u  v    is called the distance between u and v  If  u  v  is an edge in G  we represent its weight as   u  v   In case G is directed   u  v  is an edge from u to v  and  v u  means the opposite  To avoid unnecessary distraction  our examples use undirected graphs  but all the notations in our description are carefully written so that they are also correct for directed graphs  Dijkstra  Let us    rst review the Dijkstra   s algorithm  5   which is the foundation of the reach method described shortly  Dijkstra    nds S P s  t  by exploring the vertices in ascending order of their distances to the source s  At any time  each vertex u     V has a status chosen from  unseen  labeled  scanned   Furthermore  u is associated with a label l u  equal to the distance of the SP from s to u found so far  i e   an even shorter path may be discovered later   On this path  the vertex immediately preceding u is kept in pred u   referred to as the predecessor of u  At the beginning of Dijkstra  all vertices u     V have status unseen  l u        and pred u         The only exception is s  whose status is labeled  with l s    0 and pred u         At all times  the vertices of status labeled are managed by a min heap Q  using their labels as the sorting key  In each iteration  the algorithm  i  de heaps the vertex u     Q with the minimum l u    ii  relaxes all edges  u  v  such that the status of v is not scanned  and  iii  changes the status of u to scanned  Speci   cally  relaxation of  u  v  involves the following steps  relax  u  v  1  if the status of v is unseen then 2  l v      l u      u  v  3  the status of v     labeled 4  else if l v    l u      u  v  thena b c d e f g 4 5 5 5 1 3 3  4 5 s t 1 1  a  The input graph a b c d e f g s t 1 0 6 5 6 0 1 0 0  b  Global reaches of the vertices Figure 2  Illustration of reach 5  l v      l u      u  v  6  pred v      u The present l u  is guaranteed to be  S P s  u    The algorithm terminates as soon as t  the destination  is selected by an iteration  To illustrate  assume that we want to compute S P s  t  in Figure 2a  In the    rst iteration  Dijkstra scans s and relaxes  s  a   after which Q    a  and l a    1  The next iteration de heaps a and relaxes  a  b   a  c   Note that  a  s  is not relaxed because the status of s has become scanned  Now Q contains b  c with labels 5  6  respectively  The algorithm then scans b and relaxes  b  d   which labels d with 10  This is followed by de heaping c  and relaxing  c  d   c  e   Note that the relaxation of  c  d  decreases l d  to 9  The rest of the algorithm proceeds in the same manner  By tracing the execution  one can verify that  at termination  all the vertices except t have already been scanned  Dijkstra can be implemented in O m  n log n  time  3   where n  m are the number of vertices and edges  respectively  i e   n    V    m    E    In a road network  each vertex has an O 1  degree  so the time complexity is essentially O n log n   Bi directional  Dijkstra starts a graph traversal from s and gradually approaches t  call this a forward search  Immediately by symmetry  the SP problem can also be solved by a backward search from t to s  if G is directed  the backward search implicitly reverses the direction of each edge   The bi directional algorithm  10  20  achieves better ef   ciency by performing both searches synchronously  the effect of which is essentially to explore the vertices u     V in ascending order of rs t u   where rs t u    min  S P s  u     S P u  t      1  In fact  if u is closer to s  than to t   then it will    rst be touched in the forward search  otherwise  the backward search will    nd it    rst  The forward backward search proceeds as in the standard Dijkstra   s algorithm  as if the other search did not exist   Let Qf  Qb  be the heap of the forward  backward  direction  Synchronization is carried out by advancing in each iteration the direction that has a smaller label at the top of the heap  Bi directional monitors the distance    of the shortest s to t path found so far     is set to     initially  and may be updated when the forward search  the backward case is symmetric  de heaps a vertex u whose status in the backward direction is labeled 1   Specifically  at this time  the algorithm computes        lf  u    lb u   1 This status cannot be scanned  otherwise  the algorithm would have terminated right after u was de heaped by the forward search  as will be clear shortly  s t Figure 3  Comparison of Dijkstra  bi directional  and reach where lf  u  and lb u  are the labels of u in the forward and backward search  respectively     is reduced to      if              since it implies the existence of a shorter s to t path that concatenates S P s  u    u  v   and S P v t   where v is the current predecessor of u in the backward search  The algorithm terminates when either direction de heaps a vertex already scanned in the other direction  Let us demonstrate bi directional on the graph in Figure 2a  After three iterations of each direction  which are the same as in Dijkstra  the forward  backward  search has scanned s  a  b  t  g  f   such that Qf  Qb  contains vertices c  d  e  d  with labels 6  10  respectively  Currently            Next  the forward search de heaps c     Qf and relaxes edges  c  d    c  e   after which lf  e     7  lf  d    9  Similarly  the backward search then de heaps e     Qb  As the status of e in the forward direction is labeled  the algorithm updates    to lf  e   lb e    7   6     13  before it relaxes  e  c   e  d   The forward search continues by de heaping e     Qf   which  however  has been scanned in the backward search  The algorithm therefore terminates  without deheaping d in either direction  Reach  Intuitively  if    is the length of S P s  t   Dijkstra searches a ball that centers at s  and has radius     shown as the dashed circle in Figure 3  Bi directional  on the other hand  explores two balls with radius    2 that center at s  t respectively  the two solid circles in Figure 3   The reach algorithm  which is the current state of the art  dramatically shrinks the search area to a small dumb bell shape  as illustrated by the shaded region in Figure 3  Let us start the explanation with the notion of local reach  Let u be a vertex on S P s  t   The local reach of u in S P s  t  equals rs t u  as calculated in Equation 1  Note that this notion relies on a particular SP  Any vertex v      S P s  t  has no local reach de   ned in S P s  t   Now we extend the de   nition to global reach  DE FI N I T I O N 1  GL O BA L  R E AC H   1 2       The global reach  denoted as r u   of a vertex u is the maximum local reach of u in all the shortest paths that pass u  Formally  r u     max s t u   S P  s t  rs t u    2  The reach r u  can be understood intuitively as follows  If u is on S P s  t   then either s or t must have distance at most r u  to u  In other words  in case neither s nor t is within distance r u  from u  u can be safely pruned in computing S P s  t   Consider  for instance  vertex c in Figure 2a  To decide its global reach r c      rst collect the set S of all the SPs that pass c  namely  S    S P s  t   S P s  g        SP a  e         We then calculate the local reach of c in each SP of S  For example  its local reach in S P a  e  is min  S P a  c     S P c  e      1  The    nal r c equals the maximum of all the local reaches  which can be veri   ed to be 6  as is its local reach in S P s  t    Figure 2b shows the global reaches of all the vertices  In computing S P s  t   the reach algorithm  10  12  proceeds in the same way as bi directional  but may prune a vertex in relaxing an edge  u  v   Without loss of generality  suppose that the relaxation takes place in the forward search  the backward case is symmetric   This implies that the forward search just scanned u  but has never scanned v  otherwise the edge would not have been relaxed   The pruning of reach takes place as follows  RU L E 1  Vertex v is pruned if both of the following hold  1  v has status labeled or unseen in the backward direction 2  r v    lf  u  where lf  u  is the label of u in the forward search  In general  reach can be understood as the execution of bidirectional on the vertices that survive pruning  In Figure 2  for example  reach    nds S P s  t  by scanning only s  a  c  e  g  t  As in bi directional  reach    rst scans s  t  in the forward  backward  direction  after which Qf  Qb  includes a  g  with label 1  Next  the forward search de heaps a from Qf   and relaxes  a  b   a  c   The relaxation of  a  b  prunes b by Rule 1 because r b     0   lf  a     1  The backward search then eliminates f in a similar fashion  The rest of the algorithm proceeds as in bi directional  Vertex d does not need to be scanned for the reason explained earlier for bi directional  The space consumption of reach is very economical because  except G itself  only one extra value needs to be stored for each vertex  However  it can be expensive to calculate the exact reach of every vertex  Fortunately  there are fast algorithms  10  12  to obtain approximate reaches that are guaranteed to upper bound their original values  Pruning remains the same except that r u  should be replaced by its upper bound  The outstanding ef   ciency of this algorithm on road networks has been theoretically justi   ed  1   2 2 More results on SP computation The A   algorithm  13  is a classic SP solution that captures Dijkstra as a special case  The effectiveness of A   relies on a method to calculate  typically in constant time  a lower bound of  S P u  v   for any two vertices u  v  Apparently  0 can be a trivial lower bound  but in that case A   degenerates into Dijkstra  In general  the tighter the lower bounds are  the faster A   is than Dijkstra  Motivated by this  Agrawal and Jagadish  2  proposed to precompute the distances between each vertex and a special set of vertices called landmarks  In answering a SP query  those distances are deployed to derive lower bounds based on triangle inequality  This idea is also the basis of the ALT algorithm developed by Goldberg and Harrelson  9   which has lower query time  than  2   at the tradeoff of consuming more space  The experiments of  10  indicate that ALT is slower than the reach method described in Section 2 1  Note  however  that ALT and reach are compatible  in the sense that their heuristics can be applied at the same time to maximize ef   ciency  10   Sanders and Schultes  8  25  26  presented a highway hierarchy  HH  technique  whose rationale is to identify some edges that mimic the role of highways in reality  In computing a SP  the algorithm of HH modi   es Dijkstra so that the search can skip as many non highway edges as possible  and thus  terminate earlier  Based on the empirical evaluation of  10  25   HH has comparable performance with respect to reach  The separator technique is another popular approach  15  16  17  to preprocess a graph G for ef   cient SP retrieval  The idea is to divide the vertices of G into several components  and for each component  extract a set of boundary vertices such that the SP between two vertices in different components must go through a boundary vertex  Query ef   ciency bene   ts from the fact that  most computation of a SP can be restricted only to the boundary vertices  According to  25   however  separator based methods are not as ef   cient as HH on road networks  Another disadvantage is that these methods often have expensive space consumption  For example  the space of  15  is at the order of n 1 5  where n is the number of vertices   which is prohibitive for massive graphs  In  28   Wagner et al  described a geometric container technique  which associates each edge  u  v  in G with a geometric region  The region covers all the vertices t such that the SP from u to t goes through v  In running Dijkstra  such regions can be used to prune many vertices that do not appear in the SP  Lauther  19  integrated a similar idea with the bi directional algorithm discussed in Section 2 1  Samet et al   24  proposed to break the geometric regions into smaller disjoint pieces that can be indexed by a quadtree  Their solution lowers the cost of SP calculation  compared to  19  28    but entails O n 1 5   space  A common shortcoming of  19  24  28  is that their preprocessing essentially determines the SPs between all pairs of vertices  The all SP problem is notoriously dif   cult  Even on planar graphs  the fastest solution requires O n 2   time  7   which is practically intractable for large n  Recently  Sankaranarayanan et al   27  proposed a path oracle that is constructed from well separated pair decomposition  and can be used to accelerate SP retrieval  Such an oracle consumes O s 2 n  space in 2 d space  where s is shown to be around 12 for practical data  Xiao et al   30  showed that SP queries can be accelerated by exploiting symmetry in the data graph  Their approach  however  is limited to the case where all edges have a unit weight  Wei  29  developed an alternative solution by resorting to tree decomposition  23   Rice and Tsotras  22  studied the shortest path problem with label restrictions  Some other work considers how to estimate shortest path distances  e g    11  21   We emphasize that all the works aforementioned calculate traditional  complete  SPs  The concept of k skip SP has not appeared previously  and is formalized in the next section for the    rst time in the literature  3  K SKIP SHORTEST PATHS For simplicity  we assume that the data graph G    V   E  is undirected  and will discuss directed graphs only when the extension is not straightforward  The following de   nition formalizes kskip SP  DE FI N I T I O N 2  k S K I P  S H O RT E S T  PAT H   Let S P s  t  be the shortest path from source s to destination t  Label the vertices in S P s  t  in the order they appear  v1  v2       vl  i e   v1   s  vl   t   where l is the total number of vertices in the path  If l     k  let P   be an ordered subset of  v1       vl   where the ordering respects that in S P s  t   P   is a k skip shortest path from s to t if P        vi       vi k   1          3  for every 1     i     l     k   1 a b c d e f a b c d e f     3 3 3 3 3 3 2  a  Black vertices  b  3 skip graph constitute a 3 skip cover Figure 4  Preprocessing for 3 skip SP computation Even for    xed s and t  there can be multiple P   satisfying the above condition  In other words  k skip SPs are not unique  although all of them have to be subsets of S P s  t   Our objective is to preprocess G into a space economical structure such that  given any s  t at run time  we can    nd a k skip SP between any s and t ef   ciently  The    rst step of our preprocessing is to eliminate the redundant vertices in G  The intuition is that  since most vertices in a SP need not be reported under the k skip de   nition  certain vertices would end up being never returned  For example  consider the graph G in Figure 4a where all edges have a unit weight  Observe that  for k   3  any SP with 3 vertices must contain at least one black vertex  In other words  it suf   ces to form k skip SPs using just the black vertices  whereas the other  white  vertices can be discarded  We refer to the set of black vertices in the above example as a 3 skip cover  The next de   nition generalizes the notion to kskip cover  which contains a subset of the vertices that need to be considered for k skip SPs  DE FI N I T I O N 3  k S K I P  C OV E R   Let V   be a subset of the vertices in G  V   is a k skip cover if it has the property that  for arbitrary s  t     V such that S P s  t  has at least k vertices  V       S P s  t  is a k skip SP from s to t  after ordering the vertices of V       S P s  t  appropriately  As will be clear in Section 5  the ef   ciency of our k skip SP algorithm crucially depends on the fact that  it does not need to process the vertices of V   V     Hence  the minimization of  V     is essential  but it turns out to be rather challenging on an arbitrarily complex G  In fact  it is non trivial even if one simply wants to know whether a small V   always exists  These issues will be resolved in the next section  We want to capture the adjacency of the vertices in V   as far as k skip SPs are concerned  For example  the black vertex a in Figure 4a can be consecutive only to e and f in a 3 skip SP  observe that e and f have blocked all the possible ways that can lead a to any other black vertex   We represent this by generating two    super edges    that link a to e  f respectively  After this  the original edges  of G  incident on a can be ignored in computing any k skip SP beginning from a  Let us formalize the rationales behind the preceding paragraph in the next two de   nitions  DE FI N I T I O N 4  k S K I P  N E I G H B O R   Let u  v be two vertices in a k skip cover V     We say that v is a k skip neighbor of u if S P u  v   namely  the shortest path from u to v in G  does not pass any other vertex in V     It is easy to see that the SP from u to any of its k skip neighbor v can have at most k edges  In an undirected G  the relation implied by the above de   nition is symmetric  i e   u is a k skip neighbor of v if and only if v is a k skip neighbor of u  This is not necessarily true for directed graphs  In any case  let Nk u  be the set of all k skip neighbors of u  For instance  in Figure 4a  N3 a     e  f   DE FI N I T I O N 5  k S K I P  G R A P H   A k skip graph of G is a graph G      V     E      where     V   is a k skip cover of G      for every vertex u     V     E   has an edge  u  v  for each k skip neighbor v of u  namely  v     Nk u   The weight of  u  v      E   is  S P u  v    We call each edge  u  v      E   a super edge  Figure 4b demonstrates the 3 skip graph G   of the graph G in Figure 4a  The weight of  a  e  in G   equals 3  which is the distance of a and e in G  As shown later     nding a k skip SP on the original graph G can be reduced to computing a  traditional  SP on a k skip graph G     In the next section  we will delve into the properties of G     and give a method to construct it ef   ciently  Then  the reduction and the accompanied algorithms will be elaborated in Section 5  4  K SKIP GRAPH The effectiveness of our pre computed structure  namely a kskip graph G      V     E      relies on the size of the k skip cover V     No performance gain would be possible if a small V   could not be guaranteed  Fortunately  we will show that such a good V   always exists  Section 4 1   Sections 4 2 and 4 3 then elaborate the algorithms for building G     4 1 Size of k skip cover This subsection will establish  TH E O R E M 1  Let G    V   E  be a graph with n    V   vertices  For any 1     k     n  G has a k skip cover V   of size O  n k log n k    Note that the above result applies to all graphs  that is  regardless of whether they are directed  planar  etc  Our proof leverages the theory of Vapnik Chervonenkis  VC  dimension  which quanti   es how decomposable a search problem is  Speci   cally  let D be a dataset and Q be a set of queries that can be issued on D  Given a query q     Q  de   ne q D  to be the result of q on D  A shatterable set S     D is such that  for any S       S  there is always a query q     Q with q D      S   S     In other words  any subset S       S needs to have the property that  a query q     Q retrieves all the items of S     and nothing from S   S    the result of q  however  may also include items from D   S   The VC dimension of  D  Q  is the size of the largest shatterable subset of D  Note that the VC dimension is not de   ned on a dataset  but instead  on a pair of a dataset and a query set  Now let us analyze the VC dimension of the SP problem  Here  we have an input graph G    V   E   The D mentioned earlier corresponds to V   A SP query qs t is uniquely de   ned by a source vertex s and a destination vertex t  Hence  the result qs t V   of qs t is S P s  t   Let Q be the union of all the possible SP queries  namely  Q     s t   V qs t  thus   Q    n 2    Next  we give a crucial lemma LE M M A 1  For any graph G  the VC dimension of  V   Q  is 2  PRO O F  Assume for contradiction that the VC dimension d of  V   Q  is at least 3  note that d must be an integer   Hence  there is a shatterable set S with size d  which belongs to the SP returned by a query q     Q  Let u1  u2       ud be the vertices of S ordered by their appearance on the SP  Hence  u2 is on the SP from u1 to ud  In this case  however  S      u1  ud  cannot be in any SP that does not contain u2  contradicting the fact that S is shatterable  It remains to verify that the VC dimension can be 2  We omit the details as this is trivial  The rest of the proof  for Theorem 1  concerns   net  Let D  Q be as de   ned earlier in our introduction to VC dimension  A set S     D is an   net of  D  Q  if S     q D         for any q satisfying  q D         D   In other words  if q retrieves no less than   D  items  at least one of these items must appear in S  The lemma below draws the correspondence between   net and k skip cover  LE M M A 2  A   k n   net V   of  V   Q  is a k skip cover of G  PRO O F  Let Q   be the set of queries q     Q such that the SP q V   returned by q has exactly k vertices  By the de   nition of  k n  net  for each q       Q     V       q    V           Now consider a query q     Q   Q   whose result q V   has more than k vertices  Clearly  any sub path of q V   including k vertices is the result q    v  of some q       Q     from which V   contains at least a vertex  Therefore  V   is a k skip cover  The   net theorem  14  dictates that any  D  Q  with VC dimension d has an   net of size O  d   log 1      This  combined with Lemmas 1 and 2  gives Theorem 1  Remark 1  Effectively  the proof of the   net theorem  14  shows that a random subset of D with size O  d   log 1     is an   net with high probability  Hence  we can    nd a k skip cover V   by simply taking O  n k log n k   vertices from V randomly  Remark 2  There is a trivial lower bound of n k on the size of a kskip cover  Hence  the upper bound in Theorem 1 is asymptotically tight  up to only a logarithmic factor  4 2 Computing a k skip cover As mentioned in the previous subsection  a k skip cover V   can be obtained by taking O  n k log n k   random vertices from V   It is well known that randomized techniques generally work much better in practice than predicted by theory  Therefore  the V   of a real graph would be much smaller  rendering a sample set of size O  n k log n k   most probably unnecessarily large  Of course  we could arti   cially reduce the number of samples  but a good sample size appears to be dif   cult to decide  A large size gives only marginal improvement  whereas a small size has the risk that the sample set may no longer be a k skip cover  We propose an adaptive sampling  AS  algorithm to resolve the above issue  Before explaining the algorithm  we need a few more de   nitions  Let the    hop neighbor set of a vertex u     V   denoted as V   u   be the set of all vertices v     V that can be reached from u by crossing at most    edges  Each v     V   u  is called a    hop neighbor of u  For example  assume an input graph G as shown in Figure 5a  where all edges  except those explicitly labeled  have weight 1  The 2 hop neighbor set V2 u  of u contains all the vertices in the shaded diamond  Note that V2 u  is decided without taking the edge weights into account  Denote by G   u  the graph induced by V   u   which is referred to as the    hop vicinity of u  That is  G   u  includes all and only the edges in G that are between the vertices of V   u   For instance  in Figure 5a  the edges of G2 u  are those that fall entirely in the diamond  By computing the SPs inside G   u  from u to all the other vertices in V   u   that is  each SP uses only the edges of G   u    one obtains a    hop shortest path tree T   u  rooted at u  For every vertex v in G   u   the u to v path in T   u is the SP from u to v inside G   u   Figure 5b demonstrates the 2 hop SP tree T2 u  of u  It is worth noting the difference between a SP inside the tree and a SP in the whole graph  For example  the u to c path in T2 u  has length 5  Although this is the shortest when only the edges of G2 u  are considered  there is an even shorter path u     f     e     d     c which uses two edges  e  d   d  c  outside the 2 vicinity of u  We now arrive at the most crucial concept underlying the AS algorithm  Let V   be a subset of the vertices in G  We say that a    hop SP tree T   u  is covered by V     if every u to v path in T   u  having at least    edges goes through at least one vertex in V     where v is a vertex in T   u   In Figure 5a  for example  let V   be the set of black vertices  Then  the T2 u  in Figure 5b is not covered because the path u to b has 2 edges but passes no black vertex  The next lemma gives an important property  LE M M A 3  V   is a k skip cover if it covers the Tk   1 u  of all u     V   PRO O F  Assume for contradiction that a V   ful   lling the ifcondition of the lemma is not a k skip cover  It follows that there exist two vertices u  v     V such that S P u  v  has k vertices none of which is in V     Since S P u  v  has k   1 edges  by the construction of Tk   1 u   we know  i  all the vertices on S P u  v  are in Tk   1 u   and  ii  the u to v path in Tk   1 u  is exactly S P u  v   This means  however  that Tk   1 u  has not been covered by V     thus reaching a contradiction  We are ready to clarify the AS algorithm  adaptive sampling 1  V         2  randomly permutate the vertices of V 3  for each vertex u     V 4  if Tk   1 u  is not covered by V   then 5  add u to V   The correctness of the algorithm follows from Lemma 3  and the fact that  if Tk   1 u  has not been covered by V   yet  including u itself to V   immediately makes Tk   1 u  covered  For instance  as shown earlier  the T2 u  in Figure 5b is not covered  but it will be  once u is added to V     Heuristic  We can further reduce the size of V   by    rst sorting the vertices of V in descending order of their degrees  and then  randomly permute the vertices with an identical degree  This increases the chance of sampling a vertex with a higher degree  which is bene   cial because such a vertex tends to lie on more SPs  and therefore  have stronger covering power  Time complexity  We close the subsection by analyzing the execution time of the algorithm  assuming that each vertex has an O 1  degree  which is true in road networks  The randomization at Line 1 can be implemented in O n  time  6  where n    V    the sorting in the high degree favoring heuristic can be done in O n 4 5 2 2 2 2 2 2 2 2 2 u a b e c d f 2     u a b e f c 2  a  2 hop vicinity of u and its  b  2 hop SP tree of u Figure 5  Deciding whether to sample u into a 3 skip cover time when there are only a constant number of possible degrees   The    hop vicinity of a node u can be found by a standard breath    rst traversal  BFT  initiated at u  which terminates in O      u   time where      u  is the number of    hop neighbors of u  i e        u     V   u    Then  the    hop SP tree T   u  can be extracted from G   u  using the Dijkstra   s algorithm in O      u  log      u   time  Checking whether T   u  is covered by V   demands a single traversal of T   u  in O      u   time  Hence  the total cost of the algorithm is O n    k   1 log     k   1   where     k   1 is the average number of  k     1  hop neighbors of the vertices in V   The value of     k   1 depends on the structure of the road network  but not the number n of the vertices  For example  consider two simple road networks that are a 100    100 and 1000    1000 grid  respectively  Although the second grid has 100 times more vertices  the two grids have the same     k   1      k 2    Hence  when k is far smaller than n  O n    k   1 log     k   1  grows linearly with n  4 3 Computing a k skip graph Recall that the goal of our preprocessing is to produce a k skip graph G      V     E      V   is simply a k skip cover  whose derivation has been clari   ed in Section 4 2  Next we complete the puzzle by explaining the derivation of E     Our discussion concentrates on the subproblem that  given a vertex u     V     how to calculate the set Nk u  of its k skip neighbors  De   nition 4   Once this is done  it is trivial to obtain E   according to De   nition 5  because we only need to add to E   a super edge from each u to every vertex v     Nk u   We will call each vertex of V   a sample from now on  due to the sampling nature of the AS algorithm  A naive approach to acquire Nk u  is to    rst    nd the SP from u to every other sample v     V     and then add v to Nk u  if the path passes no other sample  However  since  V         n k  see Remark 2 of Section 4 1   doing so for all u     V   would incur     n 2  k  time  which is prohibitive for large n  We circumvent the performance pitfall by aiming at a superset Mk u  of Nk u   As will be clear shortly  despite that Mk u  may result in a G   with more edges  it has the advantage of being computable in signi   cantly less time  thus allowing our technique to support gigantic graphs  Mk u  can be conveniently de   ned by recycling the notations of the previous subsection  Let us    rst have the k hop SP tree Tk u  of u  as opposed to the Tk   1 u  in the AS algorithm   Then  Mk u  includes all the samples v    u in Tk u  such that the uto v path in Tk u  does not pass any other sample  For illustration  Figure 6a shows a 3 skip cover V    the black vertices  on the data graph of Figure 5a  To determine M3 u   we    rst extract the 3  hop SP tree T3 u  as in Figure 6b  Then  it becomes clear that u a b e c d f g h 5 5 2 2 2 2 2 2 2 2 2 2 u a b e c d f g h  a  3 hop vicinity of u  b  3 hop SP tree of u Figure 6  Deciding M3 u  M3 u     b  f  g  h   Note that a  for example  is not in M3 u  due to the presence of b that blocks the path from u to a  The lemma below establishes the relationship of Mk u  and Nk u   LE M M A 4  Nk u      Mk u   PRO O F  By De   nition 4  the fact v     Nk u  implies that  i  S P u  v  does not pass any other sample  and  ii  S P u  v  has no more than k edges  Property  ii  indicates that all the vertices of S P u  v  must be in the k hop vicinity of u  and hence  are present in Tk u   It follows that the u to v path in Tk u  is exactly S P u  v   Property  i  further shows that the path cannot contain any other sample  Therefore  by the construction of Mk u   we know v     Mk u   We call Mk u  a super neighbor set of u  After acquiring it  we create a super edge  u  v  in E   from u to each vertex v     Mk u   and set its weight to the length of the u to v path in Tk u   In other words  the super edge represents a path in the original graph  The    nal E   is complete after carrying out the above procedure for all the vertices u     V   Time complexity  After Tk u  is ready  Mk u  can be easily obtained by a single traversal of Tk u  in O   k u   time  where   k u  is the number of the k hop neighbors of u  i e   the number of vertices in Tk u    A straightforward adaptation of the analysis in Section 4 2 shows that the cost of processing all u     V   is O n    k log     k   where     k is the average number of khop neighbors size of the vertices in V   For k far lower than n  O n    k log     k  is linear in n  due to the reasons explained at the end of Section 4 2  5  QUERY ALGORITHM Given vertices s  t in the data graph G  next we will explain how to obtain a k skip SP from s to t using the k skip graph G   precomputed  Section 5 1    rst gives an overview of our algorithm  A part of the algorithm needs to retrieve a traditional SP from G     for which Section 5 2 presents an improved version of the reach method  Finally  Section 5 3 will discuss how to perform a zoomin operation ef   ciently  5 1 High level description Recall that the vertex set of G   is a k skip cover V     The task of k skip SP calculation is simple when both s and t are samples  namely  they belong to V     In this case  we only need to    nd thes t Mk s  Mk t  G  Figure 7  In place sampling of s and t SP from s to t in G     that is  traveling only on the super edges  By the de   nition of G     this SP is guaranteed to be a k skip SP in the original graph G  Let us focus on the situation where neither s nor t is a sample  Our solution is to sample them into G   right away  so that the case can be converted to the previous scenario where s and t are samples  The inclusion of s  t as samples is temporary  after query processing  they will be removed from G     whose size therefore does not change  Incorporation of s  t in G   involves two steps  First  s and t are inserted in V     Second  some super edges are created to re     ect the appearance of s  t  in the same way the existing superedges are computed  That is  given s  similarly for t   we    rst obtain the super neighbor set Mk s  of s  and then add to E   a super edge from s to each u     Mk s   all exactly as described in Section 4 3  This process is illustrated in Figure 7  The analysis of Section 4 3 shows that  the above strategy runs in O   s k  log   s k    t k  log   t k   time  Remember that   s k   the number of k hop neighbors of s  is low when k is small  similarly for   t k    Hence  sampling in place s and t incurs insigni     cant overhead  Let G   s t be the resulting G   with the new super edges  and S P    s  t  be the SP from s to t on G     The rest of our algorithm returns directly S P    s  t   whose correctness is shown in the lemma below  LE M M A 5  S P    s  t  is a k skip SP of S P s  t   PRO O F  Let u be the    rst sample  counting also t  when we walk from s along S P s  t   By the de   nition of k skip SP  S P s  u  has at most k edges  all of which appear in the k hop SP tree of s  Hence  u     Mk s   otherwise  there would be another sample on S P s  u   contradicting the choice of u   which means  s  u  is a super edge in G   s t   Let v be the last sample  counting also s  on S P s  t  before we arrive at t  A similar argument shows that  v t  is also a superedge  The correctness of the lemma then follows from the fact that every super edge has a weight equal to the length of the path it represents  Remark on directed graphs  Let S be the set of super edges on t newly computed for G   s t   In the above  we computed S by    rst    nding the super neighbor set Mk t  of t  and then inserting a super edge  u  t  for each u     Mk t   If G is directed  the derivation of S is slightly different  in the sense that we need to    rst reverse the directions of the edges in G  before proceeding as described earlier  Intuitively  Mk t  should contain the samples that can reach t    directly     without passing another sample   Reversing directions allows us to apply the same algorithm in Section 4 2 to extract Mk t   originally designed to    nd samples reachable from t directly  The reversing incurs no additional execution time  beu v a b c d e 3 2 4 10 5 4 2 Figure 8  Super reach calculation cause a direction change can take place only when the relevant edge is touched by the algorithm  5 2 Reach   Now that we have converted k skip SP computation to    nding S P    s  t  on G    in case s  t are not samples  simply treat G   s t as G      many SP algorithms such as Dijkstra  bi directional  and reach  can be plugged in to obtain S P    s  t   However  as those methods are not designed for our context  they may be improved by taking into account the characteristics of G     Next  we achieve the purpose for reach  As discussed in Section 2 1  reach prunes a vertex v in relaxing an edge  u  v   if the global reach r v  of v is small  compared to the distance that the algorithm has traveled in the forward backward search  see Rule 1   On a k skip graph  using only r v  for pruning may miss plenty of pruning opportunities  The reason is that  a super edge  u  v  implicitly captures a path in the original graph  and hence  can be pruned as long as any vertex on that path has a low global reach  Motivated by this  we formulate a new notion  DE FI N I T I O N 6   SU P E R  R E AC H   Let  u  v  be a super edge in G     and P be the path in G that  u  v  represents  The super reach of  u  v   denoted as sr u  v   equals the minimum h w  of all the vertices w     P  where h w    r w      min  P1    P2    4  where r w  is the global reach of w  and P1  P2  is the path on P from u to w  w to v   To illustrate  assume that the path P captured by a super edge  u  v  is as shown in Figure 8  The number above each vertex is its global reach  e g   r u    3   and suppose for simplicity all the edges have weight 1  To decide  for example  h b   we    rst observe  P1    2 and  P2    4  and then calculate by Equation 4 h b    4     min 2  4    2  After S    h u   h a        h e   h v   is ready  the super reach sr u  v  can be determined as the minimum of S  i e   h a    1  Apparently  sr u  v  can be computed in time linear to the number of vertices in P  i e   at most k  Also  preserving all the super reaches entails small space  as only one extra value per super edge is stored  Next  we propose a new rule to enhance the pruning power of reach  RU L E 2  When the forward search is about to relax a super edge  u  v   prune the edge if sr u  v    lf  u   where lf  u  is the label of u  A similar rule applies to the backward search  More precisely  pruning the super edge  u  v  means that  i  the relaxation is not performed  and  ii  v is not en heaped at this time  it is possible for v to get en heaped due to another later relaxation though   Also note that the pruning happens regardless of the status of v in the other direction  unlike Rule 1 which requires v to have status labeled or unseen in the opposite search   This turns out to be a valuable property that permits the development of a crucial heuristic for maximizing ef   ciency  as discussed shortly  Our algorithm  named reach     for SP computation over G   is identical to bi directional  see Section 2 1   except that Rule 2 ischecked prior to every edge relaxation in an attempt to avoid the relaxation  The following theorem establishes the correctness of the algorithm  TH E O R E M 2  Reach      nds a SP on G   correctly  PRO O F  If the forward or backward search applies Rule 2 to prune a super edge on S P    s  t   we say that a blow occurs  The lemma is obvious if no blow ever happens  so the subsequent discussion considers that there was at least one blow  Our argument proceeds in two steps  First  we will show that there can be only one blow during the execution of reach     This implies that every super edge in S P    s  t  must be eventually relaxed in at least one direction  since two blows are needed to eliminate a super edge from both directions  Equipped with these facts  we will prove the lemma in the second step  Step 1  Without loss of generality  assume that the    rst blow occurred in the forward search  and eliminated super edge  u  v      S P    s  t   It is easy to see that  when the blow happened   i  the forward direction had scanned all the vertices in S P      s  u   and  ii  sr u  v    lf  u   by Rule 2   where lf  u  equals  S P    s  u      S P s  u    Thus  sr u  v     S P s  u     5  Let P be the path in G that  u  v  represents  and w be the vertex in P that minimizes h w   see Equation 4   i e   sr u  v    r w      min  S P u  w     S P w  v     Hence  r w       S P u  w       sr u  v   6  r w       S P w  v       sr u  v    7  Inequalities 5 and 6 lead to r w     S P s  u      S P u  w      S P s  w    By de   nition  r w  is at least min  S P s  w     S P w  t     So we know r w       S P w  t   which  together with Inequality 7  gives  sr u  v       S P w  t         S P w  v      S P v t     8  Inequalities 5 and 8 indicate  S P s  u      S P v t    Due to  i  the way bi directional synchronizes the two directions and  ii  the choice of  u  v   the backward search must have scanned all the vertices on S P v t  before the blow happened  If there was a second blow  either the forward search needed to de heap a vertex in S P v t   or the reverse search needed to deheap a vertex in S P s  u   But both events would have terminated the algorithm immediately  because bi direction ends when the forward backward search de heaps a vertex already scanned in the other direction  Therefore  no second blow could have occurred  Step 2  The analysis of Step 1 shows that  at the moment the blow took place  the status of v was scanned in the backward search  This means that  u had a status of labeled in the backward direction  Consequently  when the forward search de heaped u  right before the blow   as in bi directional  reach   updated     which records the length of the SP found so far  to lf  u    lb u     S P    s  u      S P    u  t      S P    s  t    In other words  reach   found the correct SP successfully  After the forward  or backward  search de heaps a vertex u  our current reach   attempts to prune each out going super edge at u with Rule 2  Hence  the rule has to be applied numerous times if many super edges out of u can be eliminated  This can harm the ef   ciency because each vertex in G   may have a large degree  unlike G  where each vertex   s degree is bounded   the result of which is that we may end up applying the rule a huge number of times during the entire algorithm  The next heuristic allows us to signi   cantly reduce the cost  while still eliminating as many super edges as before  with no increase in the space assumption at all  The idea is to store the outgoing super edges of each vertex u in G   in descending order of their super reaches  After de heaping u in the algorithm  we attempt to prune those edges in the sorted order  The bene   t is that once an edge has been eliminated by Rule 2  we can assert that all the remaining edges can be pruned as well  because all of them must have lower super reaches  than the one pruned   and therefore  will satisfy Rule 2 for sure  The above heuristic is made possible by the fact that  to prune an edge  u  v   Rule 2 does not require checking the status of v  in the search opposite to the one where the pruning happens   If this was not the case  the heuristic would virtually promise no performance gain as checking the status of v takes nearly the same amount of time as applying Rule 2 on  u  v   Remark  It is worth pointing out that  the mechanism behind reach     namely the integration of bi directional and the no statuschecking pruning strategy of Rule 2  actually extends the algorithmic paradigm for SP computation as illustrated in Section 2 1  In retrospect  reach   is an immediate bene   t of this extension  5 3 Zoom in As mentioned in Section 1  the concept of k skip SP is naturally accompanied by a zoom in operator  Given consecutive vertices u  v on a k skip S P    s  t   the operator    nds all the vertices between u and v on the full S P s  t   A naive way to zoom in is to run Dijkstra to extract the SP from u to v  A faster solution applies bi directional  In fact  one can do even better using reach  However  simply executing reach afresh to compute S P u  v  is not likely to outperform bi directional much  This is because the pruning of reach  i e   Rule 1 in Section 2 1  is effective only if the vertex u de heaped in the  for example  forward search has a large label lf  u   This requires the forward search to have come a long way from the source  a situation that will not happen between u and v  because they are at most k vertices apart on their SP  There is a simple remedy to signi   cantly boost the ef   ciency  The main idea is to pretend as if we were running reach to compute S P s  t   as opposed to S P u  v    and that the algorithm had just come to u and v in the forward and backward searches  respectively  We    continue    the forward direction by setting lf  u    min  S P s  u     S P v t     and making u the only vertex in the heap Qf  which implies giving u the status labeled   Note that both  S P s  u   and  S P v t   are available in the k skip S P    s  t  already calculated  Similarly  the backward direction is also continued by setting lb v    lf  u  and creating a heap Qb with only v inside  We then start a normal iteration and proceed as in reach  6  EXPERIMENTS In this section  we empirically evaluate the performance of the proposed solutions  Our experimentation used four spatial networks 2 whose speci   cations are summarized in Table 1  Specifically  NY  BAY  CA NV  and USA contain the road networks in New York city  San Francisco Bay area  California and Nevada combined  and the entire US  respectively  The weight of an edge 2 All datasets can be downloaded from http   www dis uniroma1 it    challenge9 download shtml dataset num  of vertices num  of edges NY 264 346 733 846 BAY 321 270 800 172 CA NV 1 890 815 4 657 742 USA 23 947 347 58 333 344 Table 1  Dataset speci   cations k dataset 4 6 8 10 12 14 16 NY 51  38  31  26  22  20  18  BAY 46  33  26  22  18  16  14  CA NV 46  33  21  19  16  16  15  USA 45  32  25  21  18  16  15   a  Vertex ratio dataset 4 6 8 10 12 14 16 NY 72  65  61  58  56  53  52  BAY 66  57  52  48  45  43  42  CA NV 63  54  49  45  43  41  40  USA 61  51  47  44  41  39  38   b  Edge ratio Table 2  Sizes of k skip graphs equals the travel time on the corresponding road segment  All of our results were obtained on a computer equipped with an Intel Core 2 DUO 3 0Ghz CPU and 2 Giga bytes memory  running Fedora Linux 13  Size of the pre computed structure  Our technique has the feature of demanding only a structure with sub linear size  i e   a kskip graph G      V     E     occupies less space than the underlying road network G    V   E   The    rst set of experiments demonstrates this by proving that G   has fewer vertices and edges than G  Equivalently  if we de   ne the vertex ratio to be  V      V   and the edge ratio to be  E      E   the goal is to show that both ratios are below 1 by a comfortable margin  Moreover  remember that V   is a k skip cover  De   nition 3   Hence  the vertex ratio also re   ects the effectiveness of the AS algorithm in Section 4 2  Table 2a presents the vertex ratios of each dataset as k varies from 4 to 16  Interestingly  we noticed that the ratio roughly equals 2 k in all cases  that is  AS    nds a k skip cover with size about 2 V   k  In the same style  Table 2b shows the corresponding edge ratios  which are also much lower than 1  and decrease with the increase of k  A general observation is that  both ratios tend to be smaller  i e   greater size reduction  when the underlying network is sparser  NY is the densest among all the datasets   Que</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>GBLENDER  Visual Subgraph Query Formulation Meets Query Processing ### Changjiu Jin    Sourav S Bhowmick       Xiaokui Xiao    Byron Choi     Shuigeng Zhou        School of Computer Engineering  Nanyang Technological University  Singapore    Singapore MIT Alliance  Nanyang Technological University  Singapore     Department of Computer Science  Hong Kong Baptist University  Hong Kong     Fudan University  China cjjinjassouravjxkxiao ntu edu sg  choi hkbu edu hk  sgzhou fudan edu cn ABSTRACT Due to the complexity of graph query languages  the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of graph data management tools to wider community  We present a novel HCI  human computer interaction  aware graph query processing paradigm  where instead of processing a query graph after its construction  it interleaves visual query construction and processing to improve system response time  We demonstrate a system called GBLENDER that exploits GUI latency to prune false results and prefetch candidate data graphs by employing a novel action aware indexing scheme and a data structure called spindle shaped graphs  SPIG   We demonstrate various innovative features of GBLENDER and its promising performance in evaluating subgraph containment and similarity queries  Categories and Subject Descriptors H 2 4  Database Management   Systems   Query processing General Terms Algorithms  Experimentation  Performance Keywords Graph Databases  Graph Indexing  Visual Query Formulation  Frequent Subgraphs  Infrequent Subgraphs  Prefetching 1 ### INTRODUCTION Querying graph databases has emerged as an important research problem due to explosive growth of graph structured data in recent years  A wide variety of graph queries in many applications involve the core substructure search problem  also called subgraph containment query   In this problem  given a graph database D and a query graph q  the aim is to    nd all data graphs in D in which q is a subgraph  Note that q is a subgraph of a data graph g 2 D if there exist a subgraph isomorphism from q to g  A common problem for this type of query is that in many occasions there may not exists any g 2 D that matches the query  In this case  it is often useful to    nd out data graphs that    nearly  contain the query graph  which is called the substructure similarity search problem  5   also called subgraph similarity query   Copyright is held by the author owner s   SIGMOD   11  June 12   16  2011  Athens  Greece  ACM 978 1 4503 0661 4 11 06  A number of graph query languages  e g   SPARQL  have been proposed that can be used to formulate subgraph queries  Unfortunately  in many real life domains it is unrealistic to assume that users are pro   cient in expressing graph queries using these languages  The traditional approach to address this query formulation challenge is to build a user friendly visual framework on top of a state of the art graph query processing technique  e g    5    In this traditional visual query processing paradigm  although the    nal query that a user intends to pose is revealed gradually in a step bystep manner during query construction  it is not exploited by the query processor prior to clicking of the Run icon to execute the query  That is  query processing is initiated only after the user has    nished drawing the query  This often results in slower system response time  SRT  1 as the query processor remains idle during the entire query formulation process  2  3   In this demonstration  we present GBLENDER  Graph blender   2  3    a novel HCI aware visual subgraph querying system that challenges the aforementioned traditional paradigm of visual querying by blending the two orthogonal areas of visual graph query formulation and query processing  The key bene   ts of this novel query evaluation paradigm are two fold  First  it ensures that the query processor does not remain idle during visual query formulation  Second  it signi   cantly improves the SRT  2  3   In traditional paradigm  SRT is identical to the time taken to evaluate the entire query  In contrast  in this new paradigm SRT is the time taken to process a part of the query that is yet to be evaluated  if any   At each visual query formulation step taken by the user  GBLENDER employs a novel action aware indexing scheme and a data structure called SPIG  spindle shaped graphs  to ef   ciently compute candidate data graphs that contain  approximately if necessary  the current query fragment by exploiting the GUI latency  It also supports modi   cations to a query gracefully as a user may change her mind or commit mistakes during query construction  In this demonstration  we shall demonstrate various interactive and innovative features of GBLENDER that are necessary to realize the proposed visual query processing paradigm  2  SYSTEM OVERVIEW Figure 2 shows the system architecture of GBLENDER and mainly consists of the following modules  The reader may refer to  2  3  for details related to these modules  The GUI module  Figure 1 a  depicts the screenshot of the visual interface of GBLENDER  A user begins formulating a query by 1 Duration between the time a user presses the Run icon to the time when the user gets the query results 1 5 6 4 3 2 8 7 Panel 2 Panel 1 Panel 3 Panel 4  a  Visual interface   b  The Interaction Viewer module   c  The Modi   cation Handler module  Figure 1  The GBLENDER system  The identi   ers on the edges represent the sequence of visual steps for query formulation   Matching Query GBLENDER GUI Actions Database Graph Query Fragment Verifier Candidates Verificationfree  candidates User Frequent Fragment Extractor Constructor Index Results Visualizer Action Aware Indices Results Generator SPIG  Interaction  Viewer Modification  Handler Figure 2  Architecture of GBLENDER  choosing a database as the query target and creating a new query canvas using Panel 1  The left panel  Panel 2  displays the unique labels of nodes that appear in the dataset in lexicographic order  In the query formulation process  the user chooses labels from Panel 2 for creating the nodes in the query graph  Panel 3 depicts the area for formulating graph queries  A user drags a node that is part of the query from Panel 2 and drops it in Panel 3  Next  she adds another node in the same way  Then  she creates an edge between the added nodes by left and right clicking on them  Additional nodes and edges are added to the query graph by repeating these steps  Finally  the user can execute the query by clicking on the Run icon in Panel 1  Panel 4 displays the query results  The Frequent Fragment Extractor module  This module mines the frequent fragments from the graph database D using an existing frequent graph mining technique  the current version uses gSpan  6    Informally  we use the term fragment  resp  query fragment  to refer to a small subgraph existing in graph databases  resp  query graphs   Given a fragment g which is a subgraph of G  denoted as g   G  and G 2 D  we refer to G as the fragment support graph  FSG  of g  Since each data graph in D is denoted by an unique identi   er  fsgIds g  denotes the set of identi   ers of FSGs of g  A fragment g is frequent in D if its support is no less than   jDj where 0        1 is the minimum support threshold  Otherwise  g is an infrequent fragment  The Action Aware Index Constructor module  The action aware frequent index  A 2 F  is a graph structured index having a memoryresident and a disk resident components  We refer to them as memory based frequent index  MF index  and disk based frequent index  DF index   respectively  Speci   cally  small sized frequent fragments  frequently utilized  are stored in MF index whereas larger frequent fragments  less frequently utilized  reside in DF index  The DF index is an array of fragment clusters  A fragment cluster is a directed graph C    VC  EC  where each node v 2 VC is a frequent fragment f where the size of f  denoted as jfj  is greater than the fragment size threshold     i e   jfj        There is an edge  v       v  2 EC iff f     is a proper subgraph of f  denoted as f       f  and jfj </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>BitPath     Label Order Constrained Reachability in Large Graphs Technical Report ### Medha Atre 1   Vineet Chaoji 2   Mohammed J  Zaki 1   and James A  Hendler 1 1 Dept  of Computer Science  Rensselaer Polytechnic Institute  Troy NY  USA 2 Yahoo  Labs  Bangalore  India ABSTRACT With ever expanding size of the web  sizes of graphs with semantic relationships are growing very rapidly  The semantic relationships are typically presented as edge labeled graphs where nodes are the entities and the edge label signi   es the relationships between the two entities  RDF graphs are an example of such graphs  Pure reachability queries on these graphs do not give any information about the semantic association between two remote entities  On the other hand  the typical size of semantic graphs present on the web has made general regular path query processing infeasible  In this paper we focus on the following graph path query problem     given a source node s  a destination node d and a sequence of ordered edge labels    label seq     does there exist any path between the two nodes which has the edges labels in    label seq     in the given order  on the path  We solve this problem by a combination of graph indexing  and a query processing algorithm based on a divide and conquer procedure and greedy pruning of the search space  We have evaluated our technique on graphs with more than 22 million edges and 6 million nodes     much larger compared to the datasets used in published work on path queries  We compare our approach with optimized DFS  optimized focusedDFS  and bidirectional BFS methods  1 ###  INTRODUCTION More and more data is represented as graphs these days resulting from applications that span social networks  friendof a friend network 1    life sciences  e g  UniProt  3    security and intelligence networks  and even Government data  data gov project 2    For instance  in case of the UniProt protein sequence and annotation data in RDF format  the edge labels may provide important information about the interaction between two proteins or a possible relationship between proteins and genes  Similarly  in social network graphs  the edge labels de   ne a relationship between two persons or entities  e g   a university and a person  Within security and intelligence networks  these semantically rich graphs are analyzed to identify connections between two seemingly unrelated entities  Also with the advent of Semantic Web  Resource Description Framework  RDF  is becoming a de facto standard 1 http   www foaf project org  2 http   www data gov semantic Copyright  c is held by the authors  Dept  of Computer Science  Rensselaer Polytechnic Institute  Troy NY February 28  2011   to present semantic relationships between entities  RDF graphs are edge labeled directed graphs  Due to the prevalence of graph data  path queries have become an important area of research  The approaches taken to evaluate path queries on XML data  e g   XPath  cannot be applied to general graphs  since XML is assumed to be tree structured data for all practical purposes  General regular path queries on any graph ask for all pairs of nodes which have at least one path between them that satis   es the given regular path expression  This problem has been proved to be NP hard  18   On the other hand  pure reachability queries on graphs do not give any information about the semantic relationship between two remote nodes  Hence researchers have started to work on a hybrid problem     constrained reachability  14   The problem of constrained reachability asks if a destination node y is reachable from x through a path which satis   es the constraints in the query  The de   nition of constraints depends on the targeted problem  e g   number of hops  type of edge labels  or a regular path expression  With the growing size of graph datasets  there is also more variety in the semantic relationships between nodes  e g   an RDF representation of DBLP and IMDB graph contains 145 and 222 distinct edge labels over 13 million and 5 million total edges  respectively   This poses two main challenges      1  without knowing the exact structure or schema of the graph  it can be di   cult to pose a precise query between two nodes  It can take several iterations of various combination of edge labels to arrive at the one which fetches the correct answer  and  2  the amount of preprocessing required for answering a path query can limit the scale of the graphs that can be handled by an algorithm  Hence a method which achieves a trade o    between  a  the expressiveness of path query   b  indexing  preprocessing  time  and  c  query execution time  is more desirable  In this work  we address these issues by proposing a light weight method to process label order constrained reachability  LOCR  queries  Given a source node x  a destination node y and an ordered sequence of edge labels    a    b    c    d      an LOCR query represented by Q x  y   a  b  c  d   asks if there exists a path between x and y  such that edge labels a  b  c and d appear somewhere on the path in the given order  The query returns a binary response     Yes or No  The         and          in the label order allow any other labels  0 or more  to appear in between on the path  These symbols are used in accordance with the Perl regular expression  regex  syntax  Note that as per Perl regex     a     means 0 or more    a   s which alters the meaning of our query  hence we use    a        which enforces at least one    a    followed by any other labels allowed by           If there exists a path between x and y with follow ing edge labels    d d a b e f g c h d i j     the answer to the query is YES  On this path given labels a  b  c  d appear in the given order but also have other labels interleaved among them  But if the only path between x and y has edge labels    d a e f b b c     then answer to the query is NO  because a  b  c  d labels do not appear in the given order in the query  Such queries can be utilized in social networks or intelligence services graphs  wherein we are not only interested in the presence of a path between the two entities but also the interconnections along the path  For instance  let us assume we are interested in    nding the trace to a fraudulent organization XYZ through the connections of a political    gure ABC  Speci   cally  we are interested in the query expression      friendOf    hasMembers    worksFor    funds        In a di   erent setting  consider social network graphs such as Facebook and LinkedIn  LinkedIn contains a variety of relationships among people and between people and organizations groups  e g   classmates  colleagues  ex colleagues  worked at  working at  group memberships and so on  Without complete knowledge of the interconnection between various relationships  we may be interested in    nding out if there exists a path between person DEF and an organization in San Francisco satisfying the following path      classmates    workingAt    locatedIn        We believe that LOCR queries achieve a faster response to    exible path queries over large graphs  The problem of LOCR queries addressed in this paper is di   erent from the problem of LCR queries handled by Jin et al in  14   The di   erences are pointed out in Section 2  Our main contributions in this work are  1  Introduction of label order constrained reachability problem in the context of edge labeled graphs  2  A light weight graph indexing scheme based on compressed bit vectors     BitPath     which stores all the successor and predecessor edge sets of nodes in the graph  Successor edges are edges that can be reached from a given node through a depth    rst or breadth     rst traversal from the given node  Similarly  predecessor edges are edges that appear on any path leading to the given node  The proposed indexing scheme requires just two depth    rst passes on the graph  3  A divide and conquer query algorithm using greedypruning strategy for e   cient pruning of the search space using the aforementioned index  The algorithm recursively divides the given query into sub queries to arrive at the answer  4  Experiments on large graphs     more than 22 million edges and 253 distinct edge labels  To the best of our knowledge these graphs are largest amongst the published literature on path queries  2  RELATED WORK Early work by Mendelzon and Wood  18  shows that the general regular path query problem on graphs is NP hard  In general the indexing schemes on graphs for evaluation of path queries can be grouped into the following three categories   1  P indexes  path indexes which typically index every unique path    2  D indexes  16   node index     used for determining in constant time ancestor descendant relationship    3  T indexes  for path pattern such as twig in case of XML   23  5  6  17  9   D indexes and T indexes can only be used in the context of XML graphs as it is non trivial to decide ancestor descendant relationship in constant time on a directed graph which does not assume tree structure  21   Some approaches build P indexes with equivalence classes of nodes based on the incoming or outgoing paths to and from the nodes  19  4  15  8  13   Some other approaches that suggest building P indexes are Index Fabric  11  and APEX index  10   Most of the work for building P indexes has been in the context of XML graphs  except early proposals of P indexes  e g   1 index  2 index by Milo and Suciu  19    Among these approaches Bitmapped Path Index  BPI   12  and BitCube  22  use bitvector based indexes  The BitPath scheme proposed in this paper also uses compressed bit vectors for indexes  While BPI and BitCube use the bitmapped indexes to index paths in the XML graphs  BitPath uses compressed bit vectors to only index the unique edges in the graph  While a vast number of techniques have been proposed for path indexing in XML graphs  those approaches cannot be used for general edge labeled graphs since XML is widely viewed as tree structured data  On the other hand  number of paths in an RDF graph with 10 15 million unique edges over 5 6 million nodes can be of the order of 10 25 or more  It is computationally infeasible to index such a large number of paths due to space and runtime constraints  Recent work by Jin et al   14   proposes a novel approach for evaluating label constrained reachability  LCR  queries  Given a source node x and destination node y and a set of edge labels S  an LCR query checks the existence of at least one path between x and y such that each edge label on that path is in S  No labels other than those in S can appear on that path  Their solution uses either building complete transitive closure or by utilizing approximate maximal spanning tree on the given graph  Approximate maximal spanning tree is found by recursively constructing di   erent approximate spanning trees  The construction of an approximate maximal spanning tree is to reduce the index space  The LCR query processing algorithm utilizes indexes built on top of the approximate maximal spanning tree  interval labeling  and kd trees  range search tree   The computational complexity of their index building procedure using approximate maximal spanning tree construction is O n V   E     P     P   2    n n0  E     V  log V      and using the generalized transitive closure M is O  V   2 2 2  P      where P is the set of unique edge labels in the graph  and n and n0 are the sample sizes for approximate spanning tree  For example  with 253 labels  as for some of the graphs in our study  their complexity would be  253 126   or 2 2   253 which is prohibitive  For LCR queries  the set of unique edge labels appearing on the satisfying path is a subset of the labels speci   ed in the query  The satisfying path can have these edge labels in any order  On the other hand  LOCR queries handled by BitPath allow any other edge labels to appear on the path as long as the given order of edge labels in the query is satis   ed  Our algorithm tries to keep the index construction light weight by a  not indexing all paths in the graph  b  utilizes compressed bit vectors for index representation thereby optimizing the index size  and c  using a divideand conquer approach for recursively splitting the query into sub queries  For splitting the query into two sub queries it utilizes a greedy pruning heuristic based on the BitPath indexes  3  BITPATH INDEXES Let G    V  E  L  f  be a directed edge labeled graph  where V is the set of nodes  E is the set of edges  L is the   the thirteenth floor  the matrix  movie  1999   the matrix reloaded 1 2 3 4 5 6 Edge List EID  the thirteenth floor    releasedIn     1999   the thirteenth floor    similar to        the matrix  the matrix                 similar to        the matrix reloaded  the matrix                 releasedIn       1999   the matrix                 rdf type          movie  the thirteenth floor    rdf type          movie 1 2 3 4 5 6  the thirteenth floor  1999  111111  the matrix  the matrix reloaded  movie 000000 Node N   SUCC   E N   PRED   E 000000 000000 001110 000000 010011 011000 010000 110100 100100 011000 Edge Label EL   ID  releasedIn  similar to rdf type 000011  similar to  similar to  releasedIn  releasedIn rdf type rdf type Figure 1  BitPath indexes set of unique edge labels  and f is a edge labeling function f   E     L  for each edge  vi  vj    Before building the BitPath indexes  we transform the given graph into a directed acyclic graph by collapsing the strongly connected components  SCCs   Since the LOCR queries specify constraints on the edge labels on the path  it is imperative to capture the edges  and their labels  that get collapsed in a SCC  Following steps outline the process  1  Identify SCCs in the graph using Tarjan   s algorithm3   2  Let z be a new node representing the SCC    C     For each edge e in C that gets removed as a result of collapsing the SCC  add a self edge  z  z  with label f e  in the graph  The purpose of adding self edges with same edge labels is to keep track of the edge labels that appear in a given SCC  These edges help in determining paths going through an SCC without having to traverse the entire SCC subgraph at query time  A label order constrained reachability  LOCR  query requires the knowledge of relative order of edge labels occurring in a given path  Basic methods to answer these queries can run a DFS  depth    rst search  or BFS  breadth    rstsearch  traversal on the subgraph below the source node while examining each path for label ordering  These approaches are acceptable on small graphs and when the query has a valid answer  But as shown in our evaluation  when these properties are not satis   ed  the query performance suffers with such baseline approaches  Another way to answer these type of queries can be by storing each unique path in the graph separately indexed by its source and destination node  But as pointed out in Section 2  it is computationally infeasible to index paths of the order of 10 25 or more due to time and space constraints  In view of the above mentioned challenges  we solve the LOCR problem by creating 4 types of indices on the graph  and designing a query answering algorithm  based on a combination of greedy pruning and a divide and conquer heuristic  The 4 types of indices are as follows  1  EID  edge to ID   For each edge e     E is mapped to a unique integer ID  For instance  for the graph shown in Figure 1  edge   the matrix  movie  with label rdf type is mapped to ID 5  3 http   en wikipedia org wiki Tarjan   s strongly connected  components algorithm 2  N SUCC E  node   s successor edges   For each node  we index IDs of all the successor edges  i e   edges that will get visited if we traverse the subgraph under the given node  The self edges added to the graph as a result of collapsing an SCC can be handled trivially by examining the head and tail of the given edge  Following the example in Figure 1  node  the thirteenth    oor will have edge IDs 1  2  3  4  5  6 in its successor list  3  N PRED E  node   s predecessor edges   Similarly  for each node we index the predecessor edges  i e   edges that will get visited if we make a backward traversal on the entire subgraph above the given node  In Figure 1  node  movie will have edge IDs 2  5  6 in its predecessor list  4  EL ID  edge label to edge ID   For each unique edge label l     L  we index IDs of all the edges in E which have edge label l  In Figure 1  edge label rdf type will have IDs 5 and 6 in its list  In practice  we use bit vectors of length  E   total number of edges in the graph   for building N SUCC E  N PREDE  and EL ID indexes  Each bit position in the bit vector corresponds to the unique ID assigned to an edge as per the EID index  For the node  the thirteenth    oor in Figure 1  its N SUCC E bit vector index will be    111111     for the node  movie its N PRED E index will be    010011     and the ELID index of edge label rdf type will be    000011     We apply run length encoding on N SUCC E and N PRED E indices of each node depending on the compression ratio  Typically  run length encoding delivers high compression ratio if there are large gaps in the bitvector  A bit vector with large gaps is one where a lot of 0s or 1s appear together  For instance  a bit vector    111111000001111111    has large gaps as opposed    11001010101010    which has smaller gaps  The structure of the graph and the ordering of edges impacts the gaps in these indices  Typically the unique edge labels in the graph are much fewer compared to the number of nodes  Hence in an EL ID index of an edge label  there are many more interleaved 0s and 1s as compared to an N SUCC E or N PRED E index  Since the EL ID index typically has many small gaps  we do not apply run length encoding on the EL ID index  Note that at the time of querying we do not uncompress any compressed indices  All the algorithms are implemented to perform bitwise operations on both the gap compressed indices as well as non compressed indices  3 1 BitPath Index Creation AlgorithmWe create EID  N SUCC E  and EL ID indexes in one DFS pass over the DAG generated after collapsing the SCCs as outlined in Section 3  N PRED E index is created by making one backward DFS pass on the graph and using the EID mapping generated in the    rst pass  In the    rst pass  we    nd all the nodes with 0 in degree  the root nodes   Starting with the    rst root node  we make a DFS pass over the entire subgraph below it  and do the same for other roots  Starting with ID 1  every new edge encountered in the DFS pass is given a new ID sequentially  For instance  in Figure 1  starting at root node  the thirteenth    oor  we visit edge   the thirteenth    oor    1999     with label  releasedIn    rst and assign ID    1    to it  Next when we visit edge   the thirteenth    oor     the matrix     with label  similar to it is given ID    2     Since this is a DFS traversal  we continue traversing the subgraph below node  the matrix  and sequentially assign ID    3    to next edge visited  While assigning IDs to edges  we simultaneously maintain N SUCC E list for each node encountered  Every time we visit a new node  it is pushed on a DFS node stack  This stack keeps track of all the nodes that were visited on a path from root node to the current node  While exploring an unvisited node  we add all the outgoing edges of that node in the N SUCC E list of each node in the DFS stack  The node is popped out of the stack when it is marked as    visited     i e   when the entire subgraph below the node has been traversed  If a    visited    node is encountered through a di   erent path  instead of exploring it again  we simply add all the edge IDs in its N SUCC E list to the N SUCC E list of all the nodes in the DFS node stack  Once a node is marked    visited     we build a bit vector of N SUCC E index  Each bit position marked 1 in this bitvector corresponds to the EIDs of the node   s successor edges  Since IDs to the edges were assigned as they are visited  while constructing the EID index   this scheme generates N SUCC E bit vectors with large gaps for most nodes  We make use of this fact to apply run length encoding on NSUCC E bitvectors  Note that this was a heuristic observed for many real life graphs  and it is possible to generate a pathological graph where run length encoding does not fetch the desired bene   t  Since for a typical RDF or any edge labeled graph   L  is much smaller than  V    EL ID index is typically smaller than EID and N SUCC E indexes  and even N PRED E discussed below   At the end of the    rst DFS pass  we get EID  NSUCC E  and EL ID indexes  In the second pass  we start from the leaf nodes     nodes with 0 out degree     and make a backward DFS traversal on the graph  N PRED E index for each node is built in the same way as the N SUCC E index  The only di   erence is that in the second pass  we utilize the EID index that was populated in the    rst pass  Hence every time we encounter an edge  we simply look up its ID in the EID index and use it to construct the N PRED E index  When a node is marked    backward visited    in this pass  we generate its bitvector N PRED E index in the same manner as N SUCC E index  While building N PRED E indexes  sometimes we encounter bit vectors with a lot of small gaps  Hence we use following heuristic     if the gap compressed size of the bit vector is larger than half of the size of the corresponding uncompressed version of the bit vector  then store the bitvector in uncompressed form  At the end of the second pass  all the indexes are written to the disk  In the next section  we describe the LOCR query processing algorithm using these 4 indexes  4  BITPATH QUERY ALGORITHM In an LOCR query  x  y   a  b  c    l       we want to    nd if there exists any path between a source x and destination y  such that labels  a  b  c    l  appear on that path in the given order  other edge labels can appear on this path as well  ref  Section 1   The           denotes that there can be any number of labels speci   ed in the order constraint  The evaluation of an LOCR query can be broken down into the following steps  1  Is y is reachable from x  2  If the earlier condition is satis   ed  we want to    nd if there exists any path between x and y  such that all of the labels a  b  c     l appear somewhere on that path in that order  3  Suppose we know that there exist some path where label b appears somewhere on the path  We want to    nd an edge  m  n  with label b on that path  4  Next  we want to    nd if there exists a path between x and m  such that label a appears somewhere on it  5  If we    nd that such a path exists between x and m  we want to    nd if there exists a path between n and y  such that labels c     l appear somewhere on it in the given order  As can be seen by the steps outlined above  we recursively divided the original query into smaller sub queries for evaluation  We make use of the 4 indexes     N SUCC E  NPRED E  EL ID  and EID     to evaluate the sub queries at every step  The N SUCC E index of a node gives us all the edges that can be reached from that node and N PRED E index of a node gives us the edges that can eventually lead to that node  So if the intersection of N SUCC E index of node x and N PRED E index of node y is non empty  i e   if they have at least one edge common between them  node y is reachable from x  This answers the    rst point above  If  N SUCC E x      N P RED E y      EL ID b   6      i e   the intersection of successor index of x  predecessor index of y and EL ID index of label b is non empty  there is at least one path from x to y  where edge label b appears somewhere on the path  This solves our second step  N SUCC E x   N PRED E y  and EL ID b  are bit vectors and their intersections requires two bitwise AND operations  but this operation costs O  E       O  V    for sparse graphs  Let the result of this bitwise AND operation be INTSECT  Position of a 1 bit in INTSECT bit vector gives EID of an edge with label b  A reverse look up in the EID index gives us an edge  say  m  n   with label b  This satis   es the third step above  If  N SUCC E x    N P RED E m    EL ID a   6      it means that there exists at least one path between x and m where edge label a appears somewhere on the path  This addresses the forth step  If we put the earlier result and this result together  it tells us that there exists at least one path between x and y  such that edge label a appears somewhere before label b  Recursively  we solve our    fth step to    nd if there exists any path between n and y such that edge labels c   l appears somewhere on it  In the example above  we chose to split the edge label order    a b c   l    on label b    rst for the ease of understanding  But for further optimization  we can choose the split point depending on the selectivity of the intersection of N SUCC x    a  b  c  d  e    y Greedy   pruning Split on    c    Split on    b    x    a    s p    e    y Split on    d    Greedy   pruning x    a  b    k l    d  e    y Greedy   pruning Figure 2  Divide and conquer with Greedy pruning E x   N PRED E y   and EL ID for each label in the query 4   We will always choose to split on the edge label with high selectivity  This is the greedy pruning step  The divide and conquer strategy along with greedy pruning is depicted in Figure 2 with a di   erent example of label order  a  b  c  d  e  between nodes x and y  A    Yes    answer at each sub query node is reported to its parent query node and the query stops its evaluation when a    Yes    answer reaches the root of this query tree or when all the candidate edges are exhausted  Figure 2 shows that at the very beginning of the query  edge label c has highest selectivity among all the labels  hence the label order was split on c  Here INTSECT    N SUCC E x      N P RED E y      EL ID c    Edge  k  l  with label c was picked    rst from INTSECT and the query is split as      1  does there exist a path between x and k such that edge labels  a  b  appear somewhere on the path in the given order  and  2  does there exist a path between l and y such that edge labels  d  e  appear somewhere on the path in the given order  If the answer to any of the two sub queries is    No     we pick the second edge  say  s  t   with label c from INTSECT and continue the evaluation until we    nd a    Yes    answer or we exhaust all edges in INTSECT  The greedy pruning strategy     used to choose the    splitpoint        is outlined below  Algorithm 1 greedy pruning x  y  label seq  1  min label   0 2  min edges       3  for each l in label seq do 4  intsect   N SUCC E x      N P RED E y      EL ID l  5  if  intsect     min edges  then 6  min edges   intsect 7  min label   l 8  return pair min edges  min label  Algorithm 1 outlines the greedy pruning strategy  label seq is the order of edge labels in the query  If at any given subquery node in the divide and conquer tree  see Figure 2  there are more than one edge labels in the given label seq  the greedy pruning strategy takes intersection of  N SUCC E x      N P RED E y      EL ID l   for each label l  lines 3   7 in Algorithm 1    The edge label  min label  which generates the smallest intersection set  min edges  are returned by the greedy pruning method  Line 8 in Algorithm 1   and are subsequently used to partition the initial query into two sub queries  For a typical real life graph like RDF  there are few distinct edge labels  L is very small  as compared to the total number of edges  For instance  the UniProt RDF graph of 22 million edges has only 91 distinct edge labels  Moreover  the distribution of these edge labels is not uniform  a large number of edges have few distinct edge labels  In the UniProt dataset  the edge label    rdf type    appears on 4 Selectivity is inversely proportional to the number of edges in the intersection     high selectivity means fewer edges and low selectivity means more edges     5 million edges  about 10 edge labels appear on 1 million edges each and about 20 edge labels occupy    100 000  200 000 edges each  We exploit this skewed label distribution to e   ectively prune the potentially large search space of edges  Although the skewed edge distribution holds true for most real life graphs  it is possible to synthetically build graphs where there is one root node  one sink node and a set of edge labels that follow uniform distribution  For such a graph  if we are given the root and sink nodes and a list of edge labels in a query  the greedy selection will not be able to achieve any pruning because every edge will be in the N SUCC E index of source node and N PRED E index of the sink node  Now  Algorithm 2 describes the divide andconquer strategy  Algorithm 2 divide and conquer x  y  label seq  1  res   FAIL 2  if topo order y    topo order x    label seq size   then 3  return FAIL 4  5  pair min edges  min label    greedy pruning x  y  label seq  6  7  if min edges       then 8  return FAIL 9  if label seq size      1 then 10  return SUCCESS 11  12  lseq1   get seq label seq begin    min label 1   13  lseq2   get seq min label 1  label seq end     14  15  for each eid in min edges do 16  edge e   eid to edge eid     for edge  k l   k is the tail  and l is the head of the edge 17  res   divide and conquer x  e tail  lseq1  18  if res    SUCCESS then 19  res   divide and conquer e head  y  lseq2  20  if res    SUCCESS then 21  break 22  return res For the sake of illustration  let us assume that the LOCR query checks if the label order  a  b  c  d  e  is satis   ed between source node x and destination node y  To evaluate this query  Algorithm 2  is called on x and y nodes with label seq containing all the edge labels a  b  c  d  e   Although this example considers a sequence for length    ve  the algorithm is invariant to the length of the query  Also  an edge label can be repeated any number of times in the sequence  e g    a  b  a  c  a   or  a  a  a  a  b  b   The label seq can as well be empty  in which case it simply translates to a reachability query  If di   erence between the topological order of x and y is lesser than the length of label seq  it means that there is no path from x to y of length  label seq  or more  Divideand conquer uses this simple heuristic to preclude exploring paths shorter than  label seq   Line 2 in Algorithm 2    Using greedy pruning  line 5 in Algorithm 2   we    rst get the minimal set of EIDs  min edges  such that they are common between the successor edges of x  predecessor edges of y  and also contain a label min label     label seq  If the minimum set of edges for any label l in label seq is empty  it clearly implies that nodes x and y do not have any path with label l between them  In this case  we stop exploring the paths further and return  line 8 in Algorithm 2    Otherwise  we split the label seq into 2 parts  such that  if the min label is c and the original label seq is  a  b  c  d  e   we split it into  a  b  and  d  e   lines 12  13 in Algorithm 2    For each edge e in min edges  f e    c  Let e be an edge over nodes  k  l  such that e tail   k  e head   l  line 16   Now the original query is divided into 2 parts      1  is there any path from node x to e tail such that it satis   es a label order  a b     2  is there any path from node e head to y such that it satis   es a label order  d  e   If the label seq is split over a or e  one of the sub queries will have an empty label seq  A sub query with empty label seq is just a reachability query  Such sub query is skipped as the reachability of the nodes is previously evaluated in the greedy pruning step  With respect to the runtime complexity  divide and conquer algorithm   s in memory program stack size is at most the size of original label sequence of the query  If we assume a uniform distribution of edge labels in the graph  in the worst case the divide and conquer algorithm can get called as many as  2     E   L   times  Lines 15   21 in Algorithm 2   for a given call to the divide and conquer method  Hence the worst case complexity of the entire runtime of the algorithm is O   E   L    label seq        since the BitPath algorithm will be called at most  E   L  times for each label in the sequence  This is true for a query on a graph with one super root and one super sink node  where the root and sink nodes are the target nodes in the query  But as for most real life graphs  the edge labels follow a non uniform distribution and rarely there is a single root and sink node in the graph  The complexity of greedy pruning is O  label seq     E    But since we use bit vectors for storing N SUCC E  N PRED E  and ELID indexes  for all practical purposes greedy pruning does not imply traversing the entire graph  label seq  times  4 1 Handling Nodes and Paths in SCC Since every node in an SCC is reachable to every other node in the same SCC  it is di   cult to decide the start and end of a path going through SCC and the order of the edgelabels on that path     which is required in LOCR query processing  Hence presently we process the paths going through SCCs by simply checking the self edges introduced while merging the SCCs  ref  Section 3   Let an LOCR query be  x  y   a  b  c  d  e    such that x and y are part of the same SCC in the original graph  They are represented by node z in the graph obtained by merging SCCs  For such a query  we simply check if there exist 5 self edges  z  z  with labels a  b  c  d  and e  As another example  a query with label seq  a  a  a  b  c  can be satis   ed by traversing the edge  z  z  labeled    a    thrice and checking other self edges  z  z  for labels    b    and    c     If either x or y are part of di   erent SCCs  say t and u respectively  we evaluate the original LOCR query as  t  u   a  b  c  d  e    We use this same technique for BitPath as well as the baseline methods used for performance evaluation  hence for all practical purposes we have sampled queries for experimental evaluation on the directed acyclic graph  with self edges  obtained after merging the SCCs  5  EVALUATION BitPath indexing and query processing algorithm is developed in C and compiled using g    v4 4  with  O3 optimization    ag  We used an OpenSUSE 11 2 machine with Intel Xenon X5650 2 67GHz processor  48 GB RAM with 64 bit Linux kernel 2 6 31 5 for our experiments  Although we used a desktop with 48 GB memory  the BitPath index size for the datasets is much smaller than that  refer Section 5 5   5 1 Competitive Methods As outlined in Section 2  path indexing approaches suggested in the context of XML XPath query processing cannot be used to evaluate LOCR queries on general graphs  RDF graphs can be represented in XML format 5   hence we explored the options of evaluating LOCR queries on XML representation of an RDF graph  This requires translating the given LOCR query into equivalent XML path query  A faithful translation of an LOCR query into equivalent query on the XML graph of RDF does not represent a path query  It often has to be processed using iterative join of two or more tree patterns  twig  queries  Native XQuery speci   cations do not support recursive joins of tree pattern queries  where the number of recursions are not known at the query time  An example of this is given in Appendix A  Also the BitPath method of indexing and processing LOCR queries can be applied to any other edge labeled directed graph  But any edge labeled directed graph     which does not satisfy RDF constraints     cannot be represented as an XML graph  Hence for our evaluation we used optimized versions of DFS and BFS as our baseline methods for comparative performance  1  Optimized DFS  DFS   Given an LOCR query between nodes x  source  and y  destination   we check the out degree of x and in degree of y  If the out degree of x is lesser than in degree of y  we start the DFS walk from x and continue until y is reached and the given path satis   es LOCR label seq  If y   s in degree is lesser  we start a reverseDFS walk from y with reversed order of labels in the query and continue up to x  This method is referred to as    DFS    in the rest of the text  2  Optimized Focused DFS  F DFS   This method is same as optimized DFS  but additionally at every node we check the reachability of the destination node y   or reachability of node x if we perform reverse DFS  We check reachability by using the intersection of N SUCC E and N PREDE BitPath indices  If y is not reachable from the given node n  N SUCC E n      N P RED E y        This method is further enhanced as follows  Maintain a reachability array  The very    rst time node n is explored  update reachability n  to note if y is reachable from n  If node n is visited again through a di   erent path  next time just look up reachability n  to decide if the paths below n should be explored or discontinued  3  Optimized Bidirectional BFS  B BFS   In bidirectional BFS  we traverse down the subgraph below x and above y one step at a time  maintaining forward and reverse BFS queues  Each node enqueued in the BFS queue has its own label seqn associated with it  which tells which labels in the original sequence have been seen on a path to node n  Similarly we maintain a reverse label seqn for nodes in the reverse BFS queue  At every iteration we perform an intersection of nodes in the forward and reverse BFS queues  If there is a common node in two BFS queues  we join their label seq to check the satis   ability of the query  We further optimize bidirectional BFS as follows  if a node in the BFS queue is reached through another    better    path 6 before being taken out of the BFS queue  that node   s label seqn is updated with the label seq seen on the    better    path  In our experience  the optimized bidirectional BFS performed better than the naive bidirectional BFS by an order of magnitude  For simplicity the optimizedbidirectional BFS method is referred to as    B BFS    in the rest of the text  5 2 Datasets and Queries We used 2 real RDF datasets  RDFized DBLP dataset by LSDIS lab     SwetoDBLP  2  and a smaller subset of UniProt 5 http   www w3 org TR rdf syntax grammar  6 A path is    better    if it has seen more labels in the LOCR label order Table 1  Datasets Characteristics  Edges  Nodes  Edge labels Max indeg Max outdeg Avg in outdeg Largest depth SCCs R Mat 14 951 226 4 085 180 253 19 13 053 3 65 12 0 UniProt 22 589 927 6 634 185 91 800 127 1046 3 35 10 423 SwetoDBLP 13 378 152 5 458 220 145 907 731 9245 2 44 66 146  0  0 001  0 002  0 003  0 004  0 005  0 006  0 007  0 008  0 009  2 4 6 8 10 12 Runtime  sec  Query length RMat BitPath   ve   0  1  2  3  4  5  6  7  8  2 4 6 8 10 Runtime  sec  Query length RMat BBFS   ve   0  0 0005  0 001  0 0015  0 002  0 0025  0 003  0 0035  0 004  2 4 6 8 10 Runtime  sec  Query length RMat FDFS   ve   0  0 0005  0 001  0 0015  0 002  0 0025  0 003  0 0035  0 004  2 4 6 8 10 Runtime  sec  Query length RMat DFS   ve   0  5e 05  0 0001  0 00015  0 0002  2 4 6 8 10 Runtime  sec  Query length RMat BitPath   ve   0  20  40  60  80  100  120  140  2 4 6 8 10 Runtime  sec  Query length RMat BBFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  2 4 6 8 10 Runtime  sec  Query length RMat FDFS   ve   0  0 005  0 01  0 015  0 02  0 025  0 03  0 035  2 4 6 8 10 Runtime  sec  Query length RMat DFS   ve   0  0 0001  0 0002  0 0003  0 0004  0 0005  0 0006  0 0007  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP BitPath   ve   0  0 5  1  1 5  2  2 5  3  3 5  4  4 5  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP BBFS   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP FDFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  0 07  0 08  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP DFS   ve   0  5e 05  0 0001  0 00015  0 0002  0 00025  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP BitPath   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP BBFS   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP FDFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  0 07  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP DFS   ve   0  0 02  0 04  0 06  0 08  0 1  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BitPath   ve   0  0 5  1  1 5  2  2 5  3  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BBFS   ve   0  0 002  0 004  0 006  0 008  0 01  0 012  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BitPath   ve   0  2  4  6  8  10  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BBFS   ve  Figure 3  Mean runtime with standard deviation for varying query size  Row 1  Positive queries for R Mat  Row 2  Negative queries for R Mat  Ro
</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management"/>
        <doc>Simple Data Storage and Manipulation For Scientists Charles Noneman ### Leonard McMillan Abstract When choosing a data management system  scientists are forced to select from using either spreadsheets or a relational database  However  neither of these systems is both  exible and powerful enough to ful ll all of a scientist s needs  We have developed a system that combines the simplicity of table creation and modi cation of a classical spreadsheet with the querying power of a full relational database  This system allows users to simply and e ciently input  query  and transform their data ###  1 Introduction Scientists are faced with a di cult trade o  regard  ing data management  This trade o  is between the two most common approaches to data manage  ment  spreadsheets and databases  Spreadsheets are very  exible  but lack the querying capabilities of a database  A database has powerful querying mecha  nisms and consistency guarantees  but is bound by a rigid schema  A spreadsheet is a group of cells organized on a grid  Each cell can contain data  typically in the form of strings  numbers  dates  and formulas  Cells are addressed by their column number  in practice represent by a letter  and their row number  Scien  tists  for the most part  are familiar with spreadsheets and are comfortable using them to store data and to do basic analysis  Typically no data management system is in place at the start of a project  so members of a group will each create ad hoc spreadsheets to store the data for their portion of the experiment  This leads to many di culties and complications as the project continues and as the team tries to analyse their data  One risk of using spreadsheets is the possibility of data loss or corruption  If one of the  les is accidentally deleted or incorrect data is written  there is no way to re  cover the information  Another problem is access to the spreadsheets  Teams often email sheets between members  or put the sheets on shared drives  The time and coordination required for this form of com  munication  combined with the lack of atomic trans  actions  means that data is only shared at the end of the experiment and that the integrity of the data is likely compromised  A better system would al  low researchers to query and reorganise data as it is generated so preliminary analysis can begin imme  diately and inconsistencies and missing data can be found and resolved before they are impossible to cor  rect  Another problem with the many spreadsheets approach is that someone must merge and factor the sheets by hand  This is a time consuming and error prone process  In addition to errors generated by lost columns or copying mistakes  the sheets themselves contain implicit information  such as batch numbers or the dates of experiments  which is destroyed in the combining process  Even if all of these issues are ad  dressed by a data management process and discipline on the part of the users  such as the process described in  6   the lack of proper joining and querying mech  anisms in spreadsheet systems means that the data will need to be imported into a database eventually  A relational database is collection of relations  commonly called tables  A relation has a  xed set of attributes and domain descriptions called a schema  A relation also has a set of tuples which are used to store data and all share the same attributes  At  tributes are commonly called columns and tuples are commonly called rows  When designing a relational database  one strives to represent the data in a nor  mal form  Normal form for relational data is a group of schemas where no data is replicated  This is im  1portant for e ciency and crucial for maintaining data consistency  Most relational databases systems are accessed via a query language  of which Structured Query Language  or SQL  is the most common  Like the spreadsheet approach to data storage  an approach based on a relational database system also does not  t the needs of scientists  Databases are more di cult to use than spreadsheets  Users lack the training in database theory to generate schemas in normal form and to write queries in SQL  This results in a need to hire someone to handle the database s design  This person will  rst have to de  sign the database schemas and then provide an in  terface for the researchers to use  This is the normal approach to database design and is called schema   rst 7   since the schema is completely or mostly de  signed before any data is entered  Schema  rst design assumes that the nature of the data is known a priori  but this is not often the case for real world projects  In the event that the researchers decide to run a new experiment  or simply want to add a new measure  ment to an existing experiment  the database schema no longer meets their needs  Users will need to re  purpose an existing attribute to  t their new needs or they will be forced to contact the database admin  istrators to perform the appropriate change in the schema  Additionally  if the researchers want to look at the data in a new way  they must have the adminis  trators write a new query before they can begin their new analysis  In practice there will be enough of these changes to the nature of the project that database ad  ministrators must be retained for the entirety of the project  Also  these changes take time  so users may revert to spreadsheets for everyday use  bringing all of the problems associated with spreadsheets into the system  Actively maintaining the schema in this way is both ine cient and expensive  Since the schema often cannot be determined be  fore data collection has started and updating the schema is challenging and fails to keep up with users  needs  it may be tempting to construct a database after the data is collected  This approach is called schema later  3   By using spreadsheets during data collection  researchers have the  exibility they want  and by then loading this data into a database  they will get the querying power of a relational database  The failing of this approach is the enormous task of converting the data into a form suitable for a database  There are many issues with data stored in a spread  sheet and many of them are di cult to resolve  Spreadsheet users will often put data of incorrect type into columns  For example   NA  into a column of real numbers  Dealing with these situations must cause either an over general typing strategy  such as making everything strings  or result in lost data  Users will include decorative information into a sheet such as a title or empty rows and columns  These must be removed before an import can occur  Users will include statistics in the bottom rows of a table  such as sums and averages  which must be removed  Since users lack a joining mechanism  they will create separate columns for a repeated measurement  such as  Weight 4 8 2010  and  Weight 4 22 2010   in  stead of putting these into a new table  This type of data must be transformed by hand or by writing code to parse the sheet  convert the data  and save the resulting sheets  Many of the design issues in spreadsheets come from one to many and many to  many relationships  such as the repeated measure  ment issue  These relationships are di cult to create in a spreadsheet and  in the many to many case  are di cult for users to understand since a table must represent a relationship and not just an entity  The process of converting from spreadsheet to database can easily become more work than actively maintain  ing the schema of a database  There are some partial solutions to these problems  Online spreadsheets facilitate sharing of information between users and provide access to old versions of the spreadsheet  but combining and querying sheets remains a challenge  Visual databases ease the cre  ation of queries 1   but still require an understanding databases and do little to aid in database design  In particular a user who has no database training is un  likely to store data in a normal form  even with visual tools  Additionally  schema rigidity is not solved by visual tools  Database usability is an active area of research 3   In this paper  we provide a description of a system  called S3  that is as simple to use as a spreadsheet  but also has the full power of a relational database  2This system allows each researcher to produce ta  bles and run queries that are easy to create  use  and change  By giving researchers the power man  age their own data  they can easily keep the schema and queries up to date and applicable to their cur  rent needs  Importantly  the system is always usable as a relational database with a changing schema  in  cluding the full expressive power of SQL  This form of technique is called schema during 7   The system is accessed using a web interface  Fig  ures 1 and 2  Having the database online ensures that users are always able to access their and other s information  Additionally  data can be entered di  rectly into the database  which avoids a typical time consuming and error prone approach of writing data by hand  entering it into a spreadsheet  merging the spreadsheet with other spreadsheets  transforming the data  and loading the  nal sheet into a database  Despite the  exibility in S3  users are encouraged and helped to convert their data into a more database appropriate form  For example  if the type of a col  umn is set to an integer  any cells that are not in  tegers are highlighted in red  By having an e cient and easy to use querying mechanism  users are more open to storing data in separate tables  unlike their tendency to want to force data into one large sum  mary sheet  By including statistics for columns and allowing aggregate functions in queries  users will not need to clutter the sheet by  lling cells with that type of summary information  The system is designed with a cell centric ap  proach  which allows for much of the  exibility seen in spreadsheets  This approach also enables the sys  tem to maintain cell history  which allows for users to recover historical information and  nd when it was entered  Since all of the data is stored in a relational database  the full power of the existing joining and querying system is always available  S3 provides a  gentle slope  approach to database design  It enables the transitions from data col  lection  to organization  to reorganization and to the transparent creation of a full  edged relational database  2 Related Work Researchers have been developing user friendly querying mechanisms for nearly as long as there have been databases  Query by Example  12  was one of the  rst  It involved users  lling in example values into columns to serve as placeholders for the values in the tuples that the database would return  Visual Query Engines  of which  1  is an example  show a digram of the schema of the relations and draw lines between the relations to represent foreign keys  These systems still require the user to create the query in SQL  or may support simple queries using a point  and click interface  Liu et al   4  describe an iter  ative querying mechanism which is displayed like a spreadsheet  Combinations of spreadsheets and databases have been proposed before  Tyszkiewicz  9  describes a method for translating SQL statements into equa  tions in a spreadsheet  Query by Excel  11  translates spreadsheets into a database  The data from cells is stored similarly to the way that S3 stores cells and the functions are translated using an extended SQL syntax described in  10   While previous work has focused on well designed databases in normal form  S3 is designed to address data input  querying  and manipulation on schema that may be poorly designed  Although S3 includes an easy to use querying interface and combines the concepts of spreadsheets and databases  these are only pieces of the whole  It provides a dynamic  yet fully functional  schema during the entirety of the data collection process  Joins  by default  are done using a method that allows scientists to easily access all relevant entities without having to consider the order of the joins  This means that data is simple to query and that it is easy to  nd where data is missing or incomplete  Finally  database refactoring is simple and is often done transparently for the user  3 Model First we will describe S3 from the user s perspec  tive  This will demonstrate the capabilities of the system and further clarify the problems being ad  3Figure 1  The hub page  Here  users can access and manage tables  enumerations  and reports  Figure 2  A typical table containing actual data  Users edit data as they would in a spreadsheet  Attribute information is edited in the sidebar  4dressed  The tasks performed by users are  adding data  querying that data  and manipulating the data to be closer to normal form  3 1 Adding Data A relational database requires the creation of a schema before data can be entered into a table  A schema is a list of the names of all of the columns and the data type number  text  date  etc  allowed in each column  Additionally column names must be unique within the table  Although these require  ments are good practice  in S3 this information is not required to create a table  Users may simply put data into the table and specify column name and type in  formation later  The system is designed to handle du  plicate or unnamed columns  Columns are assumed to store text by default  but this can be changed at any time  In a relational database  a column s type can only be changed if all of the data can immediately be converted to the new type  S3 always allows the change to happen and highlights problematic  elds in red  Additionally  extra information can be attached to a column  This includes units and a full text de  scription of the column s contents and intended use  A typical table is shown in Figure 2  In addition to standard types  S3 supports enumer  ation and  le uploads cells  An enumeration is a list of valid values for a column  For example  red  or  ange  and yellow could be listed as valid colors  Enu  merations are bene cial since they enforce a standard nomenclature  Files can be placed in cells just like data  this allows for images  video  or any  le type to be easily associated with applicable data  Users can then search for a sample and then simply click on the  le cell to open the  le  Sometimes  les are not merely stored in cells  but are a source of derived  elds  Data needs to be ex  tracted from these  often machine generated   les  These  les are handled by uploading them into cells like normal  les  A user with knowledge of the format writes a  lter to process one of these  les and asso  ciates the  lter with the column  S3 will run the  lter on all existing  les  new  les as they are uploaded  and new versions of  les  Each  le will result in data added to the current row or rows of data added to another table  as speci ed by the  lter  Since data often exists outside of S3  especially when converting from an existing project  importing directly from spreadsheets is supported by S3  Sim  ple data can be copied and pasted into tables  Ex  isting workbooks can be uploaded and split into any number of tables  As with typing into cells  these methods allow data of incorrect type to be entered into columns and highlights them in red  so no data is lost in the importing process  3 2 Querying Data The query interface is designed with simplicity in mind  Users  rst select which tables to include in the query  Then  for each table  users select which column contains data that is equivalent across the tables  A suggestion is given for a compatible col  umn for each table based on heuristics  such as col  umn name  matching data types  and joins in other queries  Users can then limit the selected columns to a subset of the tables  columns  Additionally users may add restrictions based on the data  For example limiting the data to mice born after a certain date  These queries can be saved as reports  Tables and re  ports can be exported in Common Separated Value  CSV  format  so they can be trivially imported into other applications  While the report generation interface is designed to handle common report cases  If more advanced queries are required  users can write arbitrary SQL queries based on the tables  All tables and reports result in actual views in the DBMS  3 3 Manipulating Data Most experiments involve a repeating measure of the same entity under di erent conditions  For exam  ple  a mouse s weight will be measured repeatedly at di erent points in its life  In a spreadsheet  a sci  entist will often create a column for each measure  ment  These columns might be   Weight 10 4 10    Weight 10 11 10    Weight 10 18 10  or simply   Weight 1    Weight 2    Weight 3   This is a bad data model  for a few reasons  Most crucially  im  portant information is stored in the column name  5where it cannot be accessed in a query  Additionally  the schema has to change whenever a new measure  ment is added  In the relational data storage model  the schema would look like   Mouse ID    Date    Weight  and there would be a row for every time any mouse was measured  The many columns method is commonly used in spreadsheets since users have never seen the correct way to store this data and even if they had  the lack of a sophisticated query mecha  nism would make data in normal form di cult to ac  cess  Additionally  users  nd a view with one line per mouse much easier to read  understand  and perform input on than one line per mouse measurement  To solve this problem  a combination of heuristics and user selection is used to identify these repeated measurement columns  Once the system identi es them  users have the option of viewing this data in three ways  The  rst is in the repeated columns that they entered  The second is a summary view that compresses the columns into a single column using an aggregate function such as mean  median  or sum  The  nal view has the data converted into the many  rows form of a database  This is achieved by taking every cell in the repeated columns and converting it into a row  This row contains a cell with the data part of the original column name  for example  10 4 10   a cell for the value in the original cell value  and    nally an exact copy of the data in the non repeated columns  Giving users access to this data in multiple forms means that they can interact with their data in the way that is easiest for the task at hand  4 System Design S3 is implemented in a relational DBMS  which gives S3 and its users access to the querying power of SQL  Despite storing information in a database  the data  model is more closely related to a spreadsheet  By representing the data in manner similar to that of a spreadsheet  S3 is able to avoid the limitations of a database  S3 makes cells the focus of the data model  instead of tuples like in a database  This allows for the data in cells to manipulated in ways that are not possible with a tuple centric model  4 1 Schema The basis of S3 is a representation of a  virtual  table  By representing a table instead of creating an actual database table  the system gains signi cant  exibility  This  exibility is apparent when considering the most important of the actual tables  the Cells table  Table 1  Table 1  Cells Table Cells value attribute id agglomeration id created replaced Each value in a virtual table has an entry in the Cells table  Each Cell has a reference to the Agglomerations table and the Attributes ta  ble  The Agglomerations table represents the vir  tual rows  The Attributes table represents the vir  tual columns and stores the name of the column  the data type that the system will use in views  and op  tionally units or any user notes about the column  Table 2  These are used to place the cell within a virtual table  An advantage of this approach to storing data is that NULL values take no space  They are stored implicitly by not having a Cell with a matching agglomeration id and attribute id  In a tradi  tional database  NULLs are stored by setting a speci c bit  or possibly even a byte  in the row header  Table 2  Attributes and Agglomerations Tables Attributes id name type id units notes Agglomerations id 6By storing the user s input as a string in the value  eld  the user s original input  and thus intention  can always be recovered  This method allows for the typing of a column to change without any risk of permanent information loss  Additionally it allows users to input data that does not match the column type  and then correct this mismatch at a later time  The enumeration and  le types are handled slightly di erently  For the  le type  when the user uploads a  le  it is stored and a record is added to the Files table which contains a unique id and some informa  tion about the  le  The id number is stored in the value  eld of the appropriate cell  For the enumer  ation type  the source attribute id is stored in the referring attribute s record  The Cells themselves have the agglomeration id of the wanted cell stored in value  like a foreign key  Unlike a traditional schema  entries in the Cells table are not updated when a user changes a value  A new entry is placed in the table  with created set to the current time  and the replaced  eld of the old Cell is changed from NULL to the current time  This maintains the history of the virtual table and allows for users to see the changes to the table over time  A few auxiliary tables exist to manage the vir  tual tables  columns  and rows  The Tables ta  ble contains the name and creation date of a table  The Table Columns table is a listing of all of the Attributes in a given Table and Table Rows is a list of all of the rows for any Table  This data model creates  exibility in not only the typing of the Cells  but in the location of the Attributes and Agglomerations  Attributes and Agglomerations can not only be moved within a ta  ble  but can be trivially moved to new tables  This provides a mechanism for schema refactoring  which is needed keep the schema applicable to current needs and to move the schema towards normal form  4 2 Views and Querying Both the virtual tables and the reports are mani  fest as views in the DBMS  The view for a Table is created by  rst creating a view for each Attribute  This is created by taking the Cells table and se  lecting only those cells where replaced is NULL  to get the current versions of the cells and the cells with the requisite attribute id  Only the agglomeration id and the value  cast as the appro  priate type  are projected  For the attribute with id equal to n  this can be stated in relational algebra as Equation 1   An   agglomeration id  value     replaced NULL  attribute id n  Cells     1  The Table view is then created by taking the Table Rows table  selecting the correct table id  t  and performing left joins against against each of the attribute views on agglomeration id  This is given by the expression in Equation 2    agglomeration id    table id t  Table Rows   n2NAn  2  To preform a query  users select which tables they want to join and an equivalent column for each of these tables  Optionally they can specify additional conditions  Users choose if they want the intersection of the values in the equivalent columns or the union  For an intersection an inner join is done between the views of the selected Tables  For a union  an SQL statement is constructed that  rst takes the union of the equivalent columns and then does a left join with each of the selected tables  With C as the set of views of compatible attributes and T as the set of tables  this yields Equation 3      A2C   value  A    V 2T V  3  This produces the report that users expect  In par  ticular every identi er is included no matter which Table it was in and each identi er appears only once  If instead of using the union method a chain of left joins was used  identi ers that did not appear in the  rst Table in the join would not appear in the result  If a full outer join is used  identi ers that are the same  but are in Tables that are not directly joined  will produce two rows if the identi er does not exist in any one of the intermediary tables  7Queries can be saved using a table called Reports  which stores the name of the Re  port  Report Wheres  Report Columns  and Report Tables store the selection criteria  cho  sen columns  and joined tables respectively  Report Tables additional</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management"/>
        <doc>Managing Structured Collections of Community ### Data Wolfgang Gatterbauer University of Washington gatter cs washington edu Dan Suciu University of Washington suciu cs washington edu ABSTRACT Data management is becoming increasingly social  We observe a new form of information in such collaborative scenarios  where users contribute and reuse information  which resides neither in the base data nor in the schema information  This    superimposed structure    derives partly from interaction within the community  and partly from the recombination of existing data  We argue that this triad of data  schema  and higher order structure requires new data abstractions that     at the same time     must e   ciently scale to very large community databases  In addition  data generated by the community exposes four characteristics that make scalability especially di   cult   i  inconsistency  as different users or applications have or require partially overlapping and contradicting views   ii  non monotonicity  as new information may be able to revoke previous information already built upon   iii  uncertainty  as both user intent and rankings are generally uncertain  and  iv  provenance  as content contributors want to track their data  and    content re users    evaluate their trust  We show promising scalable solutions to two of these problems  and illustrate the general data management challenges with a seemingly simple example from community e learning     ce learning ###      1  A VISION  MASSIVE COMMUNITY E LEARNING WITH PAIRSPACE We will argue that management of collections of community data requires a new abstraction that does not    t well in the common dichotomy of data and schema information  We illustrate this idea with the vision of a massive online question answer learning community of users  grouped around a hypothetical tool we refer to as Pai rSpace  We prefer to keep the overall setup simple  This is a concrete community data management scenario that illustrates the main issues in this paper  while at the same time  seems to have a simple relational implementation  Note that the underlying challenges naturally extend to more complex and general community content management scenarios  5 th Biennial Conference on Innovative Data Systems Research  CIDR    11  January 9 12  2011  Asilomar  California  USA  This article is published under a Creative Commons Attribution License  http   creativecommons org licenses by 3 0    which permits distribution and reproduction in any medium as well allowing derivative works  provided that you attribute the original work to the author s  and CIDR 2011  1 day  3 days  1 week  1 month  6 months  correct  incorrect  Figure 1  Spaced repetition with    ashcard learning  Repetition intervals increase for subsequent boxes  Pai rSpace is a huge shared repository of learning nuggets organized into question answer  Q A  pairs that combines  a     ashcard learning with  b  spaced repetition and  c  a community built around it  Flashcards are sets of cards with a question on one side and an answer on the other  These cards are used as a learning drill to aid memorization of learning material through what is called    active recall 1      given a question  one produces the answer  Furthermore  those Q A pairs are usually grouped into collections of a similar nature  i e  meaningful learning units  Examples are the vocabularies of one lesson in a high school book  or the standardized questions to pass the US driving license in the State of Washington  Almost any cognitive subject can be translated into such a Q A format 2   Spaced repetition is a learning technique with increasing intervals of time between subsequent reviews of learned material  Items to memorize are entered into Pai rSpace as Q A pairs  virtual    ashcards   When a pair is due to be reviewed  the question is displayed  the user attempts to answer the question  and     after seeing the answer     decides whether he answered it correctly or not  If he succeeds  then the pair gets sent to the next box  if he fails it gets sent back to the    rst box  Each subsequent box has a longer period of time before pairs are revisited  Fig  1  3   Imagine a daily 1 In active recall  pieces of information are actively retrieved from memory as opposed to passive review  See  10  for a recent discussion  2 Further examples are  general cultural facts  such as world countries and their capitals   competition results for sports fans  e g   Who won the 2010 World Cup       lm facts for movie bu   s  e g   Who played William of Baskerville in    The name of the rose    of 1986    often asked terms during GRE and their synonyms  important paragraphs or cases in law  details on the periodic table in chemistry  multiplication tables in mathematics  names of bones and their location in the human body for medical students  basic formulae in any science  or lists of common abbreviations in computer science  e g   What does MVD stand for    3 The idea of spaced repetition traces back to the early 1930s  but only became later widely know as Pimsleur   s graduated interval recall  15   or    Leitner system     or    Ebbinghaus Forgetting Curve     While not widely popular in the USA     ashcard learning is hugely popular in Europe with several hundred  mostly o   ine    ashcard programsmorning routine in which a user repeats the learning nuggets that are due that day as suggested by the system  The third aspect is that those collections of pairs can be shared  re used  and even re combined  We envision one centralized and massive repository of Q A pairs for all disciplines  languages and kinds of human knowledge  This is the one central place  with obvious positive externalities  where learners go for repetitive learning needs to    nd relevant collections of information nuggets  to upload or combine pairs into new collections  and to train regularly  Major value of the stored information lies not just in the individual Q A pairs  e g   the translation of English    go    into Spanish    ir    can be easily found in any free online dictionary   but in the collection of these information nuggets into meaningful units of information whose mastery together allow the learner to acquire a certain skill level  e g   passing the knowledge based driving test   And this value is important to leverage when helping users    nd the right pairs  collections  or even other users with similar interests  Example 1  PairSpace Scenario   Alice is learning Spanish  She uplods Q A pairs of her    rst lesson  Bob is learning Spanish too and discovers Alice   s Spanish 1 lesson in Pai rSpace  His girlfriend is Mexican and has taught him to use andar instead of ir for go  He changes his Q A pair  go  ir  to  go  andar   Next assume Charlie is searching for basic Spanish lessons  What should the system return to Charlie  and how should it present this result  2  CHALLENGES FOR MANAGING COLLECTIONS OF COMMUNITY DATA The simple scenario of Example 1 already poses several challenges of how to search  return and present the results      What to return  Should the system return the collection Spanish 1 of either Alice or Bob  Should it present them as a derivation of each other with Bob   s collection as the most recent  or Alice   s as the original  Should it return just the intersection of Alice   s and Bob   s collections as new derived collection  Should it present and mark the tuples  go  ir  and  go  andar  as possibly con   icting pairs  Stated more abstractly  given two or more collections as input  how to inform and return to the user the structural variation in collections  How can the system learn to suggest new derived collections that    t the purpose of the user      How to bundle and present the results to the user  Can we take advantage of new    return structures    imposed by collections instead of returning individual pairs in an alltoo familiar list based fashion  cf  discussion in  3    If we have several partially overlapping and often complementary or contradicting collections  should we return collections or tuples by majority or by diversity  cf   20    Should we cluster these collection into meta collections in the search results  i e  go one level further in the abstraction      How to search  How does the user specify the information she is looking for  i e  what is the appropriate search paradigm for query formulation  What is the appropriate found on the Web  For example  Phase 6 is a German company entirely built around an o   ine    ashcard learning software that is used at 3 000 German schools and has supposedly been sold more than 500 000 times  source  http   www phase 6 com   The fact that there are hundreds of software tools available supports the thesis that one unique  and online Pai rSpace would a valuable tool to users  but nobody has yet    gured out the perfect solution to bring this to massive dimensions  cp  Friendster and LinkedIn before Facebook   User uses Collection contains Pai r  a  cname Alice go ir Bob uname Spanish 1 Spanish 1 User Collection Pai r Charlie go andar         Q A    b  Figure 2  An attempt at a relational encoding of Pai rSpace   a  an ER model  and  b  an instance  query language that     though possibly hidden from the user     allows to express the user   s search needs      What to include in ranking  What are those explicit or implicit features or associations that can be leveraged to learn and return relevant information to the user  Obvious candidates are the following   a  Semantic or syntactic similarity  How can one address synonymy and polysemy  For example  the expression bank can represent    river bank    in English     bench    in German  and a       nancial institution    in both languages   b  Structure  What is the generally appropriate way to think about the relative importance between pairs or items and collection of items   c  Trust or reputation  What is the appropriate abstraction of trust between users in this scenario  How can di   erent levels of trust be combined  i e  should they be de   ned either in a vote based  democratic  weight based  manner  or rather a rule based  strict  preference based  manner  For example  Charlie may specify to trust his school teacher strictly more than any other people  In such preference or rule based scenarios  the actual value of the weights does not matter  but rather the partial order between preference relations  18    d  Provenance  What kinds of provenance are suggested by this scenario  such as    social provenance     3   or derivative provenance  Should identical pairs speci   ed by di   erent users be linked to each other  How can one de   ne provenance on collections of items  How to incorporate all those forms of provenance into an appropriate ranking function  How to support querying those combined forms of provenance  e g  to support explanatory queries over this repository  One fundamental problem is already the question of how to best store  manipulate  and update all involved information over time  Figure 2a shows a simple ER model of the relation between users  collections and pairs in our scenario  Figure 2b is a simpli   ed depiction of the scenario of Example 1  If the collection of Alice contains 100 tuples  then storing Bob   s incremental variation would take   2 100 th of the space of Alice   s original lesson  For the sake of discussion  let   s call replicating all pairs as the explicit representation  alternatively eager or materialized   and some other representation that stores only the di   erence as implicit  alternatively lazy or virtual   But space is not the major issue here  even if the explicit representation is stored in a compressed form  valuable information about the relative derivation or evolution of content is lost  Note  that during querying  value lies not so much in the individual pairs or collections  but rather in the knowledge of how close two or more collections relate to each other  In turn  storing only the implicitinformation may decrease the access to the actual data considerably  Hence  there is this inherent trade o    between having the data explicit  or the relative derivations explicit  How to update those derivations if content evolves and users add  update  delete or transfer pairs between collections  Summing up the three main challenges that this seemingly simple scenario of managing three kinds of entities  items  collections  users  in a community scenario raises are   1  What is the right abstraction for the logical and physical representations of this partly redundant  partly overlapping information  grouped into vastly overlapping bundles   2  What is the right abstraction of a data manipulation and query language that allows one to reason in terms of collections rather than items   3  How to evaluate relative importance over the triple concepts of  items  collections  users  in a sound and principled way  In addition  how to reason about  i  inconsistency   ii  non monotonicity   iii  uncertainty  and  iv  provenance at the level of collections  3  WHY EXISTING MODELS AND APPROACHES DON   T SUFFICE Here we brie   y summarize related work that focuses on these challenges but fails short in solving them entirely  The overall area falls into what is classi   ed as sharing systems in  5  where users together build shared structured knowledge bases or a consistent data synthesis  In our scenario  the users do not share the goal of structured knowledge creation  but rather want to    nd individually    tting collections of Q A pairs that    t their respective learning needs  Our scenario is clearly di   erent from data integration or data fusion where the goal is to create one uni   ed view on the data  12   Instead  we want to e   ciently manage     nd  and compose meaningful collections bundles structures of base data that evolve over time  Our challenges are also reminiscent to those of dataspaces  11   where the focus is on incremental     pay as you go     integration  The value of the system increases over time with the number of matches between the data  In our scenario  collections of items have di   erent meanings to di   erent users at di   erent times and need to be managed from day one  The scenario is also related to the problem of con   ict resolution in community databases with the goal of automatically assigning each user in the system a unique value to each key  18  9   However  in our scenario  content import is    pull    instead of    push     i e  users actively search for content  In the scenario of searching over Yahoo  answers  1  2   the goal is to order the set of question answer pairs according to their relevance to the query  In our scenario  the goal not just to rank just pairs  i e  user generated content  but rather  or alternatively  collections of pairs  which may exist or may be re combined   or to suggest relevant users  Our notion of collections is also very reminiscent of superimposed information  14   i e  data that is placed over existing information to help organize and reuse items in these sources  One main di   erence to superimposed information management is the community aspect  we have di   erent alternative and overlapping collections of base information  and value lies not just in the groupings  but also the di   erence between alternative groupings  The concept of    nding and managing associations on top of base data is also related to inductive databases  16  and pattern base management systems  4   Both try to manage rules built upon base data as separate information inside an enhanced DBMS  The di   erence is that the collections that we are interested in do not have in general an intensional semantics  That means they cannot by themselves be expressed in a short implicit form  e g   by a query  cf   17    Rather  we are interested in the incremental and evolving di   erences between collections of base data  And we want to leverage this information about    data interference    during the ranking process  Our scenario is also reminiscent of revision control systems  RCS   such as SVN  which manage incremental changes to documents  programs  and other information  and optionally include compression  But while RCSs can store di   erences e   ciently  they do not expose general query facilities to sea</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sie09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sie09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction"/>
        <doc>Workload Balanced Processing of Top K Join Queries on Cluster Architectures ### Renwei Yu  Mithila Nagendra  Parth Nagarkar  K  Selc  uk Candan  Jong Wook Kim CIDSE  Arizona State University Tempe  AZ 85287  USA  renwei yu mnagendr pnagarka candan jong  asu edu Abstract    The observation that a signi   cant class of data processing and analysis applications can be expressed in terms of a small set of primitives that are easy to parallelize has resulted in increasing popularity of batch oriented  highly parallelizable cluster frameworks  These frameworks  however  are known to have shortcomings for certain application domains  For example  in many data analysis applications  the utility of a given data element to the particular analysis task depends on the way the data is collected  e g  its precision  or interpreted  However  since existing batch data processing frameworks do not consider variations in data utility  they are not able to focus on the best results  Even if the user is interested in obtaining a relatively small subset of the best result instances  these systems often need to enumerate entire result sets  even if these sets contain lowutility results  In this paper  we introduce and describe uSplit  a data partitioning strategy for processing top k join queries in batch oriented cluster environments  In particular  we describe how uSplit adaptively samples data from    upstream    operators to help allocate resources in a work balanced and wasted work avoiding manner for top k join processing  Experimental results show that the proposed sampling  data partitioning  and join processing strategies enable uSplit to return top k results with high con   dence and low overhead  up to     9   faster than alternative schemes on 10 servers ###   I    IN T RO D U C T I O N In many applications  the utility of the elements in the database to a particular task varies from data instance to data instance and users are interested not in all the data  but the ones that are best suited for the given task  Locating such high utility data is known as top k query processing  Applications that involve varying data and feature utilities include decision support and text and media analysis  in the case of text  the popular TF IDF keyword score is an example   In this paper  we investigate whether applications that require top k processing over large volumes of data can bene   t from recent developments in highly parallel  cluster based data processing  The observation that     while not all  30      a signi   cant class of data processing applications can be expressed in terms of a small set of primitives that are in many cases easy to parallelize  has led to popularity of batch oriented  highlyparallelizable cluster frameworks  such as MapReduce  18    3   Dynamo  19   Scope  10   PNUTS  17   HadoopDB  4    These systems have been successfully applied in data processing  mining  and information retrieval domains  27    32   Given an atomic task  these rely on the simple semantic properties of the task to partition the work onto many machines  R1 score  X                          R2 score  X  0 90 0 85 0 80 0 80 0 70 0 75 X2 X5 X6 X3 X5 X2 0 825 0 8             sorted access sorted access 0 70 0 75 0 60 0 74 0 50 0 74 0 40 0 70 X6 X4 X1 X3 X2 X6 X1 X4 0 72 0 40 0 625 X3 0 70 X4           random access           Fig  1  Typical top 3 join processing  a sorted access phase provide initial candidate results  a random access phase contributes additional candidates tat may have been missed during sorted access  the monotonicity of the combination function  average in this example  makes it possible to stop without having to consider all the data Example 1 1  MapReduce   MapReduce is a functional list processing language  18  where a data processing work   ow is constructed using map and reduce primitives  The userprovided map primitive takes a set of input key value pairs and produces a set of intermediate key value pairs  In essence  the map function extracts a set of features from the input data  The intermediate values from the map primitive are often supplied to a reduce function  The reduce function takes these intermediate key value pairs  i e   features  and produces its output by applying an aggregate function on all input values with the same key  The user provided map and reduce primitives can be combined into more complex work   ows  These systems achieve high degrees of scalability by carefully allocating resources to available processing elements and leveraging any opportunities to parallelize basic processing tasks  Signi   cant savings in execution times are obtained by independently parallelizing each step of the work   ow and executing them  except for some very recent efforts  such as  16   in a batched manner over a cluster of servers  It is important to note that such batch oriented processing is not limited to systems based on MapReduce  but also to most parallel DBMSs  such as Vertica  2   which rely on row  or column based data partitioning to support parallelism  In this paper  we note that existing batch data processing frameworks  whether based on MapReduce or traditional row or column partitioning in parallel databases  have dif   culties in domains where data utility is variable and present a novel data partitioning strategy called uSplit  which support workload balanced processing of top k join queries on server clusters 2 high utility T Top k results a Source  t Enumerated  but  pruned candidates T k Data Source 1 Dat high low utility low Data Source 1 high utility utility Fig  2  Relying on sorted  and random access operations  top k algorithms focus their processing to the high utility corner of the utility space  here  the curve represents the threshold    k  de   ned by the lowest of the scores of the best k results  consequently  and results below this threshold will be pruned  small circles under the threshold correspond to enumerated but pruned results due to random accesses  A  Related Work on Data with Non Uniform Utilities When data utility is non uniform  users are often not interested in obtaining all possible results to a query  but only the k best results  While batch based systems promise large scale parallelism  since they do not consider variations in data utility  they are unable to scale most effectively by focusing on the data that is likely to produce the best results  To avoid waste  data processing systems need to employ data structures and algorithms that can prune unpromising data objects from consideration without having to evaluate them  This is often referred to as ranked or top k query processing  5    8    11    20    21    24   Most existing ranked query processing algorithms  including Fagin   s algorithm  FA   20    21   threshold algorithm  TA   22    NRA   22   and others  such as  12    28    23    26   assume that one  or both  of the following data access strategies is available   a  streaming pipelined access to the sorted data to identify a set of candidates  and  b  index based random access to verify if these are good matches or not  Given monotonic 1 queries on the data  these help identify good candidates and prune non promising ones quickly  Figure 1   Figure 2 graphically represents the portion of the utility space covered during the top k join operation  Here  the curve represents the threshold    k  de   ned by the lowest of the scores of the best k results to be returned to the user  the shape of the curve depends on the combination function  Consequently  all the join work to produce the results above this curve is useful  Note that  as shown in Figure 2  top k join algorithms may also identify candidate results below this threshold and these need to be pruned during post processing  In systems which partition data and process them in batches  however  top k query processing cannot be ef   ciently supported  A critical dif   culty with parallelizing the top k algorithms  such as FA  TA  and NRA  is that the process is inherently sequential  as shown in Figure 1  in the    rst sorted access phase one pointer per data source is used for scanning each data stream in decreasing order of utility  This prevents ef   cient parallel implementations of these algorithms  Consider for example a naive parallelization scheme where the 1 An object that is as good as another one in all individual features is also better than the other object when these features are considered simultaneously  ty 1 1 high utili 2 2 1 2 w utility 1 Data low utility high utility low 1 1 Data 1 1 2 1 2 2 high utility T k Global top k results a Source  t Data Source 1 Da high low utility low Enumerated  but   locally  pruned  candidates Data Source 1 high utility low utility Fig  3  Random partitioning of the data will waste resources for producing large number of candidates that will be eventually pruned  points with different shapes correspond to enumerated but pruned results in different servers  ility LH HH high ut ow utility low utility high utility LL HL ol 2 3 2 high utility Top  k results 4 ata Source 1 T k Estimated boundary Data Source 1 Da low utility lowlow Data Source 1 high utility high utility Fig  4  The proposed algorithm estimates the cut off   k to prevent redundant work and partitions the useful join work above the threshold to available servers to speed up the join processing  This enables utility aware resource allocation for top k join processing join space is randomly split among U servers such that each server produces its own k results to be combined into a    nal candidate set from which the top k results will be selected  Since  top k join algorithms have O k 1 m J 1    1 m   complexity with high probability for m way joins  where J is the number of all potential join results  21   in this approach  each server would need to perform O k 1 m   J U  1    1 m   work to produce its local top k results  When for example m   2  the amount of per server work would be O    1 U     kJ   In other words  given U servers  the per server work is    1 U of the total 2 way top k join operation  not close to 1 U that would be the case if there were no wasted work 2  Figure 3   B  Our Contributions  Utility Sensitive Data Partitioning for Parallel Top k Join Processing In this paper  we describe a data partitioning strategy called uSplit  to support utility sensitive top k data processing  uSplit targets the needs of applications where  a  utilities of the data elements are non uniform  and  b  signi   cant gains in performance can be obtained by focusing on data that will produce highly ranked results  Figure 4   More speci   cally  our goal is to  a  estimate the threshold   k  the lowest score of the top k results  precisely to prevent wasted work and  b  develop a data and work partitioning strategy that can partition and assign the useful join work above this threshold to the available servers  Figure 4   2 Note that this naive partitioning strategy could provide close to    1 U gain when m is large  however  top k algorithms are often avoided for large m  because this would result in a total of O J  work  I e   a full join would need to be done anyhow  this is known as the curse of dimensionality  1  Data and Query Models  We assume a very simple key value data model   key value   Without loss of generality  each key value pair is extended with a utility score  u  between 0 and 1   key value u   In this paper  we focus on topk join queries over such data  For example  in a video recommendation application  the query top pairs   top k join male actors by movieID  female actors by movieID  avg m actScore f actScore   100 on two tables male actors m actorID  movieID  m actScore  and female actors f actorID  movieID  f actScore   would locate    highest scoring male female actor pairs who have played together in at least one movie    by equi joining the two tables on movieID and identifying the top 100 results on avg m actScore f actScore   More generally  we consider two way  top k join operations of the form R1   C R2  where C is the join condition  Each data element  t  in R1 and R2 has an associated utility score between 0 and 1  i e   t u      0  1   There is also a monotonic utility combination function        associated with the join operation  Given two elements  t1     R1 and t2     R2  possi   ble combination functions include Euclidean    Euc t1  t2     t1 u  2    t2 u  2   Average    avg t1  t2    t1 u t2 u 2   Product    prod t1  t2    t1 u    t2 u  Min    min t1  t2    min t1 u  t2 u   and Max    max t1  t2    max t1 u  t2 u  functions  6    20   Given a target score  these functions de   ne different utility boundaries on the utility space of results  2  Utility Sensitive Data Partitioning  In this paper  we develop utility based partitioning and resource allocation schemes based on two major criteria   a  wasted work criterion  this takes into account how much of the system resources are allocated to produce results that are not in top k and  b  partition work balance criterion  any imbalance in the work assignment means that certain system resources will stay under utilized  while others are over utilized  uSplit repartitions input data sets on the    y  during their assignment to the servers for join processing  in terms of data utility  To support this  uSplit collects relevant statistics to estimate the utility score    k  of the k th output result  and avoids allocating processing resources to those data combinations that will produce results with utilities less than   k  Note that  since the results may not be uniformly distributed in the utility space  both the shape of the utility boundary and statistics about the result distribution as a function of the input data utilities are needed to estimate the   k lower bound  Intuitively  this is similar to the    ltering strategy that has been successfully applied for top k query processing in relational databases  14    7    8    9   In the context of batched data processing  however  existing    ltering techniques cannot be directly applied due to the lack of index structures that provide ef   cient    ltering  and the lack of prior statistics about the intermediary data produced and consumed within a work   ow  3  Con   dence Target Parameter  Since we are relying on statistics to estimate the   k lower bound  we also take a      p11 p12 p1U      p21 p22 p2U      r11 r12 r1n      r21 r22 r2m selectivities R1  partitioned in utility  R2  partitioned in utility  R1  partitioned in value  R2  partitioned in value   c  Estimate join work distribution as a function of  d  Repartition those input data  which may result  in top   k results  in a balanced manner data utilities  b  Estimate result utility lowerbound   k based on      a  Estimate join selectivity as a function of data utilities  e  At each server  perform the corresponding join  f  Combine the results Fig  5  Top k join processing with uSplit con   dence target as a query processing parameter from the user  For example      95  con   dence target would mean that the user allows for up to 5  chance that the system may return less than k top results  In other words  high con   dence target indicates that the user wants the system return closer to k  top  results with high probability  whereas a low target indicates that the user might be satis   ed with less than k matches in most cases  Otherwise  the query model is deterministic in that the system will not return any results that are not in topk  As we will see in Section III  this con   dence target helps the system to choose tighter or looser   k thresholds based on the available statistics  Tighter thresholds means less wasted work  but may result in some misses  4  Summary  Figure 5 visualizes the steps of the uSplit top k join processing   a  uSplit    rst estimates the join selectivity as a function of the input data utilities and then  b  computes the lower bound    k  on the utilities of the top k results based on these selectivities  Next   c  uSplit estimates the amount of  useful  work needed for computing the topk results  and based on this estimate   d  it repartitions the input data in a work balanced manner  before assigning these partitions onto the available servers for processing   e  Each server creates the necessary data structures and performs the assigned join task  Finally   f  the results from the individual servers are combined to select the best k results  I I   RU N T I M E STAT I S T I C S CO L L E C T I O N Note that we cannot assume independence of join selectivities from the score distribution  Therefore  we need statistics to identify the join selectivities as a function of the input utilities  this enables that  given the con   dence limit  the cutoff can be estimated as tightly as possible  Since we cannot assume that statistics about the intermediary  transient data within the work   ow are available in advance  uSplit    rst collects any statistics needed to estimate the join selectivities as a function of the data utility 3   uSplit performs this by sampling intermediary data along the utility scores  3 Note that this is especially important when the join attributes and the utilities are not completely independent 0DS RU 5HGXFH  QSXW  DWD 2XWSXW  DWD 2XWSXW 6DPSOHV                     6WDWLVWLFV 6DPSOLQJ 6WUDWHJ  LQSXW GDWD WR WRS   MRLQ SURFHVVLQJ PHWDGDWD WR KHOS DOORFDWH UHVRXUFHV IRU WRS   MRLQ SURFHVVLQJ Fig  6  Piggy backed sampling  Given a sampling strategy  each  upstream  operator produces samples to support down stream operations R1 R2 top  k join out1 Samples1 out2 Samples2 S11 N d 11 out11 out12 out13 out14 out21 out22 out23 out24 S12 N d 12 S13 N d 13 S14 Node14 S21 N d 21 S22 Node22 S23 Node23 S24 Node11 Node12 Node13 Node14 Node21 Node22 Node23 Node24 Node24 in1 in2 Fig  7  Piggy backed sampling at the upstream operators Since the input data to the top k join operation itself may be produced using a parallel executing operator  we have three alternative runtime sampling options      Piggy backed sampling  For intermediary data generated within a work   ow  in order to eliminate unnecessary passes over the data  one option is to piggy back the sampling work on the    upstream    operators  each pararellel instance of the    upstream    operator produces a set of samples  using reservoir sampling technique  35   as it produces its output data  as shown in Figures 6 and 7      Partial output sampling  In this alternative  each upstream machine produces samples on its local output and later these individual samples are combined to obtain the full set of samples  Figure 8   Note that in this method  a full scan of the data is required  but each upstream machine can sample its data in parallel with the others      Combined output sampling  In this option  after the outputs of the upstream operators are merged  one server node samples the full combined data set  Figure 9   Note that  also in this method  a full scan of the data is required  Note that piggy backed sampling is the most ef   cient alternative as it avoids an extra scan of the data and allows each upstream machine to sample its own output data in parallel to the others  Partial output sampling is faster than combinedoutput sampling  since it also allows the data sampling process to be run in parallel in upstream operators  The combinedoutput sampling option requires a full  sequential scan of the combined data  however  this option may be necessary to avoid bias in sampling when the data partitioning attributes for the upstream operator and the data utilities relevant for the downstream top k operator are not independent  R1 R2 top  k join Samples1 S11 out1 S12 S13 S14 Samples2 S21 out2 S22 S23 S24 Node11 out11 out12 out13 out14 Node12 Node13 Node14 Node21 out21 out22 out23 out24 Node11 Node12 Node13 Node14 Node22 Node23 Node24 in1 Node21 Node22 Node23 Node24 in2 Fig  8  Partial output sampling at the upstream operators R1 R2 top  k join out1 Samples1 out2 Samples2 Node11 out11 out12 out13 out14 Node12 Node13 Node14 Node21 out21 out22 out23 out24 Node11 Node12 Node13 Node14 Node22 Node23 Node24 in1 Node21 Node22 Node23 Node24 in2 Fig  9  Combined output sampling at the upstream operators It is important to note that obtaining samples from joins is extremely dif   cult  In  15   Chaudhuri et al  showed the need for non oblivious and non uniform sampling strategies   15  also showed that there are lower bounds on the sizes of the samples even when these samples are generated in a nonoblivious manner  Here  we propose to address this dif   culty by budgeting the samples for top k processing intelligently where they matter the most  A  Allocation of the Data Sampling Budget Let us reconsider the ranked join operation  R   R1   C R2  where C is the join condition   R1    N  and  R2    M  Each data element  t  in R1 or R2 has an associated utility score  t u      0  1   The    rst task of uSplit is to sample R1 and R2 to estimate the join selectivities as a function of data utilities  Since the overall goal of the sampling process is to support top k joins  it is most important that the selectivities are estimated with higher precision towards the higher end of the utility spectrum  Therefore  instead of sampling R1 and R2 uniformly across the utility space to obtain histograms that are equally precise in all regions  as is commonly done in traditional databases  13    29    31    uSplit targets higher precision for higher utilities  Thus  uSplit segments the utility range and allocates samples to different ranges  in particular  as shown in Figure 10  the higher the utility of the segment  the larger the number of samples allocated  Let 0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1 denote the xand y boundaries 4   which are used for segmenting the two dimensions of the utility space  i e   xi    xi   1  xi   and yj    yj   1  yj  are the utility segments corresponding to the two dimensions of the utility space 5   For 1     i     n and 4We discuss how to set segment boundaries in Section II C  5 The segments x1    0  x1  and y1    0  y1   which are closed in both sides  are minor exceptions  Uniform  Sample Distribution 0 0  0 2 0 2  0 4 0 4  0 6 0 6  0 8 0 8  1 0 Utility Ranges Range  Adaptive Sample Distribution 0 0  0 4 0 4  0 84 0 84  0 96 0 96  0 99 0 99  1 0 Utility Ranges  a  Uniform sampling  b  Adaptive sampling  c  Join result samples produced with adaptive sampling Fig  10  Utility range adaptive sampling of input data ensures that more join result samples are collected at the high utility range of the utility space 1     j     m  let R1 i and R2 j   be the subsets of R1 and R2 in the corresponding utility ranges  and let r1 i    R1 i   and r2 j    R2 j   denote the number of data elements in each segment  In addition  let s1 i and s2 j   where   1   i   n s1 i   s1 and   1   j   m s2 j   s2  denote the number of samples uSplit allocates to the utility segments xi and yj  Here  s1   N is the overall sampling budget for R1  and s2   M is the sampling budget for R2  Given this sampling budget  uSplit estimates the join selectivities across the utility space by  a     rst sampling the input data sets R1 and R2  for each segment xi  1     i     n  and yj  1     j     m   for s1 i and s2 j data elements  respectively  and then  b  joining all resulting pairs of sample sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    where  S1 i     s1 i and  S2 j     s2 j   This provides a count  Ci j    S1 i   C S2 j    of results for each pair of utility segments xi and yj   1     i     n and 1     j     m  Ci j is then used for estimating the number of results in R   R1   C R2   R1 i   C R2 j       Ci j    r1 i    r2 j s1 i    s2 j   In other words  the join selectivity for a given pair  xi and yj  of segments is     Ci j s1 i  s2 j   B  Impact of the Sample Size on the Quality of Join Selectivity Estimates When s1 i   r1 i and s2 j   r2 j   the total cost of the sample based selectivity estimation process will be much less than the cost of the full join of the data  Despite these savings  however  it is important that the samples are allocated to the segments in such a way that the sampling errors are minimized at the higher end of the utility range  To achieve this in a systematic way  uSplit quanti   es the error margins associated with the selectivity estimates and identi   es a sampling strategy that minimizes these margins  Let us consider a pair  xi and yj  of utility segments and the corresponding sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    of samples  When a sample vl     S2 j joins with uh     S1 i   we denote this as matchh l   1  Similarly  a mismatch between vl     S2 j and uh     S1 i is denoted as matchh l   0  Let us assume that uh     S1 i matches with counth many samples out of the s2 j samples in S2 j   The mean likelihood    h  of a match to uh     S1 i by data in S2 j can be computed as   h   1 s2 j   1   l   s2 j matchh l   counth s2 j   uSplit similarly computes the standard deviation  stdh  of the matches to uh as std 2 h   1 s2 j   1   l   s2 j  matchh l       h  2   Lemma 2 1  Selectivity of a Single Entry   Given the above de   nitions of mean likelihood of match    h  and the standard deviation  stdh  if we want to be 100     1          con   dent of the rate of match we are computing for uh  then  assuming that the likelihood of matches are normally distributed 6   samples are randomly selected  and the number of samples are much smaller than the amount of data in the utility segment     i e   s2 j   r2 j   7 we can associate the following con   dence interval for the true likelihood of match         h     h     t s2 j   1     2     s2 j   stdh            h       h   t s2 j    1     2     s2 j   stdh    Here  the value t s2 j   1     2 is the t distribution  with  s2 j     1  degrees of freedom and corresponding to 100     1         percent con   dence  34   This con   dence interval implies that the margin of error  Eh  in the expected rate of matches for uh is Eh   2    t s2 j    1     2     s2 j    stdh  Since the t values tend to be relatively constant for degrees of freedom greater than 5  in general  for a given con   dence target     the error margin is inversely proportional to     s2 j   Theorem 2 1  Join Selectivity Estimate   Given tuples t1     R1 i and t2     R2 j   the likelihood of the combined tuple 6 It is empirically known that the con   dence intervals for    based on normality is highly reliable  in the sense of providing good coverage  even when the real distribution of the data is much different from normal  34   7When  S2 j    R2 j     s2 j r2 j     0 05  the con   dence interval can be corrected by reducing the margin of error with a    nite population correction factor  f     r2 j   s2 j r2 j   1  34   t1  t2  being in R1 i   C R2 j is   i j   1 s1 i   1   h   s1 i   h   1 s1 i   1   h   s1 i counth s2 j   Ci j s1 i    s2 j   The error margin  Ei j   corresponding to this rate of match at 100     1         percent con   dence is Ei j   2    t s1 i   1     2     s1 i    stdi j     Proof Sketch  Let us consider a pair  xi and yj  of utility segments and the corresponding sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    of samples  When a sample vl     S2 j joins with uh     S1 i   we denote this as matchh l   1  Similarly  a mismatch between vl     S2 j and uh     S1 i is denoted as matchh l   0  Let us assume that uh     S1 i matches with counth many samples out of the s2 j samples in S2 j  Now  let us consider a set of samples  S1 i    u1  u2          uh          us1 i    from the segment xi  and the corresponding rate    h  of matches with the tuples in yj   we can compute the average rate of matches of tuples in Si 1 with the tuples in yj as   i j   1 s1 i   1   h   s1 i   h  Similarly  the corresponding standard deviation  stdi j   is computed as std 2 i j   1 s1 i   1   h   s1 i    h       1 i   2   Thus  once again  if the likelihood of matches are normally distributed 8   samples are randomly selected  and s1 i   r1 i   then  targeting a statistical con   dence rate of 100   1         we can associate the con   dence interval     i j     t s1 i   1     2     s1 i    stdi j     i j   t s1 i   1     2     s1 i    stdi j   to the true rate of matches of tuples in xi with the tuples in yj  Here  the value t s1 i   1     2 is the t distribution  with  s1 i   1  degrees of freedom and corresponding to 100   1       percent con   dence  34   Note that  this true rate of matches  of tuples in xi with the tuples in yj   is nothing but the selectivity of the join between the tuples in partitions xi and yj  Thus  the theorem follows    Theorem 2 2  Sample Allocation   Given parameters   1    2   1 0 allocation of the sample budget such that    i j s1 i      2 1    s1  i   1  and s2 j      2 2    s2 j   1   subject to the overall sampling budget constraints 1   i   n s1 i   s1 and   1   j   m s2 j   s2  ensures that 8 See Footnote 6  the margins of errors decrease exponentially with increasing segment index  i e   with rates      i 1 and       j 2 for partitions xi and yj  respectively     Proof Sketch  As Theorem 2 1 implies  for a given statistical con   dence target 100     1           higher sampling rates s1 i and s2 j will lead to lower margins of errors  More speci   cally  since the t values tend to be relatively constant for degrees of freedom greater than 5  for a given con   dence target     this error margin is inversely proportional to     s1 i   From this  it follows that if we allocate more samples at the higher end of the utility spectrum  i e      i j s1 i   s1  i   1  and s2 j   s2 j   1 such that the ratio of the samples for consecutive segments are proportional with   1 and   2  respectively  the margins of errors decrease exponentially with increasing segment index    C  Selecting the Utility Partition Boundaries The sample allocation scheme described above considers the order of the segments  but is agnostic to where the segment boundaries are actually located in the utility space  This  however  can pose two dif   culties   a  Firstly  since our goal is to compute selectivities more precisely at the upper end of the utility spectrum  setting the boundaries of the segments  xi and yj   tighter for those segments that are nearer to 1 0 should help reduce errors at the high utility spectrum   b  Secondly  the error margin computation in Theorem 2 2 assumes that   h and stdh properly model the distribution of the likelihood of matches  This assumption will often hold for suf   ciently large segments  but will be more likely to be violated if the segment contains a small number of elements  Consequently  for extremely biased data distributions  such as the Zip   an  34   where there are only few high utility elements  naively selected segment boundaries can result in high utility segments with too few elements to support sampling with predictable error margins  Therefore  segment boundaries need to be selected carefully  To ensure tighter segments closer to 1 0  we adjust the segment boundaries  0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1  using two segment scaling factors    1    2   1 0  such that    2   i   n   1      xi     xi   1    2   j   m   2      yj     yj   1  subject to the constraints    1   i   n    n   i 1   xn   1  and   1   j   m   m   j 2   ym   1  where   xi   xi     xi   1 and   yj   yj     yj   1  For data sets where the utilities are highly biased  such as normal or Zip   an  where there are insuf   ciently many high utility data  sampling irregularities need to be prevented  We refer to this as the de biasing of the utilities  In general  scores in a biased distribution can be de biased 9 by considering highlevel data distribution parameters  9 De biasing is used only when determining the partition boundaries  during query processing and ranking  the actual utilities are used Theorem 2 3  De biasing   Let   D denote the utility distribution of a given data set  D  Given a utility score  u  let us replace it with u     such that u     C DF   D  x      a   a     D     a u     u    D    where C DF is the cumulative distribution function of   D  For any two ranges   v        v      and  w        w       in the transformed utility space  if  v         v            w         w         then the number of data entries within these two ranges are the same    Proof Sketch  Given  u        u      in the transformed utility space  it is easy to see that u         u           a   a   D   u    a s   u     D    for some u    and u   Similarly  v       v           a   a   D   v    a s   v     D    for some v    and v   Thus  if  u         u           v         v        then it follows that   a   a     D   u      a s     u        a   a     D   v      a s     v       Note that in a Zip   an data set with N  elements  if the number of elements above a given utility score  u  is proportional to 1 u   then N      c u elements have utilities less than or equal to u  for some c  Thus  the de biasing theorem implies that rescaling the utility scores of a Zip   an distribution as u     N     u     c N     u   de biases the data  Similarly  in data with normal utility distributions  rescaling the utility scores as u     C DF N ormal avg  stdev   x    1 2   1 2 Erf   u     avg stdev     2     where avg is the mean  stdev is the standard deviation  and Erf is the Gauss error function  34   de biases the data  In uSplit  extreme bias in the data utilities is detected and high level distribution parameters are estimated by a partial pre sampling step in the upstream operator  carried out before the overall data sampling strategy is decided  For example  whether a data set is normal is validated through statistical tests  such as the Kurtosis test which measures the peakedness or    atness of a distribution relative to the normal distribution  applied on a small initial sample  D  Discussion In practice  the sampling budget needs to be selected in such a way that the cost of statistics collection  scan   sample join  is below a maximum target sampling cost  This can be achieved using standard scan and join cost models  There are two concerns in selecting segment and sampling scaling factors     and     respectively   Firstly  as Figures 15 and 17 in the Experiments section  Section V  show  these cannot be too close to 1 0  since uniform sampling introduces higher errors   Similarly  values that are much larger than 1 0 can negatively impact performance by requiring too many samples in too small regions of the utility space  Experiments have shown good results for        1 15 and        2 0  Finally  again experiments reported in Section V show debiasing ensures that the con   dence targets on the estimation of   k can be matched even if the underlying distributions are signi   cantly skewed  Therefore  despite its minor additional cost  the pre sampling step should not be skipped  I I I   CO M P U T I N G  T H E LOW E R  B O U N D    k In the previous section  we discussed how uSplit segments the utility space and samples the input data sets in a way that enables the join selectivities to be estimated with small margins of error  closer to the high utility portion of the utility space  In this section  we discuss how to compute the utility score    k  associated with the k th best result of the join operation based on these selectivities  In the literature  this problem has been considered under different assumptions  14    15    25   In the most related work  25   authors have proposed a method to estimate the depth of a top k join query based on sample based estimators  however  none of these provide guarantees as to the error rate  while our goal is to match user provided con   dence targets  Let us consider two data sets  R1 i and R2 j  where  R1 i     r1 i and  R2 j     r2 j    Let us also consider a set  P  of utility segment pairs  or utility cells  and ask the question     What is the probability that the cells in P will  collectively  return exactly k results      This requires modeling the probability distribution of the sums of independent  but not identical  random variables  which themselves are distributed in a binomial fashion  While this is not straightforward  a reasonable approximation can be obtained by approximating each pi j  k  with a Poisson distribution  poisson     k   Theorem 3 1  Probability of Having k Results   The probability  pP  k  with which the set P of utility cells returns exactly k tuples is pP  k    poisson   P   k    e     P    P   k k    where   P      xi yj     P   i j     r1 i    r2 j      Proof Sketch  Let us consider two data sets  R1 i and R2 j  where  R1 i     r1 i and  R2 j     r2 j     and a pair of utility segments  xi and yj  from each data set  In the previous section  we computed the likelihood of a match between a given t1     R1 i and t2     R2 j   and represented this likelihood as   i j   Here  we note that  for each pair t1     R1 i and t2     R2 j   the corresponding likelihood of match can be modeled as a Bernoulli trial with a success probability of   i j   and a failure probability of  1       i j    34   Based on this  uSplit describes the probability  pi j  k   that the pair of segments  xi and yj  will result in exactly k results relying on a binomial model  pi j  k      r1 i    r2 j k         k i j     1       i j   r1 i  r2 j   k  Now  let us also consider a set  P  of utility segment pairs  or utility cells  and ask the question     What is the probability that the cells in P will  collectively  return exactly k results      This requires modeling the probability distribution of the sums of independent  but not identical  random variables  which themselves are distributed in a binomial fashion  While this is not straightforward  a reasonable approximation can be obtained by approximating each pi j  k  with a Poisson distribution  poisson     k   A commonly used statistical rule of thumb  34  is that it is possible to approximate a given binomial distribution  binom a  b  p      a b   p b  1   p  a   b   with a Poisson distribution  poisson     k    e           b b    where      ap  as long as a     20 and p     0 05  Therefore  if r1 i    r2 j     20  which is often the case   and the join selectivity    i j   is relatively small  we can approximate pi j  k  using Poisson distributions 10   Since the sum of independent Poisson processes with g parameters     1            g  itself is a Poisson process with parameter      1   i   g   i  34   the theorem follows    Given this  the probability  p     P  k   with which    P will return at least k tuples    can be computed as p     P  k      k   h pP  h     1               0   h k pP  h           Let p  be the maximum probability of error the user can accommodate in the estimation of   k  in other words  the con   dence lowerbound for the estimation of   k is 1   p    Let p     P     k  denote the probability with which the set P of utility cells will collectively return at least k tuples above the utility threshold     Given these  if C    x1        xn       y1        ym  denotes the set of all cells in the utility space  then we are looking for a utility score   k  where   k   arg max    p     C     k      1     p   Since  given     we are interested in only those results with utility scores above     we normalize the Poisson distribution parameter    C     in a way that takes into account the likelihood of the results having scores above     Given the above de   nition of p     C     k   the algorithm for computing   k   arg max   p     C     k  is shown in Figure 11  Note that  when the while loop of Step 5 of the algorithm ends  the    nal value of       is within      of the true value of   k  with con   dence 1     p    and thus  for suf   ciently small values of             can be used as an approximation of   k  Also  note that the Poisson distribution parameter    C     is computed assuming that the results are uniformly distributed in each utility cell   xi   yj       C  The utility space partitioning strategy presented in Section II which results in smaller cells nearer to the high utility region of the utility space  helps ensure that the assumption is more likely to hold at more critical  high utility  parts of the utility space  10 If the join selectivity is higher than 0 05  the Poisson approximation of the binomial distribution may not be appropriate  In that case  we compute the expected number of results    i j     r1 i    r2 j    that the pair  xi and yj   will return and we deduct this amount from the target  k  Inputs      The set  C    x1        xn      y1        ym   of utility cells      The error upper bound  p   1  given the segment boundaries  0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1  compute the utility score    i j   corresponding to each segment boundary intersection   xi  yj    2  compute the con   dence  p    Pi j    i j  k   for each segment boundary intersection   xi  yj    where Pi j     C is the set of utility cells to the north east of the segment boundary intersection   xi  yj    3     nd the largest   i j   such that p    Pi j    i j  k      1     p   and let           i j   4  let     be the next larger utility score of a segment boundary intersection  at this point we have             k         5  while                       do a  let                  2 b  if p    Pi j      k      1     p   then let            else let          6    k         7  return   k Fig  11  Algorithm for estimating   k IV  BA L A N C E D WO R K AL L O C AT I O N Once the target utility lowerbound    k  is computed within an acceptable error bound  the next step in the process is to partition the  suf   ciently high utility  portions of the input data  and to assign the resulting work quanta onto the available servers for join processing  In the simplest random split strategy  the data elements can be assigned to the available servers randomly  e g   hash partitioning   This approach is obviously very easy to implement  and could  in fact  ensure that the amount of input data assigned to the servers is balanced  On the other hand  in general  there is no guarantee that the useful workload  i e   workload for candidate input pairs with utility score greater than or equal to   k  will be balanced across different servers in the system  In contrast  a boundary aware  utility driven split strategy would rely on the utility statistics  which were already collected for obtaining the utility boundary   k     see Section II  to estimate the distribution of the workload in the utility space above   k  and partition the data along the utility dimensions in such a way that the useful workload  consisting of candidate input pairs  with utility score greater than or equal to   k  is balanced  and non useful workload is avoided as much as possible  As described in Section II  uSplit segments the two dimensions of the utility space  corresponding to data sources R1 and R2   for helping with the allocation of the sampling budget for estimating the join selectivities  For work partitioning  uSplit considers a second set of utility boundaries  0     0     1                u   1     u   1 and 0     0     1               v   1     v   1  which are uniformly spaced  i e        1     i     u and    1     j     v    i       i   1       xn and    j       j   1       ym  where   xn and   ym denote the sizes of the smallest segments for which statistics are available  see Section II C   During the statistics collection phase  see Section II and Figure 6   for each data partition    i      i   1    i   or   j      j   1    j    uSplit collects additional statistics including the numbers    1 i and   2 j   of data elements in these partitions 1000000 2000000 Count of Male Actors Score Distribution for Male Actors Count of Male Actors 0 1000000 2000000 Score Range Fig  12  Score distribution for male actors  the distribution for the female actors is similar uSplit selects one of the two sources  R1 or R2  as the pivot to drive the partitioning process  Without loss of generality  let us assume that the dimension corresponding to the utility partitions    i      i   1    i    is selected as the pivot  The work above the utility boundary  de   ned by    k and the combination function      for each pivot partition    i      i   1    i    is estimated using the selectivities that have been computed in Section II  Let W     W   i   denote the total work above the utility threshold for all partitions  If U servers are available for processing  we expect that each server will be assigned roughly W U units of work  Thus  uSplit aggregates the utility partitions into U roughly equi work  contiguous groups  or slices   each with     W U workload  The data corresponding to each slice are then allocated for one of the servers for join processing  V  EX P E R I M E N T S In this section  we evaluate the effectiveness of uSplit in  a  predicting join selectivities   b  estimating the utility threshold   k  and  c  end to end query processing time  For evaluation purposes  we used data sets with different characteristics and sizes   a  We have created synthetic data sets with up to 500M data elements  14GB  per source  We considered uniform  normal  mean   0 5  var   0 15   and Zip   an utility distributions   b  We have also run experiments using the Internet Movie Database  IMDB  data set  1   we focused on male and female actors  1M entries each   and used the average rating of the movies a given actor played in as the corresponding utility  Figure 12 shows that these ratings show a normal distribution  On the synthetic data set  we considered an equijoin  with     10 matches per data entry   For the actors data  we considered the following top k join query        nd the K highest scoring male female actor pairs who have played in at least one movie together     In order to observe the impact of de biasing  we de biased the data sets with Zip   an distributions  which has extreme bias   but left the actors and normal data sets intact  We used average  product  and Euclidean combinations  The experiments were executed over the Amazon EC2 platform with up to 20 RedHat CentOS 5 2 machines  each machine had 7 5 GB memory  4 EC2 64 bit Compute Units 1 E  04 2 E  04 3 E  04 Error Rate Error Rate   Predicted Selectivity   True Selectivity   with Adaptive Sampling              1 0 92 0 8 0 63 0 37 0 E 00 1 E  04 2 E  04 3 E  04 0 11 0 29 0 45 0 57 0 68 0 76 0 83 0 89 0 94 0 98 Error Rate Input  Data Utilities Error Rate   Predicted Selectivity   True Selectivity   with Adaptive Sampling              Fig  13  With adaptive sampling  errors in selectivity estimation are lower at high utility ranges     the true selectivity is     0 001  Uniform 10K data set  20 segments  segment scaling factor   1 1  sampling scaling factor   1 05  sampling budget   1000  Number of Elements above a Threshold Range Real Count Adaptive Seg  Uniform Seg      0 95 877 891 953     0 9 3393 3440 3704     0 8 22356 22402 23301     0 7 90965 92059 91696 Fig  14  Especially for high utility thresholds  adaptive segmentation provides highly precise predictions of the number of results  1M data set  segment scaling factor   1 1  num  of segments   23  0 1 1 10   Real   Uniform  Real   log scale  Adaptive vs  Uniform Sampling  as a function of the Sampling Scale Factor  Uniform Zipfian 0 01 0 1 1 10 1 05 1 1 1 15 2 3 4  Adaptive  Real   Uniform  Real   log scale  Sampling Scale Factor Adaptive vs  Uniform Sampling  as a function of the Sampling Scale Factor  Uniform Zipfian Normal Actors Fig  15  Effect of sampling scale factor  1M data  Euclidean combination  segment scaling factor   1 1  20 segments  sampling budget   1  of data   2 virtual cores with 2 EC2 compute units each   with high I O performance option  Once the data is sampled and partitioned  the actual joins on the servers were done using Vertica  2  DBMS  Vertica Analytic Database 3 5 10 0   Unless speci   ed otherwise  the results are averages of three runs  A  Effectiveness of the Adaptive Sampling Impact of Non Uniform Sampling  We    rst provide a highlevel overview of the impact of the adaptive approach to sampling  presented in Section II  As Figure 14 and Figure 13 show  the proposed strategies ensure that the selectivity estimation errors are very close to zero for the high utility region  which is the only region that matters for top k joins   instead of being uniform across the utility space  Figure 15 quanti   es the bene   ts of using non uniform sampling rates by varying the sampling scale parameter      between 1 05 and 4 0 and comparing the errors in the estimation of the number of results above a given threshold       1 33  to the errors resulting when uniform sampling is used  As the    gure shows  when        2 adaptive sampling leads to errors that are only 10  to 50  of the errors that occur with0 1 1 10 form  Real    Real   log scale  Error in the Estimation of number of Results  Uniform Sampling  Uniform Zipfian Normal Actors 0 01 0 1 1 10 0 10  0 50  1   Uniform  Real    Real   log scale  Sampling Budget  percentage of Data Size  Error in the Estimation of number of Results  Uniform Sampling  Uniform Zipfian Normal Actors  a  Uniform sampling 0 1 1 10 ptive  Real   Real   log scale  Error in the Estimation of number of Results  Adaptive Sampling  Uniform Zipfian Normal Actors 0 01 0 1 1 10 0 10  0 50  1   Adaptive  Real   Real   log scale  Sampling Budget  percentage of Data Size  Error in the Estimation of number of Results  Adaptive Sampling  Uniform Zipfian Normal Actors  b  Adaptive sampling Fig  16  Effect of the sampling budget  1M data  Euclidean comb     k   1 33  segment scaling factor   1 1  sampling scaling factor   1 05  20 segments  uniform sampling  Note also that using too large a    does not help as it concentrates too large a number of samples to a too small a portion of the utility space  resulting in degradations elsewhere  Impact of the Sampling Budget  Figure 16 veri   es that  as expected  a higher sampling budget helps reduce the amount of errors  but a relatively small number of samples are suf   cient for accurate estimates  Moreover adaptive sampling provides signi   cantly better result estimates  Impact of Using Non Uniform Utility Segments during Sampling  Figure 17 a  shows the bene   ts of using nonuniform segment boundaries during sampling  in the    gure  the segment size scale parameter      is varied between 1 05 and 2 0 and the corresponding errors in the estimation of the number of results above      1 33 are compared against the errors that occur when using uniform partition boundaries  i e        1 0   When the segment size scale parameter is 1 15  non uniform segmentation results in sampling errors that are only 10  to 40  of the errors that occur when using uniform boundaries  Further increasing the segment scaling factor however is not useful  while the very high utility region gets    nely segmented  the granularity drops quickly for values slightly lower than 1 0  resulting in degradations in accuracy  Figure 17 b  shows that using a larger number of segments does not imply better sampling  this is because  as discussed in Section II C  when the number of data elements in a segment is too small  this may negatively effect the estimation of error margins  This is con   rmed by Figure 17 b  which shows that the problem is more pronounced when the utilities are biased  Impact of the De biasing  Figure 18 tracks the success rate of obtaining top k results using different target con   dence 0 1 1 10 e  Real   Uniform  Real   log scale  Adaptive vs  Uniform Sampling  as a function of the Utility Segment Scaling factor  Uniform Zipfian 0 01 0 1 1 10 1 05 1 1 1 15 1 5 2  Adaptive  Real   Uniform  Real   log scale  Utility Segment Scaling Factor Adaptive vs  Uniform Sampling  as a function of the Utility Segment Scaling factor  Uniform Zipfian Normal Actors  a  Segment scaling factor 1 10 ve Real   Uniform    Real      log scale  Adaptive vs  Uniform Sampling  as a function of the number of utility segments  Uniform Zi fi  DE BIASED  0 1 1 10 20 50 100  Adaptive Real   Uniform    Real      log scale  Number of Segments Adaptive vs  Uniform Sampling  as a function of the number of utility segments  Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actor  NOT DE  BIASED   b  Number of segments Fig  17  Effects of  a  segment size scaling factor and  b  the number of segments  1M data  Euclidean combination  sampling scaling factor   1 05  20 segments  sampling budget   1   60  70  80  90  100  Percent Success Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  30  40  50  60  70  80  90  100  30  50  70  90  95  99  99 99  Percent Success Target Confidence Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actors  NOT DE  BIASED   a  k   100 60  70  80  90  100  ercent Success Success Rate vs  Confidence Target Uniform 30  40  50  60  70  80  90  100  30  50  70  90  95  99  99 99  Percent Success Target Confidence Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actors NOT DE  BIASED   b  k   1000 Fig  18  Effect of de biasing  10K data  average  sampling scaling factor   1 05  50 segments  segment scaling factor   1 1  sampling budget   10   rates  1     p   over 100 runs  As the    gure shows  for the uniform and the de biased Zip   an data sets  despite the initial extreme bias of the Zip   an distribution  the algorithm is able to provide success rates larger than or almost equal to the target con   dence  For non de biased data sets  Normal and Actors   on the other hand  the success rates are lower  especially for large ks and high target con   dences  This is because  when the utilities are skewed  this increases the chances of having sampling errors  The problem is compounded for large k10 100 1000 10000 btained no  of Results Target Number of Results vs  Obtained Number of Results  Confidence Target   99   Euclidean Comb   Uniform Zipfian Normal Actors 1 10 100 1000 10000 100 1000 10000 Obtained no  of Results Target no  of Results  k  Target Number of Results vs  Obtained Number of Results  Confidence Target   99   Euclidean Comb   Uniform Zipfian Normal Actors  a  Data distribution 10 100 1000 10000 tained no  of Results Target Number of Results vs  Obtained Number of Results  Confidence  target  99   Zipfian data  Average Product E lid 1 10 100 1000 10000 100 1000 10000 Obtained no  of Results Target no   of Results k  Target Number of Results vs  Obtained Number of Results  Confidence  target  99   Zipfian data  Average Product Euclidean  b  Combination function Fig  19  Targeted vs  obtained results  1M data  sampling scaling factor   1 05  20 segments  segment scaling factor   1 1  sampling budget   1   values  which lead to relatively lower   k values  thus moving into the regions of the utility space where sampling errors are relatively larger  This highlights the importance of debiasing the input data sets through an initial high level utility distribution analysis  Target Number of Results vs  Obtained Number of Results  Figure 19 shows the relationship between the target number of results  k  and the obtained number of results  As shown in this    gure  uSplit is highly accurate in matching the target number of results and this is true for different data distributions  Figure 19 a   and combination functions  Figure 19 b    B  Impact of uSplit on Processing Times After the above experiments in which we have explored the impact of uSplit based sampling on the success rate in top k result enumeration  in this section  we consider the execution time and scalability  In these experiments  we use two data sources  each with 500M entries  Each entry joins with     10 entries in the other source  The score merge function is average and the data utilities are uniformly distributed  Experiments are run on EC2  In implementing uSplit partitioning  we used the combined output sampling strategy  the costliest    i e   worst case    of the sampling strategies  which requires a full scan over the data  see Section II   Cost of top k Retrieval w o uSplit  As Figure 20 a  shows  the full join of these two sources using the Vertica  2  column DBMS  Vertica Analytic Database 3 5 10 0  on 10 EC2 machines takes 20 2 hours  Even using the    limit k    option with an order by clause  the execution time drops only to 358mins for k   100M and to 56mins for k   1000  Cost of Retrieval using uSplit based Thresholding  As described in Section II  uSplit piggy backs the data sampling process on the up stream operator  Nevertheless  523 389 358 56 1255 200 400 600 800 1000 1200 M i n u t e s Time with Vertica   limit K   option  10 servers  523 389 358 56 1255 0 200 400 600 800 1000 1200 900M 400M 100M 1000 FULL M i n u t e s Target   of Results Time with Vertica   limit K   option  10 servers  data loading q</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Hybrid In Database Inference for Declarative Information Extraction ### Daisy Zhe Wang University of California  Berkeley Michael J  Franklin University of California  Berkeley Minos Garofalakis Technical University of Crete Joseph M  Hellerstein University of California  Berkeley Michael L  Wick University of Massachusetts  Amherst ABSTRACT In the database community  work on information extraction  IE  has centered on two themes  how to effectively manage IE tasks  and how to manage the uncertainties that arise in the IE process in a scalable manner  Recent work has proposed a probabilistic database  PDB  based declarative IE system that supports a leading statistical IE model  and an associated inference algorithm to answer top k style queries over the probabilistic IE outcome  Still  the broader problem of effectively supporting general probabilistic inference inside a PDB based declarative IE system remains open  In this paper  we explore the in database implementations of a wide variety of inference algorithms suited to IE  including two Markov chain Monte Carlo algorithms  the Viterbi and the sum product algorithms  We describe the rules for choosing appropriate inference algorithms based on the model  the query and the text  considering the trade off between accuracy and runtime  Based on these rules  we describe a hybrid approach to optimize the execution of a single probabilistic IE query to employ different inference algorithms appropriate for different records  We show that our techniques can achieve up to 10 fold speedups compared to the non hybrid solutions proposed in the literature  Categories and Subject Descriptors H 2 4  Database Management   Systems   Textual databases  Query Processing  G 3  Mathematics of Computing   Probability and statistics   Probabilistic algorithms  including Monte Carlo  General Terms Algorithms  Performance  Management  Design Keywords Probabilistic Database  Probabilistic Graphical Models  Information Extraction  Conditional Random Fields  Viterbi  Markov chain Monte Carlo Algorithms  Query Optimization Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee ### SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  1 Introduction For most organizations  textual data is an important natural resource to fuel data analysis  Information extraction  IE  techniques parse raw text and extract structured objects that can be integrated into databases for querying  In the past few years  declarative information extraction systems  1  2  3  4  have been proposed to effectively manage IE tasks  The results of IE extraction are inherently uncertain  and queries over those results should take that uncertainty into account in a principled manner  Research in probabilistic databases  PDBs  has been exploring scalable tools to reason about these uncertainties in the context of structured query languages and query processing  5  6  7  8  9  10  11  12   Our recent work  4  13  has proposed a PDB system that natively supports a leading statistical IE model  conditional random    elds  CRFs    and an associated inference algorithm  Viterbi   It shows that the in database implementation of the inference algorithms enables   1  probabilistic relational queries that returns top k results over the probabilistic IE outcome   2  a tight integration between the relational and inference operators  which leads to signi   cant speed up by performing query driven inference  While this work is an important step towards building a probabilistic declarative IE system  the approach is limited by the capabilities of the Viterbi algorithm  which can only handle top k style queries over a limited class of CRF models  linear chain models  which do a poor job capturing features like repeated terms  Different inference algorithms are needed to deal with non linear CRF models  such as skip chain CRF models  complex IE queries that induce cyclic models over the linear chain CRFs  and marginal inference queries that produce richer probabilistic outputs than top k  The broader problem of effectively supporting general probabilistic inference inside a PDB based declarative IE system remains open  In this paper  we    rst explore the in database implementation of a number of inference algorithms suited to a broad variety of models and outputs  two variations of the general sampling based Markov chain Monte Carlo  MCMC  inference algorithm   Gibbs Sampling and MCMC Metropolis Hastings  MCMC MH    in addition to the Viterbi and the sum product algorithms  We compare the applicability of these four inference algorithms and study the data and the model parameters that affect the accuracy and runtime of those algorithms  Based on those parameters  we develop a set of rules for choosing an inference algorithm based on the characteristics of the model and the data  More importantly  we study the integration of relational query processing and statistical inference algorithms  and demonstrate that  for SQL queries over probabilistic extraction results  the proper choice of IE inference algorithm is not only model dependent  but also query  and text dependent  Such dependencies arise when re lational queries are applied to the CRF model  inducing additional variables  edges and cycles  and when the model is instantiated over different text  resulting in model instances with drastically different characteristics  To achieve good accuracy and runtime performance  it is imperative for a PDB system to use a hybrid approach to IE even within a single query  employing different algorithms for different records  In the context of our CRF based PDB system  we describe query processing steps and an algorithm to generate query plans that apply hybrid inference for probabilistic IE queries  Finally  we describe example queries and experiment results showing that such hybrid inference techniques can improve the runtime of the query processing by taking advantage of the appropriate inference methods for different combinations of query  text  and CRF model parameters  Our key contributions can be summarized as follows    We show the ef   cient implementation of two MCMC inference algorithms  in addition to the Viterbi and the sumproduct algorithms  and we identify a set of parameters and rules for choosing different inference algorithms over models and datasets with different characteristics    We describe query processing steps and an algorithm to generate query plans that employ hybrid inference over different text within the same query  where the selection of the inference algorithm is based on all three factors of data  model  and query    Last  we evaluate our approaches and algorithms using three real life datasets  DBLP  NYTimes  and Twitter  The results show that our hybrid inference techniques can achieve up to 10 fold speedups compared to the non hybrid solutions proposed in the literature  Based on our experience in implementing different inference algorithms  we also present four design guidelines for implementing statistical methods in the database in the Appendix  2 Related Work In the past few years  declarative information extraction systems  1  2  3  4  13  have been proposed to effectively manage information extraction  IE  tasks  The earlier efforts in declarative IE  1  2  3  lack a uni   ed framework supporting both a declarative interface as well as the state of the art probabilistic IE models  Ways to handle uncertainties in IE have been considered in  14  15   A probabilistic declarative IE system has been proposed in  4  13   but it only supports the Viterbi algorithm  which is unable to handle complex models that arise naturally from advanced features and relational operators  In the past decade  there has been a groundswell of work on Probabilistic Database Systems  PDBS   5  6  7  8  9  10  11  12   As shown in previous work  8  10  12   Graphical Modeling techniques can provide robust statistical models that capture complex correlation patterns among variables  while  at the same time  addressing some computational ef   ciency and scalability issues as well  In addition   8  showed that other approaches to represent and handle uncertainty in database  5  6   can be uni   ed under the framework of Graphical Models  which express uncertainties and dependencies through the use of random variables and joint probability distribution  However  there is no work addressing the problem of effectively supporting and optimizing different probabilistic inference algorithms in a single PDB  especially in the IE setting  3 Background This section covers our de   nition of a probabilistic database  the conditional random    elds  CRF  model and the different types of inference algorithms over CRF models in the context of information extraction  We also introduce a template for the types of IE queries studied in this paper  3 1 Probabilistic Database As we described in  10   a probabilistic database DB p consists of two key components   1  a collection of incomplete relations R with missing or uncertain data  and  2  a probability distribution F on all possible database instances  which we call possible worlds  and denote by pwd DB p    An incomplete relation R 2R is de   ned over a schema A d  A p comprising a  non empty  subset A d of deterministic attributes  that includes all candidate and foreign key attributes in R   and a subset A p of probabilistic attributes  Deterministic attributes have no uncertainty associated with any of their values  A probabilistic attribute A p may contains missing or uncertain values  The probabilistic distribution F of these missing or uncertain values is represented by a probabilistic graphical model  such as Baysian Networks or Markov Random Fields  Each possible database instance is a possible completion of the missing and uncertain data in R  3 2 Conditional Random Fields The linear chain CRF  16  17   similar to the hidden markov model  is a leading probabilistic model for solving IE tasks  In the context of IE  a CRF model encodes the probability distribution over a set of label random variables  RVs  Y  given the value of a set of token RVs X  We denote an assignment to X by x and to Y by y  In a linear chain CRF model  label yi is correlated only with label yi1 and token xi  Such correlations are represented by the feature functions ffk yi  yi1  xi g K k 1   EXAMPLE 1  Figure 1 a  shows an example CRF model over an address string x    2181 Shattuck North Berkeley CA USA     Observed  known  variables are shaded nodes in the graph  Hidden  unknown  variables are unshaded  Edges in the graph denote statistical correlations  The possible labels are Y   fapt num  streetnum  streetname  city  state  countryg  Two possible feature functions of this CRF are  f1 yi  yi1  xi     xi appears in a city list     yi   city  f2 yi  yi1  xi     xi is an integer     yi   apt num    yi1   streetname  A segmentation y   fy1       yT g is one possible way to tag each token in x of length T with one of the labels in Y   Figure 1 d  shows two possible segmentations of x and their probabilities  DEFINITION 3 1  Let ffk yi  yi1  xi g K k 1 be a set of realvalued feature functions  and     f kg 2 R K be a vector of real valued parameters  a CRF model de   nes the probability distribution of segmentations y given a speci   c token sequence x  p y j x    1 Z x  expf XT i 1 XK k 1  kfk yi  yi1  xi g   1  where Z x  is a standard normalization function that guarantees the probability distribution sums to 1 over all possible extractions    3 3 Relational Representation of Text and CRF We implement IE algorithms over the CRF model within a database using the relational representations of text and the CRF based distribution in the token table TOKENTBL and the factor table MR respectively         a  2181 Shattuck North Berkeley CA USA X tokens Y labels id docID pos token Label 1 1 0 2181 2 1 1 Shattuck 3 1 2 North 4 1 3 Berkeley 5 1 4 CA 6 1 5 USA  b  token prevLabel label score 2181  DIGIT  null street num 22 2181  DIGIT  null street name 5           Berkeley street street name 10 Berkeley street city 25           c   d  x 2181 Shattuck North Berkeley CA USA y1 street num street name city city state country  0 6  y2 street num street name street name city state country  0 1  Figure 1   a  Example CRF model   b  Example TOKENTBL table   c  Example MR table   d  Two possible segmentations y1  y2  Token Table  The token table TOKENTBL  as shown in Figure 1 b   is an incomplete relation R in DB p   which stores a set of documents or text strings D as a relation in a database  in a manner akin to the inverted    les commonly used in information retrieval  TOKENTBL  id  docID  pos  token  label p   TOKENTBL contains one probabilistic attribute   label p   and the main goal of IE is to perform inference on label p   As shown in the schema above  each tuple in TOKENTBL records a unique occurrence of a token  which is identi   ed by the text string ID  docID  and the position  pos  the token is taken from  The id    eld is simply a row identi   er for the token in TOKENTBL  Factor Table  The probability distribution F over all possible    worlds    of TOKENTBL can be computed from the MR  The MR is a materialization of the factor tables in the CRF model for all the tokens in the corpus D  The factor tables   yi  yi1 j xi   as shown in Figure 1 c   represent the correlation between xi  yi  and yi1  and are computed by the weighted sum of a set of feature functions in the CRF model    yi  yi1 j xi    PK k 1  kfk yi  yi1  xi   As in the following schema  each unique token string xi is associated with an array  which contains a set of scores ordered by fprevLabel  labelg  MR  token  score ARRAY    3 4 Inference Queries over a CRF Model There are two types of inference queries over the CRF model  17     Top k Inference  The top k inference computes the label sequence y  i e   extraction  with the top k highest probabilities given a token sequence x from a text string d  Constrained top k inference  18  is a special case  where the top  k extractions are computed  conditioned on a subset of the token labels  that are provided as evidence    Marginal Inference  Marginal inference computes a marginal probability p yt  yt 1       yt kjx  s  over a single label or a sub sequence of labels conditioned on the set of evidence s   fs1       sT g  where si is either NULL  i e   no evidence  or the evidence label for yi  Many inference algorithms are known that can answer the above inference queries over the CRF models  varying in their effectiveness for different CRF characteristics  e g   shape of the graph   In the next sections  three inference algorithms will be described  Viterbi  sum product  Markov chain Monte Carlo  MCMC  methods  3 5 Viterbi Algorithm Viterbi  a special case of the Max Product algorithm  19  20  can compute top k inference for linear chain CRF models  Viterbi is a dynamic programming algorithm that computes a two dimensional V matrix  where each cell V  i  y  stores a ranked list of partial label sequences  i e   paths  up to position i ending with label y and ordered by score  Based on Equation  1   the recurrence to compute the top 1 segmentation is as follows  V  i  y    8     maxy0  V  i  1  y 0     PK k 1  kfk y  y 0   xi    if i   0 0  if i   1   2  The top 1 extraction y   can be backtracked from the maximum entry in V  T  yT    where T is the length of the token sequence x  The complexity of the Viterbi algorithm is O T   jY j 2    where jY j is the number of possible labels  The constrained top k inference can be computed by a variant of the Viterbi algorithm which restricts the chosen labels y to conform with the evidence s  3 6 Sum Product Algorithm Sum product  i e   belief propagation  is a message passing algorithm for performing inference on graphical models  such as CRF  19   The simplest form of the algorithm is for tree shaped models  in which case the algorithm computes exact marginal distributions  The algorithm works by passing real valued functions called messages along the edges between the nodes  These contain the    in   uence    that one variable exerts on another  A message from a variable node yv to its    parent    variable node yu in a tree shaped model is computed by summing the product of the messages from all the    child    variables of yv in C yv  and the feature function f yu  yv  between yv and yu over variable yv   yv yu  yu    X yv f yu  yv  Y y  u2C yv   y  u yv  yv    3  Before starting  the algorithm    rst designates one node as the root  any non root node which is connected to only one other node is called a leaf  In the    rst step  messages are passed inwards  starting at the leaves  each node passes a message along the edge towards the root node  This continues until the root has obtained messages from all of its adjoining nodes  The marginal of the root note can be computed at the end of the    rst step  The second step involves passing the messages back out  starting at the root  messages are passed in the reverse direction  until all leaves have received their messages  Like Viterbi  the complexity of the sum product algorithm is also O T   jY j 2    Variants of the sum product algorithm for cyclic models require either an intractable junction tree step  or a variational approximation such as loopy belief propagation  BP   In this paper  we do not study these variants further as they are either intractable  junction tree   or can fail to converge  loopy BP  on models with longdistance dependencies such as those we discussed in this paper  3 7 MCMC Inference Algorithms Markov chain Monte Carlo  MCMC  methods are a class of randomized algorithms for estimating intractable probability distributions over large state spaces by constructing a Markov chain sampling process that converges to the desired distribution  Relative to other sampling methods  the main bene   ts of MCMC methods are that they  1  replace a dif   cult sampling procedure from a highdimensional target distribution   w  that we wish to sample with an easy sampling procedure from a low dimensional local distribution q  jw   and  2  sidestep the  P hard computational problem of computing a normalization factor  We call q  jw  a    pro       GIBBS  N  1 w0   INIT    w   w0     initialize 2 for idx   1       N do 3 i   idx n     propose variable to sample next 4 w0     wi j wi     generate sample 5 return next w0    return a new sample 6 w   w0      update current world 7 endfor Figure 2  Pseudo code for Gibbs sampling algorithm over a model with n variables  posal distribution     which   conditioned on a previous state w    probabilistically produces a new world w 0 with probability q w 0 jw   In essence  we use the proposal distribution to control a random walk among points in the target distribution  We review two MCMC methods we will adapt to our context in this paper  Gibbs sampling and Metropolis Hastings  MCMC MH   3 7 1 Gibbs Sampling Let w    w1  w2       wn  be a set of n random variables  distributed according to   w   The proposal distribution of a speci   c variable wi is its marginal distribution q  jw      wijwi  conditioned on wi  which are the current values of the rest of the variables  The Gibbs sampling algorithm  i e   Gibbs sampler     rst generates the initial world w0  for example  randomly  Next  samples are drawn for each variable wi 2 w in turn  from the distribution   wijwi   Figure 2 shows the pseudo code for the Gibbs sampler that returns N samples  In Line 4    means a new sample w 0 is drawn according to the proposal distribution   wijwi   3 7 2 Metropolis Hastings  MCMC MH  Like Gibbs  the MCMC MH algorithm    rst generates an initial world w0  e g   randomly   Next  samples are drawn from the proposal distribution w 0   q wijw   where a variable wi is randomly picked from all variables  and q wijw  is a uniform distribution over all possible values  Different proposal distribution q wijw  can be used  which results in different convergence rates  Lastly  each resulting sample is either accepted or rejected according to a Bernoulli distribution given by parameter      w 0   w    min 1    w0  q wjw0     w q w0 jw     4  The acceptance probability is determined by the product of two ratios  the model probability ratio   w 0     w   which captures the relative likelihood of the two worlds  and the proposal distribution ratio q wjw 0   q w 0 jw   which eliminates the bias introduced by the proposal distribution  3 8 Query Template Over the CRF based IE from text  the queries we consider are probabilistic queries  which inference over the probabilistic attribute label p in the TOKENTBL table  Each TOKENTBL is associated with a speci   c CRF model stored in the MR table  Such CRFbased IE is captured by a sub query with logic that produces the IE results from the base probabilistic TOKENTBL tables  The subquery consists of a relational part Qre over the probabilistic token tables TOKENTBL and the underlying CRF models  followed by an inference operator Qinf   A canonical    query template    captures the logic for the    SQL IE    sub query in Figure 3  It supports SPJ queries  aggregate conditions and two types of inference operators  Top k and Marginal  over the probabilistic TOKENTBL tables  The relational part of a    SQL IE    query Qre    rst speci   es  in the FROM clause  the TOKENTBL table s  over which the query and extraction is performed  SELECT Top k T1 docID  T1 pos exist      Marginal T1 docID  T1 pos exist       Top k T1 docID  T2 docID  T1 pos T2 pos exist       Marginal T1 docID T2 docID  T1 pos T2 pos exist    FROM TokenTbl1 T1   TokenTbl2 T2  WHERE T1 label      bar1     and T1 token      foo1      and T1 docID   X   and T1 pos   Y   and T1 label   T2 label   and T1 token   T2 token   and T1 docID   T2 docID  GROUP BY T1 docID   T2 docID  HAVING  aggregate condition  Figure 3  The    SQL IE    query template  The WHERE clause lists a number of possible selection as well as join conditions over the TOKENTBL tables  These conditions when involve label p are probabilistic conditions  and deterministic otherwise  For example  a probabilistic condition label    person    speci   es the entity types we are looking for is    person     while a deterministic condition token    Bill    speci   es the name of the entity we are looking for is    Bill     We can also specify a join condition T1 token T2 token and T1 label T2 label that two documents need to contain the same entity name with the same entity type  In the GROUP BY and the HAVING clause  we can specify conditions on an entire text    document     An example of such aggregate condition over a bibliography document can be that all title tokens are in front of all the author tokens  Following the Possible World Semantics  5   the execution of these relational operators involve modi   cation to the original graphical models as will be shown in Section 4 1  The inference part Qinf   of a    SQL IE    query  takes the docID  the pos  and the CRF model resulting from Qre as input  The inference operation is speci   ed in the SELECT clause  which can be either a Top k or a Marginal inference  The inference can be computed over different random variables in the CRF model   1  a sequence of tokens  e g   a document  speci   ed by docID  or  2  a token at a speci   c location speci   ed by docID and pos  or  3  the    existence     exist  of the result tuple  The    existence    of the result tuple becomes probabilistic with a selection or join over a probabilistic attribute  where exist variables are added to the model  10   For example  the inference Marginal T1 docID T1 pos   for each position  T1 docID T1 pos  computed from Qre  returns the distribution of the label variable at that position  The inference Marginal T1 docID exist  computes the marginal distribution of exist variable for each result tuple  We can also specify an inference following a join query  For example  the inference Top k T1 docID T2 docID   for each document pair  T1 docID T2 docID   returns the top k highest probability joint extractions that satisfy the join constraint  4 In Database MCMC Inference In this section  we    rst describe IE models that are cyclic  e g   the skip chain CRF model  and review the way that simple relational queries can often induce cyclic models   even over text that is itself modeled by simple linear chain CRFs  Such cyclic models call for an ef   cient general purpose inference algorithm such as an MCMC algorithm  Next  we describe our ef   cient in database implementation of the Gibbs sampler and MCMC MH  Finally  we discuss query driven sampling techniques that push the query constraints into the MCMC sampling process  4 1 Cycles from IE Models and Queries In many IE tasks  good accuracy can only be achieved using nonlinear CRF models like skip chain CRF models  which model the            IBM Corp  said that IBM for IBM  Figure 4  A skip chain CRF model that includes skip edges between non consecutive tokens with the same string  e g     IBM       a   b   c  Bill by Bill                                                                       assigned Clinton today CEO Gates talked about Bill Clinton met with Bill assigned by Bill Clinton today The Viterbi Algorithm Dave Forney Figure 5   a  and  b  are example CRF models after applying a join query over two different pairs of documents   c  is the resulting CRF model from a query with an aggregate condition  correlation not only between the labels of two consecutive tokens as in linear chain CRF  but also between those of non consecutive tokens  For example  a correlation can be modeled between the labels of two tokens in a sentence that have the same string  Such a skip chain CRF model can be seen in Figure 4  where the correlation between non consecutive labels  i e   skip chain edges  form cycles in the CRF model  In simple probabilistic databases with independent base tuples  the    safe plans     5  give rise to tree structured graphical models  8   where the exact inference is tractable  However  in a CRF based IE setting  an inverted    le representation of text in TOKENTBL inherently has cross tuple correlations  Thus  even queries with    safe plans    over the simple linear chain CRF model  result in cyclic models and intractable inference problems  For example  the following query computes the marginal inference Marginal T1 docID T2 docID exist   which returns pairs of docIDs and the probabilities of the existence  exist  of their join results  The join query is performed between each document pair on having the same token strings labeled as    person     The join query over the base TOKENTBL tables adds cross edges to the pair of linear chain CRF models underlying each document pair  Figure 5 a   b  shows two examples of the resulting CRF model after the join query over two different pairs of documents  As we can see  the CRF model in  a  is tree shaped  and the one in  b  is cyclic  Q1   Probabilistic Join Marginal  SELECT Marginal T1 docID T2 docID exist  FROM TokenTbl1 T1  TokenTbl2 T2 WHERE T1 label   T2 label and T1 token   T2 token and T1 label      person     Another example is a simple query to compute the top k extraction conditioned on an aggregate constraint over the label sequence of each document  e g   all    title    tokens are in front of    author    tokens   This query induces a cyclic model as shown in Figure 5 c   Q2   Aggregate Constraint Top k  SELECT Top k T1 docID  FROM TokenTbl1 T1 GROUP BY docID HAVING  aggregate constraint    true  1 CREATE FUNCTION Gibbs  int  RETURN VOID AS 2    3    compute the initial world  genInitWorld   4 insert into MHSamples 5 select setval    world id    1  as worldId  docId  pos  token  trunc random   num label 1  as label 6 from tokentbl  7    generate N sample proposals  genProposals   8 insert into Proposals with X as   select foo id  foo docID   tmp  bar doc len  as pos 9 from   select id    id 1    1 numDoc  1  as docID    id 1     1 numDoc   as tmp 10 from generate series 1  1  id  foo  doc id tbl bar 11 where foo doc id   bar doc id   12 select X id S docId S pos S token  null as label  null  integer   as prevWorld  null  integer   as factors 13 from X  tokentbl S 14 where X docID   S docID and X pos   S pos  15    fetch context  initial world and factor tables 16 update proposals S1 17 set prev world    select   from getInitialWorld S1 docId   18 from proposals S2 19 where S1 docId    S2 docId and S1 id   S2 id 1  20 update proposals S1 21 set factors    select   from getFactors S1 docId   22 from proposals S2 23 where S1 docId    S2 docId and S1 id   S2 id 1  24    generate samples  genSamples   25 insert into MHSamples 26 select worldId  docId  pos  token  label 27 from   28 select nextval    world id     worldId  docId  pos  token  label  getalpha agg  docId pos label prev world factors    getalpha io  over  order by id  alpha 29 from  select   from proposals order by id  foo  foo  30    31 LANGUAGE SQL  Figure 6  The SQL implementation of Gibbs sampler takes input N     the number of samples to generate  Next  we describe general inference algorithms for such cyclic models  4 2 SQL Implementation of MCMC Algorithms Both the Gibbs sampler and the MCMC MH algorithm are iterative algorithms  which contain three main steps  1  initialization  2  generating proposals  and 3  generating samples  They differ in their proposal and sample generation functions  We initially implemented the MCMC algorithms in the SQL procedure language provided by PostgreSQL   PL pgSQL   using iterations and three User De   ned Functions  UDF   s     GENINITWORLD   to compute the initialized world  line 1 for Gibbs in Figure 2     GENPROPOSAL   to generate one sample proposal  line 3 for Gibbs in Figure 2     GENSAMPLE   to compute the corresponding sample for a given proposal  line 4 for Gibbs in Figure 2   However  this implementation ran hundreds of times slower than the Scala Java implementation described in  12   This is mainly because calling UDF   s iteratively a million times in a PL pgSQL function is similar to running a SQL query a million times  A more ef   cient way is to    decorrelate     and run a single query over a million tuples  The database execution path is optimized for this approach  With this basic intuition  we re implemented the MCMC algorithms  where the iterative procedures are translated into set operations in SQL The ef   cient implementation of the Gibbs sampler is shown in Figure 6  which uses the feature of window functions introduced in PostgreSQL 8 4  MCMC MH can be implemented ef   ciently in a similar way with some simple adaptations  This implementation achieves similar  within a factor of 1 5  runtime compared to the Scala Java implementation of the MCMC algorithms  as shown in the results in Section 7 1  4 3 Query Driven MCMC Sampling Previous work  4  has developed query driven techniques to integrate probabilistic selection and join conditions into the Viterbi algorithm over the linear chain CRF model  However  the kind of constraint that Viterbi can handle is limited and speci   c to the Viterbi and potentially the sum product algorithm  In this section  we explore query driven techniques for the sampling based MCMC inference algorithms  Query driven sampling is needed to compute inference conditioned on the query constraints  Such query constraints can be highly selective  where most samples generated by the vanilla MCMC methods do not    qualify     i e   satisfy the constraints   Thus  we need to adapt the MCMC methods by pushing the query constraints into the sampling process  Note that our adapted  query driven MCMC methods still converge to the target distribution as long as the proposal function can reach every    quali   ed    world in a    nite number of steps  There are three types of query constraints   1  selection constraints   2  join constraints  and  3  aggregate constraints  Both  1  and  2  were studied for Viterbi in  4   The following query contains an example selection constraint  which is to    nd the top k highest likelihood extraction that contains a    person    entity    Bill     Q3   Selection Constraint Top k  SELECT Top k T1 docID  FROM TokenTbl1 T1 WHERE token      Bill    and label      person    An example of a join constraint can be found in Q1 in Section 4 1  and an example of an aggregate constraint can be found in Q2 in the same Section  The naive way to answer those conditional queries using MCMC methods is to     rst  generate a set of samples using Gibbs sampling or MCMC MH regardless of the query constraint  second     lter out the samples that do not satisfy the query constraint  last  compute the query over the remaining    quali   ed    samples  In contrast  our query driven sampling approach pushes the query constraints into the MCMC sampling process by restricting the worlds generated by GENINITWORLD    GENPROPOSALS   and GENSAMPLES   functions  so that all the samples generated satisfy the query constraint  One of the advantages of the MCMC algorithms is that the proposal and sample generation functions can naturally deal with the deterministic constraints  which might induce cliques with high tree width in the graphical model  Such cliques can easily    blow up    the complexity of known inference algorithms  19   We exploit this property of MCMC to develop query driven sampling techniques for different types of queries  The query driven GENINITWORLD   function generates an initial world that satis   es the constraint  The    rst    quali   ed    sample can either be speci   ed according to the query or generated from random samples  The query driven GENPROPOSAL   and GENSAMPLES   functions are called iteratively to generate new samples that satisfy the constraint  The next    quali   ed    jump  i e   new sample  can be generated by restricted jumps according to the query constraints or from random jumps  5 Choosing Inference Algorithms Different inference algorithms over the probabilistic graphical models have been developed in a diverse range of communities  e g   natural language processing  machine learning  etc   The characteristics of these inference algorithms  e g   applicability  accuracy  convergence rate  runtimes  over different model structures have since been studied to help modeling experts select an appropriate inference algorithm for a speci   c problem  19   In this section  we    rst compare the characteristics of the four inference algorithms we have developed over the CRF model  Next we introduce parameters that capture important properties of the model and data  Using these parameters  we then describe a set of rules to choose among different inference algorithms  5 1 Comparison between Inference Algorithms We have implemented four inference algorithms over the CRF model for IE applications   1  Viterbi   2  sum product  and two samplingbased MCMC methods   3  Gibbs Sampling and  4  MCMC MetropolisHastings  MCMC MH   In Table 1  we show the applicability of these algorithms to different inference tasks  e g  top k  or marginal  on models with different structures  e g   linear chain  tree shaped  cyclic   As we can see  Viterbi  Gibbs and MCMC MH can all compute top k queries over the linear chain CRF models  sum product  Gibbs and MCMC MH can all compute marginal queries over the linear chain and tree shaped models  while only MCMC algorithms can compute queries over cyclic models  Although there are heuristic adaptations of the sum product algorithm for cyclic models  past literature found MCMC methods to be more effective in handling complicated cyclic models with long distance dependencies and deterministic constraints  21  22   In terms of handling query constraints  Viterbi and sum product algorithms can only handle selection constraints  Gibbs sampling can handle selection constraints and aggregate constraints that do not break the distribution into disconnected regions  On the other hand  MCMC MH can handle arbitrary constraints in the    SQL IE    queries  5 2 Parameters Next  we introduce a list of parameters that affect the applicability  accuracy and runtime of the four inference algorithms that we have just described  1  Data Size  the size of the data is measured by the total number of tokens in information extraction  2  Structure of Grounded Models  the structural properties of the model instantiations over data   a  shape of the model  i e   linear chain  tree shaped  cyclic    b  maximum size of the clique   c  maximum length of the loops  e g   skip chain in linear CRF  3  Correlation Strength  the relative strength of transitional correlation between different label variables  4  Label Space  the number of possible labels  The data size affects the runtime for all the inference algorithms  The runtime of Viterbi and sum product algorithms is linear to the data size  The MCMC algorithms are iterative optimizations that can be stopped at any time  but the number of samples needed to converge depend linearly on the size of the data  The structure of the grounded model can be quanti   ed with three parameters  shape of the model  maximum size of the clique and the maximum length of the loops  The    rst parameter determines the applicability of the models  and is also the most important factorinference Top k Marginal Constraints algorithm Chain Tree Cyclic Chain Tree Cyclic Some Arbitrary Viterbi X X sum product X X X MCMC Gibbs X X X X X X X MCMC MH X X X X X X X Table 1  Applicability of different inference algorithms for different queries  e g   top k  marginal  over different model structures  e g   linear chain  tree shaped  cyclic   and in handling query constraints  in the accuracy and the runtime of the inference algorithms over the model  Although not studied in this paper  the maximum clique size and the length of the loops play an important role in the runtime of several known inference algorithms  including  for example  the junction tree and the loopy belief propagation algorithms   19   The correlation strength is the relative strength of the transition correlation between different label variables over the state correlation between tokens with their corresponding labels  The correlation strength does not in   uence the accuracy or the runtime of the Viterbi or the sum product algorithm  However  it is a significant factor in the accuracy and runtime of the MCMC methods  especially the Gibbs algorithm  Weaker correlation strengths result in faster convergence for the Gibbs sampler  At the extreme  zero transition correlation results in complete label independence  rendering consecutive Gibbs samples independently  which would converge very quickly  The size of the label space of the model is also an important factor of the runtime of all the inference algorithms  The runtime of the Viterbi and sum product algorithms is quadratic in the size of the label space  while the runtime of the Gibbs algorithm is linear in the label space because each sampling step requires enumerating all possible labels  5 3 Rules for Choosing Inference Algorithms Among the parameters described in the previous section  we focus on  1  the shape of the model   2  the correlation strength  and  3  the label space into consideration  because the rest are less in   uential in the four inference algorithms we study in this paper  The data size is important for optimizing the extraction order in the join over top k queries as described in  4   However  since the complexity of the inference algorithms we study in this paper are all linear in the size of the data  it is not an important factor for choosing inference algorithms  Based on analysis in the last section on the parameters  the following are the rules to choose an inference algorithm for different data and model characteristics  quanti   ed by the three parameters  and the query    For cyclic models    If cycles are induced by query constraints  choose querydriven MCMC MH over Gibbs Sampling    Otherwise  choose Gibbs Sampling over MCMC MH  As shown in our experiments in Sections 7 3 7 4  the Gibbs Sampler converges much faster than the random walk MCMCMH for computing both top k extractions and marginal distribution    For acyclic models    For models with small label space  choose Viterbi over MCMC methods for top k and sum product over MCMC methods for marginal queries    For models with strong correlations  choose Viterbi and sum product over MCMC methods    For models with both large label space and weak correlations  choose Gibbs Sampling over MCMC MH  Viterbi  and sum product  For a typical IE application  the label space is small  e g   10   and the correlation strength is fairly strong  For example  title tokens are usually followed by the author tokens in a bibliography string  Moreover  strong correlation exists with any multitoken entity names  e g   a person token is likely to be followed by another person token   Thus  the above rules translate in most cases in IE to  choose Viterbi and sum product over MCMC methods for acyclic models for top k and marginal queries respectively  choose Gibbs Sampling for cyclic models unless the cycles are induced by query constraints  in which case choose query driven MCMC MH  In this paper  we use heuristic rules to decide the threshold for a    small    label space and for a    strong    correlation for a data set  Developing a cost based optimizer to make such choices based on the data and model is one of our future directions  6 Hybrid Inference Typically  for a given model and dataset  a single inference algorithm is chosen based on the characteristics of the model  In this section  we    rst show that in the context of SQL queries over probabilistic IE results  the proper choice of IE inference algorithm is not only model dependent  but also query  and text dependent  Thus  to achieve good accuracy and runtime performance  it is imperative for a PDB system to use a hybrid approach to IE even within a single query  employing different inference algorithms for different records  We describe the query processing steps that employ hybrid inference for different documents within a single query  Then we describe an algorithm  which  given the input of a    SQL IE    query  generates a query plan that applies the hybrid inference  Finally  we show the query plans with hybrid inference generated from three example    SQL IE    queries to take advantage of the appropriate IE inference algorithms for different combinations of query  text and CRF models  6 1 Query Processing Steps In the context of SQL queries over probabilistic IE results  the proper choice of the IE inference algorithm is not only dependent on the model  but also dependent on the query and the text  First of all  the relational sub query Qre augments the original model with additional random variables  cross edges and factor tables  making the model structure more complex  as we explained in Section 4 1  The characteristics of the model may change after applying the query over the model  For example  a linear chain CRF model may become a cyclic CRF model  after the join query in Q1 or the query with aggregate constraint in Q2  Secondly  when the resulting CRF model is instantiated  i e   grounded  over a document  it could result in a grounded CRF model with drastically different model characteristics  For example  the CRF model  resulting from a join query over a linear chain CRF model  when instantiated over different documents  can result in either a cyclic or a tree shaped model  as we have shown in Figure 5 a  and  b   The applicability  accuracy and runtime of different inference algorithms vary signi   cantly over models with different characteristics  which can result from different data for the same query andHYBRID INFERENCE PLAN GENERATOR  Q  1 apply Qre over the base CRF models   CRF   2 apply deterministic selections in Q over base TOKENTBLs   fTig 3 apply deterministic joins in Q over fTig   T 4 apply model instantiation over T using CRF     groundCRFs 5 apply split operation to groundCRFs   linearCRFs  treeCRFs  cyclicCRFs 6 if Qinf is Marginal then 7 apply sum product to  linearCRFs   treeCRFs    res2 8 apply Gibbs to  cyclicCRFs    res3 9 else if Qinf is Top k then 10 apply Viterbi to  linearCRFs    res1 11 if Qre contains aggregate constraint but no join then 12 apply Viterbi to  cyclicCRFs   treeCRFs    res 13 apply aggregate constraint in Q over res   res1 14 apply query driven MCMC MH to  res  res1  T   res3 15 else 16 apply Gibbs to  cyclicCRFs   treeCRFs    res3 17 endif endif 18 if Qinf is Top k then 19 apply union of res1 and res3 20 else if Qinf is Marginal then 21 apply union of res2 and res3 22 endif Figure 7  Pseudo code for the hybrid inference query plan generation algorithm  model  As a result  to achieve good accuracy and runtime  we apply different inference algorithms  i e   hybrid inference  for different documents within a single query  The choice of the inference algorithm over a document is based on the characteristics of its grounded model  and rules for choosing inference algorithms we described in Section 5 3  The main steps in query processing with hybrid inference are as follows  1  Apply Query over Model  Apply the relational part of the query Qre over the underlying CRF model  2  Instantiate Model over Data  Instantiate the resulting model from the previous step over the text  and compute the important characteristics of the grounded models  3  Partition Data  Partition the data according to the properties of grounded models from the previous step  In this paper  we only partition the data according to the shape of the grounded model  More complicated partitioning techniques  such as one based on the size of the maximum clique can be considered for future work  4  Choose Inference  Choose the inference algorithms to apply according to the rules in Section 5 3 over the different data partitions based on the characteristics of the grounded models  5  Execute Inference  Execute the chosen inference algorithm over each data partition  and return the union of the results from all partitions  6 2 Query Plan Generation Algorithm We envision that the query parser takes in a    SQL IE    query and outputs  along with others non hybrid plans  a query plan which applies hybrid inference over different documents  The algorithm in Figure 7 generates a hybrid inference query plan for an input    SQL IE    query Q  consisting of the relation part Qre  and the subsequent inference operator Qinf   In Line 1  the relational operators in Qre are applied to the CRF models underlying the base TOKENTBL tables  resulting in a new CRF model CRF     In Lines 2 to 3  selection and join conditions on the deterministic attributes  e g   docID  pos  token  are applied to the base TOKENTBL tables  resulting in a set of tuples T  each of which represents a document or a document pair  In Line 4  the model instantiation is applied over T using CRF   to generate a set of    ground    models groundCRFs  In Line 5  a split operation is performed to partition the groundCRFs according to their model structures into linearCRFs  treeCRFs and cyclicCRFs  Lines 6 to 19 capture the rules for choosing inference algorithms we described in Section 5 3  Finally  a union is applied over the result sets from different inference algorithms for the same query  Lines 11 to 14 deals with a special set of queries  which compute the top k results over a simple query with aggregate conditions that induce cycles over the base linear chain CRFs  The intuition is that it is always bene   cial to apply the Viterbi algorithm over the base linear chain CRFs as a fast    ltering step before applying the MCMC methods  In Line 12  it    rst computes the top k extractions res without the aggregate constraint using Viterbi  In Line 13  it applies the constraint to the top k extractions in res  which results in a set of top k extractions that satisfy the constraints in res1  In Line 14  the query driven MCMC MH is applied to the documents in T with extractions that do not satisfy the constraint   res res1  T  An example of this special case is described in Section 6 3 3  Complexity  The complexity of generating the hybrid plan depends on the complexity of the operation on Line 5 in Figure 7  where the groundCRFs are split into subsets of linearCRFs  treeCRFs and cyclicCRFs  The split is performed by traversing the ground CRFs to determine their structural properties  which is linear to the size of the ground CRF O N   where N is the number of random variables  The complexity of choosing the appropriate inference  lines 6 to 21  is O 1   On the other hand  the complexity of Viterbi  sum product algorithms over linearCRFs and treeCRFs is O N  with a much larger constant  because of the complex computation  i e   sum  product  over jY j   jY j matrices  where jY j is the number of possible labels  The complexity of exact inference algorithm over cyclicCRFs is NP hard  Thus the cost of generating the hybrid plan is negligible from the cost of the inference algorithms  6 3 Example Query Plans In this section  we describe three example queries and show the query plans with hybrid inference generated from the algorithm in Figure 7  which take advantage of the appropriate inference algorithms for different combinations of query  text and CRF models  6 3 1 Skip Chain CRF In this query  we want to compute the marginal distribution or the top k extraction over an underlying skip chain CRF model as shown in Figure 4  The query is simply  Q4   Skip Chain Model  SELECT  Top k T1 docID    Marginal T1 pos exist   FROM TokenTbl T1 As described in  12   the MCMC methods are normally used to perform inference over the skip chain CRF model for all the documents  like the query plan in Figure 8 b   The Viterbi algorithm fails to apply because the skip chain model contains skip edges  which make the CRF model cyclic  However  the existence of such skip edges in the grounded models  instantiated from the documents  is dependant on the text  There exist documents  like the one shown in Figure 4  in which one string appears multiple times  Those documents result in cyclic CRF models  But  there also exist documents  in which only unique tokens are used except for    stop words     such as    for        a     Those documents result in linear chain CRF models  TokenTbl Skip Chain CRF model instantiation zero skip edge  linear  at least one skip edge  cyclic  Sum Product Viterbi Gibbs MCMC MH Union TokenTbl Skip Chain CRF Gibbs MCMC MH model instantiation  a   b  Figure 8  The query plan for an inference  either top k or marginal  over a skip chain CRF model  The query plan generated with hybrid inference is shown in Figure 8 a   After the model instantiation  the ground CRF model is inspected  if no skip edge exists  i e   no duplicate strings exist in a document   then the Viterbi or the sum product algorithm is applied  otherwise  the Gibbs algorithm is applied to the cyclic ground CRFs  Compared to the non hybrid query plan  the query plan with hybrid inference is more ef   cient by applying more ef     cient inference algorithms  e g   Viterbi  sum product  over the subset of the documents  where the skip chain CRF model does not induce cyclic graphs  The speedup depends on the performance of Viterbi sum product compared to Gibbs Sampling  and on the percentage of such documents that instantiate a skip chain CRF model into a grounded linear chain CRF models  6 3 2 Join over Linear chain CRF In this example  we use the join query Q1 described in Section 4 1  which computes the marginal probability of the existence of a join result  The join query is performed between each document pair on having tokens with the same strings labeled as    person     Such a join query over the underlying linear chain CRF models induces cross edges and cycles in the resulting CRF model  A typical non hybrid query plan  shown in Figure 9 with black edges  perform MCMC inference over all the documents  However  as we see in Figure 5 a  and  b   depending on the text  the joint CRF model can be instantiated into either a cyclic graph or a tree shaped graph  The red edge in Figure 9 shows the query plan with hybrid inference for the join query Q1  As we can see  instead of performing MCMC methods unilaterally across all    joinable    document pairs  i e   contain at least 1 pair of common tokens   the sum product algorithm is used over the document pairs that contain only 1 pair of common tokens  Compared to the non hybrid query plan  the hybrid inference reduces the runtime by applying the more ef   cient inference  i e   sum product  when possible  The speedup depends on the performance of sum product compared to the MCMC methods  and the percentage of the    joinable    document pairs that only share one pair of common tokens that are not    stop words     6 3 3 Aggregate Constraints In this example  we use Q2  the query with an aggregate constraint  described in Section 4 1  As shown in Figure 5 c   the aggregate constraints can induce a big clique including all the label variables in each document  In other words  regardless of the text  based on the model and the query  each document is instantiated into a cyclic graph with high tree width  Again  typically  for such a high tree width cyclic model  MCMCMH algorithms are used over all the documents to compute the topTokenTbl1 TokenTbl2 CRF1 CRF2 model instantiation CRF1 2 only 1 cross edge  tree  more than 2 cross edges  cyclic  Join token1 token2  Join token1 token2   label1 label2  Sum product Gibbs MCMC MH Union Figure 9  The query plan for the probabilistic join query followed by marginal inference  TokenTbl CRF with cliques Viterbi   Model instantiation aggregate condition MCMC Query Driven  Union  MH Linear chain CRF satisfy un satisfy aggregate condition Model instantiation TokenTbl CRF with cliques MCMC Query Driven   MH Linear chain CRF aggregate condition Model instantiation  a   b  Figure 10  The query plan for the aggregate selection query followed by a top k inference  k extractions that satisfy the constraint  Such a non hybrid query plan is shown in Figure 10 b   However  this query falls into the special case described in the query plan generation algorithm in Section 6 2 for hybrid inference  The query is to return the top k extractions over the cyclic graph induced by an aggregate constraint over a linear chain CRF model  Thus  the resulting query plan is shown in Figure 10 a   In the query plan with hybrid inference  the Viterbi algorithm runs    rst to compute the top k extraction without the constraint  Then  the results are run through the aggregate  those that satisfy the constraint are returned as part of the results  and those that do not satisfy the constraint are fed into the query driven MCMC MH algorithm  7 Evaluation So far  we have described the implementation of the MCMC algorithms  and the query plans for the hybrid inference algorithms  We now present the results of a set of experiments aimed to  1  evaluate the ef   ciency of the SQL implementation of the MCMC methods and the effectiveness of the query driven sampling techniques   2  compare the accuracy and runtime of the four inference algorithms  Viterbi  sum product  Gibbs and MCMC MH  for the two IE tasks   top k and marginal  and  3  analyze three real life text datasets to quantify the potential speedup of a query plan with hybrid inference compared to one with non hybrid inference  Setup and Dataset  We implemented the four inference algorithms  Viterbi  sum product  Gibbs and MCMC MH in PostgreSQL 8 4 1  We conducted the experiments reported here on a 2 4 GHz Intel Pentium 4 Linux system with 1GB RAM  For evaluating the ef   ciency of the in database implementation of the MCMC methods  and for comparing the accuracy and run 0 10 20 30 40 50 60 70 80 90 100 0 200000 400000 600000 800000 1000000 Runtime  sec  Number of Samples MCMC MH Runtime  SQL vs  Scala Java SQL MCMC MH SQL Gibbs Scala Java MCMC MH Figure 11  Runtime comparison of the SQL and Java Scala implementations of MCMC MH and Gibbss algorithms over DBLP  time of the inference algorithms  we use the DBLP dataset  23  and a CRF model with 10 labels and features similar to those in  18   The DBLP database contains more than 700  000 papers with attributes  such as conference  year  etc  We generate bibliography strings from DBLP by concatenating all the attribute values of each paper record  We also have similar results for the same experiments over NYTimes dataset  which we include in our technical report  For quantifying the speedup of query plans with hybrid inference  we examine the New York Times  NYTimes  dataset  and the Twitter dataset in addition to the DBLP dataset  The NYTimes dataset contains ten million tokens from 1  788 New York Times articles from the year 2004  The Twitter dataset contains around 200  000 tokens from over 40  000 tweets obtained in January 2010  We label both corpora with 9 labels  including person  location  etc  7 1 MCMC SQL Implementation In this experiment  we compare the runtime of the in database implementation of the MCMC algorithms  including Gibbs Sampling and MCMC MH  with the runtime of the Scala Java  with Scala 2 7 7 and Java 1 5  implementation of MCMC MH described in  12  over linear chain CRF models  The runtime of Scala Java implementation is measured on a different machine with better con   gurations  2 66GHz CPU Mac OSX 10 6 4 with 8G RAM   As we can see in Figure 11  the runtime of the MCMC algorithms grow linearly with the number of samples for both the SQL and the Java Scala implementations  While the Scala Java implementation of MCMC MH can generate 1 million samples in around 51 seconds  it takes about 78 seconds for the SQL implementation of the MCMC MH  and about 89 seconds for that of the Gibbs Sampling  This experiment shows that the in database implementations of the MCMC sampling algorithms achieve comparable  within a factor of 1 5  runtimes to the Java Scala implementation  7 2 Query Driven MCMC MH In this experiment  we evaluate the effectiveness of the query driven MCMC MH algorithm described in Section 4 3 with the vanilla MCMC MH in generating samples that satisfy the query constraint  The query we use is Q2 described in Section 4 1  which computes the top 1 extractions that satisfy the aggregate constraint that all title tokens are in front of the author tokens  We run the query driven MCMC MH and the vanilla MCMCMH algorithm over a randomly picked 10 documents from the DBLP dataset  Figure 12 shows the number of    quali   ed    samples that are generated by each algorithm in 1 second  As we can see  the querydriven MCMC MH generates more    quali   ed    samples  roughly 1200 for all the documents  and for half of the documents the query driven MCMC MH generates more than 10 times more quali   ed samples than vanilla MCMC MH  0 200 400 600 800 1000 1200 1400 1600 1 2 3 4 5 7 8 9 10 Number of Samples Document Number MCMC MH vs  Query driven MCMC MH MCMC MH Query driven MCMC MH Figure 12  The number of quali   ed samples generated by the querydriven MCMC MH and the vanilla MCMC MH algorithm in 1 second for different documents in DBLP  0 10000 20000 30000 40000 50000 0 50 100 150 Compuation Error  Number of Labels  Execution Time  sec  Top 1  MCMC MH  Gibbs vs  Viterbi MCMC MH Viterbi Gibbs Figure 13  Runtime Accuracy graph comparing Gibbs  MCMC MH and Viterbi over linear chain CRF for top 1 inference on DBLP  7 3 MCMC vs  Viterbi on Top k Inference This experiment compares the runtime and the accuracy of the Gibbs  the MCMC MH and the Viterbi algorithms in computing top 1 inference over linear chain CRF models  The inference is performed over 45  000 tokens in 1000 bibliography strings from the DBLP dataset  We measure the    computation error    as the number of labels different from the exact top 1 labelings according to the model 1   The Viterbi algorithm only takes 6 1 seconds to complete the exact inference over these documents  achieving zero computation error  For the MCMC algorithms  we measure the computation error and runtime for every 10k more samples  starting from 10k to 1 million samples over all documents  As we can see in Figure 13  the computation error of the Gibbs algorithm drops to 22  from 45  000 to 10  000 when 500k samples are generated  This takes around 75 seconds  more than 12 times longer than the runtime of the Viterbi algorithm  The MCMC MH converges much slower than the Gibbs Sampling  As more samples are generated  the top  1 extractions generated from the MCMC algorithms get closer and closer to the exact top 1  however very slowly  Thus  Viterbi beats the MCMC methods by far in computing top 1 extractions with linear chain CRF models  more than 10 times faster with more than 20  fewer computation errors  7 4 MCMC vs  Sum Product on Marginal Inference This experiment compares the runtime and the accuracy of the Gibbs  MCMC MH and the sum product algorithms over tree shaped graphical models induced by a join query similar to Q1  described in Section 4 1  The query computes the marginal probability of the existence of a join result for each document pair in DBLP  joining on the same    publisher     The query is performed over a set of 10  000 pairs of documents from DBLP  where the two documents in each pair have exactly one token in common  1 The top 1 extractions with zero computation error may still contain mistakes  which are caused by inaccurate models 0 0 2 0 4 0 6 0 8 1 Average Probability Difference 0 100 200 300 400 500 Execution Time  sec  MCMC MH  Gibbs vs  Sum Product Gibbs Sum Product MCMC MH Figure 14  Runtime Accuracy graph comparing Gibbs  MCMC MH and sum product over tree shaped models for marginal inference on DBLP  Data Skip chain Probabilistic Aggregate Corpora CRF Join Constraint NYTimes  5 0  4 5  10 0 Twitter  5 0  2 6 N A DBLP  1 0  1 0 N A Table 2  Speed ups achieved by hybrid inference for different queries  The sum product algorithm over these 10  000 tree shaped graphical models takes about 60 seconds  As an exact algorithm  the sum product algorithm achieves zero computation error  We measure the    computation error    as the difference between the marginal probabilities of join computed from the MCMC MH algorithms and the sum product algorithm  averaging over all document pairs  For the MCMC algorithms  we measure the computation error and runtime for every 200k more samples  starting from 200k to 2 million samples over all document pairs  As we can see in Figure 14  the probability difference between Gibbs and sum product converges to zero quickly  at 400 second  the probability difference is dropped to 0 01  The MCMC MH on the other hand  converges much slower than the Gibbs  This experiment shows that MCMC algorithms performs relatively better in computing marginal distributions than in computing top 1 extractions  However  sum product algorithm still outperforms MCMC algorithms in computing marginal probabilities over tree shaped models  more than 6 times faster with about 1  less computation error  7 5 Exploring Model Parameters In this experiment  we explore how different correlation strengths  one of the parameters we discussed in Section 5 2  affect the runtime and the accuracy of the inference algorithms  As we explained earlier  the correlation strength does not affect the accuracy or the runtime of the Viterbi algorithm  On the other hand  weaker correlation between different random variables in the CRF model leads to faster convergence for the MCMC algorithms  The setup of this experiment is the same as in Section 7 3  In Figure 15  we show the runtime accuracy graph of the Viterbi and the Gibbs algorithm to compute the top 1 extractions over models with different correlation strengths  We synthetically generated models with correlation strengths of 1  0 5  0 2 and 0 001 by dividing the original scores in the transition factors by 1  2  5 and 1000 respectively  As we can see  the weaker correlation strengths lead to faster convergence for the Gibbs algorithm  When correlation strength is 0 001 the computation error reduces to zero in less than twice that of the Viterbi runtime  The correlation strength of the CRF model depends on the dataset on which the CRF model is learned  The model we learned over NYTimes and DBLP dataset both contains strong correlation strength  0 10000 20000 30000 40000 50000 0 5 10 15 20 Computation Error   Number of Labels  Execution Time  sec  Correlation Strength  Gibbs vs  Viterbi Gibbs corr 1 Gibbs corr 0 5 Gibbs corr 0 2 Gibbs corr 0 001 Viterbi Figure 15  Runtime Accuracy graph comparing Gibbs and Viterbi over models with different correlation strengths on DBLP  7 6 Hybrid Inference for Skip chain CRF In this and the next two sections  we describe the results  in terms of</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Faerie  Ef   cient Filtering Algorithms for Approximate Dictionary based Entity Extraction ### Guoliang Li Dong Deng Jianhua Feng Department of Computer Science and Technology  Tsinghua University  Beijing 100084  China  liguoliang tsinghua edu cn  buaasoftdavid gmail com  fengjh tsinghua edu cn ABSTRACT Dictionary based entity extraction identi   es prede   ned entities  e g   person names or locations  from a document  A recent trend for improving extraction recall is to support approximate entity extraction  which    nds all substrings in the document that approximately match entities in a given dictionary  Existing methods to address this problem support either token based similarity  e g   Jaccard Similarity  or character based dissimilarity  e g   Edit Distance   It calls for a uni   ed method to support various similarity dissimilarity functions  since a uni   ed method can reduce the programming e   orts  hardware requirements  and the manpower  In addition  many substrings in the document have overlaps  and we have an opportunity to utilize the shared computation across the overlaps to avoid unnecessary redundant computation  In this paper  we propose a uni   ed framework to support many similarity dissimilarity functions  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance  We devise e   cient    ltering algorithms to utilize the shared computation and develop e   ective pruning techniques to improve the performance  The experimental results show that our method achieves high performance and outperforms state of the art studies  Categories and Subject Descriptors H 2  Database Management   Database applications  H 3 3  Information Search and Retrieval  General Terms  Algorithms  Experimentation  Performance Keywords  Approximate Entity Extraction  Uni   ed Framework  Filtering Algorithms  Pruning Techniques ### 1  INTRODUCTION Dictionary based entity extraction identi   es all the substrings from a document that match the prede   ned entities     This work is partly supported by the National Natural Science Foundation of China under Grant No  61003004  the National Grand Fundamental Research 973 Program of China under Grant No  2011CB302206  and National S T Major Project of China under Grant No  2011ZX01042 001 002  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  in a given dictionary  For example  consider a document    An E   cient Filter for Approximate Membership Checking  Venkaee shga Kamunshik kabarati  Dong Xin  Surauijt ChadhuriSIGMOD     and a dictionary with two entities    Surajit Chaudhuri    and     Dong Xin     Dictionary based entity extraction    nds the prede   ned entity    Dong Xin    from the document  This problem has many real applications in the    elds of information retrieval  molecular biology  bioinformatics  and natural language processing  However  the document may contain typographical or orthographical errors and the same entity may have di   erent representations  22   For example  the substring    Surauijt Chadhuri    in the above document has typographical errors  The traditional  exact  entity extraction cannot    nd this substring from the document  since the substring does not exactly match the prede   ned entity    Surajit Chaudhuri     Approximate entity extraction is a recent trend to address this problem  which    nds all substrings from the document that approximately match the prede   ned entities  To improve extraction recall  we study the problem of approximate dictionary based entity extraction  which  given a dictionary of entities and a document     nds all substrings from the document similar to some entities in the dictionary  Many similarity dissimilarity functions have been proposed to quantify the similarity between two strings  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance  For instance  in the above example  suppose we use edit distance and the threshold is 3  Approximate entity extraction can    nd the substring    Surauijt Chadhuri    which is similar to entity    Surajit Chaudhuri     Although there have been some recent studies on approximate entity extraction  22  4   they support either tokenbased similarity  e g   Jaccard Similarity  or character based dissimilarity  e g   Edit Distance   It calls for a uni   ed method to support di   erent similarity dissimilarity functions  since the uni   ed method can reduce not only the programming efforts  but also the hardware requirements and the manpower needed to maintain the codes for di   erent functions  In addition  we have an observation that many substrings in the document have overlaps  For example  consider the above document  many substrings  e g     Surauijt Chadhuri        urauijt ChadhuriSIG        rauijt ChadhuriSIGMOD     h ave ove r   laps  e g      Chadhuri      We have an opportunity to utilize this feature to avoid the redundant computation across overlaps of di   erent substrings  For example  we can share the computation on    Chadhuri    for di   erent substrings  To remedy the above problems  we propose a uni   ed framework to support various similarity dissimilarity functions  529To avoid redundant computation across overlaps  we develop e   cient filtering algorithms for approximate dictionary based entity extraction  called    Faerie     To summarize  we make the following contributions      We propose a uni   ed framework to support many similarity dissimilarity functions  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance      We devise e   cient    ltering algorithms  which can utilize the shared computation across the overlaps of multiple substrings of the document      We develop e   ective pruning techniques and devise ef     cient algorithms to improve the performance      We have implemented our method  and the experimental results show that our method achieves high performance and outperforms state of the art approaches  The remainder of this paper is organized as follows  We propose a uni   ed framework to support various similarity functions in Section 2  Section 3 introduces a heap based    ltering algorithm to utilize shared computation  We develop pruning techniques in Section 4 and introduce our algorithm Faerie in Section 5  We conduct extensive experimental studies in Section 6  Related works are provided in Section 7  Finally  we conclude the paper in Section 8  2  A UNIFIED FRAMEWORK We    rst formulate the problem of approximate  dictionarybased  entity extraction  Section 2 1   and then introduce a uni   ed method to support various similarity dissimilarity functions  Section 2 2   Finally we introduce a concept of    valid substrings   to prune unnecessary substrings  Section 2 3   2 1 Problem Formulation Definition 1  Approximate Entity Extraction   Given a dictionary of entities E    e1  e2          en   a document D  a similarity function  and a threshold  it    nds all     similar    pairs  s  ei  with respect to the given function and threshold  where s is a substring of D and ei     E  This paper focuses on token based similarity  characterbased similarity  and character based dissimilarity  Token based Similarity  It includes Jaccard Similarity  Cosine Similarity  and Dice Similarity  The token based similarity takes a string as a set of tokens  Let jac  cos  and dice respectively denote the jaccard similarity  cosine similarity  and dice similarity  Given two strings r and s  let  r  denote the number of tokens in r  jac r  s     r   s   r   s    cos r  s        r   s   r    s    and dice r  s    2 r   s   r   s    For example  jac    sigmod 2011 conference        sigmod 2011       2 3   cos    sigmod 2011 conference        sigmod 2011          2 6   and dice    sigmod 2011 conference        sigmod 2011       4 5   Charater based Dissimilarity  It includes Edit Distance  The charater based dissimilarity takes a string as a sequence of characters  The edit distance between two strings r and s  denoted by ed r  s   is the minimum number of singlecharacter edit operations  i e   insertion  deletion  and substitution  needed to transform string r to string s  For example  ed    surajit        surauijt       2   Table 1  A dictionary of entities and a document   a  Dictionary E ID Entity e len e   e     of q grams with q   2  1 kaushik ch 10 9 2 chakrabarti 11 10 3 chaudhuri 9 8 4 venkatesh 9 8 5 surajit ch 10 9  b  Document D an e   cient    lter for approximate membership checking  venkaee shga kamunshik kabarati  dong xin  surauijt chadhurisigmod  Charater based Similarity  It includes Edit Similarity  The edit similarity between two strings r and s is de   ned as eds r  s   1    ed r s  max   len r  len s      where len s  denotes the length of s  For instance  eds    surajit        surauijt      1    2 8   3 4   In this paper two strings are said to be similar  if their  jaccard  cosine  dice  edit  similarity is no smaller than a given similarity threshold     or their edit distance is no larger than a given edit distance threshold      For instance  consider the document D and dictionary E in Table 1  Suppose the edit distance threshold      2      venkaee sh        venkatesh          surauijt ch        surajit ch      and     chadhuri        chaudhuri     are three example results  Especially  although the substring    chadhurisigmod    in the document misses a space between    chadhuri    and     sigmod     a typographical error   our method still can    nd   chadhuri    similar to entity   chaudhuri       It has been shown that approximate entity extraction can improve recall  22   For example  the recall can increase from 65 4  to 71 4  when performing protein name recognition  In this paper  we emphasize on improving the performance  We focus on extracting textual entities  We assume the thresholds     and      are pre de   ned and take the selection of such thresholds as a future work  2 2 A Uni   ed Method In this section  we propose a uni   ed framework to support various similarity dissimilarity functions  We model both the entity and document as a set of tokens  Especially for edit distance and edit similarity  we take qgrams of an entity as tokens  A q gram of a string s is a substring of s with length q  The q gram set of s  denoted by G s   is the set of all of s   s q grams  For example  the 2 gram set of    surajit ch    is  su  ur  ra  aj  ji  it  t    c  ch   If the context is clear  we use token to denote token gram  for edit distance and edit similarity  we use e to denote G e   e   s to denote G r    G s   and  e  to denote  G e    i e    e    len e      q   1   Given an entity e and a substring s  we transform di   erent similarities dissimilarities to the overlap similarity   e     s   and use the overlap similarity as a uni   ed    ltering condition  if e and s are similar  then  e     s  must be not smaller than a threshold T   0  where T can be computed as follows      Jaccard Similarity  T      e     s          1           Cosine Similarity  T        e      s               Dice Similarity  T      e     s          2        Edit Distance  T   max  e    s              q      Edit Similarity  T    max  e    s        max  e    s    q    1        1           q   530The correctness of these thresholds is stated in Lemma 1  Lemma 1  Given an entity e and a substring s  we have 1       Jaccard Similarity   If jac e  s           e   s       e   s         1           Cosine Similarity   If cos e  s           e   s         e      s             Dice Similarity   If dice e  s           e   s       e   s         2        Edit Distance   If ed e  s            e   s    max  e    s          q      Edit Similarity   If eds e  s           e   s       max  e    s        max  e    s   q   1       1         q   In this way  we can transform various similarities dissimilarities to the overlap similarity  and develop a uni   ed    ltering condition  if  e     s    T  we prune the pair  s  e   Note that given any similarity function and a threshold  if we can deduce a lower bound for the overlap similarity of two strings  our method can apply to this function  Specially  the    ve similarity distance functions we studied are commonly used in information extraction and record linkage  22  4   2 3 Valid Substrings We have an observation that some substrings in D will not have any similar entities  For instance  consider the dictionary and document in Table 1  Suppose we use edit distance and      1  Consider substring    surauijt chadhurisigmod    with length 23  As the lengths of entities in the dictionary are between 9 and 11  the substring cannot be similar to any entity  Next we discuss how to prune such substrings  Given an entity e and a substring s  if s is similar to e  the number of tokens in s   s   should be in a range     e   e   that is    e      s       e  where    e and  e are respectively the lower and upper bound of  s   computed as below      Jaccard Similarity     e     e          and  e      e            Cosine Similarity     e     e         2   and  e      e    2        Dice Similarity     e     e         2        and  e     e      2                Edit Distance     e    e         and  e    e             Edit Similarity     e      e    q     1              q     1   and  e      e  q   1         q     1    where    is the similarity threshold and    is the edit distance threshold  The correctness of the bounds is stated in Lemma 2  Lemma 2  Given an entity e  for any substring s  we have     Jaccard Similarity   if jac e  s            e             s       e            Cosine Similarity   if cos e  s            e        2      s       e    2        Dice Similarity   if dice e  s          e        2           s      e     2                Edit Distance   if ed e  s            e              s       e             Edit Similarity   if eds e  s             e  q   1          q   1        s         e    q     1        q   1    Based on Lemma  2   given an entity e  only those substrings with token numbers between    e and  e could be similar to entity e  and others can be pruned  Especially  let    E   min    e e     E  and  E   max  e e     E   Obviously the substrings in D with token numbers between    E and  E may have a similar entity in the dictionary E  and others can be pruned  Based on this observation  we introduce the concept of    valid substring     1 In this paper  we omit the proof due to space constraints  Definition 2  Valid Substring   Given a dictionary E and a document D  a substring s in D is a valid substring for an entity e     E if    e      s       e  A substring s in D is a valid substring for dictionary E if    E      s       E  For instance  consider the dictionary and document in Table 1  Suppose we use edit similarity  and      0 8 and q   2  Consider entity e5      surajit ch        e5      e5    q     1              q     1     7  and  e5     e5  q   1         q     1    11  Only the valid substrings with token numbers between 7 and 11 could be similar to entity e5  As    E   7  and  E   12  only the valid substrings with token numbers between 7 and 12 could have similar entities in the dictionary  and all other substrings  e g      surauijt chadhurisigmod     c a n b e p r u n e d   We employ a    lter and verify framework to address the problem of approximate entity extraction  In the    lter step  we generate the candidate pairs of a valid substring in document D and an entity in dictionary E  whose overlap similarity is no smaller than a threshold T  and in the verify step  we verify the candidate pairs to get the    nal results  by computing the real similarity disimilarity  In this paper  we focus on the    lter step  3  HEAP BASED FILTERING ALGORITHMS In this section  we propose heap based    ltering algorithms to utilize the shared computation across overlaps  We    rst introduce an inverted index structure  Section 3 1   and then devise a multi heap based algorithm  Section 3 2  and a single heap based algorithm  Section 3 3   3 1 An Inverted Index Structure A valid substring is similar to an entity only if they share enough common tokens  To e   ciently count the number of their common tokens  we use the inverted index structure  We build an inverted index for all entities  where entries are tokens  for jaccard similarity  cosine similarity  and dice similarity  or q grams  for edit similarity and edit distance   and each entry has an inverted list that keeps the ids of entities that contain the corresponding token gram  sorted in ascending order  For example  Figure 1 gives the inverted list for entities in Table 1 using q grams with q   2  ka au us sh hi ik ab ba ar rt ti te es su aj ji it t  1 4 1 3 1 1 4 1 1 1 1 5 1 2 3 5 2 3 2 3 3 5 5 2 2 2 2 2 2 3 3 4 4 4 5 5 5 5 k   c ch ha ak kr dh hu ur ri ve ra 2 5 ud 3 at 4 en 4 4nk Figure 1  Inverted indexes for entities in Table 1  For each valid substring s in D  we    rst get its tokens and the corresponding inverted lists  Then for each entity in these inverted lists  we count its occurrence number in the inverted lists  i e   the number of inverted lists that contain the entity  Obviously  the occurrence number of entity e is exactly  e     s  2   For each entity e with occurrence number no smaller than T   e     s      T    s  e  is a candidate pair  For example  consider a valid substring    surauijt ch     We    rst generate its token set  su  ur  ra  au  ui  ij  jt  t    c  ch  and get the inverted lists  the italic ones in Figure 1   Suppose we use edit distance and      2   For entity e5  T   max  e5    s            q   6   As e5   s occurrence number is 6      surauijt ch     e5    surajit ch     is a candidate pair  2 In this paper  we take e and s as multisets  since there may exist duplicate tokens in entities and substrings of the document  Even if they are taken as sets  we can also use our method for extraction  531Figure 2  A heap structure for    surauijt ch    3   For simplicity  given an entity e and a valid substring s  we use e   s occurrence number in s  or s   s inverted lists  to denote e   s occurrence number in the inverted lists of tokens in s  To e   ciently count the occurrence numbers  we propose heap based    ltering algorithms in the following sections  3 2 Multi Heap based Method In this section  we propose a multi heap based method to count the occurrence numbers  We    rst enumerate the valid substrings in D  with token number between    E and  E   Then for each valid substring  we generate its tokens and construct a min heap on top of the non empty inverted lists of its tokens  Initially we use the    rst entity of every inverted list to construct the minheap  For the top entity on the heap  we count its occurrence number on the heap  i e   the number of inverted lists that contain the entity   If the number is not smaller than T  the pair of this valid substring and the entity is a candidate pair  Next  we pop the top entity  add the next entity of the inverted list from which the top entity is selected into the heap  adjust the heap  and count the occurrence number of the new top entity  Iteratively we    nd all candidate pairs  For example  consider a valid substring    surauijt ch     We    rst generate its token set and construct a min heap on top of the    rst entities of every inverted list  Figure 2   Next we iteratively adjust the heap and get the entities  1  1  1  2  2  3  3  3  5  5  5  5  5  5  in ascending order  We count the occurrence numbers of each entry  For example  the occurrence numbers of e1  e2  e3  and e5 are respectively 3  2  3  and 6  Suppose we use edit distance and      2  For entity e5  T   max  e5    s              q   6  The pair of the substring and entity e5 is a candidate pair  Finally  we verify the candidate pair and get the    nal result  Complexity  For a valid substring with l tokens  its corresponding heap contains at most l non empty inverted lists  Thus the space complexity of the heap is O l   As we can construct heaps one by one  the space complexity is the space of the maximum heap  i e   O  E   Table 2 a    The time complexity for constructing a heap of a valid substring with l tokens is O l   As there are  D     l 1 valid substrings with l tokens  the heap construction complexity for such valid substrings is O     D     l   1      l     and the total heap construction complexity is O    E l    E   D      l   1      l    Table 2 b    In addition  for each entity  we need to adjust the heap containing the entity  Consider such heap with l inverted lists  The time complexity of adjusting the heap once is O log l    There are l such heaps that contain the entity  Figure 3   Thus for each entity  the time complexity of adjusting the heaps is O    E l    E log l      l     Suppose N is the total numbers of entities in inverted lists of tokens in D  The total time complexity of adjusting the heaps is O    E l    E log l      l     N    Table 2 b    3 For ease of presentation  we use a loser tree to represent a heap structure in our examples   D  l  l tokens l D i  l         D i l    l D i  l l  l  e  i e   D i l    l      D i l tokens inverted lists D i  l l i l 1     Figure 3  A multi heap structure  Table 2  Complexity of multi heap based methods   a  Space Complexity Maximum Heap O  E   b  Time Complexity Heap Construction O    E l    E   D      l   1      l   Heap Adjustment O    E l    E log l      l     N   The multi heap based method needs to access inverted lists multiple times and does large numbers of heap adjustment operations  To address this issue  we propose a new method which accesses every inverted list only once in Section 3 3  3 3 Single Heap based Method In this section  we propose a single heap based method  We    rst tokenize the document D and get a list of tokens  For each token  we retrieve the corresponding inverted list  We use token i  to denote the i th token  and I L i  to denote the inverted list of the i th token  We construct a single minheap on top of non empty inverted lists of all tokens in D  denoted by H  and use the heap to    nd candidate pairs  For ease of presentation  we use a two dimensional array V  1           D      E           E  to count an entity   s occurrence numbers in every valid substring   s inverted lists  Formally  let D i  l  denote a valid substring of D with l tokens starting with the i th token  Given an entity e  we use V  i  l  to count e   s occurrence number in D i  l    s inverted lists  i e   V  i  l     e     D i  l    We compute V  i  l  as follows  V  i  l  is initialized as 0 for 1     i      D  and    E     l      E  For the top entity e on the heap selected from the i th inverted list  we increase the values of relevant entries in the array by 1 as follows  Without loss of generality     rstly consider the heap with l tokens  Obviously only D i   l 1  l         D i  l  contain the i th inverted list  Figure 4   thus V  i     l   1  l           V  i  l  are relevant entries  Similarly for    E     l      E  V  i     l   1  l           V  i  l  are relevant entries  We increase the value of each relevant entry by 1  If V  i  l      T   D i  l   e  is a candidate pair  Then  we pop the top entity  add the next entity in I L i  into the heap  adjust the heap and get the next entity  and count the occurrence number of the new entity  We repeat the above steps  and iteratively we can    nd all candidate pairs  Actually  for entity e  only the valid substrings with token numbers between    e and  e could be similar to entity e  4 Thus we only need to maintain array V  1           D      e           e   Next  we give a running example to walk through the single heap based method  For example  in our running example  consider a document    venkaee shga kamunshi     we construct a single heap on top of the document as shown in Figure 5  Suppose we use edit distance and      2     E   6 and  E   12  For the entity e4 selected from the    rst token  we only need to increase its occurrence numbers in valid 4 Note that  we can get entity e   s token number  e  using a hash map  which keeps the pair of an entity and its token number  thus we can get the token number of an entity in O 1  time complexity  532                                e i  e D i  l e             l       Te       tokens Inverted lists occurrence numbers occurrence numbers occurrence numbers  e D i l    l i l  Te Figure 4  A single heap structure  Table 3  Complexity of single heap based methods   a  Space Complexity Single Heap O  D   Counting Occurrence Numbers O  D         E   1    b  Time Complexity Heap Construction O  D   Heap Adjustment O   log  D       N   Counting Occurrence Numbers O   N     max    e l    e l e   E    substrings D 1  l  for    E     l      E  i e   D 1  6           D 1  12   We increase the values of V  1  6           V  1  12  by 1  For the next entity e4 selected from the second token  we increase its occurrence numbers in valid substrings D 1  l   D 2  l  for    E     l      E  Similarly  we can count all occurrence numbers  For instance  the occurrence number of entity e4     venkatesh     in D 1  9  is 5  As the occurrence number is no smaller than T   max  e4    D 1  9            q   9   2   2   5  the pair of D 1  9      venkaee sh     a n d e nt ity e4     venkatesh     is a candidate pair  Actually as    e4   6  and  e4   10  we only need to consider the entries in V  1          20  6          10   Complexity  The space complexity of the single heap is O  D    Table 3 a    To count the occurrence numbers of an entity  we do not need to maintain the array and propose an alternative method  We    rst pop all entities with the same id from the heap  with  D  space to store them   Suppose the entity is e  Then we increase e   s occurrence numbers in V  1           D     l  1  l  by varying l from    e to  e  In this way  we only need to maintain a one dimensional array  Thus the space complexity for counting the occurrence number is O max  D       e  1 e     E     O  D       E  1   Table 3 a    The time complexity of heap construction is O  D    Table 3 b    To compute the occurrence numbers of each entity  we need to adjust the heap  and the total time complexity of adjusting the heap is O log  D       N   where N is the total number of entities in every inverted list  In addition  for each entity  we need to increase its occurrence numbers  For entity e  there are   e l    e l entries needed to be increased by 1  Figure 4   and the maximum number of such entries  for any entity  is max    e l    e l e   E   Thus the total time complexity is O   N     max    e l    e l e   E     Table 3 b    Note that the single heap based method has used the shared computation across the overlaps  tokens  of di   erent valid substring  since it only needs to scan each inverted list once  It has much lower time complexity than multi heap based method  and achieves much higher performance  Section 6   4  IMPROVING THE SINGLE HEAP BASED METHOD In this section  we propose e   ective techniques to improve ve   en  nk   ka    ae    ee   e    s   sh  hg    ga   a    k    ka   am mu   un   ns   sh   hi 1 4 1 4 1    2   3   4     5    6     7   8    9   10  11  12  13   14  15  16   17  18 19  20 1 4 D 1  9  venkaee sh and entity e4 venkatesh have 5 common tokens 4    3   2   2    1    1      1   1    2    1    1    1    1    2     1                       4    3   3   2    1    1      1   2    2    1    1    1    2    2                              4    4   3   2    1    1      2   2    2    1    1    2    2                                5    4   3   2    1    2      2   2    2    1    2    2 5    4   3   2    2    2      2   2    2    2    2      5    4   4   3    2    2      2   3    3 6 7 8 9 10 11 12 Length D 5    4   3   3    2    2      2   2    3    2 1 4 1 Less than l tokens  e4   E  TE  e4  Te4  4 4 4 Occurrence numbers Figure 5  An example for the single heap based method on a document    venkaee shga kamunshi     the performance of the single heap based method  Li et al   18  have studied e   cient heap merge algorithms to    nd similar entities for a single substring  In this paper  we propose e   ective algorithms to simultaneously    nd similar entities for multiple substrings  with large number of overlaps   which are orthogonal to the heap merge algorithms  4 1 Pruning Techniques In this section  we propose several pruning techniques by estimating the lower bounds of  e     s   Lazy Count Pruning  For each top entity on the heap  we will not count its occurrence numbers for every valid string immediately  Instead  we count the numbers in a lazy manner  We    rst count its occurrence number in the heap  and if the number is small enough  we can prune the entity  Formally  given an entity e  we use a sorted position list Pe    p1             pm  to keep its occurrences in the heap  in ascending order   Each element in Pe is the position of the corresponding token whose inverted list contains entity e  We can easily get the position list using the heap structure  Then we count e   s occurrence number in the heap  i e   the number of elements in Pe   Pe    If the number is smaller than a threshold  denoted as Tl  we prune the entity  otherwise we count its occurrence number in its valid substrings with token numbers between    e and  e  Section 3 3   For example  in Figure 5  Pe1    4  9  14  19  20  and  Pe1     5  Next we discuss how to compute the threshold Tl  Recall the threshold T for the overlap similarity  Section 2 2   T depends on both  e  and  s   To derive a lower bound of T which only depends on  e   we use    e to replace  s  and the new bound Tl is computed as below      Jaccard Similarity  Tl     e               Cosine Similarity  Tl     e         2        Dice Similarity  Tl     e         2             Edit Distance  Tl    e             q      Edit Similarity  Tl     e          e    q     1       1              q      Obviously Tl     T  If  Pe    Tl     T  e cannot be similar to any substring  and thus we can prune e  Section 2 2   For instance  in Figure 5  consider e1  Suppose      1   e1    9  Tl    e1             q   9     2   7  As  Pe1     5   Tl  e1 can be pruned  The correctness is stated in Lemma 3  Lemma 3  Given an entity e on the single heap  if its occurrence number in the heap   Pe   is smaller than Tl  e will not be similar to any valid substring  533Bucket Count Pruning  Consider an entity e  If a valid substring s is similar to e  s must have at most  e tokens and shares at least Tl tokens with e  In other words  if s is similar to e  they must have no larger than  e     Tl mismatched tokens  We can use this property for e   ective pruning  Formally  given two neighbor elements in Pe  pi and pi 1  any substring containing both the two tokens  token pi  and token pi 1   has at least pi 1     pi     1 mismatched tokens  If pi 1    pi    1    e    Tl  any substrings containing both the two tokens cannot be similar to e  Thus we do not need to count e   s occurrence numbers for any substrings  To use this feature  we partition the elements in Pe into di   erent buckets  If the number of elements in a bucket is smaller than Tl  we can prune all the elements in the bucket  lazy count pruning   otherwise we use the elements in the bucket to count e   s occurrence number for valid substrings with token numbers between    e and  e  Section 3 3   Next we introduce how to do the partition  Initially  we create a bucket b1 and put the    rst element p1 into the bucket  Then we consider the next element p2  If p2     p1     1    e     Tl  we create a new bucket b2 and put p2 into bucket b2  otherwise  we put p2 into bucket b1  Iteratively we can partition all elements into di   erent buckets  We give a tighter bound for di   erent similarity functions  For example  consider edit distance  We can use pi 1     pi     1          q for pruning  since there exists at least        q   1 mismatched tokens between pi and pi 1  which need at least     1 single character edit operations to destroy these       q 1 mismatched tokens  Similarly for edit similarity  we can use pi 1     pi     1       e  q   1          1             q  for pruning  For example  in Figure 5  suppose we use edit distance and      1   Consider Pe4    1  2  3  4  9  14  19   Tl    e4           q   8     1     2   6  Obviously  e4 can pass the lazy count pruning as  Pe4       Tl  Next we check whether it can pass the bucketcount pruning  We    rst partition Pe4 into di   erent buckets  Initially  we create a bucket b1 and put p1 into the bucket  Next for p2   2   as p2     p1     1            q   2  we put p2 into bucket b1  Similarly p3   3  and p4   4 are added into b1  For p5   9   as p5     p4     1          q  we create a new bucket b2 and add p5 into bucket b2  We repeat theses steps and    nally get b1    1  2  3  4   b2    9   b3    14   and b4    19   As the size of each bucket is smaller than Tl  we can directly prune the elements in each bucket  Thus  we do not need to count the occurrence number of e4 in any valid substrings  Moreover  we can generalize this idea  Given two elements pi and pj  i   j   if pj    pi     j   i     e    Tl  any substrings containing both the two tokens  token pi  and token pj    cannot be similar to entity e  Next we will introduce how to use this property to do further pruning  Batch Count Pruning  We have an observation that we do not need to enumerate each element in the position list Pe to count the occurrence numbers for every valid substring  Instead  we check sublists of Pe and test whether the sublists can produce candidate pairs  If so  we    nd candidate pairs in the sublists  otherwise we prune the sublists  Formally  if a valid substring s is similar to entity e  they must share enough common tokens   e     s      Tl   In other words  we only need to check the sublist with no smaller than Tl elements  Consider a sublist Pe i         j  with  Pe i         j   j    i   1   Tl  Let D pi          pj   denote the substring exactly containing tokens token pi   token pi 1              token pj    Figure 6   If  D pi          pj    pj   pi 1  e  any valid substring containe                         i j  m p1 p2 pi pj pm Pe i    j if Tl Pe i   j Te Pe i     j is a  if Tl  Pe i     j Te and   e D pi     pj Te         pi pj lo max pj  Te  pi 1                                 up min pi Te  pj 1 Candidates of e are in D lo     up D pi pj Pe i  j Figure 6  Candidate window and Valid window  ing all tokens in D pi          pj   has larger than  e tokens  Thus we can prune Pe i         j   On the contrary  D pi          pj   may be similar to e if    e      D pi          pj         e  This pruning technique is much power than the mismatch based pruning  since if pj    pi    j   i     e   Tl  then pj    pi 1    e  on the contrary if pj    pi  1    e  pj    pi     j   i     e    Tl may not hold  In addition  as  Pe i         j        D pi          pj      Pe i         j   should be not larger than  e  thus Tl      Pe i         j        e  Based on this observation  we can    rst generate sublists of Pe with sizes  number of elements  between Tl and  e  i e   Tl      Pe i         j        e  Then for each such list Pe i         j   if  D pi          pj       e  we prune the sublist  otherwise if    e      D pi          pj         e  we    nd candidates of entity e  a substring s is a candidate of e if  e     s      T and    e      s       e   For each candidate s of entity e   s  e  is a candidate pair  Next we discuss how to    nd candidates of e based on Pe  For ease of presentation  we    rst introduce two concepts  Definition 3  valid window and candidate window   Consider an entity e and its position list Pe    p1          pm   A sublist Pe i         j  is called a window of Pe for 1     i     j     m  Pe i         j  is called a valid window  if Tl      Pe i         j        e  Pe i         j  is called a candidate window  if Pe i         j  is a valid window and    e      D pi          pj         e  The valid window restricts the length of a window  The candidate window restricts the number of tokens of a valid substring  If a valid substring is a candidate of entity e  it must contain a candidate window  Figure 6   For example  consider the document and entities in Table 1  Pe4    10  17  33  34  43  58  59  60  61  66  71  76  81  86   Suppose we use edit distance and      2   e4    8  Tl    e4             q   4     e4    e4           6 and  e4    e4         10  Pe4  1          4     10  17  33  34   Pe4  1          5     10  17  33  34  43   and Pe4  6          9     58  59  60  61  are three valid windows  Pe4  1          4  is not a candidate window as p4   p1 1 34   10  1    e4   The reason is that D p1          p4  contains more than  e4 tokens and any substring containing Pe4  1          4  must have more than  e4 tokens  Although p9     p6   1      e4   Pe4  6          9  is not a candidate window as p9     p6   1      e4   Notice that we can optimize the pruning condition for jaccard similarity  cosine similarity  and dice similarity  since they depend on  e     s   Given a valid window Pe i         j   let s   D pi          pj     Pe i         j        e     s  5   Take jaccard similarity as an example  If s and e are similar   e   s   e   s           D pi          pj       s       e     s       e   s         min  e   Pe i      j         Thus we give a tighter bound of  D pi          pj     For jaccard similarity     e      D pi          pj        min  e   Pe i      j         for dice similarity  5 As D pi          pj   may contain duplicate tokens   Pe i          j        e     s  and  Pe i          j   may also be larger than  e   534   e      D pi          pj        min  e    Pe i  j        2           for cosine similarity     e      D pi          pj        min  e   Pe i      j      2   Now we introduce how to    nd candidates of e from candidate windows Pe i         j   The substrings that contain all tokens in D pi          pj   may be candidates of e  We can    nd these substrings as follows  As these substrings must contain token pi   the    maximum start position    of such substrings is pi and the    maximum end position    is up pi   e    1  Similarly  as these substrings must contain token pj    the    minimum start position    is lo   pj      e   1 and the    minimum end position    is pj   Thus we only need to    nd candidates among substrings D pstart          pend  where lo     pstart     pi  pj     pend     up  Substring s   D pstart          pend  is a candidate of e if    e      s    pend    pstart   1      e and  e   s      T   Here we use threshold T as s   D pstart          pend  is known   However this method may generate duplicate candidates  For example  suppose pj      e   1   pi   1   1  D pi   1   e    D pi   1          pi   1    e     1   could be a candidate generated from Pe i         j   In this case  as    e     pj    pi   1     pj    pi   1   1      e and Tl      Pe i         j        Pe i     1          j     pj     pi   1   1      e  Pe  i     1          j  is also a candidate window  Thus Pe  i   1          j  also generates the candidate D pi   1   e   For Pe i         j   to avoid generating duplicates with Pe i   1         j  and Pe i         j 1   we will not extend Pe i         j  to positions smaller than pi   1 1 and larger than pj 1     1  and set lo   max pj      e   1  pi   1 1   up   min pi   e    1  pj 1    1   In this way  our method will not generate duplicate candidates  In summary  to    nd all candidates for an entity  we    rst get the entity   s position list  and then generate the valid windows and candidate windows  Next we identify its candidates from candidate windows  Finally the pair of each candidate and the entity is a candidate pair  The correctness and completeness of our method is formalized as below  Theorem 1  Correctness and Complexness   Our method    nds the candidate pairs completely and correctly  4 2 Finding Candidate Windows Ef   ciently Given an entity e  as there are larger numbers of valid windows     e l Tl  Pe      l   1   it could be expensive to enumerate the valid windows for    nding all candidate windows  To improve the performance  this section proposes e   cient methods to    nd candidate windows  Span and Shift based method  For ease of presentation  we    rst introduce a concept    possible candidate windows     A valid window Pe i         j  is called a possible candidate window if pj    pi   1      e  Based on this concept  we introduce two operations  span and shift  Given a current valid window Pe i         j   we use the two operations to generate new valid windows as follows  Figure 7       span  If pj     pi   1      e  for k     j  Pe i         k  may be a possible candidate window  We span it to generate all possible candidate windows starting with i  Pe i         j 1             Pe i         x   where x satis   es px     pi   1      e and px 1   pi 1  e  For j     k     x  if pk     pi   1      e  Pe i         k  is a candidate window  On the contrary  if pj     pi   1    e  for k     j  as pk    pi   1   pj    pi   1  e  Pe i         k  cannot be a candidate window  Thus we do not need to span Pe i         j       shift  We shift to a new valid window Pe  i 1          j 1    We use the two operations to    nd candidate windows as follows  Initially  we get the    rst valid window Pe 1          Tl   We do span and shift operations on Pe 1          Tl   For the new valid windows generated from the span operation  we check whether they are candidate windows  for the new valid window generated from the shift operation  we do span and shift operations on it  Iteratively we can    nd all candidate windows from Pe 1          Tl   We give an example to show how our method works  For e4    venkatesh       Pe4    10  17  33  34  43  58  59  60  61  66  71  76  81  86   Suppose    2   e4    8  Tl    e4             q   4     e4    e4           6   e4    e4         10  The    rst valid window is Pe4  1          4     10  17  33  34   As p4     p1   1     34     10   1    e4   we do not need to do span operation  We do a shift operation and get the next window Pe4  2          5   As p5     p2   1     43     17   1    e4   we do another shift operation  When we shift to valid window Pe4  6          9   as p9   p6 1 61   58 1    e4   Pe4  6          9  is not a candidate window  We do a span operation  As p10   p6 1 9    e4 and p11   p6 1 14  e4   x   10  We get a valid window Pe4  6          10   which is a candidate window  Next we shift to Pe4  7          10   Iteratively we    nd all candidate windows  Pe4  6          10  and Pe4  7          10   Figure 8   Given a valid window Pe i         j   if pj     pi   1    e  the shift operations can prune the valid windows starting with i  e g   Pe i         k  for j   k     i    e     1  However this method still scans large numbers of candidate windows  To further improve performance  we propose a binary searchbased method which can skip many more valid windows  Binary Span and Shift based method  The basic idea is as follows  Given a valid window Pe i         j   if pj    pi   1    e  we will not shift to Pe  i   1          j   1    Instead we want to directly shift to the    rst possible candidate window after i  denoted by Pe mid          mid   j     i    where mid satis   es pmid j   i     pmid   1      e and for any i     mid     mid  pmid  j   i     pmid    1    e  Similarly  if pj     pi   1      e  we will not iteratively span it to Pe i         j   1    Pe i         j   2            Pe i         x   Instead  we want to directly span to the last possible candidate window starting with i  denoted by Pe i         x   where x satis   es px     pi   1      e and for any x     x  px      pi   1    e  If the function F x    px    pi   1  for span and F    mid    pmid j   i     pmid   1  for shift are monotonic  we can use a binary search method to    nd x and mid e   ciently  For the span operation  obviously F x    px     pi   1  is monotonic as F x   1      F x    px 1     px   0  Next we give the lower bound and upper bound of the search range  Obviously x     j  In addition  as pi   j     pi j   we have px     pi    e     1     pi  e   1 and x     i    e     1  Thus we    nd x by doing a binary search between j and i    e     1  However F    mid    pmid j   i     pmid   1 is not monotonic  We have an observation that F     mid      pj    mid    i       pmid   1 is monotonic  since F     mid   1    F     mid    pmid     pmid   1     1     0  More importantly for i     mid     j  F     mid    F    mid  as   pj    mid     i        pmid j   i  Thus if F     mid     1     e  F    mid     1     e and Pe  mid     1          mid     1   j     i   cannot be a candidate window  If F     mid       e  Pe  mid          mid   j     i   could be a candidate window  In this way  we can    nd mid by doing a binary search between i and j such that F     mid     1     e and F     mid       e  If F    mid       e  we have found the last possible candidate window  otherwise  we continue to    nd mid   between mid   1  and mid   1   j     i  Iteratively  we can    nd the last possible candidate window  535i i  j j  m pi Shift Shift Pe i   j Pe i     j                 pj                 Span only if pj  pi  Te Span Pe i   j Pe i    j     Pe i   x         pi         pj                 Pe i   x  is the last possible candidate window starting with i Current valid window New valid window Current valid window New valid windows i i  j j  x m px pi 1  e and  px 1 pi 1  e Figure 7  span and shift operations  x 10 as px pi 1  Te and px 1 pi 1 Te  1  shift  2  span  3  shift   5  shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 10 17 33 34 43 58 59 60 61 66 71 76 10 17 33 34 43 58 59 60 61 66 71 76 10 17 33 34 43 58 59 60 61 66 71 76 Pe Pe Pe Pe 1 1 2 1 9876543 10 1 2 1 1 2 1 9876543 10 1 2 1 1 2 1 9876543 10 1 2 81 86 13 14 81 86 13 14 81 86 13 14 81 86 13 14  4  span Pe 10 17 33 34 43 58 59 60 61 66 71 76 1 1 2 1 9876543 10 1 2 81 86 14 13 i New valid windows New valid windows Current valid window Current valid window j i j Current valid window Current valid window Current valid window  e4   q Tl  e4  Te4  Pe 6 10  is a candidate window but Pe 6 9  is not as p9 p6 1  e4 Pe 7 10  is a candidate window Figure 8  An example for span and shift operations  Thus given a valid window Pe i         j   we use binary span and shift operations to    nd candidate windows  Figure 9       Binary span  If pj     pi   1      e  we    rst    nd x by doing a binary search between j and i    e     1  where x satis   es px     pi   1      e and px 1     pi   1    e  and then directly span to Pe i         x       Binary shift  If pj     pi   1    e  we    nd mid by doing a binary search between i and j where mid satis   es   pj    mid     i        pmid   1      e and   pj    mid     1     i        pmid   1   1    e  If pmid j   i     pmid   1    e  we iteratively do the binary shift operation between mid   1  and mid   1   j     i  On the contrary  if pj     pi   1      e  we shift to a new valid window Pe  i 1          j 1    Given a valid window Pe i         j   the binary shift can skip unnecessary valid windows  non candidate windows   such as Pe  i  1          j  1           Pe  mid   1          mid   1  j   i    as proved in Lemma 4  For example  consider the position list in Figure 10  Suppose      2  Tl   4   e4    8   e4   10  Consider the    rst valid window Pe4  1          4   The shift operation shifts it to Pe4  2          5   Pe4  3          6              Pe4  6          9   and checks whether they are candidate windows  The binary shift operation can directly shift it to Pe4  3          6  and then to Pe4  6          9   Thus it skips many valid windows  Lemma 4  Given a valid window Pe i         j  with pj    pi   1    e  if   pj  mid     i       pmid   1      e and   pj    mid     1    i      pmid   1  1    e  Pe  i 1          j 1           Pe  mid    1             mid     1    j     i     are not candidate windows  pi Binary Shift                 pj         Binary Span  if pj  pi 1 Te            pi         pj         Span to the last possible candidate window starting with i                 mid mid j i Find mid by doing a binary search between i and j such that  pj  mid i   pmid 1  e and    pj  mid 1 i   pmid 1 1 e Find x by doing a binary search between j and i  e 1 such that px  pi 1  e and  px 1  pi 1  e Current valid window the last possible candidate window the first possible candidate window pi  2   if pj  pi 1  e  Shift to the next possible candidate window                 pj                 Current valid window New valid window  1  if pj  pi 1 Te  Shift to the first possible candidate window after pi Current valid window i i  j j  m i i  j j  m i i  j j  x  m Figure 9  span and shift in a binary search way  skip lower j x  10 as  px  pi 1 e and  px 1 pi 1 e upper i Te 1 Pe 6   10  is a candidate window but Pe 6   9  is not as p9 p6 1    e  1  binary shift  2  binary span  5  binary shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 14 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 12 13 81 86 13 14 81 86 13 14  e4   q Tl  e4  Te4  Pe Pe Pe lower j Pe 7   10  is a candidate window  4  binary span  10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 13 14 Pe upper i Te 1  3  binary shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 13 14 Pe pj  pi 1  e Current valid window Current valid window stop pj  pi 1  e Current valid window skip i i pj  pi 1  e pj  pi 1  e pj  pi 1  e Current valid window Current valid window Figure 10  An example for binary span and shift  The binary span operation can directly span to Pe i         x  and has two advantages  Firstly  in many applications  users want to identify the best similar pairs  sharing common tokens as many as possible   and the binary span can e     ciently    nd such substrings  Secondly  we do not need to    nd candidates of e for Pe i         j   1            Pe i         x  one by one  Instead since there may be many candidates between lo   pj      e   1  and up   pi x   j    e     1  we    nd them in a batch manner  We group the candidates based on their token numbers  Entities in the same group have the same number of tokens  Consider the group with g tokens  suppose Tg is the threshold computed using  e  and g  If  Pe i         x     Tg  we prune all candidates in the group  We can use the two binary operations to replace the shift and span operations in order to skip valid windows  We give an algorithm to    nd candidate windows using the two operations as illustrated in Figure 11  It    rst initializes the    rst valid window  line 2 line 4   Then it uses the two binary operations to extend the valid window until reaching the last valid window  line 3   If its token number is no larger than  e  we do a binary span operation by calling its subroutine BinarySpan  line 6  and do a shift operation  line 7   otherwise calling its subroutine BinaryShift  line 8   BinarySpan does a binary search to    nd the last possible candidate window starting with pi  lines 3 6   Then it retrieves the 536Algorithm 1  Find Candidate Windows Input  e  An entity  Pe  Position list of e on the heap  Tl   Threshold   e  The upper bound of token numbers  1 begin 2 i   1  3 while i      Pe      Tl   1 do 4 j   i   Tl     1  5 if pj     pi   1      e then 6 BinarySpan i  j   7 i   i   1     shift to the next window    8 else i   BinaryShift i  j   9 end Procedure BinarySpan i  j  Input  i  the start point  j  the end point  1 begin 2 lower   j  upper   i    e     1   3 while lower     upper do 4 mid     upper   lower  2   5 if pmid     pi   1    e then upper   mid     1  6 else lower   mid   1  7 mid   upper   8 Find candidate windows in D i        mid   9 end Procedure BinaryShift i  j  Input  i  the start point  j  the end point Output  i  the new start point  1 begin 2 lower   i  upper   j  3 while lower     upper do 4 mid     lower   upper  2   if   pj    mid     i    5     pmid   1    e then 6 lower   mid   1  7 else upper   mid     1  8 i   lower  j   i   Tl     1  9 if pj     pi   1    e then i   BinaryShift  i  j   10 else return i  11 end Figure 11  Algorithm  Find candidate windows candidate windows  line 8   BinaryShift does a binary search to    nd the    rst possible candidate window after pi  Iteratively our method    nds all candidate windows  Figure 10 illustrates an example to walk through our algorithm  5  THE Faerie ALGORITHM In this section  we propose a single heap based algorithm  called Faerie  to e   ciently    nd all answers  We    rst construct an inverted index for all entities in the given dictionary E  Then for the document D  we get its tokens and corresponding inverted lists  Next we construct a single heap on top of inverted lists of tokens  We use the heap to generate entities in ascending order  For each entity e  we get its position list Pe  If  Pe    Tl  we prune e based on lazy count pruning  otherwise we use the two binary operations to    nd candidate windows  Then based on the candidate windows  we generate candidate pairs  Finally  we verify the candidate pairs and get the    nal results  Figure 12 gives the pseudo code of the Faerie algorithm  The Faerie algorithm    rst constructs an inverted index for prede   ned entities  line 2   and then tokenizes the document  gets inverted lists  line 3   and constructs a heap  line 4   Faerie uses  ei  pi  to denote the top element of the heap  where ei is the current minimal entity and pi is the position of the inverted list from which ei is selected  Faerie constructs a position list Pe to keep all the positions of inverted lists in the heap that contain e  line 6   Then for each top element  ei  pi  on the heap  if ei   e  we add pi Algorithm 2  Faerie Algorithm Input  A dictionary of entities E    e1  e2          en   A document D  A similarity function and a threshold  Output    s  e   s and e are similar for the function and threshold   where s is a substring of D and e     E  1 begin 2 Tokenize entities in E and construct an inverted index  3 Tokenize D and get inverted lists of tokens in D  4 Construct a heap H on top of inverted lists of D  5 e is the top element of H     keep the current entity   6 Initialize a position list Pe       while    ei  pi    H top   7        do 8 if ei    e then 9 Pe       pi      ei is the new top entity     10 else Derive the threshold Tl 11 for entity e  12 if  Pe      Tl then 13 Find candidate windows using Algorithm 1  14 Get candidates using candidate windows  15 e   ei  Pe    pi      update the current entity 16 Adjust the heap  17 Verify candidate pairs  18 end Figure 12  The Faerie Algorithm  into Pe  line 8 line 9   where e is the last popped entity from the heap  otherwise Faerie checks the position list as follows  Faerie derives a threshold Tl for entity e based on the similarity function and threshold  If  Pe      Tl  there may exist candidate pairs  Faerie generates candidate windows based on Algorithm 1  line 13   and    nds candidate pairs based on candidate windows  line 14   Faerie adjusts the heap to generate candidates for the next entity  line 16   Finally Faerie veri   es the candidates to get    nal results  line 17   Next we give a running example to walk through our algorithm  Consider the entities and document in Table 1  We    st construct a single min heap  Figure 5   Then we adjust the heap to generate the position list for each entity  Consider the position list for entity e4     venkatesh     in Fig   ure 10  Suppose      2   e4    8     E   6   E   12     e4   6   e4   10  Tl   4  We use the binary shift and span operations to get candidate windows  Pe 6          10  and Pe 7          10    and then generate candidate pairs based on the candidate windows  e g    D 58  9     venkaee sh     e4    venkatesh       Finally we verify the candidates to get the    nal answers  6  EXPERIMENTS We have implemented our proposed techniques  The objective of the experiments is to measure the performance  and in this section we report experimental results  Experimental Setting  We compared our algorithms with state of the art methods NGPP  22   the best for edit distance  and ISH  4   the best for jaccard similarity and edit similarity   We downloaded the binary codes of NGPP  22  from    Similarity Joins    project website 6 and implemented ISH by ourselves  We reported the best performance of the two existing methods  The algorithms were implemented in C   and compiled using GCC 4 2 4 with  O3    ag  All the experiments were run on a Ubuntu machine with an Intel Core 2 Quad X5450 3 0GHz processor and 4 GB memory  Datasets  We used three real datasets  DBLP 7   PubMed 8   and ACM WebPage 9   DBLP is a computer science publi  6 http   www cse unsw edu au    weiw project simjoin html 7 http   www informatik uni trier de    ley db 8 http   www ncbi nlm nih gov pubmed 9 http   portal acm org 537cation dataset  We selected 100 000 author names as entities and 10 000 papers as documents  PubMed is a medicalpublication dataset  We selected 100 000 paper titles as entities and 10 000 publication records as documents  WebPage is a set of web pages  We crawled 100 000 titles as entities and 1 000 web pages as documents  thousands of tokens   Table 4 gives the dataset statistics  len denotes the average length and token denotes the average token number   We did not consider di   erent attributes in the entities and documents  Each entity in the dictionary is just a string  Table 4  Datasets  Datasets Cardinality len tokens Details DBLP Dict 100 000 21 1 2 77 Author DBLP Docs 10 000 123 3 16 99 Papers PubMed Dict 100 000 52 96 6 98 Title PubMed Docs 10 000 235 8 33 6 Papers WebPage Dict 100 000 66 89 8 5 Title WebPage Docs 1 000 8949 1268 Web Pages 6 1 Multi Heap vs Single Heap In this section  we compared the multi heap based method with the single heap based method  without using pruning techniques in Section 4   We tested the performance of the two methods for di   erent similarity functions on the three datasets  Figure 13 shows the experimental results  We see that the single heap based method outperforms the multi heap based method by 1 2 orders of magnitude  and even 3 orders of magnitude in some cases  For example  on DBLP dataset with edit distance threshold      3  the multi heap based method took more than 10 000 seconds and the single heap based method took about 180 seconds  On PubMed dataset with eds similarity threshold      0 9  the multi heap based method took more than 14 000 seconds and the single heap based method took only 600 seconds  There are two reasons that the single heap based method is better than the multi heap based method  Firstly  the multi heap based method scans each inverted list of the document many times and the single heap based method only scans them once  Secondly the multi heap based method constructs larger numbers of heaps and does larger numbers of heap adjustment than the single heap based method  As the single heap based method outperforms the multi heapbased method  we focus on the single heap based method in the remainder of the experimental comparison  6 2 Effectiveness of Pruning Techniques In this section  we tested the e   ectiveness of our pruning techniques  We    rst evaluated the number of candidates by applying di   erent pruning techniques to our algorithm  lazycount pruning  bucket count pruning  and binary span and shift pruning  As batch count pruning is a special case of binary span and shift pruning  we only show the results for binary span and shift pruning    In the paper  the number of candidates refers to the number of non zero values in the occurrence arrays  which need to be veri   ed  Figure 14 gives the results  In the    gure  we tested edit distance on DBLP dataset  jaccard similarity on WebPage dataset  and edit similarity on PubMed dataset  Note that in the    gures  the results are in 10 x formate  For example if there are 100 million candidates  the number in the    gure is 8  10 8   100M   In the paper  our method used parameters of q   16  8  5  4  3 for      0  1  2  3  4 respectively for edit distance on DBLP and q   26  11  7  5  4 for      1  0 95  0 9  0 85  0 8 edit similarity on PubMed  We observe that our proposed pruning techniques can prune large numbers of candidates  For example  on the DBLP dataset  for      3  the method without any pruning techniques involved 11 billion candidates  and the lazy count pruning reduced the number to 860 million  The bucketcount pruning further reduced the number to 600 million  The binary span and shift pruning had only 200 million candidates  On the WebPage dataset  for      0 9  the binary span and shift pruning reduced the number of candidates from 10 billion to 35  On the PubMed dataset  for      0 85  the binary span and shift pruning reduced the number of candidates from 180 billion to 120 million  The main reason is that we compute an upper bound of the overlap of an entity and a substring  and if the bound is smaller than the overlap threshold  we prune the substring  If for any substring  the bound of an entity is smaller than the threshold  we prune the entity  This con   rms the superiority of our pruning techniques  Then we evaluated the performance bene   t of the pruning techniques  Figure 15 shows the results  We observe that the pruning techniques can improve the performance  For instance  on the DBLP dataset  for      3   the  elapsed time of the method without any pruning technique was 180 seconds  and the lazy count pruning decreased the time to 43 seconds  The binary span and shift pruning reduced the time to 25 second  On PubMed dataset  for      0 9  the pruning techniques can improve the time from 600 seconds to 8 seconds  This shows that our pruning techniques can improve the performance  In the remainder of this paper  we compared the single heap based method using the binary span and shift pruning with existing methods  6 3 Comparison with State of the art Methods In this section  we compared our algorithm Faerie with state of the art methods NGPP  22   which only supports edit distance  and ISH  4   which supports edit similarity and jaccard similarity   We tuned the parameters of NGPP and ISH  e g   pre   x length of NGPP  to make them achieve the best performance  Figure 16 shows the results  We see that Faerie achieved the highest performance  Especially Faerie outperformed ISH by 1 2 orders of magnitude for edit similarity and jaccard similarity  For example  on the PubMed with edit similarity threshold      0 9  the elapsed time of ISH was 1000 seconds  Faerie reduced the time to 8 seconds  This is because Faerie used the shared computation across overlapped tokens  In addition  our pruning techniques can prune large numbers of unnecessary valid substrings and reduce the number of candidates  Although NGPP achieved high performance for smaller edit distance thresholds  it is ine   cient for larger edit distance thresholds  The reason is that it needs to enumerate neighbors of entities and an entity has larger numbers of neighbors for larger thresholds  On jaccard similarity  as each entity has a smaller number of tokens  the average number is 8  and the thresholds Tl and  e for di   erent thresholds are nearly the same  Tl   8 for    1 and Tl   10 for    0 8   Faerie varied a little for di   erent jaccard similarity thresholds  In addition  we compared index sizes of di   erence algorithms  Note that NGPP had di   erent index sizes for different edit distance threshold      as NGPP uses    to generate neighborhoods  The larger the edit distance threshold  the larger indexes are involved for the neighborhoods of an entity  since an entity has larger numbers of neighbors for larger thresholds  On DBLP dataset  for      3  NGPP con  538 0 1  1  10  100  1000  10000  0 1 2 3 Search Time  s  Threshold    Multi Heap Single Heap  a  ed  DBLP   100  1000  10000 1 0 95 0 9 0 85 Search Time  s  Threshold    Multi Heap Single Heap  b  jac  WebPage   0 1  1  10  100  1000  10000  100000 1 0 95 0 9 0 85 Search Time  s  Threshold    Multi Heap Single Heap  c  eds  PubMed  Figure 13  Performance comparison of multi heap based methods and single heap based methods   0  2  4  6  8  10  12  14  0 1 2 3   of Candidates  10 x   Threshold    None Lazy Bucket Binary  a  ed  DBLP   0  2  4  6  8  10  12  14  16 1 0 95 0 9 0 85   of Candidates  10 x   Threshold    None Lazy Bucket Binary  b  jac  WebPage   0  2  4  6  8  10  12  14  16 1 0 95 0 9 0 85   of Candidates  10 x   Threshold    None Lazy Bucket Binary  c  eds  PubMed  Figure 14  Number of candidates with di   erent pruning techniques   0 1  1  10  1</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Attribute Domain Discovery for Hidden Web Databases ### Xin Jin George Washington University Washington  DC 20052  USA xjin gwu edu Nan Zhang   George Washington University Washington  DC 20052  USA nzhang10 gwu edu Gautam Das y University of Texas at Arlington Arlington  TX 76019  USA gdas uta edu ABSTRACT Many web databases are hidden behind restrictive form like interfaces which may or may not provide domain information for an attribute  When attribute domains are not available  domain discovery becomes a critical challenge facing the application of a broad range of existing techniques on third party analytical and mash up applications over hidden databases  In this paper  we consider the problem of domain discovery over a hidden database through its web interface  We prove that for any database schema  an achievability guarantee on domain discovery can be made based solely upon the interface design  We also develop novel techniques which provide effective guarantees on the comprehensiveness of domain discovery  We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach  Categories and Subject Descriptors H 2 7  Database Administration   H 3 5  Online Information Services   Web based services General Terms Algorithms  Measurement  Performance Keywords Hidden Web Database  Domain Discovery ### 1  INTRODUCTION The Attribute Domain Discovery Problem  In this paper  we develop novel techniques to discover the attribute domains  i e   the set of possible values for each attribute  from hidden web databases  by external users  Hidden databases  as a large portion of the deep   Partially supported by NSF grants 0852673  0852674  0915834 and a GWU Research Enhancement Fund  y Partially supported by NSF grants 0812601  0915834  1018865  a NHARP grant from the Texas Higher Education Coordinating Board  and grants from Microsoft Research and Nokia Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  web  are hidden behind restrictive form like interfaces which allow a user to form a search query by specifying the desired values for one or a few attributes  and the system responds by returning a small number of tuples satisfying the user speci   ed search condition  A typical example of a hidden database is the award search database of the US National Science Foundation  NSF  1   which allows users to search for NSF award projects featuring userspeci   ed values on up to 20 attributes  Each attribute speci   able by users through the form like interface is represented by an input control on the interface  For certain types of controls  e g   drop down menus and radio buttons  the attribute domain can be readily retrieved by external users from source code of the interface  e g   an HTML    le   In the NSF example  attributes such as NSF organization and Award Instrument belong to this category  For other controls  especially text boxes  without features such as autocompletion   no domain information is provided  Attributes such as Program Manager belong to this category  The focus of this paper is to develop domain discovery techniques  restricted to accessing the database only via the proprietary form interface  that external users can employ to unveil possible attribute values that appear in tuples of the hidden databases  An important design goal is to develop techniques that unveil all  or most  attribute values by issuing only a small number of queries  and to provide analytical guarantees on the coverage and query cost of our methods  We emphasize that in this paper  we do not consider approaches to the domain discovery problem that rely on external knowledge sources or domain experts to provide the required domain values  We argue that while the use of external knowledge sources may have applicability in very general scenarios  e g   the use of a USPS data source to list all values of an attribute such as zipcode   these approaches will not work in more focused applications such as the Program Manager attribute on the NSF award search database  Our emphasis is to develop automated domain discovery algorithms that can power a a variety of third party applications only using the public interfaces of these databases  and without requiring any further collaborations or agreements with the database owners  Applications  Domain discovery from hidden databases belongs to the area of information extraction and deep web analytics  see tutorials in  6  11  and survey in  18    To the best of our knowledge  the only prior work that tackles our speci   c problem of discovering attribute domains of hidden databases is  20   This problem has a broad range of applications  First and foremost  it serves as a critical prerequisite for data analytics over hidden databases  because all existing aggregate estimation  8  and sampling  7  9  10  techniques for hidden databases require prior knowledge of the attribute 1 http   www nsf gov awardsearch tab do dispatch 4domains  In addition  the attribute domains being discovered can be of direct interest to third party web mashup applications  For example  such an application may use the discovered attribute domains to add autocompletion feature to text boxes in the original formlike interface  so as to improve the usability of the interface  The discovered domains may also be used to identify mapping between attributes in different hidden databases to facilitate the creation of mashup  Challenges  The main challenges to effective attribute domain discovery are the restrictions on input and output interfaces of hidden databases  Normally  input interfaces are restricted to issuing only search queries with conjunctive selection conditions   which means that queries like SELECT UNIQUE Program Manager  FROM D cannot be directly issued  eliminating the chance of directly discovering the domain of Program Manager through the interface  The output interface is usually limited by a top k constraint   i e   if more than k tuples match the user speci   ed condition  then only k of them are preferentially selected by a scoring function and returned to the user  This restriction eliminates the possibility of trivially solving the problem by issuing the single query SELECT   FROM D and then discovering all attribute domains from the returned results  The existing technique for attribute domain discovery is based on crawling the database  20   A simple instantiation of crawling is to start with SELECT   FROM D  then use domain values returned in the query answer to construct future queries  and repeat this process until all queries that can be constructed have been issued  One can see that no algorithm can beat the comprehensiveness of attribute domain discovery achieved by crawling  But crawling requires an extremely large number of queries to complete  and since most hidden databases enforce a limit on the number of queries one can issue over a period of time  e g   1  000 per IP address per day   or even charge per query  often the crawling process has to be terminated prematurely  Due to this reason  such techniques cannot provide any guarantees on the comprehensiveness of discovered attribute domains  A seemingly promising way to address the query cost problem of crawling is to instead perform sampling   i e   one    rst draws representative samples from the hidden database  and then discover attribute domains from the sample tuples to achieve statistical guarantees on the comprehensiveness of domain discovery  Sampling of hidden databases is an intensively studied problem in recent years  7  9  10   Nonetheless  a key obstacle here is a    chickenand egg    problem   i e   all existing sampling algorithms for hidden databases require the attribute domains to be known in the    rst place  preventing them from being applied to hidden databases with unknown attribute domains  Outline of Technical Results  In this paper we initiate a study of query ef   cient attribute domain discovery over hidden databases  We consider two types of output interfaces commonly offered by hidden databases  COUNT and ALERT  For a user speci   ed query  a COUNT interface returns not only the top k tuples  but also the number of tuples matching the query in the database  which can be greater than k   ALERT interface  on the other hand  only alerts the user with an over   ow    ag when more than k tuples match the query  indicating that not all tuples which match the query can be returned  The NSF award database features an ALERT interface  For COUNT interface  we start by studying the feasibility of developing deterministic algorithms for attribute domain discovery  Our main results are two achievability conditions   lower bounds on k and query cost  respectively   which a deterministic algorithm must satisfy to guarantee the discovery of all attribute domains  Unfortunately  neither condition is achievable in practice for a generic hidden database  Nonetheless  a promising lead we    nd from the study is that ef   cient and complete attribute domain discovery is indeed possible for a special type of  perhaps unrealistic  databases in which each attributes has only two domain values  For this special case  we develop B COUNT DISCOVER  a deterministic algorithm based on the key idea of constructing a series of nested search spaces for unknown domain values  and prove that the algorithm guarantees the discovery of all attribute domains with only O m2   queries  where m is the number of attributes  as long as k   2  Our main observation from B COUNT DISCOVER is an understanding of why it cannot be extended to generic hidden databases containing attributes with arbitrary sized domains   we    nd the main reason to be the large number of queries required by a deterministic algorithm to decide how to construct the nested search space  To address this  we consider the design of randomized algorithms which allows query ef   cient randomization of the nestedspace construction process  To this end  we develop RANDOMCOUNT DISCOVER  a Monte Carlo algorithm which only requires a small amount of queries to discover a large number of domain values  For ALERT interface  we similarly prove the infeasibility of developing a query ef   cient deterministic algorithm which guarantees complete attribute domain discovery  Our main result here is then an extension of RANDOM COUNT DISCOVER to RANDOMALERT DISCOVER  a Monte Carlo algorithm which achieves similar performance to RANDOM COUNT DISCOVER by estimating COUNTs of queries based on historic query answers  and using estimated COUNTs to bootstrap RANDOM COUNT DISCOVER to discover domain values  In summary  the main contributions of this paper are as follows    We initiate the study of attribute domain discovery over a hidden database through its restrictive web interface    We derive the achievability conditions for complete attribute domain discovery on k and the query cost  These conditions indicate the infeasibility of designing a query ef   cient deterministic algorithm which guarantees the discovery of all attribute domains    We propose two randomized algorithms  RANDOM COUNTDISCOVER and RANDOM ALERT DISCOVER  for COUNT and ALERT interfaces  respectively  Both algorithms require only a small amount of queries to develop a large number of domain values    We provide a thorough theoretical analysis and experimental studies that demonstrate the effectiveness of our proposed algorithms over the real world NSF award search database and a local patient discharge dataset  Paper Organization  In Section 2 we introduce preliminaries and discuss simple but ineffective crawling algorithms for attribute domain discovery over hidden databases  Sections 3 and 4 are devoted to the development of deterministic and randomized algorithms for COUNT interfaces  respectively  In Section 5 we extend the results to ALERT interfaces  Section 6 contains a detailed experimental evaluation of our proposed approaches  Section 7 discusses related work  followed by conclusion in Section 8  2  PRELIMINARIES2 1 Model of Hidden Databases Consider a hidden database table D with n tuples  Let there be m attributes A1          Am which can be speci   ed through the input interface  and Vi be the domain of Ai  We assume each tuple to be unique  and each value in Vi  i 2  1  m   to occur in at least one tuple in the database  because otherwise Vi can never be completely discovered from the database  Consider a prototypical interface which allows user to query D by specifying values for a subset of attributes   i e   to issue queries q of the form  SELECT   FROM D WHERE Ai1   vi1         Ais   vis   where vij 2 Vij   Let Sel q  be the set of tuples in D which satisfy q  Since the interface is restricted to return up to k tuples  a overly broad query  i e   jSel q j   k  will over   ow and return only the top k tuples in Sel q  selected according to a scoring function  In addition  a COUNT interface will return jSel q j  the number of tuples satisfying q  while an ALERT interface will return an over   ow    ag indicating that not all tuples which satisfy q can be returned  When a query does not over   ow  COUNT and ALERT interfaces return the exact same information to the user  In particular  if the query is too speci   c to return any tuple  we say that an under   ow occurs  If there is neither over   ow nor under   ow  i e   jSel q j 2  1  k    then Sel q  will be returned in its entirety and we have a valid query result  For each tuple returned by an over     owing or valid query  the values of all its attributes are displayed  enabling the discovery of domain values of an attribute with unknown domain  Running Example  We consider a running example of the above mentioned NSF award search database with k   50  There are 9 attributes  award amount  instrument  PI state     eld  program manager  NSF organization  PI organization  City  and ZIP code  with domain sizes 5  8  58  49  654  58  3110  1093  1995  respectively  these are the estimates we generated by running our domain discovery algorithms to be presented in the paper for an extended period of time  We do not have the ground truth for all attributes  But they can be safely considered as lower bounds on domain sizes   2 2 Problem De   nition We consider the problem of attribute domain discovery   i e   the discovery of V1          Vm through the restrictive interface  Since many hidden databases impose limits on the number of queries one can issue through their interfaces over a period of time  the performance of a domain discovery algorithm should be measured by not only the comprehensiveness of discovered domain values  but also the query cost  i e   the number of queries issued through the interface of hidden database for domain discovery  While the metric for query cost is straightforward  to measure the comprehensiveness of discovered domain values we consider the following two metrics    Coverage  i e   the total number of domain values discovered for one or a set of attributes    Recall  i e   the number of tuples in the database for which all attribute values  of A1          Am  have been discovered  Note that while the two metrics are positively correlated  e g   a recall of n indicates a coverage of jVij for each Ai   they may be preferred in different applications  For example  if the objective of domain discovery is to unveil the metadata  e g   to discover the list of program managers from the NSF award database  then coverage is a more important metric  If the objective is to enable data analytics over the hidden database  then recall is more important as it guarantees how many tuples will be covered by the subsequent aggregate estimation and sampling algorithms  Given the measures for comprehensiveness and query cost  we de   ne the problem of attribute domain discovery as follows  Problem Statement  How to maximize the coverage and recall of attribute domain discovery while minimizing the query cost through COUNT and ALERT interfaces  respectively  2 3 Crawling Based Algorithms In this subsection  we consider depth    rst search  DFS  and breadth     rst search  BFS   two simple crawling based algorithm for attribute domain discovery  and point out their problems in terms of the tradeoff between coverage recall and query cost  Both algorithms start with SELECT   FROM D to learn the    rst few domain values  but then take different methods to construct the subsequent queries  DFS  With DFS  one constructs the next query by    nding an attribute that is not speci   ed in the previous one  and then adding to the previous query a conjunctive condition de   ned by the attribute and one of its already discovered domain values  Such a process continues  with DFS learning all domain values from tuples returned by each issued query  until either the query returns under   ows valid or no discovered domain value is available for constructing the next query  at which time the algorithm backtracks by removing the last added condition and adding a new one based on another discovered domain value  We refer to the algorithm as DFS because it keeps increasing the number of predicates included in the query if possible  A main problem of DFS  however  is poor coverage and recall when a large number of attributes are strongly correlated with the    rst few attributes appended to SELECT   FROM D  For example  consider the case where A1   v1 is    rst appended to the SELECT   query  and A1   Am forms a functional dependency  Since DFS with a small query budget is likely to explore only queries with predicate A1   v1  the domain values of Am which are corresponding to the other values of A1 will not be discovered  BFS  With BFS  one    rst exhausts all 1 predicate queries that can be constructed from discovered domain values  before moving forward to construct and issue 2 predicate queries  etc  The main problem of BFS arises when a large number of domain values are strongly correlated with the scoring function used to select the top  k tuples  because BFS with a small query budget is likely to issue only over   owing queries  For example  consider a total order of values for Am and suppose that tuples with    larger    Am receives higher scores  Then  BFS will likely to only discover the larger values of Am  3  DETERMINISTIC ALGORITHMS FOR COUNT INTERFACES In this section  we analyze the achievability conditions for attribute domain discovery over a top k COUNT interface  and also develop deterministic algorithms which guarantee the discovery of all attribute domains when the achievability conditions are met  The purpose of introducing deterministic algorithms is to use them as the basis for our design of more ef   cient randomized algorithms in the next section  3 1 Discover Binary Domains We start with a simple case where each attribute has only two possible values  both unknown to the third party analyzer 2   An ex  2 The third party analyzer may or may not know the binary nature of all attributes   our results apply either way ample of such an attribute is transmission for an auto database which has two values     manual    and    automatic     While hardly any practical database consists solely of binary attributes  the results in this subsection are illustrative when considering extensions to handle arbitrary attribute domains  as we shall show in the next subsection  3 1 1 Achievability of Domain Discovery Before devising concrete attribute domain discovery algorithms  we    rst consider the problem of achievability   i e   whether it is at all possible for the third party analyzer to discover all attribute domains from the top k interface  There are two sub problems   1  whether complete attribute domain discovery is possible when the analyzer can afford an unlimited query cost  and  2  if the    rst answer is yes  then what is the minimum number of queries required by a deterministic algorithm to accomplish attribute domain discovery in the worst case  Our main results  as shown by the following three theorems  can be summarized as follows  The answer to the    rst question is a lower bound on k   i e   no algorithm without knowledge of the scoring function can guarantee complete discovery over any hidden database if k   1  while complete discovery is always possible for any combination of database and scoring function when k   2  For the second question  if k   2  a deterministic algorithm require  at least    m2   log m  queries to discover all attribute domains in the worst case  where m is the number of attributes  unless k   n  in which case SELECT   FROM D returns all tuples in the database  THEOREM 3 1  If k   1  then for any given binary database  there exists a scoring function such that no algorithm can discover the domains of all attributes  PROOF  This impossibility proof is simple   Let the tuple returned by SELECT   FROM D be t    01          0n   Assign the highest score to t  One can see that no algorithm can discover any domain value other than f01          0ng  THEOREM 3 2  For any given number of attributes m  there exists a binary database and a scoring function such that no deterministic algorithm can complete attribute domain discovery without incurring a worst case query cost of   m2   log m   even if k   2  PROOF  For the sake of simplicity  we consider the case where m is even  The case where m is odd can be proved in analogy  We construct a set of 2 m2  4 database instances D1         D 2m2 4 as follows  Each instance Di consists of 2 m 2  m 2 tuplest1          t 2m 2 m 2   We consider the    rst 2 m 2 tuples    rst   for these tuples  i e   with i 2  1  2 m 2     if j 2  1  m 2   ti Aj     0j  resp  1j   iff the j th most signi   cant bit of the binary representation of i  1 is 0  resp  1   if j 2  m 2   1  m   then there is always ti Aj     0j   For the latter m 2 tuples  their values of Am 2 1          Am are set such that ti Aj     1j iff i   2 m 2 m 2  j  and ti Aj     0j otherwise  For the values of A1          Am 2 of these tuples  they are set such that each database instance has a different combination of the m 2 tuples  Note that for each tuple  there are 2 m 2 possible choices  Thus  we can construct  2 m 2   m 2   2 m2  4 different database instances  The scoring function for each instance is designed with only one condition  each tuple which satis   es ti Aj     0j for all j 2  m 2   1  m  has a higher score than all tuples which do not satisfy this condition  There are two important observations one can make from the above construction when k   2  First  each database instance requires a different sequence of queries to unveil all domain values  The reason for this is because the only way to unveil 1j for j 2  m 2   1  m  is to issue a query that contains at least m 2 predicates corresponding to the values of A1          Am 2 for tuple t 2m 2m 2 j   respectively  Since each database instance has a different combination of values for t 2m 2 1          t 2m 2 m 2   each instance requires a different sequence of queries for complete attribute domain discovery  The second important observation is that the ability for any query to distinguish between the 2 m2  4 database instances is limited  Note that if only the returned tuples are concerned  then almost all queries  except those that unveil one of 1j for j 2  m 2  1  m   return the exact same tuples when k   2  The more powerful distinguishing factor is COUNT  Nonetheless  the COUNT of any query has only m 2 different answers for all database instances  because after all these instances differ by at most m 2 tuples  Since we focus on deterministic algorithms  it is impossible for an algorithm to receive the same answer for all previous queries but then issue a different query in the next step for two database instances  Thus  in order for the algorithm to have a different sequence of queries for each database instance  the query cost is at least logm 2 2 m2  4   i e     m2   log m   Given the achievability conditions shown in the above two theorems  we now show the existence of a deterministic algorithm that is capable of discovering all attribute domains when the achievability conditions are met  and achieves a near optimal query cost  within a factor of log m where m is the number of attributes   THEOREM 3 3  If k   2  there exists a deterministic algorithm which guarantees complete attribute domain discovery for all binary databases and all scoring functions with a query cost of O m2    The following deterministic algorithm  B COUNT DISCOVER  serves as the proof  One can see from this theorem that for databases with only binary domains  the problem of attribute domain discovery can indeed by solved with a small query cost as long as k   2  3 1 2 B COUNT DISCOVER B COUNT DISCOVER starts by issuing query q1  SELECT   FROM D  Without loss of generality  let t    01          0m  be a tuple returned by q1  Since k   2  there must be another tuple returned by q1 which differs from t on at least one attribute value  Again without loss of generality  let such an attribute be A1   i e   the analyzer learns from the answer to q1 at least the following m  1 values  f01          0m  11g  The objective of B COUNT DISCOVER then becomes to unveil 12          1m  To discover any of these values  say 1m  we essentially need to    nd a tuple with Am   1m  Initially  the search space   i e   the set of possible tuple values in the database featuring Am   1m   is very large  In particular  the initial space S1 1m  is formed by all possible value combinations for A1          Am1  and Am   1m  and thus of size 2 m1   Our main idea of B COUNT DISCOVER is to shrink the search space by leveraging COUNT information provided by the interface  In particular  we construct a series of nested spaces S1 1m   S2 1m          each half the size of the preceding one  while ensuring that every Si 1m  contains at least one tuple in the database with Am   1m  Then  even in the worst case scenario  we can unveil 1m when the search space is shrinked to size 1   by simply issuing an  m1  predicate query with A1          Am1 speci   ed according to the value left in the search space  To understand how the nested spaces are constructed  consider the following three queries  q2  SELECT   FROM D WHERE Am   0m q3  SELECT   FROM D WHERE A1   01 q4  SELECT   FROM D WHERE A1   01 AND Am   0m       Let C qi  be the COUNT returned alongside query qi  We can compute from the query answers C1   COUNT A1   01 AND Am   1m  and C2   COUNT A1   11 AND Am   1m  as C1   C q3   C q4    1  C2   C q1   C q2    C q3   C q4     2  Note that either C1 or C2  or both  must be greater than 0 if 1m occurs in the database  Suppose that C1   0  We can now shrink our search space by half to size 2 m2   by constructing S2 1m    S1 1m  with only values in S1 1m  which satisfy A1   01  One can see that  since C1   0  at least one value in S2 1m  appears in the database and has Am   1m  The shrinking process continues from here  To see how  note that since C1   0  q3 either returns a tuple with Am   1m or over   ows  If q3 over   ows without returning 1m  there must exist another attribute  in A2          Am1  which has both values appearing in the k tuples returned for q3  Without loss of generality  let the attribute be A2  We now issue the following two queries  q5  SELECT   FROM D WHERE A1   01 AND A2   02 q6  SELECT   FROM D WHERE A1   01 AND A2   02 AND Am   0m Similar to the last step  we compute C3   COUNT A1   01 AND A2   02 AND Am   1m  and C4   COUNT A1   01 AND A2   12 AND Am   1m  as C3   C q5   C q6    3  C4   C q3   C q4    C q5   C q6     4  Again  at least one of C3 and C4 must be greater than 0 because C1   0  Suppose that C4   0  We can then further shrink our search space to size 2 m3 by constructing S3 1m    S2 1m  with only values in S2 1m  which satis   es A2   12  Once again  at least one value in S3 1m  appears in the database and has Am   1m because C4   0  Note that before the next shrinking step  we might need to issue q7  SELECT   FROM D WHERE A1   01 AND A2   12 in order to discover the complete domain of another attribute  in A3          Am1   so that we can continue the shrinking process  Note that similar to the discovery of A2   s domain from q3  this discovery is guaranteed by q7 because it either returns a tuple with Am   1m  which accomplishes our task  or over   ows  in which case at least one attribute unspeci   ed in q7 must have different values appear in the k   2 tuples returned by q7  One can see that eventually  we can always discover 1m after issuing at most 3m queries  because the search space would then become Sm 1m  of size 1  Thus  B COUNT DISCOVER requires a query cost of O m2    This concludes the proof of Theorem 3 3  3 2 Discover Arbitrary Domains We now extend our results for binary domains to arbitrary domains  We start by showing that the originally practical achievability conditions for binary domains  i e   lower bounds on k and query cost  now become unrealistic for arbitrary domains  Then  we illustrate the fundamental reason for such a change which motivates our design of randomized algorithms in the next section  3 2 1 Achievability of Attribute Domain Discovery For arbitrary domains  the achievability condition for attribute domain discovery imposes a much large lower bound on k  as indicated by the following theorem  Recall that jVij is the domain size of Ai  THEOREM 3 4  If k   1   Qm i 1  jVij1   then no deterministic algorithm can guarantee the discovery of all attribute domains for all database instance scoring function combinations  PROOF  Arbitrarily pick one attribute value vi from each Vi Q i 2  1  m    respectively  Consider a database D formed by 1   m i 1  jVij  1  tuples  Qm i 1  jVij  1  of them are the Cartesian product of Vinfvig for all i 2  1  m   The one additional tuple is t    v1          vm   Suppose that t has the lowest score  One can see that no algorithm can discover any of v1          vm if t is not returned by SELECT   FROM D  Thus  no algorithm can guarantee complete attribute domain discovery when k   1  Qm i 1  jVij1   One can see from Theorem 3 4 that the binary case result  Theorem 3 1  is indeed a special case when jV1j           jVmj   2  Another observation from Theorem 3 4 is that the lower bound on k has now become hardly reachable because real world database often features a number of attributes with large domains  Running Example  The NSF award search database requires k   1   Qm i 1  jVij  1    1 93   10 19 to guarantee complete attribute domain discovery  What reinforces this impracticability result is the following theorem  Note that since the achievability condition on k is now infeasible  it makes little practical sense for us to assume k   1   Qn i 1  jVij  1   as in the binary case  when deriving the lower bound on query cost  As such  we consider the following question  for a given value of k  is it possible for a deterministic algorithm to use a small number of queries to discover all domain values that can be discovered from such a top k interface  given unlimited query cost   The following lower bound on k shows that the answer to this question is also no for hidden databases with arbitrary domains  THEOREM 3 5  For given k and m  there exists a database and a scoring function such that no deterministic algorithm can complete the discovery of all domain values that can be discovered from the top k COUNT interface without incurring a worst case query cost of     m2   jVmaxj   log jVmaxj k   log m   jVminj     5  where jVmaxj and jVminj are the maximum and minimum values in jV1j          jVmj  respectively  The proof of this theorem follows in analogy to Theorem 3 2  We do not include it due to the space limitation  One can see that the binary case lower bound on query cost  Theorem 3 2  is indeed a special case of Theorem 3 5 when jV j   2 and k   2  Another observation from Theorem 3 5 is that this arbitrary domain lower bound on query cost  just like the lower bound on k  is hardly achievable in practice  especially when the hidden database contains one or a few attributes  e g   ZIP code  with large domains  Running Example  With the setting of our NSF award search example  m2   jVmaxj   log jVmaxj  k   log m   jVminj     1 06   10 4   While the bound may differ by a constant factor  10 4 is at least an order of magnitude larger than what one would desire for attribute domain discovery  e g   in our experimental results   Although the achievability conditions for both k and query cost are unrealistic for arbitrary domains  in the following we still extend B COUNT DISCOVER to COUNT DISCOVER  a deterministic algorithm that is capable of discovering all attribute domains  Of course  the extended algorithm has to follow the achievability conditions and therefore cannot provide meaningful worst case performance  Our main purpose of studying it is to identify which part of the extension causes the signi   cant increase on query cost  Theorem 3 6 summarizes the query cost of COUNT DISCOVER                   QTHEOREM 3 6  For an m attribute database with k   1   n i 1  jVij  1   there exists an algorithm which requires at most Xm i 1    2jVdi j  1    mXi 1 j 1   jVdj j    jVdj j  1  2     6  queries to discover all attribute domains  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  Running Example  With NSF award search example  the worst case query cost of COUNT DISCOVER is 1 07   10 11   orders of magnitude larger than what one can afford  Similar to the binary case  we explain the proof of this theorem in the description of COUNT DISCOVER  Note yet again that the binary case result  Theorem 3 3  is a special case of Theorem 3 6 when jVij   2 for all i 2  1  m   Nonetheless  unlike in the binary case where B COUNT DISCOVER has a query cost within a factor of O log m  from optimal  the query cost of COUNT DISCOVER is further away from the lower bound in Theorem 3 5  In particular  observe from Theorem 3 6 the worst case query cost of COUNTDISCOVER is   jVd1 j 2   jVdmj 3    a factor of O jVd1 j   jVdmj 2   from optimal  We did not pursue to close this gap for two reasons  First  since the lower bound itself is infeasible in practice  even an optimal algorithm would not be practical anyway  Second  our purpose of introducing COUNT DISCOVER is not to promote its practical usage  but to use the comparison between B COUNTDISCOVER and COUNT DISCOVER to illustrate the main obstacle facing the discovery of arbitrary attribute domains  which motivates our design of ef   cient  and thus practical  randomized algorithms  3 2 2 COUNT DISCOVER We now extend B COUNT DISCOVER to arbitrary domains  given the condition that k   1   Qn i 1  jVij  1   Note that this lower bound on k guarantees that SELECT   FROM D must reveal the complete domain for at least one attribute  assumed to be A1 without loss of generality  In addition  it must reveal at least one domain value for the remaining m  1 attributes  assumed to be 02          0m  As such  the objective of COUNT DISCOVER is to unveil 1i          jVij i for all i 2  2  m   Like in the binary case  we consider the discovery of an arbitrary attribute domain  say Vm  by constructing a series of nested spaces that are guaranteed to contain at least one tuple in D which has its value of Am yet to be discovered  Consider the worst case scenario where the only value of Vm revealed by SELECT   FROM D is 0m  The starting search space S1 Vm  then has a size of  jVmj  1    Qm1 i 1 jVij  Since we already know the complete domain of A1  the next step is to    nd one value in V1 which can be used to construct the next nested space   which must guarantee the appearance of at least one value in 1m          jVij m  In the binary case  we only need to determine whether 01 can be used because if it cannot  then 11 can always be used because A1 has only these two possible values  In particular  the judgement for 01 can be done with just two queries   WHERE A1   01 and WHERE A1   01 and Am   0m   because we can then infer the COUNT of tuples that satisfy A1   01 and Am   1m  Note that for arbitrary domains  we can still determine whether 01 can be used with the two exact same queries  because they now reveal the COUNT of tuples that satisfy A1   01 and have their values of Am yet to be discovered  Nonetheless  since the domain of A1 is much larger  than binary   we must continue testing other values of A1 if 01 cannot be used  In the worst case scenario  we may not be able to    nd the next search space until testing jV1j  1 possible values of A1  Thus  at this step  the query cost for arbitrary domain becomes jV1j  1 times as large as that for binary domain  Let w1 2 V1 be the value we    nd and use to construct the next nested search space  We can now construct the next nested space S2 Vm  by only including value combinations in S1 Vm  which satisfy A1   w1  One can see that the size of search space now becomes  jVmj  1    Qm1 i 2 jVij  COUNT DISCOVER continues the shrinking process in the same way as the binary case   i e   by issuing query q  SELECT   FROM D WHERE A1   w1  if it has not already been issued   One can see that either q returns a yet tobe discovered value in Vm  or it over   ows  in which case we can always    nd the complete domain of an attribute in A2          Am1 from the k tuples returned by q because k   1   Qn i 1  jVij  1   As such  eventually we can always unveil one value in Vm that has not yet been discovered  Nonetheless  since we do not maintain in the search space all yet to be discovered value in Vm  the entire shrinking process may have to be repeated multiple times to unveil the entire Vm  Theorem 3 6 can be derived accordingly  A key observation from the design of COUNT DISCOVER is that the signi   cant increase on query cost  from the binary case  is not because the shrinking of search spaces becomes less powerful   indeed  ALERT DISCOVER still needs only m nested spaces S1 Vm           Sm Vm  to reach size 1  The increase on query cost is largely due to the cost of determining how to construct the next nested space  i e  by    ltering the current one    a task previously accomplished by just two queries now requires many more  In the next section  we shall show that randomizing such nested space construction process is our key idea for building ef   cient randomized algorithms for discovering arbitrary attribute domains  4  RANDOMIZED ALGORITHMS FOR COUNT INTERFACES In the last section  we developed a query ef   cient algorithm for discovering binary domains  but found its extension to arbitrary domains to be extremely inef   cient  We also found the main reason behind this obstacle to be the increased cost of determining how to construct the nested search spaces   in particular  for a given search space and an attribute Ai  how to select a domain value v of Ai such that Ai   v can be used to    lter the current search space while maintaining at least one tuple with yet to be discovered domain value s  in the    ltered space  Now that any deterministic construction of nested spaces is provably expensive due to the achievability conditions we derived in the last section  our main idea in this section is to randomize nestedspace construction  so as to spend a small number of queries for search space construction while maintaining yet to be discovered domain value s  in the constructed space with high probability  In the following  we    rst describe our randomized process of nestedspace construction and the performance  i e   recall and coverage  guarantee it is able to achieve  and then combine this process with COUNT DISCOVER to form RANDOM COUNT DISCOVER  our randomized  Monte Carlo  algorithm for attribute domain discovery which can unveil attribute domains  though not necessarily completely  no matter if the achievability conditions are met  4 1 Randomize Nested Space Construction Recall that the main obstacle for nested space construction is to    nd a predicate Ai   v that can be used to    lter the current search space  In the worst case scenario  one may have to    nd m1 such conditions to discover a domain value for an attribute say Aj   this occurs when the complete domains of all attributes but Aj have been discovered  For the purpose of discussing the randomization of nested space construction  we consider a worst case scenario               where the complete domains of A1          Am1 have been discovered  and our objective now is to discover the domain of Am  As a side note   this problem is actually interesting in its own right because it captures the scenario where Am is a textbox attribute  with unknown domain  on the hidden database interface  while a large number of other attributes  i e   A1          Am1  have domains provided by the interface  e g   through options in drop down menus   We shall shown in the experiments section that our algorithms can effectively discover the domain of Am in this scenario  RANDOM SPACE  A simple idea to randomize the construction of nested spaces is to choose v uniformly at random from the domain of Ai  With this idea  every round  i e   search space shrinking process  of our RANDOM SPACE algorithm starts with SELECT   FROM D as the    rst query  Then  if no new domain value of Am is discovered from the query answer  it randomly chooses v1 from V1 and then uses A1   v1 to    lter the search space  i e   by issuing query SELECT   FROM D WHERE A1   v1   More generally  if RANDOM SPACE fails to discover a new domain value of Am from the    rst b1 queries issued in the current round  then it constructs the b query by adding a conjunctive condition Ab   vb  where vb is chosen uniformly at random from Vb  to the selection condition of the  b  1  th query  Each round of RANDOM SPACE requires at most m queries  and may lead to two possible outcomes  One is a new domain value being discovered  The other is that RANDOM SPACE reaches a valid or under   owing query without discovering any new domain value  Note that while this outcome never occurs for COUNTDISCOVER  it might occur for RANDOM SPACE because of the random construction of nested spaces  RANDOM SPACE may be executed for multiple rounds to discover more domain values  One can see that the randomization used by RANDOM SPACE guarantees a positive probability for each domain value to be discovered  Nonetheless  the main problem of it occurs when the distribution of tuples in the database is severally skewed   e g   when almost all tuples have the same values for A1  A2  etc  In this case  a large number of queries may be wasted on a small number of tuples  rendering it unlikely for RANDOM SPACE to unveil many domain values  RANDOM COUNT SPACE  The main problem of RANDOMSPACE can be solved by leveraging the COUNT information provided by the interface  In particular  RANDOM COUNT SPACE constructs the nested spaces in a similar fashion to RANDOMSPACE  with the only exception that the value v in selection condition Ai   v is no longer drawn uniformly at random from Vi  Instead  RANDOM COUNT SPACE    rst retrieves the COUNT of all jVij possible queries that de   ne the next search space  each corresponding to a different value of v   Then  it selects v in proportional to its corresponding COUNT  The construction of nested search spaces continues until either    nding a new domain value or reaching a valid query  Note that since we now leverage COUNT information  an under   owing query will never be selected to construct the next search space  An important property of RANDOM COUNT SPACE is that if we continue the search space shrinking process until reaching a valid query  then the valid query returns each tuple in the database with roughly equal probability  precisely equal if k   1  varies at most k times otherwise   This property explains why RANDOMCOUNT SPACE solves the above mentioned problem of RANDOMSPACE  Nonetheless  RANDOM COUNT SPACE also has its own problem  In particular  if Am is strongly correlated with A1  A2  etc  e g   A1  A2   Am forms a functional dependency   and all but a few tuples have the same value of Am  then RANDOMCOUNT SPACE may spend too many queries retrieving tuples featuring the    popular    value of Am  because the spaces it constructs most likely follow the    popular    value combination of A1 and A2   but fails to retrieve the other values  Interestingly  one can see that RANDOM SPACE can handle this case pretty well  motivating our idea of mixing the two randomization approaches  HYBRID SPACE  We now consider a hybrid of RANDOM  and RANDOM COUNT SPACE to avoid the problems of both  In particular  HYBRID SPACE starts with RANDOM SPACE  It is repeated executed until none of the last r1   1 times returns any new domain value  where r1 is a small number  e g   0 to 5   the setting of which shall be discussed in the next subsection and in the experiments  At this time  we switch to RANDOM COUNT SPACE  One round of RANDOM COUNT SPACE is performed  If it unveils a new domain value  then the entire process of HYBRIDSPACE is restarted by going back to the beginning of RANDOMSPACE  If no new value is discovered  we repeat RANDOM COUNTSPACE for up to r2 other rounds or until a new domain value is discovered  whichever happens    rst  Similar to r1  we shall discuss the setting of r2 in the next subsection and in the experiments  If a new domain value is discovered  then the entire HYBRID SPACE is restarted  Otherwise  If r2 1 consecutive rounds of RANDOMCOUNT SPACE fails to return any new domain value  we terminate HYBRID SPACE  Algorithm 1 depicts the pseudocode for HYBRID SPACE  Note that the input parameter in the pseudocode is designed for integration into RANDOM COUNT DISCOVER  Algorithm 1 HYBRID SPACE 1  Input parameter  q0  set to SELECT   FROM D by default 2  repeat 3    Perform one round of RANDOM SPACE 4  q   q0 5  while q over   ows AND returns no new domain value do 6  Select Ai not speci   ed in q  Select v uniformly at random from the discovered domain of Ai 7  q    q AND Ai   v   Issue q to learn new values  8  end while 9  until the last r1   1 rounds return no new domain value 10  repeat 11    Perform one round of RANDOM COUNT SPACE 12  q   q0 13  while q over   ows AND returns no new domain value do 14  Select Ai not speci   ed in q  For each discovered v 2 Vi  Query c v    COUNT of  q AND Ai   v  15  Select v with probability proportional to c v  16  q    q AND Ai   v   Issue q to learn new values  17  end while 18  if q returns a new domain value then Goto 2  19  until the last r2   1 rounds return no new domain value 4 2 HYBRID SPACE Performance Guarantee An important feature of HYBRID SPACE is the performance guarantee it provides independent of the underlying data distribution  To illustrate such a guarantee  we consider an example where A1          Am1 each has a domain of size 10  Let there be k   50  n   1 000 000 tuples and m   10 attributes in the database  note that n is readily available from the interface   Consider a parameter setting of r1   r2   0  The following derivation shows how HYBRID SPACE guarantees either a coverage of 46 values or a median recall of 500 000 tuples with only 4 036 queries  Case 1  First consider the case where HYBRID SPACE did not terminate before 4  036 queries were issued  Let wB and wT be the number of rounds RANDOM  and RANDOM COUNT SPACE     were performed  respectively  Since r1   0  the number of discovered domain values is at least wB  1  Since A1          Am1 each has 10 domain values  an RANDOM COUNT SPACE shrinking process issues at most 9 m  1  queries  Thus  the wB   wT    nished rounds consume at least 4036 9 m1 1    40469m queries because the    nal un   nished round could have issued at most 9 m  1   1 queries  As such  c   wB   9 m  1    wT   4046  9m   7  where c is the average number of queries issued by a round of RANDOM SPACE during the execution  Since r2   0  wB   wT  Thus   c   9 m  1     wB   4046  9m  If HYBRID SPACE did not discover at least 46 values  then wB   46 and therefore c    3956 46   9 m  1    5  A key step now is to prove that c   5 indicates an absolute recall of at least 500  000  This is enabled by the following theorem which illustrates a positive correlation between the absolute recall   and the average query cost c  per round  of RANDOM SPACE  THEOREM 4 1  There is     k   10 c1 with probability close to 1 when the number of rounds is suf   ciently large  PROOF  Let there be a total of s rounds of RANDOM SPACE performed  Consider the i th round  i 2  1  s    Let  i be the absolute recall of discovered domain values before the round begins  Given the discovered values  let  i be the set of queries which may be the second to last query issued by this i th round  One can see that each query in  i must over   ow and have all returned tuples featuring the already discovered domain values  Thus  we have  i   k   j ij  where j ij is the number of queries in  i  For each q 2  i  let p q  and h q  be the probability for q to be chosen in this round of RANDOM SPACE and the number of predicates in v  respectively  Since each attribute in A1          Am1 has 10 possible values  we have p q    1 10 h q    Thus  Eq2 i  k   10 h q      X q2 i k   10 h q  10 h q    k   j ij    i  8  where the expected value is taken over the randomness of nested space construction in RANDOM SPACE  Note that k   2 x is a convex function of x  According to Jensen   s inequality  Eq2 i  k   10 h q      k   10 Eq2 i  h q     Also note that for any i   i      Thus  k   10 Eq2 i  h q      i      When s  the total number of rounds of RANDOM SPACE  is suf   ciently large  according to the central limit theorem  the probability of Eq2 i  h q     c1 for every i 2  1  s  tends to 0  Thus  the probability of     k   10 c1 tends to 1 when the number of drill downs is suf   ciently large  According to the theorem  c   5 indicates an absolute recall of at least k   10 51   500000  Case 2  Now consider the other case where HYBRID SPACE terminates before 4  036 queries were issued  This indicates that the last round of RANDOM COUNT SPACE returns no new domain value  Let the absolute recall be   n  With each round of RANDOMCOUNT SPACE  the probability for the  1       n    unrecalled    tuples to be retrieved is 1   Thus  with Bayes    theorem  the posterior probability for     50  given the empty discovery of the last round of RANDOM COUNT SPACE is Prf    50 jempty discoveryg   1 2   1 2 R 1 0 p dp   1 2   9  That is  the median absolute recall is at least 500000  In summary of both cases  HYBRID SPACE either discovers at least 46 values  or has a median absolute recall of 500000  Generic Performance Guarantee  More generally  we have the following algorithm  THEOREM 4 2  For a query budget of d over a top k interface  HYBRID SPACE achieves either an absolute coverage of m0 values  or a median absolute recall of at least min   k   Y  i 1 jVij  r2 2 s 1 2 r2   2    n    10  where     d  Pm1 i 1 jVij   1 m0    r1   1   r2   1 r1   1   mX1 i 1 jVij   11  We do not include the proof due to the space limitation  One can see from the theorem that a suggest setting for r1 and r2 is r1   r2   0  as it maximizes the lower bound derived in  10   We shall verify this suggested setting in the experiments  Running Example  To discover the domain of Instrument from the NSF award search database based on pre known domains of award amount  PI state  NSF organization  and    eld  HYBRID SPACE requires at most 1563 queries to guarantee either the complete discovery of all domain values  or an absolute recall of min 4 12   10 7   n 2   4 3 RANDOM COUNT DISCOVER We now integrate HYBRID SPACE into COUNT DISCOVER to produce RANDOM COUNT DISCOVER  our randomized  Monte Carlo  algorithm for discovering attribute domains  RANDOMCOUNT DISCOVER starts with SELECT   FROM D  and uses the domain values discovered from the returned results to bootstrap HYBRID SPACE  After that  RANDOM COUNT DISCOVER executes HYBRID SPACE for multiple rounds  using the domain values discovered from previous rounds to bootstrap the next round  In particular  at any time  RANDOM COUNT DISCOVER maintains a set of domain values that have been discovered but not used in the current round of HYBRID SPACE  Let it be V     m i 1Vi  At the start  V is empty  After every round of HYBRID SPACE  the new domain values discovered are added to V   Then  in the next round of HYBRID SPACE  we randomly select an attribute with domain values in V   say Aj and fvj1          vjhg   V   Vj   and start the search space construction process with a selection condition of Aj   v where v 2 fvj1          vjhg  We then remove v from V   RANDOM COUNT DISCOVER terminates when V is empty or the query budget is exhausted  Algorithm 2 RANDOM COUNT DISCOVER 1  q0   SELECT   FROM D  V      2  repeat 3  Execute HYBRID SPACE q0  4  V   domain values discovered by HYBRID SPACE 5  Arbitrarily select j and v such that v 2 V   Vj 6  q0   SELECT   FROM D WHERE Aj   v 7  V   V nv 8  until V     or the query budget is exhausted 5  ATTRIBUTE DOMAIN DISCOVERY FOR ALERT INTERFACES We now extend our results to ALERT interfaces which does not offer COUNT  Since the information provided by an ALERT interface is a proper subset of that provided by the corresponding                          COUNT interface  the results in   3 2 already shows the impracticability of deterministic algorithms  Our results in this section further shows that ALERT interfaces require an even higher lower bound on query cost  To ef   ciently discover attribute domains  we develop RANDOM ALERT DISCOVER  a randomized algorithm which extends RANDOM COUNT DISCOVER by augmenting a query answer returned by an ALERT interface with an approximate COUNT estimated from historic query answers  5 1 Achievability for Deterministic Algorithms For ALERT interfaces  the achievability condition for k is exactly the same as the one for COUNT interfaces   which is shown in Theorem 3 4   as one can easily verify that the proof of Theorem 3 4 does not use any COUNT returned by the interface  For the condition on query cost  however  the lower bound is signi   cantly higher for ALERT interfaces  as shown by the following theorem  THEOREM 5 1  For given k and m  there exists a database and a scoring function such that no deterministic algorithm can complete the discovery of all domain values that can be discovered from the top k ALERT interface without incurring a worst case query cost of Qm i h 1 jVdi j  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  and h is the minimum value which satis   es k   1   Qh i 1  jVdi j  1   PROOF  We prove by induction  In particular  we start with constructing a database instance D1  and then show that for any b   Qm i h 1 jVdi j  if a deterministic algorithm uses exactly b queries q1          qb to discover all attribute domains from D1  then there must exist another database instance D2 which  1  provides the exact same query answers for q1          qb1  such that the deterministic algorithm will always issue qb next  and  2  ensures that q1          qb cannot discover all attribute domains  One can see that if this is proved  then any deterministic algorithm which discovers all attribute domains must has a query cost of at least Qm i h 1 jVdi j  We construct D1 with 1  k   Qm i h 1 jVdi j tuples t1          t 2m1 in the following manner  First  for each Adi  i 2  1  h    we arbitrarily choose a domain value vi 2 Adi   Then  For each value combination of Vdh 1           Vdm  we include k other tuples with Adh 1           Adm de   ned by the value combination and Adi  6 vdi for all i 2  1  h   It is always possible to    nd such k tuples because of our assumption that k   1   Qh i 1  jVdi j  1   The one additional tuple in D1 is t which satis   es Adi   vdi for all i 2  1  h  and has an arbitrary value combination for Adh 1           Adm  The scoring function is designed such that t has the lowest score  Note that the deterministic algorithm must unveil t with the    rst b queries  Without loss of generality  we assume that the b th query returns t  An important observation here is that t can only be unveiled by a query which has no predicate for Ad1           Adh  because v1          vh could not have been discovered before t is unveiled  and one predicate for each of Adh 1           Adm  because of k   If b   Qm i h 1 jVdi j  there must exist at least one value combination of Adh 1           Adm which has not yet been  fully  speci   ed by any issued query  We then construct D2 by changing the values of Adh 1           Adm of t to this not yet speci   ed combination  One can see that the answers of q1          qb1 over D2 is exactly the same as D1  Nonetheless  qb no longer returns t  mandating a query cost of at least b   1 for attribute domain discovery over D2  Running Example  With the NSF award search example  the lower bound on worst case query cost over a top 50 ALERT interface is 9 40 10 15   orders of magnitude larger than even the upper bound for COUNT interface  THEOREM 5 2  For given k and m attribute  there exists an algorithm which requires at most h  1   Qh1 i 1 jVdi j queries to discover the domains of at least h  out of the m  attributes  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  The following description of ALERT DISCOVER serves as the proof for this theorem  5 1 1 ALERT DISCOVER The deterministic COUNT DISCOVER is capable of shrinking the search space signi   cantly at each step because it can infer the COUNT of a domain value yet to be discovered from COUNTs returned by other queries  ALERT interfaces do not provide such luxury  In particular  for the discovery of Vm  there is no direct way to check whether an unknown domain value in Vm occurs in tuples satisfying an over   owing query such as q3  SELECT   FROM D WHERE A1   01  unless q3 returns the domain value in its top k result  Despite of the lack of COUNT information  we can still shrink the search space with an ALERT interface  though the shrinking may not be as signi   cant as in COUNT DISCOVER  For example  if the above mentioned q3 turns out to be valid or under   owing without returning any unknown domain value in Vm  we can safely remove from the search space S1 Vm  all values which satisfy q3   a reduction of size Qm1 i 2 jVij   while still ensuring that the reduced search space contains at least one tuple in the database which has an unknown value of Vm  ALERT DISCOVER  our deterministic algorithm for ALERT interfaces  exactly follows this strategy to construct a series of nested spaces S1 Vm   S2 Vm          for the discovery of Vm  To decide which query to issue next  ALERT DISCOVER uses the following simple rule  Suppose that the complete domains of h attributes have been discovered  say A1          Ah without loss of generality  note that h   1 at the start of the algorithm   The next query ALERTDISCOVER issues is an h predicate query with one conjunctive condition for each attribute Ai  i 2  1  h    The value vi speci   ed for Ai in the query is determined as follows  First  the value combination hv1          vhi must lead to a query that cannot be answered based solely upon the historic query answers   i e   no previously issued query is formed by a subset of A1   v1          Ah   vh and returns valid or under   ow  Within the value combinations which satisfy this condition  we select the highest ordered one according to an arbitrary global order  The above process is repeated until all domains are discovered  To understand how such a query issuing plan leads to the shrinking of nested search spaces  consider the three possible outcomes of an above constructed query q   under   ow  valid  or over   ow  If q under   ows or is valid  then we can construct the next search space by removing from the current one all value combinations that satisfy q   a reduction of size Qm1 i h 1 jVij where h is the number of predicates in q  On the other hand  if q over   ows  then we can always discover the complete domain of at least one additional attribute because k   1   Qn i 1  jVij  1   One can see that if ALERT DISCOVER has issued at least m queries with the second outcome  then it must have discovered the domains for all m attributes  The    rst outcome  on the other hand  guarantees the reduction of search space  Theorem 5 2 follows accordingly  5 2 RANDOM ALERT DISCOVER A simple approach to enable aggregate estimation is to directly integrate RANDOM COUNT DISCOVER with HD UNBIASEDAGG  the existing aggregate estimator for hidden databases  8   Intuitively  since RANDOM COUNT DISCOVER starts with RANDOM           SPACE  one can use queries answers received by RANDOM SPACE to estimate each COUNT value required by the subsequent execution of RANDOM COUNT SPACE  The estimations will become more accurate over time after more queries are issued by RANDOM SPACE or RANDOM COUNT SPACE   Nonetheless  such a direct integration has a key problem  The existing HD UNBIASED AGG can only generate a COUNT estimation from a valid query answer  Most queries issued by RANDOMCOUNT SPACE  however  are likely to over   ow as the shrinking of search space only continues  i e   with    narrower     and possibly valid  queries being issued  if no new domain value can be discovered from the previously issued over   owing queries   an unlikely event especially at the beginning of RANDOM COUNTDISCOVER  As a result  the integration with HD UNBIASEDAGG faces a dilemma  either uses only a few valid queries and suffers a high estimation variance  or issues a large number of extra queries to    narrow down    the queries issued by RANDOMCOUNT SPACE  by adding conjunctive conditions to the queries  to valid ones  Either way  the large number of over   owing queries issued by RANDOM COUNT SPACE would be wasted  To address this problem  we propose a step by step estimation process which can generate COUNT estimations based the the results of all historic queries  including the over   owing ones  For the ease of understanding  we consider an example of estimating SELECT COUNT    FROM D WHERE A1   v1  denoted by COUNT v1   Handling of other COUNT queries immediately follows  Consider one round of RANDOM SPACE or RANDOMCOUNT SPACE during which b queries q1         qb are issued in order  Recall that q1 is SELECT   FROM D  qb may be over   owing  valid or under   owing  Without loss of generality  let A1          Ai1 be the attributes involved in the selection conditions of qi  i 2  2  b    Let pi be the transition probability from qi to qi 1   i e   the probability for qi 1 to be selected  out of all possible values of Ai  after qi is issued  For example  pi   1 jVij for RANDOM SPACE  and may differ  based on the currently estimated COUNTs  for RANDOM COUNT SPACE  We estimate COUNT v1  from all b queries with the following two steps    We    rst choose i i d  uniformly at random vi 2 Vi for each i 2  b  m  1   and then construct and issue a  conjunctive  query qF formed by appending conditions  Ab   vb  AND       AND  Am1   vm1  to qc    We then return the following estimation of COUNT v1         Xb i 1 Sv1   i  Qi1 j 1 pj     Sv1   F    Qm j b jVj j Qb1 j 1 pj  12  where  i   ftjt 2 qi  t 62  q1           qi1 g   F   ftjt 2 qF  t 62  q1           qc g  and Sv1     stands for the result of applying COUNT v1  over a set of tuples  Note that here qi Qstands for the set of tuples returned by query qi  We assume i1 j 1 pj   1 when i   1  and Sv1       0  One can see that each query in q1          qb contributes an additive component to the    nal estimation     e g   qi contributes Sv1   i   Qi1 j 1 pj   which is essentially an estimated COUNT of tuples that sastify A1   v1  can be returned by a query with predicates on A1          Ai  but cannot be returned by a query with predicates only on A1          Ai1  This is why we call the idea    stepby step estimation     In terms of query cost  step by step estimation reuses all queries issued by RANDOM SPACE or RANDOMCOUNT SPACE  In addition  it issues at most one additional query qF per round  of search space shrinking   It is easy to verify that the proof of unbiasedness for HD UNBIASED AGG  8  remains valid with step by step estimation  6  EXPERIMENTAL RESULTS 6 1 Experimental Setup 1  Hardware and Platform  All our experiments were performed on a 2 6GHz Intel Core 2 Duo machine with 2GB RAM and Windows XP OS  All algorithms were implemented in C    2  Dataset  We conducted our experiments on two real world datasets  One is the NSF award search database which has been used as a running example throughout the paper  We tested our algorithms over the real world web interface of NSF award search by issuing queries through the    searc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis"/>
        <doc>Data Serving in the Cloud ### Raghu Ramakrishnan Chief Scientist  Audience and Cloud Computing Brian Cooper Adam Silberstein Utkarsh Srivastava Yahoo  Research Joint work with the Sherpa team in Cloud Computing2 Outline     Clouds     Scalable serving   the new landscape     Very Large Scale Distributed systems  VLSD      Yahoo    s PNUTS Sherpa     Comparison of several systems     Preview of upcoming Y  Cloud Serving  YCS   benchmark3 Types of Cloud Services     Two kinds of cloud services      Horizontal     Platform     Cloud Services     Functionality enabling tenants to build applications or new  services on top of the cloud     Functional Cloud Services      Functionality that is useful in and of itself to tenants  E g    various SaaS instances  such as Saleforce com  Google  Analytics and Yahoo    s IndexTools  Yahoo  properties aimed  at end users and small businesses  e g   flickr  Groups  Mail   News  Shopping      Could be built on top of horizontal cloud services or from  scratch     Yahoo  has been offering these for a long while  e g   Mail for  SMB  Groups  Flickr  BOSS  Ad exchanges  YQL 5 Yahoo  Horizontal Cloud Stack Provisioning   Self serve  YCS Horizontal YCPI  Cloud ServicesBrooklyn     EDGE Monitoring Metering Security Hadoop Horizontal   Cloud Services BATCH STORAGE PNUTS SherpaHorizontal MOBStor Cloud Services   ###  OPERATIONAL STORAGE VM OS Horizontal Cloud Services     APP VM OS Horizontal Cloud Services yApache WEB Data Highway Serving Grid PHP App Engine6 Cloud Power   Yahoo  Ads  Optimization Content  Optimization Search  Index Image Video  Storage   Delivery Machine  Learning   e g  Spam  filters  Attachment Storage7 Yahoo    s Cloud   Massive Scale  Geo Footprint     Massive user base and engagement     500M  unique users per month     Hundreds of petabyte of storage      Hundreds of billions of objects     Hundred of thousands of requests sec     Global     Tens of globally distributed data centers     Serving each region at low latencies     Challenging Users     Downtime is not an option  outages cost  millions      Very variable usage patterns8 New in 2010      SIGMOD and SIGOPS are starting a new annual  conference  co located with SIGMOD in 2010   ACM Symposium on Cloud Computing  SoCC   PC Chairs  Surajit Chaudhuri   Mendel Rosenblum GC  Joe Hellerstein Treasurer  Brian Cooper      Steering committee  Phil Bernstein  Ken Birman   Joe Hellerstein  John Ousterhout  Raghu  Ramakrishnan  Doug Terry  John Wilkes9 VERY LARGE SCALE   DISTRIBUTED  VLSD   DATA SERVING ACID or BASE  Litmus tests are colorful  but the picture is cloudy10 Databases and Key Value Stores http   browsertoolkit com fault tolerance png11 Web Data Management Large data analysis  Hadoop  Structured record  storage  PNUTS Sherpa  Blob storage  MObStor     Warehousing    Scan  oriented  workloads    Focus on  sequential  disk I O      per cpu  cycle    CRUD     Point lookups  and short  scans    Index  organized  table and  random I Os      per latency    Object  retrieval and  streaming    Scalable file  storage      per GB  storage    bandwidth12 The World Has Changed     Web serving applications need      Scalability      Preferably elastic     Flexible schemas     Geographic distribution     High availability     Reliable storage     Web serving applications can do without      Complicated queries     Strong transactions     But some form of consistency is still desirable13 Typical Applications     User logins and profiles     Including changes that must not be lost      But single record    transactions    suffice     Events     Alerts  e g   news  price changes      Social network activity  e g   user goes offline      Ad clicks  article clicks     Application specific data     Postings in message board     Uploaded photos  tags     Shopping carts14 Data Serving in the Y  Cloud Simple Web Service API   s Database PNUTS   SHERPA Search Vespa Messaging Tribble Storage MObStor Foreign key photo     listing FredsList com application ALTER Listings MAKE CACHEABLE Compute Grid Batch export Caching memcached 1234323   transportation   For sale  one  bicycle  barely  used 5523442   childcare   Nanny  available in  San Jose DECLARE DATASET Listings AS   ID String PRIMARY KEY  Category String  Description Text   32138   camera   Nikon  D40  USD 30015 VLSD Data Serving Stores     Must partition data across machines     How are partitions determined      Can partitions be changed easily   Affects elasticity      How are read update requests routed      Range selections  Can requests span machines      Availability  What failures are handled      With what semantic guarantees on data access       How  Is data replicated      Sync or async  Consistency model  Local or geo      How are updates made durable      How is data stored on a single machine 16 The CAP Theorem     You have to give up one of the following in  a distributed system  Brewer  PODC 2000   Gilbert Lynch  SIGACT News 2002       Consistency of data      Think serializability     Availability     Pinging a live node should produce results     Partition tolerance     Live nodes should not be blocked by partitions17 Approaches to CAP        BASE        No ACID  use a single version of DB  reconcile later     Defer transaction commit      Until partitions fixed and distr xact can run     Eventual consistency  e g   Amazon Dynamo      Eventually  all copies of an object converge     Restrict transactions  e g   Sharded MySQL      1 M c Xacts  Objects in xact are on the same machine      1 Object Xacts   Xact can only read write 1 object     Object timelines  PNUTS  http   www julianbrowne com article viewer brewers cap theorem18 18    I want a big  virtual database       What I want is a robust  high performance virtual  relational database that runs transparently over a  cluster  nodes dropping in and out of service at will   read write replication and data migration all done  automatically  I want to be able to install a database on a server  cloud and use it like it was all running on one  machine        Greg Linden   s blog19 PNUTS   SHERPA To Help You Scale Your Mountains of Data Y  CCDI20 Yahoo  Serving Storage Problem     Small records     100KB or less     Structured records     Lots of fields  evolving     Extreme data scale   Tens of TB     Extreme request scale   Tens of thousands of requests sec     Low latency globally   20  datacenters worldwide     High Availability   Outages cost  millions     Variable usage patterns   Applications and users change 2021 E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E What is PNUTS Sherpa  E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E CREATE TABLE Parts   ID VARCHAR  StockNumber INT  Status VARCHAR       Parallel database Geographic replication Structured  flexible schema Hosted  managed infrastructure A     42342               E B     42521               W C     66354               W D     12352               E E     75656               C F     15677               E 2122 What Will It Become   E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E Indexes and views23 Scalability     Thousands of machines     Easy to add capacity     Restrict query language to avoid costly queries Geographic replication     Asynchronous replication around the globe     Low latency local access High availability and fault tolerance     Automatically recover from failures     Serve reads and writes despite failures Design Goals 23 Consistency     Per record guarantees     Timeline model      Option to relax if needed Multiple access paths     Hash table  ordered table     Primary  secondary access Hosted service     Applications plug and play     Share operational cost24 Technology Elements PNUTS      Query planning and execution     Index maintenance Distributed infrastructure for tabular data     Data partitioning      Update consistency     Replication YDOT FS     Ordered tables Applications Tribble     Pub sub messaging YDHT FS      Hash tables Zookeeper     Consistency service YCA  Authorization PNUTS API Tabular API 2425 PNUTS  Key Components     Maintains map from  database table key totablet to SU     Provides load balancing     Caches the maps from the TC     Routes client requests to  correct SU     Stores records     Services get set delete  requests 2526 Storage units Routers Tablet Controller REST API Clients Local region Remote regions Tribble Detailed Architecture 2627 DATA MODEL 2728 Data Manipulation     Per record operations     Get     Set     Delete     Multi record operations     Multiget     Scan     Getrange     Web service  RESTful  API 2829 Tablets   Hash  Table Apple Lemon Grape Orange Lime Strawberry Kiwi Avocado Tomato Banana Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  How much did you pay for this lemon  Is this a vegetable  New Zealand The perfect fruit Name Description Price  12  9  1  900  2  3  1  14  2  8 0x0000 0xFFFF 0x911F 0x2AF3 2930 Tablets   Ordered Table 30 Apple Banana Grape Orange Lime Strawberry Kiwi Avocado Tomato Lemon Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  The perfect fruit Is this a vegetable  How much did you pay for this lemon  New Zealand  1  3  2  12  8  1  9  2  900  14 Name Description Price A Z Q H31 Flexible Schema Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Color Red Condition Good Fair32 32 Primary vs  Secondary Access Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Price Posted date Listing id 15 6 5 07 421133 86 6 1 07 763245 570 6 1 07 424252 1123 6 3 07 211242 Primary table Secondary index Planned functionality33 Index Maintenance     How to have lots of interesting indexes  and views  without killing performance      Solution  Asynchrony      Indexes views updated asynchronously when  base table updated34 PROCESSING READS   UPDATES 3435 Updates 1 Write key k 2 Write key k 7 Sequence   for key k 8 Sequence   for key k SU SU SU 3 Write key k 4 5 SUCCESS 6 Write key k Routers Message brokers 3536 Accessing Data 36 SU SU SU 1 Get key k 2 3 Record for key k Get key k 4 Record for key k37 Bulk Read 37 SU Scatter  gather  server SU SU 1  k1  k2      kn  Get k 2 1 Get k2 Get k338 Storage unit 1 Storage unit 2 Storage unit 3 Range Queries in YDOT     Clustered  ordered retrieval of records Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 1 Router Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Grapefruit   Pear  Grapefruit   Lime  Lime   Pear  Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 139 Bulk Load in YDOT     YDOT bulk inserts can cause performance  hotspots     Solution  preallocate tablets40 ASYNCHRONOUS REPLICATION  AND CONSISTENCY 4041 Asynchronous Replication 4142 Consistency Model     If copies are asynchronously updated   what can we say about stale copies      ACID guarantees require synchronous updts     Eventual consistency  Copies can drift apart   but will eventually converge if the system is  allowed to quiesce     To what value will copies converge       Do systems ever    quiesce         Is there any middle ground 43 Example  Social Alice User Status Alice Busy West East User Status Alice Free User Status Alice     User Status Alice     User Status Alice Busy User Status Alice         Busy Free Free Record Timeline  Network fault   updt goes to East   Alice logs on 44     Goal  Make it easier for applications to reason about updates  and cope with asynchrony     What happens to a record with primary key    Alice     PNUTS Consistency Model 44 Time Record  inserted Update Update Update Update Update Delete Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Update Update As the record is updated  copies may get out of sync 45 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write Current  version Stale version Stale version PNUTS Consistency Model 45 Achieved via per record primary copy protocol  To maximize availability  record masterships automaticlly  transferred if site fails  Can be selectively weakened to eventual consistency   local writes that are reconciled using version vectors 46 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write if   v 7 ERROR Current  version Stale version Stale version PNUTS Consistency Model 46 Te s t and set writes facilitate per record transactions47 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Current  version Stale version Stale version Read PNUTS Consistency Model 47 In general  reads are served using a local copy48 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read up to date Current  version Stale version Stale version PNUTS Consistency Model 48 But application can request and get current version49 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read     v 6 Current  version Stale version Stale version PNUTS Consistency Model 49 Or variations such as    read forward      while copies may lag the master record  every copy goes through the same sequence of changes50 OPERABILITY 5051 51 Server 1 Server 2 Server 3 Server 4 6 2 07 636353 Bike  86 6 5 07 662113 Chair  10 Distribution 6 1 07 424252 Couch  570 6 1 07 256623 Car  1123 6 7 07 121113 Lamp  19 6 9 07 887734 Bike  56 6 11 07 252111 Scooter  18 6 11 07 116458 Hammer  8000 Data shuffling for load balancing Distribution for parallelism52 Tablet Splitting and Balancing 52 Each storage unit has many tablets  horizontal partitions of the table  Overfull tablets split Tablets may grow over time Storage unit may become a hotspot Shed load by moving tablets to other servers Storage unit Tablet53 Consistency Techniques     Per record mastering     Each record is assigned a    master region        May differ between records     Updates to the record forwarded to the master region     Ensures consistent ordering of updates     Tablet level mastering     Each tablet is assigned a    master region        Inserts and deletes of records forwarded to the master region     Master region decides tablet splits     These details are hidden from the application     Except for the latency impact 54 54 Mastering A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                 E A     42342                E B     42521                E C     66354                W D     12352                E E     75656                C F     15677                 E C     66354                W B     42521                E A     42342                E D     12352                E E     75656                C F     15677                E55 55 Record vs  Tablet Master A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis"/>
        <doc>Architectural Considerations for Distributed RFID Tracking and Monitoring ### Zhao Cao Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA caozhao cs umass edu Yanlei Diao Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA yanlei cs umass edu Prashant Shenoy Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA shenoy cs umass edu ABSTRACT In this paper we discuss architectural challenges in designing a distributed  scalable system for RFID tracking and monitoring  We argue for the need to combine inference and query processing techniques into a single system and consider several architectural choices for building such a system  Key research challenges in de  signing our system include   i  the design of inference techniques that span multiple sites   ii  distributed maintenance of inference and query state   iii  sharing of inference and query state for scalability  and  iv  the use of writeable RFID tags to transfer state information as objects move through the supply chain  We also present the status of our ongoing research and preliminary results from an early implementation ### 1  INTRODUCTION RFID is a promising electronic identi cation technology that enables a real time information infrastructure to pro  vide timely  high value content to monitoring and tracking applications  An RFID enabled information infrastructure is likely to revolutionize areas such as supply chain manage  ment  health care and pharmaceuticals  Consider  for ex  ample  a distributed supply chain environment with multiple warehouses and millions of tagged objects that move through this supply chain  Each warehouse is equipped with RFID readers that scan objects and their associated cases and pal  lets upon arrival and departure and while they are processed in the warehouse  In order to track objects and monitor the supply chain for anomalies  several types of queries may be posed on the RFID streams generated at the warehouses    Tracking queries  Report any pallet that has deviated from its intended path  List the path taken by an item through the supply chain    Containment queries  Raise an alert if a  ammable item is not packed in a  reproof case  Verify that food containing peanuts is never exposed to other food cases for more than an hour    Hybrid queries  Report if a drug has been exposed to a temperature of more than 80 degrees for 12 hours  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  NetDB    09 Big sky  MT USA Copyright 200X ACM X XXXXX XX X XX XX     10 00  The  rst class of queries are location queries that require object locations or location history  The second class in  volves containment  i e   relationships between objects  cases and pallets  The third kind involves processing of sensor streams  e g   temperature readings  in conjunction with RFID streams to detect certain conditions  Typically raw RFID streams contain noisy data that lacks any location or containment information  Hence  such continuous queries require derivation of location and containment information from raw RFID data as well as processing of heterogeneous sensor streams along with RFID data streams  In this paper  we discuss the architectural challenges in designing a scalable  distributed stream processing system for RFID tracking and monitoring  We propose to com  bine location and containment inference with scalable query processing into a single architecture  in contrast to prior ap  proaches that dealt with these two problems separately  We present three architectural choices in instantiating such a system over large supply chains and present an analysis of their communication overheads  By doing so  we show that the choice between centralized and distributed approaches mainly depends on the read frequency of RFID readers and the number of active queries in the system  Furthermore  utilizing local storage of writeable RFID tags for inference and query processing makes the distributed approach a bet  ter solution with signi cantly reduced communication cost  In this paper  we also describe key technical challenges in designing a distributed architecture  which include  i  the design of novel inference techniques that span multiple warehouses of the supply chain   ii  distributed  consistent maintenance of inference and query state as objects move through the supply chain  and  iii  sharing of inference and query state for scalability  A novel aspect of our system is its ability to exploit writeable RFID tags  when available  and use the onboard tag storage to transfer query and infer  ence state as the object moves from one location to another  We  nally present the status of our ongoing research and preliminary results from an early implementation  2  RELATED WORK RFID stream processing  The HiFi system  9  12  o ers a declarative framework for RFID data cleaning and process  ing  It focuses on per tag smoothing and multi tag aggre  gation  but does not capture relationships between objects such as containment or estimate object locations via con  tainment  Our system can produce a rich event stream with object location and containment information  It further of  fers distributed inference and event processing methods Central Server Warehouse Warehouse Warehouse Warehouse Local Server Local Server Local Server Local Server Objects  Tags  Figure 1  A distributed RFID data management system  RFID databases  Siemens RFID middleware 20  uses ap  plication rules to archive RFID data streams into databases  Cascadia  21  supports RFID based pervasive computing with event speci cation  extraction and archival  Techniques are also available to integrate data cleansing with query pro  cessing  16   encode  ow information  13  and recover high  level information from incomplete  noisy data by exploiting known constraints  23   These techniques  however  are not designed for fast RFID stream processing and only for cen  tralized processing  Event query processing  Most event processing methods  1  14  22  use centralized processing which requires all RFID data to be transferred to a single site  incurring high com  munication cost  The system in  2  uses multi step event acquisition and processing to minimize event transmission cost  In contrast  our system performs both inference and query processing  does so at a local site whenever possible  and then transmits computation state across sites for dis  tributed inference and event pattern detection  Probabilistic inference in sensor networks  There have been several recent techniques  15  5  18  11  for inferring the true value of a phenomenon  such as temperature  light  or an object s position  that a sensor network is deployed to measure  Our inference problem di ers because we aim to infer inter object relationships  such as containment  that sensors cannot directly measure  hence the di erent design of our graph model for inference  Further  our system combines inference with query processing into a single architecture and explores advanced techniques to reduce the combined cost of inference query state migration in a distributed system  3  OVERVIEW OF THE SPIRE SYSTEM In this section  we provide an architectural overview of a distributed RFID data management system  which we call SPIRE  This system is designed to support RFID based tracking and monitoring in large scale supply chains with multiple warehouses and millions of objects  In a typical environment as depicted in Figure 1  each ob  ject is a xed with a tag that has a unique identity under the EPC standard  7   Most tags are passive tags that have no individual power systems but small amounts of memory  e g   1 4 KB in the current generation of EPC tags  17  and up to 64KB  10  in the next generation  Such tag memory can be used to store object information and facilitate query processing as we describe in the next section  An RFID reader periodically sends a radio signal to the tags in its read range  the tags use the radio energy to send back their tag ids  The reader immediately returns the sensed data in the form of  tag id  reader id  time   The local servers of a warehouse collect raw RFID data streams from all of the readers  and  lter  aggregate  and process these streams  The data streams from di erent warehouses are further ag  gregated to support global tracking and monitoring  We next illustrate two tracking and monitoring queries using an extension of the Continuous Query Language  3  with additional constructs for event pattern matching  1  22   These queries assume that events in the input stream contain attributes  tag id  time  location  container  and optional attributes describing object properties  such as the type of food and expiration date  which can be obtained from the manufacturer s database 1   It is worth noting the di erence in schema between raw RFID readings and events required for query processing  Such di erences motivate our work on inference  which is discussed shortly  Query 1 below is an example of containment queries  It sends an alert when peanut free food has been contained in the same case as food containing peanuts for more than 1 hour  The inner  nested  query block performs a self join over the input stream  where one join input retains only the events for peanut free food  the other input retains only those for food containing peanuts  and the join is based on equality on container  Each join result represents an event that a container contains both food with peanuts and foot without peanuts  This event is published immediately into a new stream  The outer query block detects a sequence pattern over the new stream  each match of the pattern contains a sequence of events that refer to the same tag id of the peanut free food and span a time period of more than 1 hour  For each pattern match  the query returns the tag id of the peanut free food and the length of the period such information can assist a retail store in deciding whether to dispose of the food  Query 1  Select tag id  A A len  time   A 1  time From   Select Rstream R1 tag id  R1 loc  From Food  Range 3 minutes  As R1  Food  Range 3 minutes  As R2 Where R1 type    peanut free  and R2 type    peanut  and R1 container   R2 container   As S   Pattern SEQ A   Where A i  tag id   A 1  tag id and A A len  time   A 1  time   1 hr  Query 2 combines RFID readings with temperature sen  sor readings and alerts if an object has been exposed to a temperature of more than 80    for 12 hours  It has a similar structure as Query 1  but returns all the sensor readings in the period when the temperature regulation was violated  Query 2  Select tag id  A   temp From   Select Rstream R tag id  R loc  T temp  From Object  Now  as R  Temperature  Rows 1 minute  as T Where R type    drug  and T temp   80     and R loc   T loc   As S   Pattern SEQ A   Where A i  tag id   A 1  tag id and A A len  time   A 1  time   12 hrs  The SPIRE system we design has two main functionali  ties  inference and query processing  Both of them can be 1 How to obtain the object properties to the site of query process  ing is an architectural issue  which we discuss in the next section  b  Inference   Inference State Event Stream      tag id  time  location  container        Raw RFID Stream    tag id  reader id  time  Local   Distributed inference  with state migration Query 2 Query m         c  Query Processing location and  containment 1 4 5 7 8 9 10 2 6 11 12 3 Query 1 Distributed  processing  w  state  migration O2 Window Window processing Local           On a 1  a i  Query  State O1 O2 Local Distributed Local Distributed  a  RFID Sensing   F Figure 2  Architectural overview of the SPIRE system  implemented in a centralized or distributed fashion  The tradeo s between these implementation choices are the fo  cus of the next section  Inference  While data stream processing has been a topic of intensive recent research  RFID data stream processing presents several new challenges  I Insu cient information  As the above examples show  query processing often requires information about ob  ject locations and inter object relationships such as containment  However  raw RFID data contains only the observed tag id and its reader id due to the limi  tations of being an identi cation technology  I Incomplete  noisy data  The problem of deriving lo  cation and containment information from raw data is compounded by the fact that RFID readings are in  herently noisy  with read rates in actual deployments often in the 60  70  range  12   This is largely due to the sensitivity of radio frequencies to environmental factors such as occluding metal objects and interfer  ence  8   Mobile RFID readers may read objects from arbitrary angles and distances  hence more susceptible to variable read rates  In SPIRE  we design an inference module that derives ob  ject locations and containment relationships despite missed readings  This module resides between the RFID sensing module and the query processing module  as shown in Fig  ure 2  We focus on containment in the following discussion  inference for object locations alone is detailed in our recent publication  19    First  an RFID reader can read several containers and all of their contained items simultaneously  which makes it di cult to infer the exact container of each item  In SPIRE  we explore the correlations of observations obtained at di erent locations at di erent times to infer con  tainment  For instance  while the containment information at the loading dock of a warehouse is ambiguous due to the readings of two containers simultaneously  true containment may be revealed at the receiving belt where containers are read one at a time  Such containment remains unchanged as containers are placed on shelves but may change later in the repackaging area  The inference algorithm needs to adapt to such changes in a timely fashion  However  missed readings signi cantly complicate the in  ference problem  Consider a scenario that an item was last seen in location A with its container  Now its container is observed in B but the item is not observed in any location  There are a few possible locations for the item  it was left behind in location A but the reading in A was missed  it moved to location B with its container and its reading in B was missed  it disappeared unexpectedly  e g   stolen   The inference algorithm needs to account for all these possibili  ties when deriving containment and location information  In SPIRE  we employ a time varying graph model to infer object location and containment information  as depicted in Figure 3  the example is taken from our ICDE poster paper  6    Our graph model G    V  E  encodes the current view of the objects in the physical world  including their reported locations and  unreported  possible containment relationships  In addition  the model incorporates statistical history about co occurrences of objects  The node set V of the graph denotes all RFID tagged objects in the physical world  These nodes are arranged into layers  with one layer for each packaging level  e g   an item  case or a pallet  which is encoded in each tag id  Nodes are assigned colors to denote their locations  The node colors are updated from the RFID readings in each epoch using the color of the location where each tag is observed  If an object is not read in an epoch  its node becomes uncolored but retains memory of the most recent observation  The directed edge set E encodes possible containment re  lationships between objects  We allow multiple outgoing and incoming edges to and from each node  indicating an object such as a case may contain multiple items  and conversely  an item may have multiple potential cases  our probabilistic inference will subsequently chose only one of these possibili  ties   To enable inference  the graph also encodes additional statistics  Each edge maintains a bit vector to record recent positive and negative evidence for the co location of the two objects  For example  the recent co location bit vector for nodes 3 and 7 in Figure 3 is 01 at time t 2 and and 011 at t 3  Further  each node remembers the last con rmed containment by a special reader such as a belt reader that scans cases one at a time  For example  at time t 2 in the  gure  the belt reader con rms that the edge from node 3 to node 7 represents the true containment  This informa  tion stays valid for a while but then becomes obsolete when containment changes  In summary  the graph model encodes the following infor  mation about each object for inference  which we call the inference state   i  the most recent observation of the object   ii  all of its possible containers   iii  its recent co  location history with each of the containers  and  iv  its con rmed container in the past by a special reader  After the graph is updated from the RFID readings in each epoch  an inference algorithm runs on the graph to es  timate the most likely container and location of each object  Our algorithm combines node inference  which derives the most likely location of an object  with edge inference  which derives the most likely container of an object  in an itera 1 2 3 4 5 6 Level 1 Locations A  loading dock Time C  packaging area t   1 t   2 t   3 Level 2 Level 3 2 3 4 5 6 7 8 2 3 4 5 6 7 10 11 9  A 1   A 1   A 1   B  2   B 2   B 2   A 1   A 1   C 3   C 3   C 3   A 1   A 1   B 3   C 3   C 2   C 3   A 1   A 1   A 1   A 1   A 1   A 1  1  A 1  B  belt B  belt 10 11 9 C  packaging area  C 2   C 2   C 2  1 Figure 3  Examples of the time varying colored graph model for containment and location inference  tive fashion through the graph  In particular  the algorithm runs  1  from the colored nodes  with known locations    2  through the edges linked to the colored nodes  where edge inference determines the container of a color node   3  to the uncolored nodes incident to these edges  where node in  ference determines the location of an uncolored node given its recent color and the colors of the processed neighboring nodes   4  to the edges linked to these nodes  and so on  As such  inference sweeps through the graph in increasing distance from the colored nodes  The above description assumes that all the data is avail  able at a central server for inference  If inference is to be made in a distributed fashion  as objects move from site to site we need to transfer inference state with the objects so that information is available for subsequent inference this process is called inference state migration  Revisit the example in Figure 3  Suppose that after time t 3  cases 2  3 and items 4  5  7 move to a new warehouse and are observed at the loading dock of the new warehouse  Now we want to infer the containers of items 4  5  7  The information in the previous inference state  such as case 3 being the con rmed container of item 7 and the co location history of items 4  5 with case 2  3  will be very useful to the inference in the new location  This indicates that inference state needs to be maintained across sites on a global scale  We detail several architectural choices for doing so in the next section  Query Processing  As the inference module streams out events with inferred location and containment information  the query processor  as shown in Figure 2 c   processes these events to answer continuous monitoring queries like the two examples above  As the  gure shows  part of query process  ing can be performed at each warehouse  such as  ltering of events based on object properties  Queries 1 and 2   a self  join over the input stream  Query 1   and a join between an object stream and a sensor stream  Query 2   A more challenging issue is with the part of query pro  cessing that spans sites  such as the detection of a complex pattern over a large period of time  see the pattern clause in Queries 1 and 2   Our discussion  rst assumes that queries run at a central server with all the information needed for query processing transfered to the server  Our query pro  cessing module employs a new type of automaton to gov  ern the pattern matching process  Each query automaton comprises a nondeterministic  nite automaton and a match bu er that stores each match of the pattern  The automa  ton for Query 1 is depicted in Figure 2 c   The start state  a 1   is where the Kleene plus operator starts to select the  rst relevant event into the match bu er  At the next state a i   it attempts to select zero  one  or more events into the bu er  The  nal state  F  represents the completion of the matching process  Each state is associated with a number of edges  representing the possible actions  Each edge has a formula expressing the condition on taking the edge  Edge formulas are evaluated using the values from the current event as well as the previous events   Details of this query automaton model are reported in our recent publication  1    In summary  the following information  called the query state  is maintained to evaluate a pattern query using our automaton model   i  the current automaton state   ii  the minimum set of values extracted from the input events that future automaton evaluation requires  e g   A 1  tag id and A 1  time for Query 1  details on extracting these values are available in  1    and  iii  the set of values that the query intends to return  which can be simple values as in Query 1  or a long sequence of readings as in Query 2  If the pattern query is de ned on a per object basis  as in both of our example queries  the system needs to maintain a copy of query state for each object  as depicted by the copies labeled O1          On in Figure 2 c   Finally  if a monitoring system supports multiple queries  the size of query state is further multiplied by the number of concurrent queries  Similar to inference  if we evaluate pattern queries in a distributed fashion  we need to perform query state mi  gration across sites so that we can resume the automaton execution in a new location and continue to expand the set of values that the query intends to return  4  ARCHITECTURAL CHOICES  BENEFITS AND DRAWBACKS There are three possible choices for instantiating the sys  tem architecture presented in the previous section  Centralized warehouse  This simplest approach is to employ a centralized architecture similar to a centralized warehouse where all RFID data is sent to a central location for stream processing  and possibly archival  The advantage of such a centralized approach is that the system has a global view of the entire supply chain  which simpli es stream pro  cessing  Inference is also simpler since all of the object state is maintained at a single location  In this case  the local servers depicted in Figure 1 only need to perform simple processing tasks such as cleaning and or compression  The primary disadvantage of the approach is the high commu  nication cost of transmitting RFID streams to the central location  since RFID data can be voluminous  the network bandwidth costs can be substantial  Analysis  Consider a supply chain with 1000 warehouses  where each warehouse stores 10 000 cases  and each case contains 10 items  Both the number of readers in a ware  house and the read frequency of each reader will vary in di erent supply chains  More readers and a higher read fre  quency yield greater monitoring accuracy  but can also lead to higher deployment and data processing costs  The choice of these parameters depends on the system budget and the monitoring requirement  Assume that  on average  there arebetween 500 to 5000 RFID readings per object every day depending on the actual deployment  Further  assume that the temperature sensors report temperature readings every 5 minutes and that there are 100 temperature sensors in each warehouse  Let each RFID reading tuple be 20 bytes  and temperature reading tuple be 9 bytes  A simple calcu  lation shows that  in this scenario  approximately 1 1 to 11 TB of data will be sent to the central location every day  Even if a compression scheme o ers a factor of 20 reduction in data volume  a  gure that we have observed in our recent research  this will still</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis"/>
        <doc>A Comparison of Approaches to Large Scale Data Analysis  ###  Andrew Pavlo Erik Paulson Alexander Rasin Brown University University of Wisconsin Brown University pavlo cs brown edu epaulson cs wisc edu alexr cs brown edu Daniel J  Abadi David J  DeWitt Samuel Madden Michael Stonebraker Yale University Microsoft Inc  M I T  CSAIL M I T  CSAIL dna cs yale edu dewitt microsoft com madden csail mit edu stonebraker csail mit edu  ###  ABSTRACT There is currently considerable enthusiasm around the MapReduce  MR  paradigm for large scale data analysis  17   Although the basic control    ow of this framework has existed in parallel SQL database management systems  DBMS  for over 20 years  some have called MR a dramatically new computing model  8  17   In this paper  we describe and compare both paradigms  Furthermore  we evaluate both kinds of systems in terms of performance and development complexity  To this end  we de   ne a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs  For each task  we measure each system   s performance for various degrees of parallelism on a cluster of 100 nodes  Our results reveal some interesting trade offs  Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system  the observed performance of these DBMSs was strikingly better  We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures  Categories and Subject Descriptors H 2 4  Database Management   Systems   Parallel databases General Terms Database Applications  Use Cases  Database Programming 1  ###  INTRODUCTION Recently the trade press has been    lled with news of the revolution of    cluster computing     This paradigm entails harnessing large numbers of  low end  processors working in parallel to solve a computing problem  In effect  this suggests constructing a data center by lining up a large number of low end servers instead of deploying a smaller set of high end servers  With this rise of interest in clusters has come a proliferation of tools for programming them  One of the earliest and best known such tools in MapReduce  MR   8   MapReduce is attractive because it provides a simple Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  model through which users can express relatively sophisticated distributed programs  leading to signi   cant interest in the educational community  For example  IBM and Google have announced plans to make a 1000 processor MapReduce cluster available to teach students distributed programming  Given this interest in MapReduce  it is natural to ask    Why not use a parallel DBMS instead     Parallel database systems  which all share a common architectural design  have been commercially available for nearly two decades  and there are now about a dozen in the marketplace  including Teradata  Aster Data  Netezza  DATAllegro  and therefore soon Microsoft SQL Server via Project Madison   Dataupia  Vertica  ParAccel  Neoview  Greenplum  DB2  via the Database Partitioning Feature   and Oracle  via Exadata   They are robust  high performance computing platforms  Like MapReduce  they provide a high level programming environment and parallelize readily  Though it may seem that MR and parallel databases target different audiences  it is in fact possible to write almost any parallel processing task as either a set of database queries  possibly using user de   ned functions and aggregates to    lter and combine data  or a set of MapReduce jobs  Inspired by this question  our goal is to understand the differences between the MapReduce approach to performing large scale data analysis and the approach taken by parallel database systems  The two classes of systems make different choices in several key areas  For example  all DBMSs require that data conform to a well de   ned schema  whereas MR permits data to be in any arbitrary format  Other differences also include how each system provides indexing and compression optimizations  programming models  the way in which data is distributed  and query execution strategies  The purpose of this paper is to consider these choices  and the trade offs that they entail  We begin in Section 2 with a brief review of the two alternative classes of systems  followed by a discussion in Section 3 of the architectural trade offs  Then  in Section 4 we present our benchmark consisting of a variety of tasks  one taken from the MR paper  8   and the rest a collection of more demanding tasks  In addition  we present the results of running the benchmark on a 100 node cluster to execute each task  We tested the publicly available open source version of MapReduce  Hadoop  1   against two parallel SQL DBMSs  Vertica  3  and a second system from a major relational vendor  We also present results on the time each system took to load the test data and report informally on the procedures needed to set up and tune the software for each task  In general  the SQL DBMSs were signi   cantly faster and required less code to implement each task  but took longer to tune and load the data  Hence  we conclude with a discussion on the reasons for the differences between the approaches and provide suggestions on the best practices for any large scale data analysis engine  Some readers may feel that experiments conducted using 100nodes are not interesting or representative of real world data processing systems  We disagree with this conjecture on two points  First  as we demonstrate in Section 4  at 100 nodes the two parallel DBMSs range from a factor of 3 1 to 6 5 faster than MapReduce on a variety of analytic tasks  While MR may indeed be capable of scaling up to 1000s of nodes  the superior ef   ciency of modern DBMSs alleviates the need to use such massive hardware on datasets in the range of 1   2PB  1000 nodes with 2TB of disk node has a total disk capacity of 2PB   For example  eBay   s Teradata con     guration uses just 72 nodes  two quad core CPUs  32GB RAM  104 300GB disks per node  to manage approximately 2 4PB of relational data  As another example  Fox Interactive Media   s warehouse is implemented using a 40 node Greenplum DBMS  Each node is a Sun X4500 machine with two dual core CPUs  48 500GB disks  and 16 GB RAM  1PB total disk space   7   Since few data sets in the world even approach a petabyte in size  it is not at all clear how many MR users really need 1 000 nodes  2  TWO APPROACHES TO LARGE SCALE DATA ANALYSIS The two classes of systems we consider in this paper run on a    shared nothing    collection of computers  19   That is  the system is deployed on a collection of independent machines  each with local disk and local main memory  connected together on a highspeed local area network  Both systems achieve parallelism by dividing any data set to be utilized into partitions  which are allocated to different nodes to facilitate parallel processing  In this section  we provide an overview of how both the MR model and traditional parallel DBMSs operate in this environment  2 1 MapReduce One of the attractive qualities about the MapReduce programming model is its simplicity  an MR program consists only of two functions  called Map and Reduce  that are written by a user to process key value data pairs  The input data set is stored in a collection of partitions in a distributed    le system deployed on each node in the cluster  The program is then injected into a distributed processing framework and executed in a manner to be described  The Map function reads a set of    records    from an input    le  does any desired    ltering and or transformations  and then outputs a set of intermediate records in the form of new key value pairs  As the Map function produces these output records  a    split    function partitions the records into R disjoint buckets by applying a function to the key of each output record  This split function is typically a hash function  though any deterministic function will suf   ce  Each map bucket is written to the processing node   s local disk  The Map function terminates having produced R output    les  one for each bucket  In general  there are multiple instances of the Map function running on different nodes of a compute cluster  We use the term instance to mean a unique running invocation of either the Map or Reduce function  Each Map instance is assigned a distinct portion of the input    le by the MR scheduler to process  If there are M such distinct portions of the input    le  then there are R    les on disk storage for each of the M Map tasks  for a total of M    R    les  Fij   1     i     M  1     j     R  The key observation is that all Map instances use the same hash function  thus  all output records with the same hash value are stored in the same output    le  The second phase of a MR program executes R instances of the Reduce program  where R is typically the number of nodes  The input for each Reduce instance Rj consists of the    les Fij   1     i     M  These    les are transferred over the network from the Map nodes    local disks  Note that again all output records from the Map phase with the same hash value are consumed by the same Reduce instance  regardless of which Map instance produced the data  Each Reduce processes or combines the records assigned to it in some way  and then writes records to an output    le  in the distributed    le system   which forms part of the computation   s    nal output  The input data set exists as a collection of one or more partitions in the distributed    le system  It is the job of the MR scheduler to decide how many Map instances to run and how to allocate them to available nodes  Likewise  the scheduler must also decide on the number and location of nodes running Reduce instances  The MR central controller is responsible for coordinating the system activities on each node  A MR program    nishes execution once the    nal result is written as new    les in the distributed    le system  2 2 Parallel DBMSs Database systems capable of running on clusters of shared nothing nodes have existed since the late 1980s  These systems all support standard relational tables and SQL  and thus the fact that the data is stored on multiple machines is transparent to the end user  Many of these systems build on the pioneering research from the Gamma  10  and Grace  11  parallel DBMS projects  The two key aspects that enable parallel execution are that  1  most  or even all  tables are partitioned over the nodes in a cluster and that  2  the system uses an optimizer that translates SQL commands into a query plan whose execution is divided amongst multiple nodes  Because programmers only need to specify their goal in a high level language  they are not burdened by the underlying storage details  such as indexing options and join strategies  Consider a SQL command to    lter the records in a table T1 based on a predicate  along with a join to a second table T2 with an aggregate computed on the result of the join  A basic sketch of how this command is processed in a parallel DBMS consists of three phases  Since the database will have already stored T1 on some collection of the nodes partitioned on some attribute  the    lter sub query is    rst performed in parallel at these sites similar to the    ltering performed in a Map function  Following this step  one of two common parallel join algorithms are employed based on the size of data tables  For example  if the number of records in T2 is small  then the DBMS could replicate it on all nodes when the data is    rst loaded  This allows the join to execute in parallel at all nodes  Following this  each node then computes the aggregate using its portion of the answer to the join  A    nal    roll up    step is required to compute the    nal answer from these partial aggregates  9   If the size of the data in T2 is large  then T2   s contents will be distributed across multiple nodes  If these tables are partitioned on different attributes than those used in the join  the system will have to hash both T2 and the    ltered version of T1 on the join attribute using a common hash function  The redistribution of both T2 and the    ltered version of T1 to the nodes is similar to the processing that occurs between the Map and the Reduce functions  Once each node has the necessary data  it then performs a hash join and calculates the preliminary aggregate function  Again  a roll up computation must be performed as a last step to produce the    nal answer  At    rst glance  these two approaches to data analysis and processing have many common elements  however  there are notable differences that we consider in the next section  3  ARCHITECTURAL ELEMENTS In this section  we consider aspects of the two system architectures that are necessary for processing large amounts of data in a distributed environment  One theme in our discussion is that the nature of the MR model is well suited for development environments with a small number of programmers and a limited application domain  This lack of constraints  however  may not be appropriate for longer term and larger sized projects 3 1 Schema Support Parallel DBMSs require data to    t into the relational paradigm of rows and columns  In contrast  the MR model does not require that data    les adhere to a schema de   ned using the relational data model  That is  the MR programmer is free to structure their data in any manner or even to have no structure at all  One might think that the absence of a rigid schema automatically makes MR the preferable option  For example  SQL is often criticized for its requirement that the programmer must specify the    shape    of the data in a data de   nition facility  On the other hand  the MR programmer must often write a custom parser in order to derive the appropriate semantics for their input records  which is at least an equivalent amount of work  But there are also other potential problems with not using a schema for large data sets  Whatever structure exists in MR input    les must be built into the Map and Reduce programs  Existing MR implementations provide built in functionality to handle simple key value pair formats  but the programmer must explicitly write support for more complex data structures  such as compound keys  This is possibly an acceptable approach if a MR data set is not accessed by multiple applications  If such data sharing exists  however  a second programmer must decipher the code written by the    rst programmer to decide how to process the input    le  A better approach  followed by all SQL DBMSs  is to separate the schema from the application and store it in a set of system catalogs that can be queried  But even if the schema is separated from the application and made available to multiple MR programs through a description facility  the developers must also agree on a single schema  This obviously requires some commitment to a data model or models  and the input    les must obey this commitment as it is cumbersome to modify data attributes once the    les are created  Once the programmers agree on the structure of data  something or someone must ensure that any data added or modi   ed does not violate integrity or other high level constraints  e g   employee salaries must be non negative   Such conditions must be known and explicitly adhered to by all programmers modifying a particular data set  a MR framework and its underlying distributed storage system has no knowledge of these rules  and thus allows input data to be easily corrupted with bad data  By again separating such constraints from the application and enforcing them automatically by the run time system  as is done by all SQL DBMSs  the integrity of the data is enforced without additional work on the programmer   s behalf  In summary  when no sharing is anticipated  the MR paradigm is quite    exible  If sharing is needed  however  then we argue that it is advantageous for the programmer to use a data description language and factor schema de   nitions and integrity constraints out of application programs  This information should be installed in common system catalogs accessible to the appropriate users and applications  3 2 Indexing All modern DBMSs use hash or B tree indexes to accelerate access to data  If one is looking for a subset of records  e g   employees with a salary greater than  100 000   then using a proper index reduces the scope of the search dramatically  Most database systems also support multiple indexes per table  Thus  the query optimizer can decide which index to use for each query or whether to simply perform a brute force sequential search  Because the MR model is so simple  MR frameworks do not provide built in indexes  The programmer must implement any indexes that they may desire to speed up access to the data inside of their application  This is not easily accomplished  as the framework   s data fetching mechanisms must also be instrumented to use these indexes when pushing data to running Map instances  Once more  this is an acceptable strategy if the indexes do not need to be shared between multiple programmers  despite requiring every MR programmer re implement the same basic functionality  If sharing is needed  however  then the speci   cations of what indexes are present and how to use them must be transferred between programmers  It is again preferable to store this index information in a standard format in the system catalogs  so that programmers can query this structure to discover such knowledge  3 3 Programming Model During the 1970s  the database research community engaged in a contentious debate between the relational advocates and the Codasyl advocates  18   The salient issue of this discussion was whether a program to access data in a DBMS should be written either by  1  Stating what you want     rather than presenting an algorithm for how to get it  Relational  2  Presenting an algorithm for data access  Codasyl  In the end  the former view prevailed and the last 30 years is a testament to the value of relational database systems  Programs in high level languages  such as SQL  are easier to write  easier to modify  and easier for a new person to understand  Codasyl was criticized for being    the assembly language of DBMS access     We argue that MR programming is somewhat analogous to Codasyl programming  one is forced to write algorithms in a low level language in order to perform record level manipulation  On the other hand  to many people brought up programming in procedural languages  such as C C   or Java  describing tasks in a declarative language like SQL can be challenging  Anecdotal evidence from the MR community suggests that there is widespread sharing of MR code fragments to do common tasks  such as joining data sets  To alleviate the burden of having to reimplement repetitive tasks  the MR community is migrating highlevel languages on top of the current interface to move such functionality into the run time  Pig  15  and Hive  2  are two notable projects in this direction  3 4 Data Distribution The conventional wisdom for large scale databases is to always send the computation to the data  rather than the other way around  In other words  one should send a small program over the network to a node  rather than importing a large amount of data from the node  Parallel DBMSs use knowledge of data distribution and location to their advantage  a parallel query optimizer strives to balance computational workloads while minimizing the amount data transmitted over the network connecting the nodes of the cluster  Aside from the initial decision on where to schedule Map instances  a MR programmer must perform these tasks manually  For example  suppose a user writes a MR program to process a collection of documents in two parts  First  the Map function scans the documents and creates a histogram of frequently occurring words  The documents are then passed to a Reduce function that groups    les by their site of origin  Using this data  the user  or another user building on the    rst user   s work  now wants to    nd sites with a document that contains more than    ve occurrences of the word    Google    or the word    IBM     In the naive implementation of this query  where the Map is executed over the accumulated statistics  the    ltration is done after the statistics for all documents are computed and shipped to reduce workers  even though only a small subset of documents satisfy the keyword    lter  In contrast  the following SQL view and select queries perform a similar computation CREATE VIEW Keywords AS SELECT siteid  docid  word  COUNT    AS wordcount FROM Documents GROUP BY siteid  docid  word  SELECT DISTINCT siteid FROM Keywords WHERE  word      IBM    OR word      Google     AND wordcount   5  A modern DBMS would rewrite the second query such that the view de   nition is substituted for the Keywords table in the FROM clause  Then  the optimizer can push the WHERE clause in the query down so that it is applied to the Documents table before the COUNT is computed  substantially reducing computation  If the documents are spread across multiple nodes  then this    lter can be applied on each node before documents belonging to the same site are grouped together  generating much less network I O  3 5 Execution Strategy There is a potentially serious performance problem related to MR   s handling of data transfer between Map and Reduce jobs  Recall that each of the N Map instances produces M output    les  each destined for a different Reduce instance  These    les are written to the local disk on the node executing each particular Map instance  If N is 1000 and M is 500  the Map phase of the program produces 500 000 local    les  When the Reduce phase starts  each of the 500 Reduce instances needs to read its 1000 input    les and must use a    le transfer protocol to    pull    each of its input    les from the nodes on which the Map instances were run  With 100s of Reduce instances running simultaneously  it is inevitable that two or more Reduce instances will attempt to read their input    les from the same map node simultaneously  inducing large numbers of disk seeks and slowing the effective disk transfer rate  This is why parallel database systems do not materialize their split    les and instead use a push approach to transfer data instead of a pull  3 6 Flexibility Despite its widespread adoption  SQL is routinely criticized for its insuf   cient expressive prowess  Some believe that it was a mistake for the database research community in the 1970s to focus on data sub languages that could be embedded in any programming language  rather than adding high level data access to all programming languages  Fortunately  new application frameworks  such as Ruby on Rails  21  and LINQ  14   have started to reverse this situation by leveraging new programming language functionality to implement an object relational mapping pattern  These programming environments allow developers to bene   t from the robustness of DBMS technologies without the burden of writing complex SQL  Proponents of the MR model argue that SQL does not facilitate the desired generality that MR provides  But almost all of the major DBMS products  commercial and open source  now provide support for user de   ned functions  stored procedures  and user de   ned aggregates in SQL  Although this does not have the full generality of MR  it does improve the    exibility of database systems  3 7 Fault Tolerance The MR frameworks provide a more sophisticated failure model than parallel DBMSs  While both classes of systems use some form of replication to deal with disk failures  MR is far more adept at handling node failures during the execution of a MR computation  In a MR system  if a unit of work  i e   processing a block of data  fails  then the MR scheduler can automatically restart the task on an alternate node  Part of the    exibility is the result of the fact that the output    les of the Map phase are materialized locally instead of being streamed to the nodes running the Reduce tasks  Similarly  pipelines of MR jobs  such as the one described in Section 4 3 4  materialize intermediate results to    les each step of the way  This differs from parallel DBMSs  which have larger granules of work  i e   transactions  that are restarted in the event of a failure  Part of the reason for this approach is that DBMSs avoid saving intermediate results to disk whenever possible  Thus  if a single node fails during a long running query in a DBMS  the entire query must be completely restarted  4  PERFORMANCE BENCHMARKS In this section  we present our benchmark consisting of    ve tasks that we use to compare the performance of the MR model with that of parallel DBMSs  The    rst task is taken directly from the original MapReduce paper  8  that the authors    claim is representative of common MR tasks  Because this task is quite simple  we also developed four additional tasks  comprised of more complex analytical workloads designed to explore the trade offs discussed in the previous section  We executed our benchmarks on a well known MR implementation and two parallel DBMSs  4 1 Benchmark Environment As we describe the details of our benchmark environment  we note how the different data analysis systems that we test differ in operating assumptions and discuss the ways in which we dealt with them in order to make the experiments uniform  4 1 1 Tested Systems Hadoop  The Hadoop system is the most popular open source implementation of the MapReduce framework  under development by Yahoo  and the Apache Software Foundation  1   Unlike the Google implementation of the original MR framework written in C    the core Hadoop system is written entirely in Java  For our experiments in this paper  we use Hadoop version 0 19 0 running on Java 1 6 0  We deployed the system with the default con   guration settings  except for the following changes that we found yielded better performance without diverging from core MR fundamentals   1  data is stored using 256MB data blocks instead of the default 64MB   2  each task executor JVM ran with a maximum heap size of 512MB and the DataNode JobTracker JVMs ran with a maximum heap size of a 1024MB  for a total size of 3 5GB per node    3  we enabled Hadoop   s    rack awareness    feature for data locality in the cluster  and  4  we allowed Hadoop to reuse the task JVM executor instead starting a new process for each Map Reduce task  Moreover  we con   gured the system to run two Map instances and a single Reduce instance concurrently on each node  The Hadoop framework also provides an implementation of the Google distributed    le system  12   For each benchmark trial  we store all input and output data in the Hadoop distributed    le system  HDFS   We used the default settings of HDFS of three replicas per block and without compression  we also tested other con   gurations  such as using only a single replica per block as well as blockand record level compression  but we found that our tests almost always executed at the same speed or worse with these features enabled  see Section 5 1 3   After each benchmark run    nishes for a particular node scaling level  we delete the data directories on each node and reformat HDFS so that the next set of input data is replicated uniformly across all nodes  Hadoop uses a central job tracker and a    master    HDFS daemon to coordinate node activities  To ensure that these daemons do not affect the performance of worker nodes  we execute both of these additional framework components on a separate node in the cluster  DBMS X  We used the latest release of DBMS X  a parallel SQL DBMS from a major relational database vendor that stores data ina row based format  The system is installed on each node and con     gured to use 4GB shared memory segments for the buffer pool and other temporary space  Each table is hash partitioned across all nodes on the salient attribute for that particular table  and then sorted and indexed on different attributes  see Sections 4 2 1 and 4 3 1   Like the Hadoop experiments  we deleted the tables in DBMSX and reloaded the data for each trial to ensure that the tuples was uniformly distributed in the cluster  By default DBMS X does not compress data in its internal storage  but it does provide ability to compress tables using a wellknown dictionary based scheme  We found that enabling compression reduced the execution times for almost all the benchmark tasks by 50   and thus we only report results with compression enabled  In only one case did we    nd that using compression actually performed worse  Furthermore  because all of our benchmarks are read only  we did not enable replication features in DBMS X  since this would not have improved performance and complicates the installation process  Vertica  The Vertica database is a parallel DBMS designed for large data warehouses  3   The main distinction of Vertica from other DBMSs  including DBMS X  is that all data is stored as columns  rather than rows  20   It uses a unique execution engine designed speci   cally for operating on top of a column oriented storage layer  Unlike DBMS X  Vertica compresses data by default since its executor can operate directly on compressed tables  Because disabling this feature is not typical in Vertica deployments  the Vertica results in this paper are generated using only compressed data  Vertica also sorts every table by one or more attributes based on a clustered index  We found that the default 256MB buffer size per node performed well in our experiments  The Vertica resource manager is responsible for setting the amount of memory given to queries  but we provide a hint to the system to expect to execute only one query at a time  Thus  each query receives most the maximum amount of memory available on each node at runtime  4 1 2 Node Con   guration All three systems were deployed on a 100 node cluster  Each node has a single 2 40 GHz Intel Core 2 Duo processor running 64  bit Red Hat Enterprise Linux 5  kernel version 2 6 18  with 4GB RAM and two 250GB SATA I hard disks  According to hdparm  the hard disks deliver 7GB sec for cached reads and about 74MB sec for buffered reads  The nodes are connected with Cisco Catalyst 3750E 48TD switches  This switch has gigabit Ethernet ports for each node and an internal switching fabric of 128Gbps  6   There are 50 nodes per switch  The switches are linked together via Cisco StackWise Plus  which creates a 64Gbps ring between the switches  Traf   c between two nodes on the same switch is entirely local to the switch and does not impact traf   c on the ring  4 1 3 Benchmark Execution For each benchmark task  we describe the steps used to implement the MR program as well as provide the equivalent SQL statement s  executed by the two database systems  We executed each task three times and report the average of the trials  Each system executes the benchmark tasks separately to ensure exclusive access to the cluster   s resources  To measure the basic performance without the overhead of coordinating parallel tasks  we    rst execute each task on a single node  We then execute the task on different cluster sizes to show how each system scales as both the amount of data processed and available resources are increased  We only report results using trials where all nodes are available and the system   s software operates correctly during the benchmark execution  We also measured the time it takes for each system to load the test data  The results from these measurements are split between the actual loading of the data and any additional operations after the loading that each system performs  such as compressing or building indexes  The initial input data on each node is stored on one of its two locally installed disks  Unless otherwise indicated  the    nal results from the queries executing in Vertica and DBMS X are piped from a shell command into a    le on the disk not used by the DBMS  Although it is possible to do an equivalent operation in Hadoop  it is easier  and more common  to store the results of a MR program into the distributed    le system  This procedure  however  is not analogous to how the DBMSs produce their output data  rather than storing the results in a single    le  the MR program produces one output    le for each Reduce instance and stores them in a single directory  The standard practice is for developers then to use these output directories as a single input unit for other MR jobs  If  however  a user wishes to use this data in a non MR application  they must    rst combine the results into a single    le and download it to the local    le system  Because of this discrepancy  we execute an extra Reduce function for each MR benchmark task that simply combines the    nal output into a single    le in HDFS  Our results differentiate between the execution times for Hadoop running the actual benchmark task versus the additional combine operation  Thus  the Hadoop results displayed in the graphs for this paper are shown as stacked bars  the lower portion of each bar is the execution time for just the speci   c benchmark task  while the upper portion is the execution time for the single Reduce function to combine all of the program   s output data into a single    le  4 2 The Original MR Task Our    rst benchmark task is the    Grep task    taken from the original MapReduce paper  which the authors describe as    representative of a large subset of the real programs written by users of MapReduce     8   For this task  each system must scan through a data set of 100 byte records looking for a three character pattern  Each record consists of a unique key in the    rst 10 bytes  followed by a 90 byte random value  The search pattern is only found in the last 90 bytes once in every 10 000 records  The input data is stored on each node in plain text    les  with one record per line  For the Hadoop trials  we uploaded these    les unaltered directly into HDFS  To load the data into Vertica and DBMSX  we execute each system   s proprietary load commands in parallel on each node and store the data using the following schema  CREATE TABLE Data   key VARCHAR 10  PRIMARY KEY  field VARCHAR 90     We execute the Grep task using two different data sets  The measurements in the original MapReduce paper are based on processing 1TB of data on approximately 1800 nodes  which is 5 6 million records or roughly 535MB of data per node  For each system  we execute the Grep task on cluster sizes of 1  10  25  50  and 100 nodes  The total number of records processed for each cluster size is therefore 5 6 million times the number of nodes  The performance of each system not only illustrates how each system scales as the amount of data is increased  but also allows us to  to some extent  compare the results to the original MR system  While our    rst dataset    xes the size of the data per node to be the same as the original MR benchmark and only varies the number of nodes  our second dataset    xes the total dataset size to be the same as the original MR benchmark  1TB  and evenly divides the data amongst a variable number of nodes  This task measures how well each system scales as the number of available nodes is increased 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 250 500 750 1000 1250 1500 seconds     17 6     75 5     76 7     67 7     75 5 Vertica Hadoop Figure 1  Load Times     Grep Task Data Set  535MB node  25 Nodes 50 Nodes 100 Nodes 0 5000 10000 15000 20000 25000 30000 seconds Vertica Hadoop Figure 2  Load Times     Grep Task Data Set  1TB cluster  1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 10000 20000 30000 40000 50000 seconds     2262 2     4369 8     4445 3     4486 2     4520 8 Vertica Hadoop Figure 3  Load Times     UserVisits Data Set  20GB node  Since Hadoop needs a total of 3TB of disk space in order to store three replicas of each block in HDFS  we were limited to running this benchmark only on 25  50  and 100 nodes  at fewer than 25 nodes  there is not enough available disk space to store 3TB   4 2 1 Data Loading We now describe the procedures used to load the data from the nodes    local    les into each system   s internal storage representation  Hadoop  There are two ways to load data into Hadoop   s distributed    le system   1  use Hadoop   s command line    le utility to upload    les stored on the local    lesystem into HDFS or  2  create a custom data loader program that writes data using Hadoop   s internal I O API  We did not need to alter the input data for our MR programs  therefore we loaded the    les on each node in parallel directly into HDFS as plain text using the command line utility  Storing the data in this manner enables MR programs to access data using Hadoop   s TextInputFormat data format  where the keys are line numbers in each    le and their corresponding values are the contents of each line  We found that this approach yielded the best performance in both the loading process and task execution  as opposed to using Hadoop   s serialized data formats or compression features  DBMS X  The loading process in DBMS X occurs in two phases  First  we execute the LOAD SQL command in parallel on each node in the cluster to read data from the local    lesystem and insert its contents into a particular table in the database  We specify in this command that the local data is delimited by a special character  thus we did not need to write a custom program to transform the data before loading it  But because our data generator simply creates random keys for each record on each node  the system must redistribute the tuples to other nodes in the cluster as it reads each record from the input    les based on the target table   s partitioning attribute  It would be possible to generate a    hash aware    version of the data generator that would allow DBMS X to just load the input    les on each node without this redistribution process  but we do not believe that this would improve load times very much  Once the initial loading phase is complete  we then execute an administrative command to reorganize the data on each node  This process executes in parallel on each node to compress data  build each table   s indexes  and perform other housekeeping  Vertica  Vertica also provides a COPY SQL command that is issued from a single host and then coordinates the loading process on multiple nodes in parallel in the cluster  The user gives the COPY command as input a list of nodes to execute the loading operation for  This process is similar to DBMS X  on each node the Vertica loader splits the input data    les on a delimiter  creates a new tuple for each line in an input    le  and redistributes that tuple to a different node based on the hash of its primary key  Once the data is loaded  the columns are automatically sorted and compressed according to the physical design of the database  Results   Discussion  The results for loading both the 535MB node and 1TB cluster data sets are shown in Figures 1 and 2  respectively  For DBMS X  we separate the times of the two loading phases  which are shown as a stacked bar in the graphs  the bottom segment represents the execution time of the parallel LOAD commands and the top segment is the reorganization process  The most striking feature of the results for the load times in 535MB node data set shown in Figure 1 is the difference in performance of DBMS X compared to Hadoop and Vertica  Despite issuing the initial LOAD command in the    rst phase on each node in parallel  the data was actually loaded on each node sequentially  Thus  as the total of amount of data is increased  the load times also increased proportionately  This also explains why  for the 1TB cluster data set  the load times for DBMS X do not decrease as less data is stored per node  However  the compression and housekeeping on DBMS X can be done in parallel across nodes  and thus the execution time of the second phase of the loading process is cut in half when twice as many nodes are used to store the 1TB of data  Without using either block  or record level compression  Hadoop clearly outperforms both DBMS X and Vertica since each node is simply copying each data    le from the local disk into the local HDFS instance and then distributing two replicas to other nodes in the cluster  If we load the data into Hadoop using only a single replica per block  then the load times are reduced by a factor of three  But as we will discuss in Section 5  the lack of multiple replicas often increases the execution times of jobs  4 2 2 Task Execution SQL Commands  A pattern search for a particular    eld is simply the following query in SQL  Neither SQL system contained an index on the    eld attribute  so this query requires a full table scan  SELECT   FROM Data WHERE field LIKE     XYZ      MapReduce Program  The MR program consists of just a Map function that is given a single record already split into the appropriate key value pair and then performs a sub string match on the value  If the search pattern is found  the Map function simply outputs the input key value pair to HDFS  Because no Reduce function is de   ned  the output generated by each Map instance is the    nal output of the program  Results   Discussion  The performance results for the three systems for this task is shown in Figures 4 and 5  Surprisingly  the relative differences between the systems are not consistent in the1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 10 20 30 40 50 60 70 seconds Vertica Hadoop Figure 4  Grep Task Results     535MB node Data Set 25 Nodes 50 Nodes 100 Nodes 0 250 500 750 1000 1250 1500 seconds Vertica Hadoop Figure 5  Grep Task Results     1TB cluster Data Set two    gures  In Figure 4  the two parallel databases perform about the same  more than a factor of two faster in Hadoop  But in Figure 5  both DBMS X and Hadoop perform more than a factor of two slower than Vertica  The reason is that the amount of data processing varies substantially from the two experiments  For the results in Figure 4  very little data is being processed  535MB node   This causes Hadoop   s non insigni   cant start up costs to become the limiting factor in its performance  As will be described in Section 5 1 2  for short running queries  i e   queries that take less than a minute   Hadoop   s start up costs can dominate the execution time  In our observations  we found that takes 10   25 seconds before all Map tasks have been started and are running at full speed across the nodes in the cluster  Furthermore  as the total number of allocated Map tasks increases  there is additional overhead required for the central job tracker to coordinate node activities  Hence  this    xed overhead increases slightly as more nodes are added to the cluster and for longer data processing tasks  as shown in Figure 5  this    xed cost is dwarfed by the time to complete the required processing  The upper segments of each Hadoop bar in the graphs represent the execution time of the additional MR job to combine the output into a single    le  Since we ran this as a separate MapReduce job  these segments consume a larger percentage of overall time in Figure 4  as the    xed start up overhead cost again dominates the work needed to perform the rest of the task  Even though the Grep task is selective  the results in Figure 5 show how this combine phase can still take hundreds of seconds due to the need to open and combine many small output    les  Each Map instance produces its output in a separate HDFS    le  and thus even though each    le is small there are many Map tasks and therefore many    les on each node  For the 1TB cluster data set experiments  Figure 5 shows that all systems executed the task on twice as many nodes in nearly half the amount of time  as one would expect since the total amount of data was held constant across nodes for this experiment  Hadoop and DBMS X performs approximately the same  since Hadoop   s startup cost is amortized across the increased amount of data processing for this experiment  However  the results clearly show that Vertica outperforms both DBMS X and Hadoop  We attribute this to Vertica   s aggressive use of data compression  see Section 5 1 3   which becomes more effective as more data is stored per node  4 3 Analytical Tasks To explore more complex uses of both types of systems  we developed four tasks related to HTML document processing  We    rst generate a collection of random HTML documents  similar to that which a web crawler might    nd  Each node is assigned a set of 600 000 unique HTML documents  each with a unique URL  In each document  we randomly generate links to other pages set using a Zip   an distribution  We also generated two additional data sets meant to model log    les of HTTP server traf   c  These data sets consist of values derived from the HTML documents as well as several randomly generated attributes  The schema of these three tables is as follows  CREATE TABLE Documents   url VARCHAR 100  PRIMARY KEY  contents TEXT    CREATE TABLE Rankings   pageURL VARCHAR 100  PRIMARY KEY  pageRank INT  avgDuration INT    CREATE TABLE UserVisits   sourceIP VARCHAR 16   destURL VARCHAR 100   visitDate DATE  adRevenue FLOAT  userAgent VARCHAR 64   countryCode VARCHAR 3   languageCode VARCHAR 6   searchWord VARCHAR 32   duration INT    Our data generator created unique    les with 155 million UserVisits records  20GB node  and 18 million Rankings records  1GB node  on each node  The visitDate  adRevenue  and sourceIP    elds are picked uniformly at random from speci   c ranges  All other    elds are picked uniformly from sampling real world data sets  Each data    le is stored on each node as a column delimited text    le  4 3 1 Data Loading We now describe the procedures for loading the UserVisits and Rankings data sets  For reasons to be discussed in Section 4 3 5  only Hadoop needs to directly load the Documents    les into its internal storage system  DBMS X and Vertica both execute a UDF that processes the Documents on each node at runtime and loads the data into a temporary table  We account for the overhead of this approach in the benchmark times  rather than in the load times  Therefore  we do not provide results for loading this data set  Hadoop  Unlike the Grep task   s data set  which was uploaded directly into HDFS unaltered  the UserVisits and Rankings data sets needed to be modi   ed so that the    rst and second columns are separated by a tab delimiter and all other    elds in each line are separated by a unique    eld delimiter  Because there are no schemas in the MR model  in order to access the different attributes at run time  the Map and Reduce functions in each task must manually split the value by the delimiter character into an array of strings  We wrote a custom data loader executed in parallel on each node to read in each line of the data sets  prepare the data as needed  and then write the tuple into a plain text    le in HDFS  Loading the data sets in this manner was roughly three times slower than using the command line utility  but did not require us to write cus 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 20 40 60 80 100 120 140 160 seconds     0 3     0 8     1 8     4 7     12 4 Vertica Hadoop Figure 6  Selection Task Results tom input handlers in Hadoop  the MR programs are able to use Hadoop   s KeyValueTextInputFormat interface on the data    les to automatically split lines of text    les into key values pairs by the tab delimiter  Again  we found that other data format options  such as SequenceFileInputFormat or custom Writable tuples  resulted in both slower load and execution times  DBMS X  We used the same loading procedures for DBMS X as discussed in Section 4 2  The Rankings table was hash partitioned across the cluster on pageURL and the data on each node was sorted by pageRank  Likewise  the UserVisits table was hash partitioned on destinationURL and sorted by visitDate on each node  Vertica  Similar to DBMS X  Vertica used the same bulk load commands discussed in Section 4 2 and sorted the UserVisits and Rankings tables by the visitDate and pageRank columns  respectively  Results   Discussion  Since the results of loading the UserVisits and Ranking data sets are similar  we only provide the results for loading the larger UserVisits data in Figure 3  Just as with loading the Grep 535MB node data set  Figure 1   the loading times for each system increases in proportion to the number of nodes used  4 3 2 Selection Task The Selection task is a lightweight    lter to    nd the pageURLs in the Rankings table  1GB node  with a pageRank above a userde   ned threshold  For our experiments  we set this threshold parameter to 10  which yields approximately 36 000 records per data    le on each node  SQL Commands  The DBMSs execute the selection task using the following simple SQL statement  SELECT pageURL  pageRank FROM Rankings WHERE pageRank   X  MapReduce Program  The MR program uses only a single Map function that splits the input value based on the    eld delimiter and outputs the record   s pageURL and pageRank as a new key value pair if its pageRank is above the threshold  This task does not require a Reduce function  since each pageURL in the Rankings data set is unique across all nodes  Results   Discussion  As was discussed in the Grep task  the results from this experiment  shown in Figure 6  demonstrate again that the parallel DBMSs outperform Hadoop by a rather signi   cant factor across all cluster scaling levels  Although the relative performance of all systems degrade as both the number of nodes and the total amount of data increase  Hadoop is most affected  For example  there is almost a 50  difference in the execution time between the 1 node and 10 node experiments  This is again due to Hadoop   s increased start up costs as more nodes are added to the cluster  which takes up a proportionately larger fraction of total query time for short running queries  Another important reason for why the parallel DBMSs are able to outperform Hadoop is that both Vertica and DBMS X use an index on the pageRank column and store the Rankings table already sorted by pageRank  Thus  executing this query is trivial  It should also be noted that although Vertica   s absolute times remain low  its relative performance degrades as the number of nodes increases  This is in spite of the fact that each node still executes the query in the same amount of time  about 170ms   But because the nodes    nish executing the query so quickly  the system becomes    ooded with control messages from too many nodes  which then takes a longer time for the system to process  Vertica uses a reliable message layer for query dissemination and commit protocol processing  4   which we believe has considerable overhead when more than a few dozen nodes are involved in the query  4 3 3 Aggregation Task Our next task requires each system to calculate the total adRevenue generated for each sourceIP in the UserVisits table  20GB node   grouped by the sourceIP column  We also ran a variant of this query where we grouped by the seven character pre   x of the sourceIP column to measure the effect of reducing the total number of groups on query performance  We designed this task to measure the performance of parallel analytics on a single read only table  where nodes need to exchange intermediate data with one another in order compute the    nal value  Regardless of the number of nodes in the cluster  this tasks always produces 2 5 million records  53 MB   the variant query produces 2 000 records  24KB   SQL Commands  The SQL commands to calculate the total adRevenue is straightforward  SELECT sourceIP  SUM adRevenue  FROM UserVisits GROUP BY sourceIP  The variant query is  SELECT SUBSTR sourceIP  1  7   SUM adRevenue  FROM UserVisits GROUP BY SUBSTR sourceIP  1  7   MapReduce Program  Unlike the previous tasks  the MR program for this task consists of both a Map and Reduce function  The Map function    rst splits the input value by the    eld delimiter  and then outputs the sourceIP    eld  given as the input key  and the adRevenue    eld as a new key value pair  For the variant query  only the    rst seven characters  representing the    rst two octets  each stored as three digits  of the sourceIP are used  These two Map functions share the same Reduce function that simply adds together all of the adRevenue values for each sourceIP and then outputs the pre   x and revenue total  We also used MR   s Combine feature to perform the pre aggregate before data is transmitted to the Reduce instances  improving the    rst query   s execution time by a factor of two  8   Results   Discussion  The results of the aggregation task experiment in Figures 7 and 8 show once again that the two DBMSs outperform Hadoop  The DBMSs execute these queries by having each node scan its local table  extract the sourceIP and adRevenue    elds  and perform a local group by  These local groups are then merged at1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 1600 1800 seconds Vertica Hadoop Figure 7  Aggregation Task Results  2 5 million Groups  1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 seconds Vertica Hadoop Figure 8  Aggregation Task Results  2 000 Groups  the query coordinator  which outputs results to the user  The results in Figure 7 illustrate that the two DBMSs perform about the same for a large number of groups  as their runtime is dominated by the cost to transmit the large number of local groups and merge them at the coordinator  For the experiments using fewer nodes  Vertica performs somewhat better  since it has to read less data  since it can directly access the sourceIP and adRevenue columns   but it becomes slightly slower as more nodes are used  Based on the results in Figure 8  it is more advantageous to use a column store system when processing fewer groups for this task  This is because the two columns accessed  sourceIP and adRevenue  consist of only 20 bytes out of the more than 200 bytes per UserVisits tuple  and therefore there are relatively few groups that need to be merged so communication costs are much lower than in the non variant plan  Vertica is thus able to outperform the other two systems from not reading unused parts of the UserVisits tuples  Note that the execution times for all systems are roughly consistent for any number of nodes  modulo Vertica   s slight slow down as the number of nodes increases   Since this benchmark task requires the system to scan through the entire data set  the run time is always bounded by the constant sequential scan performance and network repartitioning costs for each node  4 3 4 Join Task The join task consists of two sub tasks that perform a complex calculation on two data sets  In the    rst part of the task  each system must    nd the sourceIP that generated the most revenue within a particular date range  Once these intermediate records are generated  the system must then calculate the average pageRank of all the pages visited during this interval  We use the week of January 15  22  2000 in our experiments  which matches approximately 134 000 records in the UserVisits table  The salient aspect of this task is that it must consume two data different sets and join them together in order to    nd pairs of Ranking and UserVisits records with matching values for pageURL and destURL  This task stresses each system using fairly complex operations over a large amount of data  The performance results are also a good indication on how well the DBMS   s query optimizer produces ef   cient join plans  SQL Commands  In contrast to the complexity of the MR program described below  the DBMSs need only two fairly simple queries to complete the task  The    rst statement creates a temporary table and uses it to store the output of the SELECT statement that performs the join of UserVisits and Rankings and computes the aggregates  Once this table is populated  it is then trivial to use a second query to output the record with the largest totalRevenue    eld  SELECT INTO Temp sourceIP  AVG pageRank  as avgPageRank  SUM adRevenue  as totalRevenue FROM Rankings AS R  UserVisits AS UV WHERE R pageURL   UV destURL AND UV visitDate BETWEEN Date    2000 01 15     AND Date    2000 01 22     GROUP BY UV sourceIP  SELECT sourceIP  totalRevenue  avgPageRank FROM Temp ORDER BY totalRevenue DESC LIMIT 1  MapReduce Program  Because the MR model does not have an inherent ability to join two or more disparate data sets  the MR program that implements the join task must be broken out into three separate phases  Each of these phases is implemented together as a single MR program in Hadoop  but do not begin executing until the previous phase is complete  Phase 1     The    rst phase    lters UserVisits records that are outside the desired data range and then joins the qualifying records with records from the Rankings    le  The MR program is initially given all of the UserVisits and Rankings data    les as input  Map Function  For each key value input pair  we determine its record type by counting the number of    elds produced when splitting the value on the delimiter  If it is a UserVisits record  we apply the    lter based on the date range predicate  These qualifying records are emitted with composite keys of the form  destURL  K1   where K1 indicates that it is a UserVisits record  All Rankings records are emitted with composite keys of the form  pageURL  K2   where K2 indicates that it is a Rankings record  These output records are repartitioned using a user supplied partitioning function that only hashes on the URL portion of the composite key  Reduce Function  The input to the Reduce function is a single sorted run of records in URL order  For each URL  we divide its values into two sets based on the tag component of the composite key  The function then forms the cross product of the two sets to complete the join and outputs a new key value pair with the sourceIP as the key and the tuple  pageURL  pageRank  adRevenue  as the value  Phase 2     The next phase computes the total adRevenue and average pageRank based on the sourceIP of records generated in Phase 1  This phase uses a Reduce function in order to gather all of the1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 1600 1800 seconds     21 5     28 2     31 3     36 1     85 0     15 7     28 0     29 2     29 4     31 9 Vertica DBMS   X Hadoop Figure 9  Join Task Results 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 1000 2000 3000 4000 5000 6000 7000 8000 seconds Vertica Hadoop Figure 10  UDF Aggregation Task Results records for a particular sourceIP on a single node  We use the identity Map function in the Hadoop API to supply records directly to the split process  1  8   Reduce Function  For each sourceIP  the function adds up the adRevenue and computes the average pageRank  retaining the one with the maximum total ad revenue  Each Reduce instance outputs a single record with sourceIP as the key and the value as a tuple of the form  avgPageRank  totalRevenue   Phase 3     In the    nal phase  we again only need to de   ne a single Reduce function that uses the output from the previous phase to produce the record with the largest total adRevenue  We only execute one instance of the Reduce function on a single node to scan all the records from Phase 2 and    nd the target record  Reduce Function  The function processes each key value pair and keeps track of the record with the largest totalRevenue    eld  Because the Hadoop API does not easily expose the total number records that a Reduce instance will process  there is no way for the Reduce function to know that it is processing the last record  Therefore  we override the closing callback method in our Reduce implementation so that the MR program outputs the largest record right before it exits  Results   Discussion  The performance results for this task is displayed in Figure 9  We had to slightly change the SQL used in 100 node experiments for Vertica due to an optimizer bug in the system  which is why there is an increase in the execution time for Vertica going from 50 to 100 nodes  But even with this increase  it is clear that this task results in the biggest performance difference between Hadoop and the parallel database systems  The reason for this disparity is two fold  First  despite the increased complexity of the query  the performance of Hadoop is yet again limited by the speed with which the large UserVisits table  20GB node  can be read off disk  The MR program has to perform a complete table scan  while the parallel database systems were able to take advantage of clustered indexes on UserVisits visitDate to signi   cantly reduce the amount of data that needed to be read  When breaking down the costs of the different parts of the Hadoop query  we found that regardless of the number of nodes in the cluster  phase 2 and phase 3 took on average 24 3 seconds and 12 7 seconds  respectively  In contrast  phase 1  which contains the Map task that reads in the UserVisits and Rankings tables  takes an average of 1434 7 seconds to complete  Interestingly  it takes approximately 600 seconds of raw I O to read the UserVisits and Rankings tables off of disk and then another 300 seconds to split  parse  and deserialize the various attributes  Thus  the CPU overhead needed to parse these tables on the    y is the limiting factor for Hadoop  Second  the parallel DBMSs are able to take advantage of the fact that both the UserVisits and the Rankings tables are partitioned by the join key  This means that both systems are able to do the join locally on each node  without any network overhead of repartitioning before the join  Thus  they simply have to do a local hash join between the Rankings table and a selective part of the UserVisits table on each node  with a trivial ORDER BY clause across nodes  4 3 5 UDF Aggregation Task The    nal task is to compute the inlink count for each document in the dataset  a task that is often used as a component of PageRank calculations  Speci   cally  for this task  the systems must read each document    le and search for all the URLs that appear in the contents  The systems must then  for each unique URL  count the number of unique pages that reference that particular URL across the entire set of    les  It is this type of task that the MR is believed to be commonly used for  We make two adjustments for this task in order to make processing easier in Hadoop  First  we allow the aggregate to include self references  as it is non trivial for a Map function to discover the name of the input    le it is processing  Second  on each node we concatenate the HTML documents into larger    les when storing them in HDFS  We found this improved Hadoop   s performance by a factor of two and helped avoid memory issues with the central HDFS master when a large number of    les are stored in the system  SQL Commands  To perform this task in a parallel DBMS requires a user de   ned function F that parses the contents of each record in the Documents table and emits URLs into the database  This function can be written in a general purpose language and is effectively identical to the Map program discussed below  With this function F  we populate a temporary table with a list of URLs and then can execute a simple query to calculate the inlink count  SELECT INTO Temp F contents  FROM Documents  SELECT url  SUM value  FROM Temp GROUP BY url  Despite the simplicity of this proposed UDF  we found that in practice it was dif   cult to implement in the DBMSs  For DBMS X  we translated the MR program used in Hadoop into an equivalent C program that uses the POSIX regular expression library to search for links in the document  For each URL found in the document contents  the UDF returns a new tuple  URL 1  to the database engine  We originally intended to store each HTML document as a character BLOB in DBMS X and then execute the UDF on each document completely inside of the database  but were unable to do so due to a known bug in our version of the system  Instead  we modi   ed the UDF to open each HTML document on the local disk and process its contents as if it was stored in the database  Although this is similar to the approach that we had to take with Vertica  see below   the DBMS X UDF did not run as an external process to the database and did not require any bulk loading tools to import the extracted URLs  Vertica does not currently support UDFs  therefore we had to implement this benchmark task in two phases  In the    rst phase  we used a modi   ed version of DBMS X   s UDF to extract URLs from the    les  but then write the output to    les on each node   s local    lesystem  Unlike DBMS X  this program executes as a separate process outside of the database system  Each node then loads the contents of these    les into a table using Vertica   s bulk loading tools  Once this is completed  we then execute the query as described above to compute the inlink count for each URL  MapReduce Program  To    t into the MR model where all data must be de   ned in terms of key value pairs  each HTML document is split by its lines and given to the Map function with the line contents as the value and the line number in which it appeared in the    le as its key  The Map function then uses a regular expression to    nd all of the URLs in each line  For every URL found  the function outputs the URL and the integer 1 as a new key value pair  Given these records  the Reduce function then simply counts the number of values for a given key and outputs the URL and the calculated inlink count as the program   s    nal output  Results   Discussion  The results in Figure 10 show that both DBMS X and Hadoop  not including the extra Reduce process to combine the data  have approximately constant performance for this task  since each node has the same amount of Document data to process and this amount of data remains constant  7GB  as more nodes are added in the experiments  As we expected  the additional operation for Hadoop to combine data into a single    le in HDFS gets progressively slower since the amount of output data that the single node must process gets larger as new nodes are added  The results for both DBMS X and Vertica are shown in Figure 10 as stacked bars  where the bottom segment represents the time it took to execute the UDF parser and load the data into the table and the top segment is the time to execute the actual query  DBMS X performs worse than Hadoop due to the added overhead of row by row interaction between the UDF and the input    le stored outside of the database  Vertica   s poor performance is the result of having to parse data outside of the DBMS and materialize the intermediate results on the local disk before it can load it into the system  5  DISCUSSION We now discuss broader issues about the benchmark results and comment on particular aspects of each system that the raw numbers may not convey  In the benchmark above  both DBMS X and Vertica execute most of the tasks much faster than Hadoop at all scaling levels  The next subsections describe  in greater detail than the previous section  the reasons for this dramatic performance difference  5 1 System level Aspects In this section  we describe how architectural decisions made at the system level affect the relative performance of the two classes of data analysis systems  Since installation and con   guration parameters can have a signi   cant difference in the ultimate performance of the system  we begin with a discussion of the relative ease with which these parameters are set  Afterwards  we discuss some lower level implementation details  While some of these details affect performance in fundamental ways  e g   the fact that MR does not transform data on loading precludes various I O optimizations and necessitates runtime parsing which increases CPU costs   others are more implementation speci   c  e g   the high start up cost of MR   5 1 1 System Installation  Con   guration  and Tuning We were able to get Hadoop installed and running jobs with little effort  Installing the system only requires setting up data directories on each node and deploying the system library and con   guration    les  Con   guring the system for optimal performance was done through trial and error  We found that certain parameters  such as the size of the sort buffers or the number of replicas  had no affect on execution performance  whereas other parameters  such as using larger block sizes  improved performance signi   cantly  The DBMS X installation process was relatively straightforward  A GUI leads the user through the initial steps on one of the cluster nodes  and then prepares a    le that can be fed to an installer utility in parallel on the other nodes to complete the installation  Despite this simple process  we found that DBMS X was complicated to con   gure in order to start running queries  Initially  we were frustrated by the failure of anything but the most basic of operations  We eventually discovered each node   s kernel was con   gured to limit the total amount of allocated virtual address space  When this limit was hit  new processes could not be created and DBMS X operations would fail  We mention this even though it was our own administrative error  as we were surprised that DBMS X   s extensive system probing and self adjusting con   guration was not able to detect this limitation  This was disappointing after our earlier Hadoop successes  Even after these earlier issues were resolved and we had DBMSX running  we were routinely stymied by other memory limitations  We found that certain default parameters  such as the sizes of the buffer pool and sort heaps  were too conservative for modern systems  Furthermore  DBMS X proved to be ineffective at adjusting memory allocations for changing conditions  For example  the system automatically expanded our buffer pool from the default 4MB to only 5MB  we later forced it to 512 MB   It also warned us that performance could be degraded when we increased our sort heap size to 128 MB  in fact  performance improved by a factor of 12   Manually changing some options resulted in the system automatically altering others  On occasion  this combination of manual and automatic changes resulted in a con   guration for DBMS X that caused it to refuse to boot the next time the system started  As most con   guration settings required DBMS X to be running in order to adjust them  it was unfortunately easy to lock ourselves out with no failsafe mode to restore to a previous state  Vertica was relatively easy to install as an RPM that we deployed on each node  An additional con   guration script bundled with the RPM is used to build catalog meta data and modify certain kernel parameters  Database tuning is minimal and is done through hints to the resource manager  we found that the default settings worked well for us  The downside of this simpli   ed tuning approach  however  is that there is no explicit mechanism to determine what resources were granted to a query nor is there a way to manually adjust per query resource allocation  The take away from our efforts is that we found parallel DBMSs to be much more challenging than Hadoop to install and con   gure properly  There is  however  a signi   cant variation with respect to ease of installation and con   guration across the different parallel database products  One small advantage for the database systems is that the tuning that is needed is mostly done prior to query execution  and that certain tuning parameters  e g   sort buffer sizes  are suitable for all tasks  In contrast  for Hadoop we not only had totune the system  e g   block sizes   but we also occasionally needed to tune each individual task to work well with the system  e g   changing code   Finally  the parallel database products came with tools to aid in the tuning process whereas with Hadoop we were forced to resort to trial and error tuning  clearly a more mature MR implementation could include such tuning tools as well  5 1 2 Task Start up We found that our MR programs took some time before all nodes were running at full capacity  On a cluster of 100 nodes  it takes 10 seconds from the moment that a job is submitted to the JobTracker before the    rst Map task begins to execute and 25 seconds until all the nodes in the cluster are executing the job  This coincides with the results in  8   where the data processing rate does not reach its peak for nearly 60 seconds on a cluster of 1800 nodes  The    cold start    nature is symptomatic to Hadoop   s  and apparently Google   s  implementation and not inherent to the actual MR model itself  For example  we also found that prior versions of Hadoop would create a new JVM process for each Map and Reduce instance on a node  which we found increased the overhead of running jobs on large data sets  enabling the JVM reuse feature in the latest version of Hadoop improved our results for MR by 10   15   In contrast  parallel DBMSs are started at OS boot time  and thus are considered to always be    warm     waiting for a query to execute  Moreover  all modern DBMSs are designed to execute using multiple threads and processes  which allows the currently running code to accept additional tasks and further optimize its execution schedule  Minimizing start up time was one of the early optimizations of DBMSs  and is certainly something that MR systems should be able to incorporate without a large rewrite of the underlying architecture  5 1 3 Compression Almost every parallel DBMS  including DBMS X and Vertica  allows for optional compression of stored data  It is not uncommon for compression to result in a factor of 6   10 space savings  Vertica   s internal data representation is highly optimized for data compression and has an execution engine that operates directly on compressed data  i e   it avoids decompressing the data during processing whenever possible   In general  since analysis tasks on large data sets are often I O bound  trading CPU cycles  needed to decompress input data  for I O bandwidth  compressed data means that there is less data to read  is a good strategy and translates to faster execution  In situations where the executor can operate directly on compressed data  there is often no trade off at all and compression is an obvious win  Hadoop and its underlying distributed    lesystem support both block level and record level compression on input data  We found  however  that neither technique improved Hadoop   s performance and in some cases actually slowed execution  It also required more effort on our part to either change code or prepare the input data  It should also be noted that compression was also not used in the original MR benchmark  8   In order to use block level compression in Hadoop  we    rst had to split the data    les into multiple  smaller    les on each node   s local    le system and then compress each    le using the gzip tool  Compressing the data in this manner reduced each data set by 20   25  from its original size  These compressed    les are then copied into HDFS just as if they were plain text    les  Hadoop automatically detects when    les are compressed and will decompress them on the    y when they are fed into Map instances  thus we did not need to change our MR programs to use the compressed data  Despite the longer load times  if one includes the splitting and compressing   Hadoop using block level compression slowed most the tasks by a few seconds while CPU bound tasks executed 50  slower  We also tried executing the benchmarks using record level compression  This required us to  1  write to a custom tuple object using Hadoop   s API   2  modify our data loader program to transform records to compressed and serialized custom tuples  and  3  refactor each benchmark  We initially believed that this would improve CPU bound tasks  because the Map and Reduce tasks no longer needed to split the    elds by the delimiter  We found  however  that this approach actually performed worse than block level compression while only compressing the data by 10   5 1 4 Loading and Data Layout Parallel DBMSs have the opportunity to reorganize the input data    le at load time  This allows for certain optimizations  such as storing each attribute of a table separately  as done in column stores such as Vertica   For read only queries that only touch a subset of the attributes of a table  this optimization can improve performance by allowing the attributes that are not accessed by a particular query to be left on disk and never read  Similar to the compression optimization described above  this saves critical I O bandwidth  MR systems by default do not transform the data when it is loaded into their distributed    le system  and thus are unable to change the layout of input data  which precludes this class of optimization opportunities  Furthermore  Hadoop was always much more CPU intensive than the parallel DBMS in running equivalent tasks because it must parse and deserialize the records in the input data at run time  whereas parallel databases do the parsing at load time and can quickly extract attributes from tuples at essentially zero cost  But MR   s simpli   ed loading process did make it much easier and faster to load than with the DBMSs  Our results in Sections 4 2 1 and 4 3 1 show that Hadoop achieved load throughputs of up to three times faster than Vertica and almost 20 times faster than DBMS X  This suggests that for data that is only going to be loaded once for certain types on analysis tasks  that it may not be worth it to pay the cost of the indexing and reorganization cost in a DBMS   This also strongly suggests that a DBMS would bene   t from a    insitu    operation mode that would allow a user to directly access and query    les stored in a local    le system  5 1 5 Execution Strategies As noted earlier  the query planner in parallel DBMSs are careful to transfer data between nodes only if it is absolutely necessary  This allows the systems to optimize the join algorithm depending on the characteristics of the data and perform push oriented messaging without writing intermediate data sets  Over time  MR advocates should study the techniques used in parallel DBMSs and incorporate the concepts that are germane to their model  In doing so  we believe that again the performance of MR frameworks will improve dramatically  Furthermore  parallel DBMSs construct a complete query plan that is sent to all processing nodes at the start of the query  Because data is    pushed    between sites when only necessary  there are no control messages during processing  In contrast  MR systems use a large number of control messages to synchronize processing  resulting in poorer performance due to increased overhead  Vertica also experienced this problem but on a much smaller scale  Section 4 2   5 1 6 Failure Model As discussed previously  while not providing support for transactions  MR is able to recover from faults in the middle of query execution in a way that most parallel database systems cannot  Since parallel DBMSs will be deployed on larger clusters over time  the probability of mid query hardware failures will increase  Thus  for long running queries  it may be important to implement such a fault tolerance model  While improving the fault tolerance of DBMSs isclearly a good idea  we are wary of devoting huge computational clusters and    brute force    approaches to computation when sophisticated software would could do the same processing with far less hardware and consume far less energy  or in less time  thereby obviating the need for a sophisticated fault tolerance model  A multithousand node cluster of the sort Google  Microsoft  and Yahoo  run uses huge amounts of energy  and as our results show  for many data processing tasks a parallel DBMS can often achieve the same performance using far fewer nodes  As such  the desirable approach is to use high performance algorithms with modest parallelism rather than brute force approaches on much larger clusters  5 2 User level Aspects A data processing system   s performance is irrelevant to a user or an organization if the system is not usable  In this section  we discuss aspects of each system that we encountered from a userlevel perspective while conducting the benchmark study that may promote or inhibit application development and adoption  5 2 1 Ease of Use Once the system is on line and the data has been loaded  the programmer then begins to write the query or the code needed to perform their task  Like other kinds of programming  this is often an iterative process  the programmer writes a little bit of code  tests it  and then writes some more  The programmer can easily determine whether his her code is syntactically correct in b</doc>
        <doc>A Comparison of Approaches to Large Scale Data Analysis  ###  Andrew Pavlo Erik Paulson Alexander Rasin Brown University University of Wisconsin Brown University pavlo cs brown edu epaulson cs wisc edu alexr cs brown edu Daniel J  Abadi David J  DeWitt Samuel Madden Michael Stonebraker Yale University Microsoft Inc  M I T  CSAIL M I T  CSAIL dna cs yale edu dewitt microsoft com madden csail mit edu stonebraker csail mit edu  ABSTRACT There is currently considerable enthusiasm around the MapReduce  MR  paradigm for large scale data analysis  17   Although the basic control    ow of this framework has existed in parallel SQL database management systems  DBMS  for over 20 years  some have called MR a dramatically new computing model  8  17   In this paper  we describe and compare both paradigms  Furthermore  we evaluate both kinds of systems in terms of performance and development complexity  To this end  we de   ne a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs  For each task  we measure each system   s performance for various degrees of parallelism on a cluster of 100 nodes  Our results reveal some interesting trade offs  Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system  the observed performance of these DBMSs was strikingly better  We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures  Categories and Subject Descriptors H 2 4  Database Management   Systems   Parallel databases General Terms Database Applications  Use Cases  Database Programming 1  ###  INTRODUCTION Recently the trade press has been    lled with news of the revolution of    cluster computing     This paradigm entails harnessing large numbers of  low end  processors working in parallel to solve a computing problem  In effect  this suggests constructing a data center by lining up a large number of low end servers instead of deploying a smaller set of high end servers  With this rise of interest in clusters has come a proliferation of tools for programming them  One of the earliest and best known such tools in MapReduce  MR   8   MapReduce is attractive because it provides a simple Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   09  June 29   July 2  2009  Providence  Rhode Island  USA  Copyright 2009 ACM 978 1 60558 551 2 09 06     5 00  model through which users can express relatively sophisticated distributed programs  leading to signi   cant interest in the educational community  For example  IBM and Google have announced plans to make a 1000 processor MapReduce cluster available to teach students distributed programming  Given this interest in MapReduce  it is natural to ask    Why not use a parallel DBMS instead     Parallel database systems  which all share a common architectural design  have been commercially available for nearly two decades  and there are now about a dozen in the marketplace  including Teradata  Aster Data  Netezza  DATAllegro  and therefore soon Microsoft SQL Server via Project Madison   Dataupia  Vertica  ParAccel  Neoview  Greenplum  DB2  via the Database Partitioning Feature   and Oracle  via Exadata   They are robust  high performance computing platforms  Like MapReduce  they provide a high level programming environment and parallelize readily  Though it may seem that MR and parallel databases target different audiences  it is in fact possible to write almost any parallel processing task as either a set of database queries  possibly using user de   ned functions and aggregates to    lter and combine data  or a set of MapReduce jobs  Inspired by this question  our goal is to understand the differences between the MapReduce approach to performing large scale data analysis and the approach taken by parallel database systems  The two classes of systems make different choices in several key areas  For example  all DBMSs require that data conform to a well de   ned schema  whereas MR permits data to be in any arbitrary format  Other differences also include how each system provides indexing and compression optimizations  programming models  the way in which data is distributed  and query execution strategies  The purpose of this paper is to consider these choices  and the trade offs that they entail  We begin in Section 2 with a brief review of the two alternative classes of systems  followed by a discussion in Section 3 of the architectural trade offs  Then  in Section 4 we present our benchmark consisting of a variety of tasks  one taken from the MR paper  8   and the rest a collection of more demanding tasks  In addition  we present the results of running the benchmark on a 100 node cluster to execute each task  We tested the publicly available open source version of MapReduce  Hadoop  1   against two parallel SQL DBMSs  Vertica  3  and a second system from a major relational vendor  We also present results on the time each system took to load the test data and report informally on the procedures needed to set up and tune the software for each task  In general  the SQL DBMSs were signi   cantly faster and required less code to implement each task  but took longer to tune and load the data  Hence  we conclude with a discussion on the reasons for the differences between the approaches and provide suggestions on the best practices for any large scale data analysis engine  Some readers may feel that experiments conducted using 100nodes are not interesting or representative of real world data processing systems  We disagree with this conjecture on two points  First  as we demonstrate in Section 4  at 100 nodes the two parallel DBMSs range from a factor of 3 1 to 6 5 faster than MapReduce on a variety of analytic tasks  While MR may indeed be capable of scaling up to 1000s of nodes  the superior ef   ciency of modern DBMSs alleviates the need to use such massive hardware on datasets in the range of 1   2PB  1000 nodes with 2TB of disk node has a total disk capacity of 2PB   For example  eBay   s Teradata con     guration uses just 72 nodes  two quad core CPUs  32GB RAM  104 300GB disks per node  to manage approximately 2 4PB of relational data  As another example  Fox Interactive Media   s warehouse is implemented using a 40 node Greenplum DBMS  Each node is a Sun X4500 machine with two dual core CPUs  48 500GB disks  and 16 GB RAM  1PB total disk space   7   Since few data sets in the world even approach a petabyte in size  it is not at all clear how many MR users really need 1 000 nodes  2  TWO APPROACHES TO LARGE SCALE DATA ANALYSIS The two classes of systems we consider in this paper run on a    shared nothing    collection of computers  19   That is  the system is deployed on a collection of independent machines  each with local disk and local main memory  connected together on a highspeed local area network  Both systems achieve parallelism by dividing any data set to be utilized into partitions  which are allocated to different nodes to facilitate parallel processing  In this section  we provide an overview of how both the MR model and traditional parallel DBMSs operate in this environment  2 1 MapReduce One of the attractive qualities about the MapReduce programming model is its simplicity  an MR program consists only of two functions  called Map and Reduce  that are written by a user to process key value data pairs  The input data set is stored in a collection of partitions in a distributed    le system deployed on each node in the cluster  The program is then injected into a distributed processing framework and executed in a manner to be described  The Map function reads a set of    records    from an input    le  does any desired    ltering and or transformations  and then outputs a set of intermediate records in the form of new key value pairs  As the Map function produces these output records  a    split    function partitions the records into R disjoint buckets by applying a function to the key of each output record  This split function is typically a hash function  though any deterministic function will suf   ce  Each map bucket is written to the processing node   s local disk  The Map function terminates having produced R output    les  one for each bucket  In general  there are multiple instances of the Map function running on different nodes of a compute cluster  We use the term instance to mean a unique running invocation of either the Map or Reduce function  Each Map instance is assigned a distinct portion of the input    le by the MR scheduler to process  If there are M such distinct portions of the input    le  then there are R    les on disk storage for each of the M Map tasks  for a total of M    R    les  Fij   1     i     M  1     j     R  The key observation is that all Map instances use the same hash function  thus  all output records with the same hash value are stored in the same output    le  The second phase of a MR program executes R instances of the Reduce program  where R is typically the number of nodes  The input for each Reduce instance Rj consists of the    les Fij   1     i     M  These    les are transferred over the network from the Map nodes    local disks  Note that again all output records from the Map phase with the same hash value are consumed by the same Reduce instance  regardless of which Map instance produced the data  Each Reduce processes or combines the records assigned to it in some way  and then writes records to an output    le  in the distributed    le system   which forms part of the computation   s    nal output  The input data set exists as a collection of one or more partitions in the distributed    le system  It is the job of the MR scheduler to decide how many Map instances to run and how to allocate them to available nodes  Likewise  the scheduler must also decide on the number and location of nodes running Reduce instances  The MR central controller is responsible for coordinating the system activities on each node  A MR program    nishes execution once the    nal result is written as new    les in the distributed    le system  2 2 Parallel DBMSs Database systems capable of running on clusters of shared nothing nodes have existed since the late 1980s  These systems all support standard relational tables and SQL  and thus the fact that the data is stored on multiple machines is transparent to the end user  Many of these systems build on the pioneering research from the Gamma  10  and Grace  11  parallel DBMS projects  The two key aspects that enable parallel execution are that  1  most  or even all  tables are partitioned over the nodes in a cluster and that  2  the system uses an optimizer that translates SQL commands into a query plan whose execution is divided amongst multiple nodes  Because programmers only need to specify their goal in a high level language  they are not burdened by the underlying storage details  such as indexing options and join strategies  Consider a SQL command to    lter the records in a table T1 based on a predicate  along with a join to a second table T2 with an aggregate computed on the result of the join  A basic sketch of how this command is processed in a parallel DBMS consists of three phases  Since the database will have already stored T1 on some collection of the nodes partitioned on some attribute  the    lter sub query is    rst performed in parallel at these sites similar to the    ltering performed in a Map function  Following this step  one of two common parallel join algorithms are employed based on the size of data tables  For example  if the number of records in T2 is small  then the DBMS could replicate it on all nodes when the data is    rst loaded  This allows the join to execute in parallel at all nodes  Following this  each node then computes the aggregate using its portion of the answer to the join  A    nal    roll up    step is required to compute the    nal answer from these partial aggregates  9   If the size of the data in T2 is large  then T2   s contents will be distributed across multiple nodes  If these tables are partitioned on different attributes than those used in the join  the system will have to hash both T2 and the    ltered version of T1 on the join attribute using a common hash function  The redistribution of both T2 and the    ltered version of T1 to the nodes is similar to the processing that occurs between the Map and the Reduce functions  Once each node has the necessary data  it then performs a hash join and calculates the preliminary aggregate function  Again  a roll up computation must be performed as a last step to produce the    nal answer  At    rst glance  these two approaches to data analysis and processing have many common elements  however  there are notable differences that we consider in the next section  3  ARCHITECTURAL ELEMENTS In this section  we consider aspects of the two system architectures that are necessary for processing large amounts of data in a distributed environment  One theme in our discussion is that the nature of the MR model is well suited for development environments with a small number of programmers and a limited application domain  This lack of constraints  however  may not be appropriate for longer term and larger sized projects 3 1 Schema Support Parallel DBMSs require data to    t into the relational paradigm of rows and columns  In contrast  the MR model does not require that data    les adhere to a schema de   ned using the relational data model  That is  the MR programmer is free to structure their data in any manner or even to have no structure at all  One might think that the absence of a rigid schema automatically makes MR the preferable option  For example  SQL is often criticized for its requirement that the programmer must specify the    shape    of the data in a data de   nition facility  On the other hand  the MR programmer must often write a custom parser in order to derive the appropriate semantics for their input records  which is at least an equivalent amount of work  But there are also other potential problems with not using a schema for large data sets  Whatever structure exists in MR input    les must be built into the Map and Reduce programs  Existing MR implementations provide built in functionality to handle simple key value pair formats  but the programmer must explicitly write support for more complex data structures  such as compound keys  This is possibly an acceptable approach if a MR data set is not accessed by multiple applications  If such data sharing exists  however  a second programmer must decipher the code written by the    rst programmer to decide how to process the input    le  A better approach  followed by all SQL DBMSs  is to separate the schema from the application and store it in a set of system catalogs that can be queried  But even if the schema is separated from the application and made available to multiple MR programs through a description facility  the developers must also agree on a single schema  This obviously requires some commitment to a data model or models  and the input    les must obey this commitment as it is cumbersome to modify data attributes once the    les are created  Once the programmers agree on the structure of data  something or someone must ensure that any data added or modi   ed does not violate integrity or other high level constraints  e g   employee salaries must be non negative   Such conditions must be known and explicitly adhered to by all programmers modifying a particular data set  a MR framework and its underlying distributed storage system has no knowledge of these rules  and thus allows input data to be easily corrupted with bad data  By again separating such constraints from the application and enforcing them automatically by the run time system  as is done by all SQL DBMSs  the integrity of the data is enforced without additional work on the programmer   s behalf  In summary  when no sharing is anticipated  the MR paradigm is quite    exible  If sharing is needed  however  then we argue that it is advantageous for the programmer to use a data description language and factor schema de   nitions and integrity constraints out of application programs  This information should be installed in common system catalogs accessible to the appropriate users and applications  3 2 Indexing All modern DBMSs use hash or B tree indexes to accelerate access to data  If one is looking for a subset of records  e g   employees with a salary greater than  100 000   then using a proper index reduces the scope of the search dramatically  Most database systems also support multiple indexes per table  Thus  the query optimizer can decide which index to use for each query or whether to simply perform a brute force sequential search  Because the MR model is so simple  MR frameworks do not provide built in indexes  The programmer must implement any indexes that they may desire to speed up access to the data inside of their application  This is not easily accomplished  as the framework   s data fetching mechanisms must also be instrumented to use these indexes when pushing data to running Map instances  Once more  this is an acceptable strategy if the indexes do not need to be shared between multiple programmers  despite requiring every MR programmer re implement the same basic functionality  If sharing is needed  however  then the speci   cations of what indexes are present and how to use them must be transferred between programmers  It is again preferable to store this index information in a standard format in the system catalogs  so that programmers can query this structure to discover such knowledge  3 3 Programming Model During the 1970s  the database research community engaged in a contentious debate between the relational advocates and the Codasyl advocates  18   The salient issue of this discussion was whether a program to access data in a DBMS should be written either by  1  Stating what you want     rather than presenting an algorithm for how to get it  Relational  2  Presenting an algorithm for data access  Codasyl  In the end  the former view prevailed and the last 30 years is a testament to the value of relational database systems  Programs in high level languages  such as SQL  are easier to write  easier to modify  and easier for a new person to understand  Codasyl was criticized for being    the assembly language of DBMS access     We argue that MR programming is somewhat analogous to Codasyl programming  one is forced to write algorithms in a low level language in order to perform record level manipulation  On the other hand  to many people brought up programming in procedural languages  such as C C   or Java  describing tasks in a declarative language like SQL can be challenging  Anecdotal evidence from the MR community suggests that there is widespread sharing of MR code fragments to do common tasks  such as joining data sets  To alleviate the burden of having to reimplement repetitive tasks  the MR community is migrating highlevel languages on top of the current interface to move such functionality into the run time  Pig  15  and Hive  2  are two notable projects in this direction  3 4 Data Distribution The conventional wisdom for large scale databases is to always send the computation to the data  rather than the other way around  In other words  one should send a small program over the network to a node  rather than importing a large amount of data from the node  Parallel DBMSs use knowledge of data distribution and location to their advantage  a parallel query optimizer strives to balance computational workloads while minimizing the amount data transmitted over the network connecting the nodes of the cluster  Aside from the initial decision on where to schedule Map instances  a MR programmer must perform these tasks manually  For example  suppose a user writes a MR program to process a collection of documents in two parts  First  the Map function scans the documents and creates a histogram of frequently occurring words  The documents are then passed to a Reduce function that groups    les by their site of origin  Using this data  the user  or another user building on the    rst user   s work  now wants to    nd sites with a document that contains more than    ve occurrences of the word    Google    or the word    IBM     In the naive implementation of this query  where the Map is executed over the accumulated statistics  the    ltration is done after the statistics for all documents are computed and shipped to reduce workers  even though only a small subset of documents satisfy the keyword    lter  In contrast  the following SQL view and select queries perform a similar computation CREATE VIEW Keywords AS SELECT siteid  docid  word  COUNT    AS wordcount FROM Documents GROUP BY siteid  docid  word  SELECT DISTINCT siteid FROM Keywords WHERE  word      IBM    OR word      Google     AND wordcount   5  A modern DBMS would rewrite the second query such that the view de   nition is substituted for the Keywords table in the FROM clause  Then  the optimizer can push the WHERE clause in the query down so that it is applied to the Documents table before the COUNT is computed  substantially reducing computation  If the documents are spread across multiple nodes  then this    lter can be applied on each node before documents belonging to the same site are grouped together  generating much less network I O  3 5 Execution Strategy There is a potentially serious performance problem related to MR   s handling of data transfer between Map and Reduce jobs  Recall that each of the N Map instances produces M output    les  each destined for a different Reduce instance  These    les are written to the local disk on the node executing each particular Map instance  If N is 1000 and M is 500  the Map phase of the program produces 500 000 local    les  When the Reduce phase starts  each of the 500 Reduce instances needs to read its 1000 input    les and must use a    le transfer protocol to    pull    each of its input    les from the nodes on which the Map instances were run  With 100s of Reduce instances running simultaneously  it is inevitable that two or more Reduce instances will attempt to read their input    les from the same map node simultaneously  inducing large numbers of disk seeks and slowing the effective disk transfer rate  This is why parallel database systems do not materialize their split    les and instead use a push approach to transfer data instead of a pull  3 6 Flexibility Despite its widespread adoption  SQL is routinely criticized for its insuf   cient expressive prowess  Some believe that it was a mistake for the database research community in the 1970s to focus on data sub languages that could be embedded in any programming language  rather than adding high level data access to all programming languages  Fortunately  new application frameworks  such as Ruby on Rails  21  and LINQ  14   have started to reverse this situation by leveraging new programming language functionality to implement an object relational mapping pattern  These programming environments allow developers to bene   t from the robustness of DBMS technologies without the burden of writing complex SQL  Proponents of the MR model argue that SQL does not facilitate the desired generality that MR provides  But almost all of the major DBMS products  commercial and open source  now provide support for user de   ned functions  stored procedures  and user de   ned aggregates in SQL  Although this does not have the full generality of MR  it does improve the    exibility of database systems  3 7 Fault Tolerance The MR frameworks provide a more sophisticated failure model than parallel DBMSs  While both classes of systems use some form of replication to deal with disk failures  MR is far more adept at handling node failures during the execution of a MR computation  In a MR system  if a unit of work  i e   processing a block of data  fails  then the MR scheduler can automatically restart the task on an alternate node  Part of the    exibility is the result of the fact that the output    les of the Map phase are materialized locally instead of being streamed to the nodes running the Reduce tasks  Similarly  pipelines of MR jobs  such as the one described in Section 4 3 4  materialize intermediate results to    les each step of the way  This differs from parallel DBMSs  which have larger granules of work  i e   transactions  that are restarted in the event of a failure  Part of the reason for this approach is that DBMSs avoid saving intermediate results to disk whenever possible  Thus  if a single node fails during a long running query in a DBMS  the entire query must be completely restarted  4  PERFORMANCE BENCHMARKS In this section  we present our benchmark consisting of    ve tasks that we use to compare the performance of the MR model with that of parallel DBMSs  The    rst task is taken directly from the original MapReduce paper  8  that the authors    claim is representative of common MR tasks  Because this task is quite simple  we also developed four additional tasks  comprised of more complex analytical workloads designed to explore the trade offs discussed in the previous section  We executed our benchmarks on a well known MR implementation and two parallel DBMSs  4 1 Benchmark Environment As we describe the details of our benchmark environment  we note how the different data analysis systems that we test differ in operating assumptions and discuss the ways in which we dealt with them in order to make the experiments uniform  4 1 1 Tested Systems Hadoop  The Hadoop system is the most popular open source implementation of the MapReduce framework  under development by Yahoo  and the Apache Software Foundation  1   Unlike the Google implementation of the original MR framework written in C    the core Hadoop system is written entirely in Java  For our experiments in this paper  we use Hadoop version 0 19 0 running on Java 1 6 0  We deployed the system with the default con   guration settings  except for the following changes that we found yielded better performance without diverging from core MR fundamentals   1  data is stored using 256MB data blocks instead of the default 64MB   2  each task executor JVM ran with a maximum heap size of 512MB and the DataNode JobTracker JVMs ran with a maximum heap size of a 1024MB  for a total size of 3 5GB per node    3  we enabled Hadoop   s    rack awareness    feature for data locality in the cluster  and  4  we allowed Hadoop to reuse the task JVM executor instead starting a new process for each Map Reduce task  Moreover  we con   gured the system to run two Map instances and a single Reduce instance concurrently on each node  The Hadoop framework also provides an implementation of the Google distributed    le system  12   For each benchmark trial  we store all input and output data in the Hadoop distributed    le system  HDFS   We used the default settings of HDFS of three replicas per block and without compression  we also tested other con   gurations  such as using only a single replica per block as well as blockand record level compression  but we found that our tests almost always executed at the same speed or worse with these features enabled  see Section 5 1 3   After each benchmark run    nishes for a particular node scaling level  we delete the data directories on each node and reformat HDFS so that the next set of input data is replicated uniformly across all nodes  Hadoop uses a central job tracker and a    master    HDFS daemon to coordinate node activities  To ensure that these daemons do not affect the performance of worker nodes  we execute both of these additional framework components on a separate node in the cluster  DBMS X  We used the latest release of DBMS X  a parallel SQL DBMS from a major relational database vendor that stores data ina row based format  The system is installed on each node and con     gured to use 4GB shared memory segments for the buffer pool and other temporary space  Each table is hash partitioned across all nodes on the salient attribute for that particular table  and then sorted and indexed on different attributes  see Sections 4 2 1 and 4 3 1   Like the Hadoop experiments  we deleted the tables in DBMSX and reloaded the data for each trial to ensure that the tuples was uniformly distributed in the cluster  By default DBMS X does not compress data in its internal storage  but it does provide ability to compress tables using a wellknown dictionary based scheme  We found that enabling compression reduced the execution times for almost all the benchmark tasks by 50   and thus we only report results with compression enabled  In only one case did we    nd that using compression actually performed worse  Furthermore  because all of our benchmarks are read only  we did not enable replication features in DBMS X  since this would not have improved performance and complicates the installation process  Vertica  The Vertica database is a parallel DBMS designed for large data warehouses  3   The main distinction of Vertica from other DBMSs  including DBMS X  is that all data is stored as columns  rather than rows  20   It uses a unique execution engine designed speci   cally for operating on top of a column oriented storage layer  Unlike DBMS X  Vertica compresses data by default since its executor can operate directly on compressed tables  Because disabling this feature is not typical in Vertica deployments  the Vertica results in this paper are generated using only compressed data  Vertica also sorts every table by one or more attributes based on a clustered index  We found that the default 256MB buffer size per node performed well in our experiments  The Vertica resource manager is responsible for setting the amount of memory given to queries  but we provide a hint to the system to expect to execute only one query at a time  Thus  each query receives most the maximum amount of memory available on each node at runtime  4 1 2 Node Con   guration All three systems were deployed on a 100 node cluster  Each node has a single 2 40 GHz Intel Core 2 Duo processor running 64  bit Red Hat Enterprise Linux 5  kernel version 2 6 18  with 4GB RAM and two 250GB SATA I hard disks  According to hdparm  the hard disks deliver 7GB sec for cached reads and about 74MB sec for buffered reads  The nodes are connected with Cisco Catalyst 3750E 48TD switches  This switch has gigabit Ethernet ports for each node and an internal switching fabric of 128Gbps  6   There are 50 nodes per switch  The switches are linked together via Cisco StackWise Plus  which creates a 64Gbps ring between the switches  Traf   c between two nodes on the same switch is entirely local to the switch and does not impact traf   c on the ring  4 1 3 Benchmark Execution For each benchmark task  we describe the steps used to implement the MR program as well as provide the equivalent SQL statement s  executed by the two database systems  We executed each task three times and report the average of the trials  Each system executes the benchmark tasks separately to ensure exclusive access to the cluster   s resources  To measure the basic performance without the overhead of coordinating parallel tasks  we    rst execute each task on a single node  We then execute the task on different cluster sizes to show how each system scales as both the amount of data processed and available resources are increased  We only report results using trials where all nodes are available and the system   s software operates correctly during the benchmark execution  We also measured the time it takes for each system to load the test data  The results from these measurements are split between the actual loading of the data and any additional operations after the loading that each system performs  such as compressing or building indexes  The initial input data on each node is stored on one of its two locally installed disks  Unless otherwise indicated  the    nal results from the queries executing in Vertica and DBMS X are piped from a shell command into a    le on the disk not used by the DBMS  Although it is possible to do an equivalent operation in Hadoop  it is easier  and more common  to store the results of a MR program into the distributed    le system  This procedure  however  is not analogous to how the DBMSs produce their output data  rather than storing the results in a single    le  the MR program produces one output    le for each Reduce instance and stores them in a single directory  The standard practice is for developers then to use these output directories as a single input unit for other MR jobs  If  however  a user wishes to use this data in a non MR application  they must    rst combine the results into a single    le and download it to the local    le system  Because of this discrepancy  we execute an extra Reduce function for each MR benchmark task that simply combines the    nal output into a single    le in HDFS  Our results differentiate between the execution times for Hadoop running the actual benchmark task versus the additional combine operation  Thus  the Hadoop results displayed in the graphs for this paper are shown as stacked bars  the lower portion of each bar is the execution time for just the speci   c benchmark task  while the upper portion is the execution time for the single Reduce function to combine all of the program   s output data into a single    le  4 2 The Original MR Task Our    rst benchmark task is the    Grep task    taken from the original MapReduce paper  which the authors describe as    representative of a large subset of the real programs written by users of MapReduce     8   For this task  each system must scan through a data set of 100 byte records looking for a three character pattern  Each record consists of a unique key in the    rst 10 bytes  followed by a 90 byte random value  The search pattern is only found in the last 90 bytes once in every 10 000 records  The input data is stored on each node in plain text    les  with one record per line  For the Hadoop trials  we uploaded these    les unaltered directly into HDFS  To load the data into Vertica and DBMSX  we execute each system   s proprietary load commands in parallel on each node and store the data using the following schema  CREATE TABLE Data   key VARCHAR 10  PRIMARY KEY  field VARCHAR 90     We execute the Grep task using two different data sets  The measurements in the original MapReduce paper are based on processing 1TB of data on approximately 1800 nodes  which is 5 6 million records or roughly 535MB of data per node  For each system  we execute the Grep task on cluster sizes of 1  10  25  50  and 100 nodes  The total number of records processed for each cluster size is therefore 5 6 million times the number of nodes  The performance of each system not only illustrates how each system scales as the amount of data is increased  but also allows us to  to some extent  compare the results to the original MR system  While our    rst dataset    xes the size of the data per node to be the same as the original MR benchmark and only varies the number of nodes  our second dataset    xes the total dataset size to be the same as the original MR benchmark  1TB  and evenly divides the data amongst a variable number of nodes  This task measures how well each system scales as the number of available nodes is increased 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 250 500 750 1000 1250 1500 seconds     17 6     75 5     76 7     67 7     75 5 Vertica Hadoop Figure 1  Load Times     Grep Task Data Set  535MB node  25 Nodes 50 Nodes 100 Nodes 0 5000 10000 15000 20000 25000 30000 seconds Vertica Hadoop Figure 2  Load Times     Grep Task Data Set  1TB cluster  1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 10000 20000 30000 40000 50000 seconds     2262 2     4369 8     4445 3     4486 2     4520 8 Vertica Hadoop Figure 3  Load Times     UserVisits Data Set  20GB node  Since Hadoop needs a total of 3TB of disk space in order to store three replicas of each block in HDFS  we were limited to running this benchmark only on 25  50  and 100 nodes  at fewer than 25 nodes  there is not enough available disk space to store 3TB   4 2 1 Data Loading We now describe the procedures used to load the data from the nodes    local    les into each system   s internal storage representation  Hadoop  There are two ways to load data into Hadoop   s distributed    le system   1  use Hadoop   s command line    le utility to upload    les stored on the local    lesystem into HDFS or  2  create a custom data loader program that writes data using Hadoop   s internal I O API  We did not need to alter the input data for our MR programs  therefore we loaded the    les on each node in parallel directly into HDFS as plain text using the command line utility  Storing the data in this manner enables MR programs to access data using Hadoop   s TextInputFormat data format  where the keys are line numbers in each    le and their corresponding values are the contents of each line  We found that this approach yielded the best performance in both the loading process and task execution  as opposed to using Hadoop   s serialized data formats or compression features  DBMS X  The loading process in DBMS X occurs in two phases  First  we execute the LOAD SQL command in parallel on each node in the cluster to read data from the local    lesystem and insert its contents into a particular table in the database  We specify in this command that the local data is delimited by a special character  thus we did not need to write a custom program to transform the data before loading it  But because our data generator simply creates random keys for each record on each node  the system must redistribute the tuples to other nodes in the cluster as it reads each record from the input    les based on the target table   s partitioning attribute  It would be possible to generate a    hash aware    version of the data generator that would allow DBMS X to just load the input    les on each node without this redistribution process  but we do not believe that this would improve load times very much  Once the initial loading phase is complete  we then execute an administrative command to reorganize the data on each node  This process executes in parallel on each node to compress data  build each table   s indexes  and perform other housekeeping  Vertica  Vertica also provides a COPY SQL command that is issued from a single host and then coordinates the loading process on multiple nodes in parallel in the cluster  The user gives the COPY command as input a list of nodes to execute the loading operation for  This process is similar to DBMS X  on each node the Vertica loader splits the input data    les on a delimiter  creates a new tuple for each line in an input    le  and redistributes that tuple to a different node based on the hash of its primary key  Once the data is loaded  the columns are automatically sorted and compressed according to the physical design of the database  Results   Discussion  The results for loading both the 535MB node and 1TB cluster data sets are shown in Figures 1 and 2  respectively  For DBMS X  we separate the times of the two loading phases  which are shown as a stacked bar in the graphs  the bottom segment represents the execution time of the parallel LOAD commands and the top segment is the reorganization process  The most striking feature of the results for the load times in 535MB node data set shown in Figure 1 is the difference in performance of DBMS X compared to Hadoop and Vertica  Despite issuing the initial LOAD command in the    rst phase on each node in parallel  the data was actually loaded on each node sequentially  Thus  as the total of amount of data is increased  the load times also increased proportionately  This also explains why  for the 1TB cluster data set  the load times for DBMS X do not decrease as less data is stored per node  However  the compression and housekeeping on DBMS X can be done in parallel across nodes  and thus the execution time of the second phase of the loading process is cut in half when twice as many nodes are used to store the 1TB of data  Without using either block  or record level compression  Hadoop clearly outperforms both DBMS X and Vertica since each node is simply copying each data    le from the local disk into the local HDFS instance and then distributing two replicas to other nodes in the cluster  If we load the data into Hadoop using only a single replica per block  then the load times are reduced by a factor of three  But as we will discuss in Section 5  the lack of multiple replicas often increases the execution times of jobs  4 2 2 Task Execution SQL Commands  A pattern search for a particular    eld is simply the following query in SQL  Neither SQL system contained an index on the    eld attribute  so this query requires a full table scan  SELECT   FROM Data WHERE field LIKE     XYZ      MapReduce Program  The MR program consists of just a Map function that is given a single record already split into the appropriate key value pair and then performs a sub string match on the value  If the search pattern is found  the Map function simply outputs the input key value pair to HDFS  Because no Reduce function is de   ned  the output generated by each Map instance is the    nal output of the program  Results   Discussion  The performance results for the three systems for this task is shown in Figures 4 and 5  Surprisingly  the relative differences between the systems are not consistent in the1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 10 20 30 40 50 60 70 seconds Vertica Hadoop Figure 4  Grep Task Results     535MB node Data Set 25 Nodes 50 Nodes 100 Nodes 0 250 500 750 1000 1250 1500 seconds Vertica Hadoop Figure 5  Grep Task Results     1TB cluster Data Set two    gures  In Figure 4  the two parallel databases perform about the same  more than a factor of two faster in Hadoop  But in Figure 5  both DBMS X and Hadoop perform more than a factor of two slower than Vertica  The reason is that the amount of data processing varies substantially from the two experiments  For the results in Figure 4  very little data is being processed  535MB node   This causes Hadoop   s non insigni   cant start up costs to become the limiting factor in its performance  As will be described in Section 5 1 2  for short running queries  i e   queries that take less than a minute   Hadoop   s start up costs can dominate the execution time  In our observations  we found that takes 10   25 seconds before all Map tasks have been started and are running at full speed across the nodes in the cluster  Furthermore  as the total number of allocated Map tasks increases  there is additional overhead required for the central job tracker to coordinate node activities  Hence  this    xed overhead increases slightly as more nodes are added to the cluster and for longer data processing tasks  as shown in Figure 5  this    xed cost is dwarfed by the time to complete the required processing  The upper segments of each Hadoop bar in the graphs represent the execution time of the additional MR job to combine the output into a single    le  Since we ran this as a separate MapReduce job  these segments consume a larger percentage of overall time in Figure 4  as the    xed start up overhead cost again dominates the work needed to perform the rest of the task  Even though the Grep task is selective  the results in Figure 5 show how this combine phase can still take hundreds of seconds due to the need to open and combine many small output    les  Each Map instance produces its output in a separate HDFS    le  and thus even though each    le is small there are many Map tasks and therefore many    les on each node  For the 1TB cluster data set experiments  Figure 5 shows that all systems executed the task on twice as many nodes in nearly half the amount of time  as one would expect since the total amount of data was held constant across nodes for this experiment  Hadoop and DBMS X performs approximately the same  since Hadoop   s startup cost is amortized across the increased amount of data processing for this experiment  However  the results clearly show that Vertica outperforms both DBMS X and Hadoop  We attribute this to Vertica   s aggressive use of data compression  see Section 5 1 3   which becomes more effective as more data is stored per node  4 3 Analytical Tasks To explore more complex uses of both types of systems  we developed four tasks related to HTML document processing  We    rst generate a collection of random HTML documents  similar to that which a web crawler might    nd  Each node is assigned a set of 600 000 unique HTML documents  each with a unique URL  In each document  we randomly generate links to other pages set using a Zip   an distribution  We also generated two additional data sets meant to model log    les of HTTP server traf   c  These data sets consist of values derived from the HTML documents as well as several randomly generated attributes  The schema of these three tables is as follows  CREATE TABLE Documents   url VARCHAR 100  PRIMARY KEY  contents TEXT    CREATE TABLE Rankings   pageURL VARCHAR 100  PRIMARY KEY  pageRank INT  avgDuration INT    CREATE TABLE UserVisits   sourceIP VARCHAR 16   destURL VARCHAR 100   visitDate DATE  adRevenue FLOAT  userAgent VARCHAR 64   countryCode VARCHAR 3   languageCode VARCHAR 6   searchWord VARCHAR 32   duration INT    Our data generator created unique    les with 155 million UserVisits records  20GB node  and 18 million Rankings records  1GB node  on each node  The visitDate  adRevenue  and sourceIP    elds are picked uniformly at random from speci   c ranges  All other    elds are picked uniformly from sampling real world data sets  Each data    le is stored on each node as a column delimited text    le  4 3 1 Data Loading We now describe the procedures for loading the UserVisits and Rankings data sets  For reasons to be discussed in Section 4 3 5  only Hadoop needs to directly load the Documents    les into its internal storage system  DBMS X and Vertica both execute a UDF that processes the Documents on each node at runtime and loads the data into a temporary table  We account for the overhead of this approach in the benchmark times  rather than in the load times  Therefore  we do not provide results for loading this data set  Hadoop  Unlike the Grep task   s data set  which was uploaded directly into HDFS unaltered  the UserVisits and Rankings data sets needed to be modi   ed so that the    rst and second columns are separated by a tab delimiter and all other    elds in each line are separated by a unique    eld delimiter  Because there are no schemas in the MR model  in order to access the different attributes at run time  the Map and Reduce functions in each task must manually split the value by the delimiter character into an array of strings  We wrote a custom data loader executed in parallel on each node to read in each line of the data sets  prepare the data as needed  and then write the tuple into a plain text    le in HDFS  Loading the data sets in this manner was roughly three times slower than using the command line utility  but did not require us to write cus 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 20 40 60 80 100 120 140 160 seconds     0 3     0 8     1 8     4 7     12 4 Vertica Hadoop Figure 6  Selection Task Results tom input handlers in Hadoop  the MR programs are able to use Hadoop   s KeyValueTextInputFormat interface on the data    les to automatically split lines of text    les into key values pairs by the tab delimiter  Again  we found that other data format options  such as SequenceFileInputFormat or custom Writable tuples  resulted in both slower load and execution times  DBMS X  We used the same loading procedures for DBMS X as discussed in Section 4 2  The Rankings table was hash partitioned across the cluster on pageURL and the data on each node was sorted by pageRank  Likewise  the UserVisits table was hash partitioned on destinationURL and sorted by visitDate on each node  Vertica  Similar to DBMS X  Vertica used the same bulk load commands discussed in Section 4 2 and sorted the UserVisits and Rankings tables by the visitDate and pageRank columns  respectively  Results   Discussion  Since the results of loading the UserVisits and Ranking data sets are similar  we only provide the results for loading the larger UserVisits data in Figure 3  Just as with loading the Grep 535MB node data set  Figure 1   the loading times for each system increases in proportion to the number of nodes used  4 3 2 Selection Task The Selection task is a lightweight    lter to    nd the pageURLs in the Rankings table  1GB node  with a pageRank above a userde   ned threshold  For our experiments  we set this threshold parameter to 10  which yields approximately 36 000 records per data    le on each node  SQL Commands  The DBMSs execute the selection task using the following simple SQL statement  SELECT pageURL  pageRank FROM Rankings WHERE pageRank   X  MapReduce Program  The MR program uses only a single Map function that splits the input value based on the    eld delimiter and outputs the record   s pageURL and pageRank as a new key value pair if its pageRank is above the threshold  This task does not require a Reduce function  since each pageURL in the Rankings data set is unique across all nodes  Results   Discussion  As was discussed in the Grep task  the results from this experiment  shown in Figure 6  demonstrate again that the parallel DBMSs outperform Hadoop by a rather signi   cant factor across all cluster scaling levels  Although the relative performance of all systems degrade as both the number of nodes and the total amount of data increase  Hadoop is most affected  For example  there is almost a 50  difference in the execution time between the 1 node and 10 node experiments  This is again due to Hadoop   s increased start up costs as more nodes are added to the cluster  which takes up a proportionately larger fraction of total query time for short running queries  Another important reason for why the parallel DBMSs are able to outperform Hadoop is that both Vertica and DBMS X use an index on the pageRank column and store the Rankings table already sorted by pageRank  Thus  executing this query is trivial  It should also be noted that although Vertica   s absolute times remain low  its relative performance degrades as the number of nodes increases  This is in spite of the fact that each node still executes the query in the same amount of time  about 170ms   But because the nodes    nish executing the query so quickly  the system becomes    ooded with control messages from too many nodes  which then takes a longer time for the system to process  Vertica uses a reliable message layer for query dissemination and commit protocol processing  4   which we believe has considerable overhead when more than a few dozen nodes are involved in the query  4 3 3 Aggregation Task Our next task requires each system to calculate the total adRevenue generated for each sourceIP in the UserVisits table  20GB node   grouped by the sourceIP column  We also ran a variant of this query where we grouped by the seven character pre   x of the sourceIP column to measure the effect of reducing the total number of groups on query performance  We designed this task to measure the performance of parallel analytics on a single read only table  where nodes need to exchange intermediate data with one another in order compute the    nal value  Regardless of the number of nodes in the cluster  this tasks always produces 2 5 million records  53 MB   the variant query produces 2 000 records  24KB   SQL Commands  The SQL commands to calculate the total adRevenue is straightforward  SELECT sourceIP  SUM adRevenue  FROM UserVisits GROUP BY sourceIP  The variant query is  SELECT SUBSTR sourceIP  1  7   SUM adRevenue  FROM UserVisits GROUP BY SUBSTR sourceIP  1  7   MapReduce Program  Unlike the previous tasks  the MR program for this task consists of both a Map and Reduce function  The Map function    rst splits the input value by the    eld delimiter  and then outputs the sourceIP    eld  given as the input key  and the adRevenue    eld as a new key value pair  For the variant query  only the    rst seven characters  representing the    rst two octets  each stored as three digits  of the sourceIP are used  These two Map functions share the same Reduce function that simply adds together all of the adRevenue values for each sourceIP and then outputs the pre   x and revenue total  We also used MR   s Combine feature to perform the pre aggregate before data is transmitted to the Reduce instances  improving the    rst query   s execution time by a factor of two  8   Results   Discussion  The results of the aggregation task experiment in Figures 7 and 8 show once again that the two DBMSs outperform Hadoop  The DBMSs execute these queries by having each node scan its local table  extract the sourceIP and adRevenue    elds  and perform a local group by  These local groups are then merged at1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 1600 1800 seconds Vertica Hadoop Figure 7  Aggregation Task Results  2 5 million Groups  1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 seconds Vertica Hadoop Figure 8  Aggregation Task Results  2 000 Groups  the query coordinator  which outputs results to the user  The results in Figure 7 illustrate that the two DBMSs perform about the same for a large number of groups  as their runtime is dominated by the cost to transmit the large number of local groups and merge them at the coordinator  For the experiments using fewer nodes  Vertica performs somewhat better  since it has to read less data  since it can directly access the sourceIP and adRevenue columns   but it becomes slightly slower as more nodes are used  Based on the results in Figure 8  it is more advantageous to use a column store system when processing fewer groups for this task  This is because the two columns accessed  sourceIP and adRevenue  consist of only 20 bytes out of the more than 200 bytes per UserVisits tuple  and therefore there are relatively few groups that need to be merged so communication costs are much lower than in the non variant plan  Vertica is thus able to outperform the other two systems from not reading unused parts of the UserVisits tuples  Note that the execution times for all systems are roughly consistent for any number of nodes  modulo Vertica   s slight slow down as the number of nodes increases   Since this benchmark task requires the system to scan through the entire data set  the run time is always bounded by the constant sequential scan performance and network repartitioning costs for each node  4 3 4 Join Task The join task consists of two sub tasks that perform a complex calculation on two data sets  In the    rst part of the task  each system must    nd the sourceIP that generated the most revenue within a particular date range  Once these intermediate records are generated  the system must then calculate the average pageRank of all the pages visited during this interval  We use the week of January 15  22  2000 in our experiments  which matches approximately 134 000 records in the UserVisits table  The salient aspect of this task is that it must consume two data different sets and join them together in order to    nd pairs of Ranking and UserVisits records with matching values for pageURL and destURL  This task stresses each system using fairly complex operations over a large amount of data  The performance results are also a good indication on how well the DBMS   s query optimizer produces ef   cient join plans  SQL Commands  In contrast to the complexity of the MR program described below  the DBMSs need only two fairly simple queries to complete the task  The    rst statement creates a temporary table and uses it to store the output of the SELECT statement that performs the join of UserVisits and Rankings and computes the aggregates  Once this table is populated  it is then trivial to use a second query to output the record with the largest totalRevenue    eld  SELECT INTO Temp sourceIP  AVG pageRank  as avgPageRank  SUM adRevenue  as totalRevenue FROM Rankings AS R  UserVisits AS UV WHERE R pageURL   UV destURL AND UV visitDate BETWEEN Date    2000 01 15     AND Date    2000 01 22     GROUP BY UV sourceIP  SELECT sourceIP  totalRevenue  avgPageRank FROM Temp ORDER BY totalRevenue DESC LIMIT 1  MapReduce Program  Because the MR model does not have an inherent ability to join two or more disparate data sets  the MR program that implements the join task must be broken out into three separate phases  Each of these phases is implemented together as a single MR program in Hadoop  but do not begin executing until the previous phase is complete  Phase 1     The    rst phase    lters UserVisits records that are outside the desired data range and then joins the qualifying records with records from the Rankings    le  The MR program is initially given all of the UserVisits and Rankings data    les as input  Map Function  For each key value input pair  we determine its record type by counting the number of    elds produced when splitting the value on the delimiter  If it is a UserVisits record  we apply the    lter based on the date range predicate  These qualifying records are emitted with composite keys of the form  destURL  K1   where K1 indicates that it is a UserVisits record  All Rankings records are emitted with composite keys of the form  pageURL  K2   where K2 indicates that it is a Rankings record  These output records are repartitioned using a user supplied partitioning function that only hashes on the URL portion of the composite key  Reduce Function  The input to the Reduce function is a single sorted run of records in URL order  For each URL  we divide its values into two sets based on the tag component of the composite key  The function then forms the cross product of the two sets to complete the join and outputs a new key value pair with the sourceIP as the key and the tuple  pageURL  pageRank  adRevenue  as the value  Phase 2     The next phase computes the total adRevenue and average pageRank based on the sourceIP of records generated in Phase 1  This phase uses a Reduce function in order to gather all of the1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 200 400 600 800 1000 1200 1400 1600 1800 seconds     21 5     28 2     31 3     36 1     85 0     15 7     28 0     29 2     29 4     31 9 Vertica DBMS   X Hadoop Figure 9  Join Task Results 1 Nodes 10 Nodes 25 Nodes 50 Nodes 100 Nodes 0 1000 2000 3000 4000 5000 6000 7000 8000 seconds Vertica Hadoop Figure 10  UDF Aggregation Task Results records for a particular sourceIP on a single node  We use the identity Map function in the Hadoop API to supply records directly to the split process  1  8   Reduce Function  For each sourceIP  the function adds up the adRevenue and computes the average pageRank  retaining the one with the maximum total ad revenue  Each Reduce instance outputs a single record with sourceIP as the key and the value as a tuple of the form  avgPageRank  totalRevenue   Phase 3     In the    nal phase  we again only need to de   ne a single Reduce function that uses the output from the previous phase to produce the record with the largest total adRevenue  We only execute one instance of the Reduce function on a single node to scan all the records from Phase 2 and    nd the target record  Reduce Function  The function processes each key value pair and keeps track of the record with the largest totalRevenue    eld  Because the Hadoop API does not easily expose the total number records that a Reduce instance will process  there is no way for the Reduce function to know that it is processing the last record  Therefore  we override the closing callback method in our Reduce implementation so that the MR program outputs the largest record right before it exits  Results   Discussion  The performance results for this task is displayed in Figure 9  We had to slightly change the SQL used in 100 node experiments for Vertica due to an optimizer bug in the system  which is why there is an increase in the execution time for Vertica going from 50 to 100 nodes  But even with this increase  it is clear that this task results in the biggest performance difference between Hadoop and the parallel database systems  The reason for this disparity is two fold  First  despite the increased complexity of the query  the performance of Hadoop is yet again limited by the speed with which the large UserVisits table  20GB node  can be read off disk  The MR program has to perform a complete table scan  while the parallel database systems were able to take advantage of clustered indexes on UserVisits visitDate to signi   cantly reduce the amount of data that needed to be read  When breaking down the costs of the different parts of the Hadoop query  we found that regardless of the number of nodes in the cluster  phase 2 and phase 3 took on average 24 3 seconds and 12 7 seconds  respectively  In contrast  phase 1  which contains the Map task that reads in the UserVisits and Rankings tables  takes an average of 1434 7 seconds to complete  Interestingly  it takes approximately 600 seconds of raw I O to read the UserVisits and Rankings tables off of disk and then another 300 seconds to split  parse  and deserialize the various attributes  Thus  the CPU overhead needed to parse these tables on the    y is the limiting factor for Hadoop  Second  the parallel DBMSs are able to take advantage of the fact that both the UserVisits and the Rankings tables are partitioned by the join key  This means that both systems are able to do the join locally on each node  without any network overhead of repartitioning before the join  Thus  they simply have to do a local hash join between the Rankings table and a selective part of the UserVisits table on each node  with a trivial ORDER BY clause across nodes  4 3 5 UDF Aggregation Task The    nal task is to compute the inlink count for each document in the dataset  a task that is often used as a component of PageRank calculations  Speci   cally  for this task  the systems must read each document    le and search for all the URLs that appear in the contents  The systems must then  for each unique URL  count the number of unique pages that reference that particular URL across the entire set of    les  It is this type of task that the MR is believed to be commonly used for  We make two adjustments for this task in order to make processing easier in Hadoop  First  we allow the aggregate to include self references  as it is non trivial for a Map function to discover the name of the input    le it is processing  Second  on each node we concatenate the HTML documents into larger    les when storing them in HDFS  We found this improved Hadoop   s performance by a factor of two and helped avoid memory issues with the central HDFS master when a large number of    les are stored in the system  SQL Commands  To perform this task in a parallel DBMS requires a user de   ned function F that parses the contents of each record in the Documents table and emits URLs into the database  This function can be written in a general purpose language and is effectively identical to the Map program discussed below  With this function F  we populate a temporary table with a list of URLs and then can execute a simple query to calculate the inlink count  SELECT INTO Temp F contents  FROM Documents  SELECT url  SUM value  FROM Temp GROUP BY url  Despite the simplicity of this proposed UDF  we found that in practice it was dif   cult to implement in the DBMSs  For DBMS X  we translated the MR program used in Hadoop into an equivalent C program that uses the POSIX regular expression library to search for links in the document  For each URL found in the document contents  the UDF returns a new tuple  URL 1  to the database engine  We originally intended to store each HTML document as a character BLOB in DBMS X and then execute the UDF on each document completely inside of the database  but were unable to do so due to a known bug in our version of the system  Instead  we modi   ed the UDF to open each HTML document on the local disk and process its contents as if it was stored in the database  Although this is similar to the approach that we had to take with Vertica  see below   the DBMS X UDF did not run as an external process to the database and did not require any bulk loading tools to import the extracted URLs  Vertica does not currently support UDFs  therefore we had to implement this benchmark task in two phases  In the    rst phase  we used a modi   ed version of DBMS X   s UDF to extract URLs from the    les  but then write the output to    les on each node   s local    lesystem  Unlike DBMS X  this program executes as a separate process outside of the database system  Each node then loads the contents of these    les into a table using Vertica   s bulk loading tools  Once this is completed  we then execute the query as described above to compute the inlink count for each URL  MapReduce Program  To    t into the MR model where all data must be de   ned in terms of key value pairs  each HTML document is split by its lines and given to the Map function with the line contents as the value and the line number in which it appeared in the    le as its key  The Map function then uses a regular expression to    nd all of the URLs in each line  For every URL found  the function outputs the URL and the integer 1 as a new key value pair  Given these records  the Reduce function then simply counts the number of values for a given key and outputs the URL and the calculated inlink count as the program   s    nal output  Results   Discussion  The results in Figure 10 show that both DBMS X and Hadoop  not including the extra Reduce process to combine the data  have approximately constant performance for this task  since each node has the same amount of Document data to process and this amount of data remains constant  7GB  as more nodes are added in the experiments  As we expected  the additional operation for Hadoop to combine data into a single    le in HDFS gets progressively slower since the amount of output data that the single node must process gets larger as new nodes are added  The results for both DBMS X and Vertica are shown in Figure 10 as stacked bars  where the bottom segment represents the time it took to execute the UDF parser and load the data into the table and the top segment is the time to execute the actual query  DBMS X performs worse than Hadoop due to the added overhead of row by row interaction between the UDF and the input    le stored outside of the database  Vertica   s poor performance is the result of having to parse data outside of the DBMS and materialize the intermediate results on the local disk before it can load it into the system  5  DISCUSSION We now discuss broader issues about the benchmark results and comment on particular aspects of each system that the raw numbers may not convey  In the benchmark above  both DBMS X and Vertica execute most of the tasks much faster than Hadoop at all scaling levels  The next subsections describe  in greater detail than the previous section  the reasons for this dramatic performance difference  5 1 System level Aspects In this section  we describe how architectural decisions made at the system level affect the relative performance of the two classes of data analysis systems  Since installation and con   guration parameters can have a signi   cant difference in the ultimate performance of the system  we begin with a discussion of the relative ease with which these parameters are set  Afterwards  we discuss some lower level implementation details  While some of these details affect performance in fundamental ways  e g   the fact that MR does not transform data on loading precludes various I O optimizations and necessitates runtime parsing which increases CPU costs   others are more implementation speci   c  e g   the high start up cost of MR   5 1 1 System Installation  Con   guration  and Tuning We were able to get Hadoop installed and running jobs with little effort  Installing the system only requires setting up data directories on each node and deploying the system library and con   guration    les  Con   guring the system for optimal performance was done through trial and error  We found that certain parameters  such as the size of the sort buffers or the number of replicas  had no affect on execution performance  whereas other parameters  such as using larger block sizes  improved performance signi   cantly  The DBMS X installation process was relatively straightforward  A GUI leads the user through the initial steps on one of the cluster nodes  and then prepares a    le that can be fed to an installer utility in parallel on the other nodes to complete the installation  Despite this simple process  we found that DBMS X was complicated to con   gure in order to start running queries  Initially  we were frustrated by the failure of anything but the most basic of operations  We eventually discovered each node   s kernel was con   gured to limit the total amount of allocated virtual address space  When this limit was hit  new processes could not be created and DBMS X operations would fail  We mention this even though it was our own administrative error  as we were surprised that DBMS X   s extensive system probing and self adjusting con   guration was not able to detect this limitation  This was disappointing after our earlier Hadoop successes  Even after these earlier issues were resolved and we had DBMSX running  we were routinely stymied by other memory limitations  We found that certain default parameters  such as the sizes of the buffer pool and sort heaps  were too conservative for modern systems  Furthermore  DBMS X proved to be ineffective at adjusting memory allocations for changing conditions  For example  the system automatically expanded our buffer pool from the default 4MB to only 5MB  we later forced it to 512 MB   It also warned us that performance could be degraded when we increased our sort heap size to 128 MB  in fact  performance improved by a factor of 12   Manually changing some options resulted in the system automatically altering others  On occasion  this combination of manual and automatic changes resulted in a con   guration for DBMS X that caused it to refuse to boot the next time the system started  As most con   guration settings required DBMS X to be running in order to adjust them  it was unfortunately easy to lock ourselves out with no failsafe mode to restore to a previous state  Vertica was relatively easy to install as an RPM that we deployed on each node  An additional con   guration script bundled with the RPM is used to build catalog meta data and modify certain kernel parameters  Database tuning is minimal and is done through hints to the resource manager  we found that the default settings worked well for us  The downside of this simpli   ed tuning approach  however  is that there is no explicit mechanism to determine what resources were granted to a query nor is there a way to manually adjust per query resource allocation  The take away from our efforts is that we found parallel DBMSs to be much more challenging than Hadoop to install and con   gure properly  There is  however  a signi   cant variation with respect to ease of installation and con   guration across the different parallel database products  One small advantage for the database systems is that the tuning that is needed is mostly done prior to query execution  and that certain tuning parameters  e g   sort buffer sizes  are suitable for all tasks  In contrast  for Hadoop we not only had totune the system  e g   block sizes   but we also occasionally needed to tune each individual task to work well with the system  e g   changing code   Finally  the parallel database products came with tools to aid in the tuning process whereas with Hadoop we were forced to resort to trial and error tuning  clearly a more mature MR implementation could include such tuning tools as well  5 1 2 Task Start up We found that our MR programs took some time before all nodes were running at full capacity  On a cluster of 100 nodes  it takes 10 seconds from the moment that a job is submitted to the JobTracker before the    rst Map task begins to execute and 25 seconds until all the nodes in the cluster are executing the job  This coincides with the results in  8   where the data processing rate does not reach its peak for nearly 60 seconds on a cluster of 1800 nodes  The    cold start    nature is symptomatic to Hadoop   s  and apparently Google   s  implementation and not inherent to the actual MR model itself  For example  we also found that prior versions of Hadoop would create a new JVM process for each Map and Reduce instance on a node  which we found increased the overhead of running jobs on large data sets  enabling the JVM reuse feature in the latest version of Hadoop improved our results for MR by 10   15   In contrast  parallel DBMSs are started at OS boot time  and thus are considered to always be    warm     waiting for a query to execute  Moreover  all modern DBMSs are designed to execute using multiple threads and processes  which allows the currently running code to accept additional tasks and further optimize its execution schedule  Minimizing start up time was one of the early optimizations of DBMSs  and is certainly something that MR systems should be able to incorporate without a large rewrite of the underlying architecture  5 1 3 Compression Almost every parallel DBMS  including DBMS X and Vertica  allows for optional compression of stored data  It is not uncommon for compression to result in a factor of 6   10 space savings  Vertica   s internal data representation is highly optimized for data compression and has an execution engine that operates directly on compressed data  i e   it avoids decompressing the data during processing whenever possible   In general  since analysis tasks on large data sets are often I O bound  trading CPU cycles  needed to decompress input data  for I O bandwidth  compressed data means that there is less data to read  is a good strategy and translates to faster execution  In situations where the executor can operate directly on compressed data  there is often no trade off at all and compression is an obvious win  Hadoop and its underlying distributed    lesystem support both block level and record level compression on input data  We found  however  that neither technique improved Hadoop   s performance and in some cases actually slowed execution  It also required more effort on our part to either change code or prepare the input data  It should also be noted that compression was also not used in the original MR benchmark  8   In order to use block level compression in Hadoop  we    rst had to split the data    les into multiple  smaller    les on each node   s local    le system and then compress each    le using the gzip tool  Compressing the data in this manner reduced each data set by 20   25  from its original size  These compressed    les are then copied into HDFS just as if they were plain text    les  Hadoop automatically detects when    les are compressed and will decompress them on the    y when they are fed into Map instances  thus we did not need to change our MR programs to use the compressed data  Despite the longer load times  if one includes the splitting and compressing   Hadoop using block level compression slowed most the tasks by a few seconds while CPU bound tasks executed 50  slower  We also tried executing the benchmarks using record level compression  This required us to  1  write to a custom tuple object using Hadoop   s API   2  modify our data loader program to transform records to compressed and serialized custom tuples  and  3  refactor each benchmark  We initially believed that this would improve CPU bound tasks  because the Map and Reduce tasks no longer needed to split the    elds by the delimiter  We found  however  that this approach actually performed worse than block level compression while only compressing the data by 10   5 1 4 Loading and Data Layout Parallel DBMSs have the opportunity to reorganize the input data    le at load time  This allows for certain optimizations  such as storing each attribute of a table separately  as done in column stores such as Vertica   For read only queries that only touch a subset of the attributes of a table  this optimization can improve performance by allowing the attributes that are not accessed by a particular query to be left on disk and never read  Similar to the compression optimization described above  this saves critical I O bandwidth  MR systems by default do not transform the data when it is loaded into their distributed    le system  and thus are unable to change the layout of input data  which precludes this class of optimization opportunities  Furthermore  Hadoop was always much more CPU intensive than the parallel DBMS in running equivalent tasks because it must parse and deserialize the records in the input data at run time  whereas parallel databases do the parsing at load time and can quickly extract attributes from tuples at essentially zero cost  But MR   s simpli   ed loading process did make it much easier and faster to load than with the DBMSs  Our results in Sections 4 2 1 and 4 3 1 show that Hadoop achieved load throughputs of up to three times faster than Vertica and almost 20 times faster than DBMS X  This suggests that for data that is only going to be loaded once for certain types on analysis tasks  that it may not be worth it to pay the cost of the indexing and reorganization cost in a DBMS   This also strongly suggests that a DBMS would bene   t from a    insitu    operation mode that would allow a user to directly access and query    les stored in a local    le system  5 1 5 Execution Strategies As noted earlier  the query planner in parallel DBMSs are careful to transfer data between nodes only if it is absolutely necessary  This allows the systems to optimize the join algorithm depending on the characteristics of the data and perform push oriented messaging without writing intermediate data sets  Over time  MR advocates should study the techniques used in parallel DBMSs and incorporate the concepts that are germane to their model  In doing so  we believe that again the performance of MR frameworks will improve dramatically  Furthermore  parallel DBMSs construct a complete query plan that is sent to all processing nodes at the start of the query  Because data is    pushed    between sites when only necessary  there are no control messages during processing  In contrast  MR systems use a large number of control messages to synchronize processing  resulting in poorer performance due to increased overhead  Vertica also experienced this problem but on a much smaller scale  Section 4 2   5 1 6 Failure Model As discussed previously  while not providing support for transactions  MR is able to recover from faults in the middle of query execution in a way that most parallel database systems cannot  Since parallel DBMSs will be deployed on larger clusters over time  the probability of mid query hardware failures will increase  Thus  for long running queries  it may be important to implement such a fault tolerance model  While improving the fault tolerance of DBMSs isclearly a good idea  we are wary of devoting huge computational clusters and    brute force    approaches to computation when sophisticated software would could do the same processing with far less hardware and consume far less energy  or in less time  thereby obviating the need for a sophisticated fault tolerance model  A multithousand node cluster of the sort Google  Microsoft  and Yahoo  run uses huge amounts of energy  and as our results show  for many data processing tasks a parallel DBMS can often achieve the same performance using far fewer nodes  As such  the desirable approach is to use high performance algorithms with modest parallelism rather than brute force approaches on much larger clusters  5 2 User level Aspects A data processing system   s performance is irrelevant to a user or an organization if the system is not usable  In this section  we discuss aspects of each system that we encountered from a userlevel perspective while conducting the benchmark study that may promote or inhibit application development and adoption  5 2 1 Ease of Use Once the system is on line and the data has been loaded  the programmer then begins to write the query or the code needed to perform their task  Like other kinds of programming  this is often an iterative process  the programmer writes a little bit of code  tests it  and then writes some more  The programmer can easily determine whether his her code is syntactically correct in b</doc>
    </owl:NamedIndividual>
</rdf:RDF>



<!-- Generated by the OWL API (version 3.2.5.1912) http://owlapi.sourceforge.net -->

