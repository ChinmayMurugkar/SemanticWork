<?xml version="1.0"?>


<!DOCTYPE rdf:RDF [
    <!ENTITY owl "http://www.w3.org/2002/07/owl#" >
    <!ENTITY xsd "http://www.w3.org/2001/XMLSchema#" >
    <!ENTITY owl2xml "http://www.w3.org/2006/12/owl2-xml#" >
    <!ENTITY rdfs "http://www.w3.org/2000/01/rdf-schema#" >
    <!ENTITY rdf "http://www.w3.org/1999/02/22-rdf-syntax-ns#" >
]>


<rdf:RDF xmlns="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#"
     xml:base="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"
     xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
     xmlns:owl2xml="http://www.w3.org/2006/12/owl2-xml#"
     xmlns:owl="http://www.w3.org/2002/07/owl#"
     xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
     xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    <owl:Ontology rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Datatypes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Object Properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a -->

    <owl:ObjectProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#has_a"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Data properties
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#age"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#disease"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#doc"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#id"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#medication"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#name"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sex"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms -->

    <owl:DatatypeProperty rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#synonyms"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Classes
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#DBLP_contents"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#books -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#books">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_mining -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_cleaning_and_mining">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_in_cloud -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_in_cloud">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_stream -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_stream">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#distributed_database -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#distributed_database">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#energy_performance -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#energy_performance">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#er">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#icde">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#incomplete_information_and_award -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#incomplete_information_and_award">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#journals">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#mining_and_complex_events -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#mining_and_complex_events">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#olap_and_decision_support -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#olap_and_decision_support">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#popl">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pubmed -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pubmed"/>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#series">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#bibliographies"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sigmod">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pods2011"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#conferences"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2009 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2009">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2010 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2010">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2011 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2011">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2012 -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb2012">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#vldb"/>
    </owl:Class>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#xml_and_semistructured_data -->

    <owl:Class rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#xml_and_semistructured_data">
        <rdfs:subClassOf rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#edbt2010"/>
    </owl:Class>
    


    <!-- http://www.w3.org/2002/07/owl#Thing -->

    <owl:Class rdf:about="&owl;Thing"/>
    


    <!-- 
    ///////////////////////////////////////////////////////////////////////////////////////
    //
    // Individuals
    //
    ///////////////////////////////////////////////////////////////////////////////////////
     -->

    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dhp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dhp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>LazyFTL  A Page level Flash Translation Layer Optimized for NAND Flash Memory ### Dongzhe Ma Jianhua Feng Guoliang Li Department of Computer Science and Technology Tsinghua University  Beijing 100084  P R  China mdzfirst yahoo com cn   fengjh  liguoliang  tsinghua edu cn ABSTRACT Flash is a type of electronically erasable programmable readonly memory  EEPROM   which has many advantages over traditional magnetic disks  such as lower access latency  lower power consumption  lack of noise  and shock resistance  However  due to its special characteristics     ash memory cannot be deployed directly in the place of traditional magnetic disks  The Flash Translation Layer  FTL  is a software layer built on raw    ash memory that carries out garbage collection and wear leveling strategies and hides the special characteristics of    ash memory from upper    le systems by emulating a normal block device like magnetic disks  Most existing FTL schemes are optimized for some speci   c access patterns or bring about signi   cant overhead of merge operations under certain circumstances  In this paper  we propose a novel FTL scheme named LazyFTL that exhibits low response latency and high scalability  and at the same time  eliminates the overhead of merge operations completely  Experimental results show that LazyFTL outperforms all the typical existing FTL schemes and is very close to the theoretically optimal solution  We also provide a basic design that assists LazyFTL to recover from system failures  Categories and Subject Descriptors D 4 2  Operating Systems   Storage Management   Secondary storage  B 7 1  Integrated Circuits   Types and Design Styles   Memory technologies  B 8 2  Performance and Reliability   Performance Analysis and Design Aids General Terms Design  Experimentation  Performance  Reliability     This work is partly supported by the National Natural Science Foundation of China under Grant No  60873065  the National Grand Fundamental Research 973 Program of China under Grant No  2011CB302206  and the National S T Major Project of China  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior specif c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  Keywords Flash translation layer  LazyFTL  address translation  garbage collection ### 1  INTRODUCTION Recent years have witnessed a rapid development of    ash technologies  Advantages of    ash memory  such as high density  low access latency  low power consumption  and shock resistance  greatly bene   t database systems and other dataintensive applications  However     ash memory cannot be deployed directly in the place of traditional magnetic disks due to its special characteristics  Like other EEPROM devices  if a page has been programmed  an erase operation needs to take place before new data can be written  To make things worse  the granularity of    ash erase operations is much larger than that of read and write operations  If an in place update is to be performed  we need to copy all valid pages in the corresponding block into the RAM  update the page in the RAM  erase the block  and write all valid pages back  This update method will not only degrade performance of the    ash memory  but also reduce the life span of the chip and bring a potential consistency problem  To solve these problems  out of place updates are adopted  That is  when a page is to be overwritten  we allocate a new free or erased page  put the new data there  and use a software layer called FTL to indicate the physical location change of the page  In addition  after about 10 000     100 000 erase write cycles  some blocks may become unstable and malfunction  Although a few blocks are reserved to replace broken ones  these extra blocks will eventually be exhausted  A technology named wear leveling is usually employed in the FTL to prolong the life span of    ash memory by distributing erase cycles across the entire memory  A lot of work from the database community focuses on designing e   cient index structures  such as BFTL  29      Tree  13   FlashDB  25   LA Tree  2   and FD Tree  22   Some of these technologies are built upon the FTL while others work directly on raw    ash memories  realizing functionalities of the FTL themselves  for example  address translation  garbage collection  and wear leveling  In all cases  the FTL is a crucial factor to all    ash based technologies  Besides  since    ash memories are purely electronic devices and have no moving parts  they have no seek or rotation latency like magnetic disks  Therefore  random access of    ash memory can be as fast as sequential access and the latency of    ash memory is almost linearly proportional to the amount of data being accessed  no matter where the data 1Table 1  Magnetic Disk vs  NAND Flash  19  Read Write Erase Magnetic Disk 12 7 ms 13 7 ms N A NAND Flash 80   s  2 KB  200   s  2 KB  1 5 ms  128 KB  is located  Another feature making    ash memories di   erent is that the write latency of    ash memory is usually several times larger than the read latency as shown in Table 1  we adopt the con   guration described in  19   since it takes longer to physically inject electrons into a storage cell than sense its status  Therefore  most    ash technologies tend to focus on optimization of write performance  even if increase read operations sometimes  Our study makes several contributions as follows      In this paper  we propose a novel FTL scheme named LazyFTL  which is optimized for NAND type    ash memories  To avoid the heavy overhead of merge operations in existing block level and hybrid mapping FTL schemes  LazyFTL employs a page level mapping table  This makes LazyFTL a high performance FTL scheme compared with other existing ones  However  a page level mapping scheme is hard to deploy on NAND type    ash memories since they can only be programmed in pages  If any part of the mapping table is immediately written in    ash memory whenever it is modi   ed  performance will be a   ected  If dirty data is kept in the SRAM and only written in    ash memory when it is swapped out  we risk losing critical information and leaving the system in an inconsistent state  To solve this problem  LazyFTL keeps two small areas in    ash memory and updates the page level mapping table in a lazy manner      We implement a trace driven simulator to help evaluate the performance of LazyFTL and six other typical FTL schemes  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  Our empirical evaluation demonstrates that LazyFTL outperforms all typical FTL schemes while achieving consistency and reliability at the same time  Experimental results also show that LazyFTL successfully approaches the theoretically optimal solution      We test and measure the performance of LazyFTL when the ratio of the capacity of    ash memory to that of the SRAM is increased  We discover that within a certain scope  LazyFTL can still achieve an excellent performance  This experiment indicates that the scalability of LazyFTL is high since it does not require that the capacity of the SRAM is enlarged as fast as    ash memory  We also analyze the reliability of LazyFTL theoretically and present an algorithm that assists LazyFTL to recover from system failures e   ciently  The rest of this paper is organized as follows  In Section 2  we make a short introduction of    ash memory and previous FTL designs  In Section 3  we provide an overview of the proposed LazyFTL scheme  A detailed description of the major functionalities of LazyFTL is given in Section 4 and Section 5 de   nes the states of pages and demonstrates the transition of states using a simple example  Experimental results are presented and analyzed in Section 6  We also analyze the scalability and reliability issues in Section 6  Finally  Section 7 draws some conclusions and directions for future work  2  BACKGROUND 2 1 Introduction of Flash Memory There are two types of    ash memories  namely NOR and NAND  NOR provides independent address and data buses  allowing random access to any location of the memory  which makes NOR a perfect replacement of the traditional readonly memory  ROM   such as the BIOS chip of computers  On the other hand  address and data share the same I O interface in NAND  which means that NAND can only be accessed in pages  though it has a higher density and lower cost per bit than NOR  NAND is usually used as a secondary storage device  1   In the rest of this paper  we use the term    ash to refer to NAND type    ash memory unless we explicitly indicate NOR type    ash memory  Each    ash chip consists of a constant number of blocks that are basic units of erase operations  And each block consists of a constant number of pages that are basic units of read and write operations  Most    ash memories also provide a spare area for each page to store out of band  OOB  data  such as the error correction code  ECC   the logical page number  and the state    ag for the page  As technology advances  di   erent    ash memory organizations have been developed as shown in Table 2  Table 2  Organization of Flash Chips  23  28  13  Block Size Page Size OOB Size Small block SLC 16 KB 512 bytes 16 bytes Large block SLC 128 KB 2 KB 64 bytes Large block MLC 1 512 KB 4 KB 128 bytes 2 2 Overview of FTL According to the granularity of the mapping unit  existing FTL schemes can be divided into four categories  page level  block level  hybrid  and variable length mappings  Just as the name implies  in page level FTLs  a logical page number  LPN  can be directly translated to a physical page number  PPN   In other words  page level FTLs need to maintain a mapping entry for every logical page  which means that the mapping table of page level FTLs is much larger than any other types  In fact  all page level FTL schemes store the entire mapping table in    ash memory and load the currently used parts into the SRAM dynamically using the LRU algorithm or some other strategy  However  since hot and cold data can be easily separated  page level FTLs are quite e   cient and    exible  On the contrary  in block level FTLs  an LPN is    rst divided into a logical block number  LBN  and an in block o   set  Then the LBN is translated to a physical block number  PBN  and    nally some search algorithm is employed to    nd the target page  It is obvious that the mapping table of block level FTLs is quite small and can be easily stored in the SRAM  Nevertheless  due to the mixture of hot and cold data and the overhead of moving valid pages during garbage collection  the performance of block level FTL schemes is limited compared with other mapping methods  Hybrid mapping schemes try to achieve the    exibility of page level FTLs while keeping the mapping table relatively small and comparable to the block level methods by dividing the    ash memory into a data block area  DBA  and a log block area  LBA   Block level mapping is applied to the 1 An MLC device is capable of storing more than one bit of information in a single storage cell  2Figure 1  Three Types of Merge Operations DBA which occupies most of the    ash memory while each valid page in the LBA is traced by another page level mapping  The LBA is very small and generally takes less than 5 percent of the entire    ash memory  In hybrid FTLs  except HFTL  17    the LBA is used to store overwriting data and di   erent schemes adopt di   erent strategies to merge data in the LBA to the DBA to generate new space for the LBA  There are three types of merge operations as illustrated in Figure 1  A full merge is a general but expensive operation in which all up to date pages need to be copied to a new allocated block and then old blocks are erased and put back into the free block pool  The partial and switch merges are e   cient but can only be done in special cases since they can only be done when pages in the log block or the replacement block are all free or valid and each valid page is written in their own place  Although many hybrid FTL schemes try to do partial or switch merges whenever possible  full merges are di   cult to avoid with di   erent access patterns  This makes an insuperable bottleneck for all hybrid FTL schemes  It is also possible to map variable length continuous logical pages to continuous physical pages in    ash memory  In this case  granularity can be adjusted dynamically when access pattern changes  However  since sizes of di   erent mapping units are not identical and are changing  mapping entries can only be stored in some type of search tree  and as a result  the table look up overhead of variable length mappings is higher than other schemes of which the mapping table is nothing more than a simple address array  2 3 Page level FTL Schemes The    rst FTL scheme was patented by Ban in 1995  3  and was adopted by the PCMCIA as a standard for NORbased    ash memories several years later  12   There is one issue that NOR based FTLs should handle in the    rst place  When a page is overwritten  the relevant entry in    ash memory needs to be updated to keep the operation atomic and reliable   Remember that page level FTL schemes keep an entire mirror of the mapping table in    ash memory to reduce the SRAM overhead   This presents no di   culty to the NOR based FTL since NOR type    ash memories can be programmed in bytes  By assigning a replacement page list for the relevant mapping page when necessary  this mapping page can be updated  written in the    rst free entry of the same o   set in the replacement page list  several times as long as the length of the list without rewriting the entire mapping page  12  9   DFTL  Demand based FTL   10   another page level FTL scheme  makes the    rst attempt to transfer the former NORbased FTL to NAND type    ash memories  omitting the replacement page part  This scheme  though e   cient  faces a serious reliability problem since all modi   ed information in the SRAM will be lost if a system failure occurs  In this case  spare areas of all data pages need to be scanned until the system recovers to a consistent state  Therefore  DFTL is not suitable  we believe  for circumstances where    ash memory is regarded as a permanent and reliable storage device  2 4 Block level FTL Schemes Ban patented two other FTL schemes in 1999  4  8  9   These schemes are designed for NAND type    ash memories and also known as the NFTLs  In this paper  they will be cited as NFTL 1 and NFTL N  NFTL 1 is designed for    ash memories that have a spare area for each page and NFTL N is for devices without such storage  When a page is overwritten  NFTL 1    rst allocates a replacement block for the relevant logical block if there is none and writes overwriting pages one after another from the beginning of the replacement block  Since pages are written in an out of place manner in replacement blocks  NFTL 1 needs to scan all the spare areas in the replacement block in reversed order to    nd the most up to date version of a requested page  Fortunately  the spare areas in NAND type    ash memory are using a di   erent addressing algorithm that is optimized for fast reference and the overhead of this search process is relatively low  On the other hand  since some models of NAND    ash memories have no spare areas to support fast search  NFTL N keeps a replacement block list for some of the logical blocks when necessary and write requests for each logical page are    rst handled by the    rst block in the list and then the next one  keeping the in block o   set identical with that of the logical address  If all pages in the list with the request o     set have been programmed  a new block is allocated and appended to the back of the list  2 5 Hybrid FTL Schemes BAST  Block Associative Sector Translation  is the    rst hybrid FTL scheme proposed in 2002  15   which is essentially an altered version of NFTL 1  As mentioned earlier  hybrid FTL schemes build a page level mapping for the LBA  To keep this table small enough to reside in the SRAM  BAST limits the total number of replacement blocks  also known as log blocks   Obviously  the read performance of BAST is better than NFTL 1 because the SRAM is several orders of magnitude faster than    ash memories  However  BAST does not work well with random overwrite patterns which may result in a block thrashing problem  20   Since each replacement block can accommodate pages from only one logical block  BAST can easily run out of free replacement blocks and be forced to reclaim replacement blocks that have not been    lled  Therefore  the utilization ratio of replacement blocks in BAST is low both theoretically and experimentally  To solve the block thrashing problem  another hybrid FTL scheme named FAST  Fully Associative Sector Translation  was put forward  20   FAST goes to the other extreme by allowing a log block to hold updates from any data block  Although FAST successfully delays garbage collections as much as possible  the system wide latency for reclaiming a single log block may turn out to be longer than BAST  since the associativity of log blocks is only limited by the number of pages in a block  The associativity of a log block is de   ned as the number of di   erent data blocks whose most up to date pages are located in the log block  The higher the associativity of a log block is  the more expensive it is to 3reclaim it  To increase the proportion of partial and switch merges  FAST reserves a sequential log block to perform sequential updates  This optimization is also limited since in modern multi process environments  a sequential write is often interrupted by random writes and other sequential writes  18   In the following years  researchers tried to    nd some intermediate proposals to balance between the log block utilization and the reclamation overhead  There are some typical representatives such as Superblock FTL  14   SAST  SetAssociative Sector Translation   26   LAST  Locality Aware Sector Translation   18   and A SAST  Adaptive SAST   16   Both Superblock FTL and SAST share  at most  K log blocks among N data blocks  The di   erence is that Superblock FTL keeps a page level map in the spare areas of the superblock while SAST restricts the number of log blocks and maintains the page level map in the SRAM  Due to the size limitation of spare areas  Superblock FTL needs to search at most three spare areas to    nd a requested page  And in SAST  di   erent data block sets may compete for log blocks as a result of the small LBA  A common problem with these two schemes is that they both need to be tuned beforehand  which means that their performance may get worse if access pattern changes  Unlike Superblock FTL and SAST  LAST divides the LBA into several functional segments to fully utilize the log blocks while keeping the reclamation overhead as low as possible  Longer requests are written in the sequential log bu   er to perform partial or switch merges  Hot data that might be overwritten soon is written in the hot partition of the random log bu   er and other write requests are served by the cold partition  A SAST is an optimized version of SAST which loosens the restriction of maximum number of log blocks shared within a data block set and can merge and split data block sets dynamically  KAST  K Associative Sector Translation   6  is the same as FAST in essence but requires that the associativity of all log blocks should never exceed K  The scheme is designed for real time systems since its reclamation latency is controllable  KAST can be considered as another tradeo    between the log block utilization and the reclamation overhead  Unlike other hybrid schemes  HFTL  Hybrid FTL   17  does not treat the page mapping area as a bu   er of updates  Instead  HFTL employs a hash based hot data identi   cation technique  11  and traces pages from hot blocks with the page level mapping as long as they remain hot  However  when access pattern changes  some hot pages will need to be swapped out  which will introduce an extra overhead  2 6 Other FTL Schemes It is also possible to implement a variable length mapping  One such scheme was proposed in 2004  5  and in 2008 another one named    FTL  which adopts    Tree  13  as the mapping structure  was published  21   The main disadvantage of these schemes is the address translation cost since variable length mappings can only be implemented in search trees  JFTL  proposed in 2009  is a technique to e   ectively deploy journal    le systems on    ash memory using the outof place characteristic  7   which can be built on any other FTL schemes  However  JFTL cannot do anything about the consistency problem of DFTL  Figure 2  Architecture of LazyFTL 3  LAZYFTL OVERVIEW 3 1 Design Principles After explaining the merits and demerits of di   erent types of existing FTL schemes in Section 2  some design principles and considerations will be presented at the beginning of this section  First of all  a storage system should guarantee the reliability of its operations  therefore dirty or altered data should be    ushed into    ash memory before an operation can return  DFTL violates this rule in order to obtain high performance  Although the system can recover by scanning the spare area of all pages  the resulting bootup delay is unacceptable along with the increase of the density and the capacity of    ash memories  To design a highly e   cient FTL scheme  the mapping granularity should be decided in the    rst place  Among all the FTL schemes discussed in Section 2  the block level mapping cannot distinguish cold data from hot ones and has to move cold data unnecessarily during the garbage collection procedure  The variable length mapping can adjust its mapping granularity dynamically but the high complexity of address translation makes an inherent weakness  The hybrid mapping is feasible since costly full merge operations can be avoid as much as possible by partitioning the LBA or by sharing log blocks  However  no matter how subtly they are designed  hybrid mapping schemes can hardly eliminate full merge operations completely  The page level mapping is the most e   cient and e   ective mapping granularity but can hardly be applied to NAND    ash without violating the    rst rule  This is not true however  The LazyFTL scheme proposed in this paper proves that by adopting an update bu   er  like the LBA in hybrid mapping schemes  the pagelevel FTL can be transferred to NAND    ash while keeping reliability and consistency at the same time  3 2 LazyFTL Architecture The architecture of the proposed LazyFTL scheme is presented in Figure 2  As illustrated  LazyFTL divides the entire    ash memory into four parts  a data block area  DBA   a mapping block area  MBA   a cold block area  CBA   and an update block area  UBA   All these parts except the MBA are used to store user data  Pages in the DBA are tracked by a page level mapping table called the global mapping table  GMT   The GMT is organized in pages and stored in the MBA  A small cache adopting the LRU algorithm or similar is reserved in the SRAM to provide e   cient reference of the most frequently 4accessed parts of the GMT  A secondary table named the global mapping directory  GMD  is stored in the SRAM and keeps physical locations of all valid mapping pages of the GMT  The CBA is used to accommodate cold blocks and the UBA is used to accommodate update blocks as the names indicate  The main di   erence between LazyFTL and the original page level FTL scheme  3  12  is that LazyFTL reserves two small partitions  the CBA and the UBA  to delay modi     cations of the GMT caused by write requests or valid page movements  The total size of the CBA and the UBA is relatively small compared with the entire    ash memory and  like the LBAs in hybrid FTL schemes  another page level mapping table which is called the update mapping table  UMT  is built on these two areas  The UMT can be implemented as a hash table or a binary search tree to support e   cient insertion  deletion  modi   cation  and reference  The number of entries in the UMT is quite small  so these operations will not introduce too much overhead  A block in the UBA called the current update block  CUB  is used to handle write operations  When the CUB over     ows  another free block is allocated and becomes the new CUB  Similarly  there is a current cold block  CCB  in the CBA dealing with moved data pages  As a matter of fact  LazyFTL treats    lled cold blocks in the CBA and    lled update blocks in the UBA in the same way  In other words  the relative size of the CBA and the UBA can be adjusted dynamically  If the proportion of hot data rises  the convert cost  see 4 1  of blocks in the UBA will decrease more slowly and the UBA will expand  If space utilization increases  the CCB will be    lled faster than the CUB and the CBA will be enlarged  In this way  LazyFTL can tune itself automatically for di   erent access patterns  It is necessary to mention that it is also feasible to divide the CBA and the UBA into smaller functional segments like LAST  18   However  we decide to keep the design as simple as possible since the current design is quite e   cient and there is no room for performance improvement  We also maintain two bitmaps in the SRAM  the update    ag and the invalidate    ag  These two bitmaps help mark the states of all pages in the CBA and the UBA  Each bit in the update    ag indicates whether the translation information of the corresponding page needs to be updated to the GMT  And each bit in the invalidate    ag indicates whether the target page that the corresponding GMT entry points to needs to be invalidated  4  MAJOR FUNCTIONALITIES 4 1 Convert Operation Since the UMT is stored in the SRAM to support e   cient reference  the CBA and the UBA cannot be too large and will eventually over   ow  in which case  a convert operation is carried out  In hybrid FTL schemes  a merge operation needs to copy valid pages in the victim log block out of the LBA and reorganize relevant data blocks most of the time due to the in place storage pattern in the DBA  However  LazyFTL only has to convert the victim block to a normal data block logically since pages in the DBA are also stored in an out of place manner  The only overhead of the convert operation is caused by the GMT updates which will be proved to be much cheaper than reorganizing data blocks  A convert operation is achieved in four steps as illustrated in Algorithm 1  First  a    lled block in the CBA or the UBA Algorithm 1 Convert block B Input  B  a victim block in the CBA or the UBA Output  B  a normal data block 1  mapping pages         2  update entries            Gather relevant information    3  for each valid page P in B do 4  E      LPNP  PPNP  5  remove E from the UMT 6  if the update    ag of P is set then 7  P            LPNP   number of entries per page    8  mapping pages     mapping pages      P       9  update entries     update entries      E  10  end if 11  end for    Gather entries that can also be updated    12  for each entry E     in the UMT do 13  if the relevant mapping page     mapping pages and the update    ag of E     is set then 14  update entries     update entries      E       15  the update    ag of E         0 16  end if 17  end for    Make sure that each page is loaded only once    18  sort update entries by LPN    Update the GMT and invalidate old pages    19  for each entry E            update entries do 20  load the relevant mapping page P        if necessary 21  o   set     LPNE       mod number of entries per page 22  if the invalidate    ag of E        is set then 23  invalidate P         o   set  24  the invalidate    ag of E            0 25  end if 26  P         o   set      PPNE       27  if no more updates to P        then 28  write P        to the MBA 29  update the GMD 30  invalidate the old page of P        31  end if 32  end for with the lowest convert cost is selected as the victim  The convert cost of each candidate block is de   ned as the number of di   erent mapping pages that valid pages in this block whose translation information need to be updated to the GMT belong to  Second  all relevant mapping pages are found and all mapping entries in the UMT that belong to these mapping pages are collected  including entries pointing to other blocks in the CBA and the UBA  Then modi   cations of the mapping pages are performed  Finally  mapping entries in the UMT that point to the victim block are removed and the victim block is converted to a normal data block logically  One thing that should take our attention is that for the sake of e   ciency an entry in the UMT is removed only when the block where the target page is located is converted  no matter whether this entry is updated in that operation  In other words  all valid pages in the CBA and the UBA are tracked by the UMT  even if some of them have already been updated when other blocks in the CBA or the UBA are converted  As mentioned earlier  to help identify pages whose physical locations have not been updated to the GMT  an update 5Algorithm 2 Reclaim block B Input  B  a victim block in the DBA or the MBA Output  B  a free block 1  if B is a mapping block then 2  for each valid page P in B do 3  move P to the MBA 4  end for 5  else 6  for each valid page P in B do 7  if LPNP can be found in the UMT then 8  the invalidate    ag of UMT LPNP      0 9  else 10  if the CCB is    lled up then 11  if the UBA   CBA are    lled up then 12  select a victim block and convert it 13  end if 14  allocate a new block for the CCB 15  end if 16  move P to the CBA 17  add  LPNP  PPNP  to the UMT 18  the update    ag of P     1 19  the invalidate    ag of P     0 20  end if 21  end for 22  end if 23  erase B 24  put B into the free block pool    ag bitmap is maintained in the SRAM  Each bit in this bitmap is related to a page in the CBA or the UBA  If the update    ag of a page is 1  we should modify the corresponding entry in the GMT whenever possible and at least before the block this page belongs to is converted  When a page is written to the UBA or moved to the CBA  its physical location changes which means that the initial update    ag of all pages should be 1  And after the relevant GMT entry of a page in the CBA or the UBA is updated or if it is overwritten  its update    ag should be cleared  4 2 Garbage Collection When the number of free blocks decreases to a prede   ned threshold  a victim block from the DBA or the MBA is selected to be erased  The cost to reclaim a certain block B can be de   ned as  Cread   Cwrite      NB   Cerase where Cread  Cwrite  and Cerase indicate the    ash read  write  and erase operation latencies  respectively and NB represents the number of valid pages in block B  Obviously  to reduce the overhead of garbage collection process  the block with the lowest reclaimation cost should be selected as the victim most of the time  After the victim block is chosen  all pages of this block should be scanned and the valid ones should be moved to some other block  If the victim block stores mapping pages of the GMT  valid pages should be moved to a current mapping block  CMB  in the MBA that handles mapping page rewriting and the GMD is modi   ed to track these changes  If the victim block stores user data  the valid pages should be moved to the CCB in the CBA  The philosophy is that these valid pages should be relatively colder than the invalid ones  Note that there are two cases indicating a data page is invalid  If this page has been overwritten and the new verAlgorithm 3 Write page P Input  P  new data to be written 1  if the CUB is    lled up then 2  if the UBA   CBA are    lled up then 3  select a victim block and convert it 4  end if 5  allocate a new block for the CUB 6  end if 7  write P in the UBA    Set the update    ag    8  the update    ag of P     1    Inherit or set the invalidate    ag    9  if LPNP can be found in the UMT then 10  P         UMT LPNP  11  the invalidate    ag of P     the invalidate    ag of P     12  invalidate P     13  the update    ag of P         0 14  the invalidate    ag of P         0 15  else 16  the invalidate    ag of P     1 17  end if 18  add  LPNP  PPNP  to the UMT sion is still located in the UBA  the spare area of this page may have not been marked  However  if the new version has been converted  the old page in the DBA should have been invalidated  Therefore  when the state    ag in the spare area indicates that a page is valid  we should further check whether its LPN can be found in the UMT  If this page has been overwritten  we should ignore it  and at the same time  we should clear the invalidate    ag of the most up to date page since the target that the corresponding GMT entry points to has been erased and will be used to accommodate other data  It is seriously wrong to invalidate an empty page or an innocent one  After all the valid pages have been moved  the victim block is erased and put into the free block pool again  An algorithmic description of garbage collection operations is presented in Algorithm 2  4 3 Write Operation The write operation of LazyFTL is much simpler than the convert operation and the garbage collection operation  We only need to write the new data in the UBA and do some bookkeeping  That is to say  we set the update    ag  inherit or set the invalidate    ag  invalidate the old page in the CBA or the UBA if there is one  and clear its two    ags  The pseudo code of the write operation is given in Algorithm 3  5  STATE TRANSITION 5 1 State Def nition As described earlier  the update and invalidate    ags represent the state of the corresponding page in the CBA or the UBA  By taking pointers in the GMT and the UMT with the same LPN into consideration  we can    gure out all possible states a page may have in LazyFTL  To help readers understand the di   erent page states and their transition paths and conditions  a state transition diagram is given in Figure 3  In this diagram  some states have a two digit binary number on its upper right corner  The    rst digit stands for the update    ag and the second one is the invalidate    ag  Invalid pages are represented by a small 6Figure 3  State Transition of Pages square with a cross inside  such as in state G and H  and those that are pointed neither by the GMT nor by the UMT are omitted  In state F  a pointer in the GMT is pointing to nothing since its target has been moved to the CBA and the block has been erased  Transition conditions are labeled on the path  WRITE means a write operation  GC means the block this page is located in is reclaimed  CONVERT means the corresponding block is converted and UPDATE means that some other block is converted and the entry of this page in the UMT is updated  Among the eight states in Figure 3  state C is the updated state since all update paths arrive at C  When a block is reclaimed  the relevant pages should be in reclaimed state F  All CONVERT paths except one point to D quali   es state D as the converted state  Between state H and state G  there is a path labeled as CONVERT which seems to violate the rule  This is a conversion of another block that contains a valid page which needs to be updated and shares the same mapping page with the one in our discussion  and therefore  this convert operation is di   erent from others  5 2 An Example of State Transition In this section  we give an example to help readers understand how the update and invalidate    ags are manipulated  We will start from a page that has never been written and follow its state transitions as illustrated in Figure 4      A     B As demonstrated in Algorithm 3  the update    ag of each new written page is set as 1 and since no previous written page is found in the UBA or the CBA  the initial value of the invalidate    ag is also set as 1      B     D When the corresponding block is selected as the convert victim  address translation information needs to be updated to the GMT since the update    ag is set  However  no page needs to be invalidated though the invalidate    ag is set since the relevant entry in the GMT has not been used      D     E This operation is similar to the    rst write request  The only thing we should pay attention to is that we do not try to alter the GMT entry or invalidate the old page at this time      E     F In this operation  a data block in the DBA where the old page is located is reclaimed  To tell whether a page is valid  we should    rst check the state    ag in its spare area  If the    ag indicates a valid page  we should further check whether this page has been overwritten in the UBA  In this case  the same LPN is found in the UMT  meaning that this page has been overwritten and thus should be discarded  Meanwhile  the invalidate    ag of the up to date page is cleared since the old GMT entry is currently pointing to an erased page which should not be invalidated again      F     C This time  some other block is converted and the mapping information of this page is updated in passing  Do not forget to clear the update    ag      C     H This page is overwritten again  Unlike the third operation  the old page is invalidated immediately after new data is written in  Note that invalidate    ag of the new page is cleared not because the old page has just been invalidated but because its invalidate    ag is not set  This is so called the inheritance of invalidate    ags in LazyFTL      H     G The block that holds the old page is converted and nothing needs to be done      G     F Another block related to the current page is reclaimed  This time  the old page is already invalidated and there is no need to check the UMT  The di   erence is in the    rst state F of Figure 4  the invalidate    ag is cleared by the GC operation  while in the other state F  the    ag is unchanged      F     D Finally  the block that holds the up to date page is converted and we need to modi   ed the GMT entry but do not try to invalidate the page that the old entry points to just as the two    ags indicate  6  PERFORMANCE EVALUATION To help evaluate the performance and understand other characteristics of the proposed LazyFTL scheme  we implement a trace driven simulator for LazyFTL  For comparison  we also implement six other FTL schemes that are comparable with LazyFTL  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  6 1 Experimental Setup The simulator is built on a large block SLC    ash  see Table 2  which is widely used in enterprise grade    ash memories  The capacity is 1 GB and the access latencies are set as Table 1  7Figure 4  An Example of State Transition We use the Microsoft Research Cambridge block I O traces as described in  24   These traces are collected from 13 enterprise servers for di   erent applications  such as user home directories  print server     rewall web proxy  source control  web SQL server  and media server  which should cover all major access patterns  We have tested all the 36 traces in the package and obtained similar results  The space utilization of the adjusted traces varies from almost empty to 82 87   And the relative standard deviation  RSD  of the numbers of accesses of all touched addresses ranges from 35 12  to 2526 16   The larger the RSD value is  the more frequently hot data in the trace is accessed  It is necessary to point out that though our experiment touches no more than 82  of the address space  the relative performance of LazyFTL will not degrade if the device is    lled up  In fact  we believe that the performance gap between LazyFTL and other existing schemes will expand because other FTLs do not fully utilize every page in a block and a higher space utilization means more frequent garbage collection calls for them  The results presented in this paper were obtained by using trace usr 2 csv  The trace is scaled down to    t our 1 GB    ash memory and 75 9  of the entire address space is touched  with an RSD of 193 60   The largest request size is 256 pages  which are equivalent to 4 blocks or 0 5 MB  However  most requests involve less than 32 pages  We also implement six comparative experiments  namely NFTL 1  NFTL N  BAST  FAST  LAST  and A SAST  All these schemes are typical block level or hybrid mapping FTL schemes that focus on optimization of average access performance and do not have to be tuned for speci   c access patterns  We do not try to compare LazyFTL with DFTL  another NAND based page level FTL scheme  On one hand  DFTL has a consistent disadvantage  and on the other  the performance of LazyFTL and DFTL should be similar since neither of them can overcome the theoretical barrier  Suppose that 4 bytes are used to store a single page address or block address 2   then it takes    1 GB    128 KB    4 bytes      32768 bytes or 16 pages in the SRAM to store the block level mapping table in comparative experiments  Another 32768 bytes are allocated in the SRAM to accommodate the page level mapping table of hybrid FTL schemes  which means that    32768 bytes    4 bytes      8192 pages or 128 blocks can be assigned to the LBA  These 128 blocks take only 1 56  of the entire    ash memory  To keep the results comparable  the total size of the UBA and the CBA of LazyFTL is also limited to 128 blocks and at most 14 pages  another 2 pages are used to store the GMD  of the GMT can be cached in the LRU cache in the SRAM  Other data structures are either small or employed in all the schemes and therefore are not considered  All implemented schemes adopt the greedy strategy to 2 The block address is shorter than the page address and may be stored in less than 4 bytes  select garbage collection victims  NFTL 1 selects the block which has the most used pages in its replacement block  NFTL N selects the block which has the longest replacement block list  All hybrid FTL schemes and LazyFTL select the block that has the least valid pages since these pages need to be moved and thus are considered as the overhead of the garbage collection  Some may argue that by taking the access pattern into consideration  we may    gure out which free page is going to be used or which valid page is going to be invalidated  Garbage collection strategy is not within our discussion  however  and we only choose a block that has the least overhead or the most pro   t for a single operation as the victim  In addition  wear leveling is omitted in all our implements because the wear leveling mechanism is relatively independent from other components  Wear leveling involves many issues  such as how to identify worn out blocks  which blocks to reclaim and where to put valid data  Upper applications and the system architechture should also be taken into consideration  If multi process is supported  garbage collection and wear leveling can be carried out in the background without interrupting other operations  However  in embedded environments or real time systems these functions can only be done on demand  since the background way is either impossible or inacceptable  respectively  All in all  wear leveling is another interesting research topic and many existing works have studied this problem  It is unrealistic to permute all the combinations and di   cult to    nd a representative strategy  Another reason why wear leveling is omitted is to provide a clear view of the performance comparison of di   erent FTLs  For the sake of wear leveling  some data will need to be moved from cold blocks to worn out ones on purpose  which will introduce many noise operations  Since our paper focuses on address translation and data organization  it is better not to be distracted from other components  We believe that an identical wear leveling strategy will similarly in   uence all the schemes and will not a   ect our simulation results  To help evaluate the possibility of further improvement of the proposed LazyFTL  we also compare LazyFTL with the theoretically optimal solution  That is to say  each page read request causes a single page read operation  each page write request causes a single page write operation and a block erase operation is invoked every 64 page write operations  When implementing NFTL N  we discover that in the beginning  the response time for write requests decreases quickly when the length limit of replacement block lists is enlarged  However  after a certain point around 7  the write performance becomes stable and constant  Another issue that surprised us is that it seems that the search cost of read requests does not increase much when the limit is relaxed  This is probably because when a certain proportion of    ash memory is used  the replacement block list has little 8Figure 5  Comparison with Existing Schemes chance to get longer before it is reclaimed  even though the limit has been enlarged  In our experiment  the maximum length of the replacement block lists is set to 16  Tuning for LAST is relatively complex  In our experiments  8 blocks are assigned to the sequential log block area and the other 120 blocks in the LBA are random log blocks  The threshold for the hot partition utility and that of the cold partition are set to 0 2 and 0 9 respectively  Other schemes do not need special tuning and will not be presented in this section  6 2 Comparison with Other Schemes The results of our simulation are shown in Figure 5  We will focus on four parameters  the block utilization which indicates the average number of pages that have been used when a block is erased  the total time  the write response time  and the number of erase operations which will directly a   ect the life span of the    ash memory  Among all the implemented schemes  the performance of NFTL 1 is acceptable compared with NFTL N and BAST and its simplicity makes a great selling point  However  the read performance is inferior to other schemes since it needs to search in the replacement block to    nd a certain page  In our experiment  17 565 spare areas need to be scanned on average to serve a one page read request  NFTL N is designed for NAND    ash that does not have a spare area for each page  As a result  the block utilization ratio of NFTL N is very low since pages can only be written in an in place manner  This also implies that NFTL N needs to perform more erase operations than other schemes as illustrated in Figure 5 d   BAST is the    rst hybrid FTL scheme that tries to avoid the search overhead when reading a page  Due to the block thrashing problem  the block utilization ratio of BAST is also very low compared with other hybrid FTL schemes and the performance is worse than any other scheme including NFTL N  That is because NFTL N can always perform partial or switch merges while BAST needs to perform full merge almost all the time  In our experiment  we can hardly    nd any chance for BAST to perform a much cheaper partial or switch merge  The simulation results of the other three hybrid FTLs are nearly the same  A SAST is better than LAST  which is in turn better than FAST  We notice that the performance of FAST is very close to that of NFTL 1  Although FAST tries to delay merge operations as much as possible and reserves a sequential log block to perform partial and switch merges  the advantages gained are counteracted by the heavy overhead of full merges  LAST and A SAST successfully make a tradeo    between the log block utilization and the reclamation overhead  Experimental results indicate that LAST and A SAST achieve a much higher log block utilization than BAST and a much lower write response time than FAST at the same time  As Figure 5 shows  the performance of LazyFTL is much better than those of other schemes and is very close to the optimal result  First of all  LazyFTL does not have to reclaim a block before it is    lled  This implies that the block 9Figure 6  Convert Cost and Scalability of LazyFTL utilization of LazyFTL always equals the number of pages in a block  which also means that LazyFTL needs to perform fewer erase operations than other FTL schemes  If a proper wear leveling strategy is employed and the erase operations are distributed through the entire memory  LazyFTL will also prolong the life span of the    ash chip  One may argue that although LazyFTL successfully avoids merge operations  it has to convert a victim block in the UBA or the CBA when these two areas over   ow  Nevertheless  Figure 6 a  indicates that in current con   guration  LazyFTL needs to rewrite only a small number of mapping pages  We can also see from Figure 5 c  that LazyFTL has a much lower response latency for write requests than other schemes  This issue will be further d</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining"/>
        <doc>Advancing Data Clustering via Projective Clustering Ensembles ### Francesco Gullo DEIS Dept  University of Calabria 87036 Rende  CS   Italy fgullo deis unical it Carlotta Domeniconi Dept  of Computer Science George Mason University 22030 Fairfax     VA  USA carlotta cs gmu edu Andrea Tagarelli DEIS Dept  University of Calabria 87036 Rende  CS   Italy tagarelli deis unical it ABSTRACT Projective Clustering Ensembles  PCE  are a very recent advance in data clustering research which combines the two powerful tools of clustering ensembles and projective cluster  ing  Speci cally  PCE enables clustering ensemble methods to handle ensembles composed by projective clustering so  lutions  PCE has been formalized as an optimization prob  lem with either a two objective or a single objective func  tion  Two objective PCE has shown to generally produce more accurate clustering results than its single objective counterpart  although it can handle the object based and feature based cluster representations only independently of one other  Moreover  both the early formulations of PCE do not follow any of the standard approaches of clustering en  sembles  namely instance based  cluster based  and hybrid  In this paper  we propose an alternative formulation to the PCE problem which overcomes the above issues  We investigate the drawbacks of the early formulations of PCE and de ne a new single objective formulation of the prob  lem  This formulation is capable of treating the object  and feature based cluster representations as a whole  essentially tying them in a distance computation between a projective clustering solution and a given ensemble  We propose two cluster based algorithms for computing approximations to the proposed PCE formulation  which have the common merit of conforming to one of the standard approaches of clustering ensembles  Experiments on benchmark datasets have shown the signi cance of our PCE formulation  as both the proposed heuristics outperform existing PCE methods  Categories and Subject Descriptors H 3 3  Information Storage and Retrieval   Information Search and Retrieval clustering  I 2 6  Arti cial Intelli  gence   Learning  I 5 3  Pattern Recognition   Cluster  ing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  General Terms Algorithms  Theory  Experimentation Keywords Data Mining  Clustering  Clustering Ensembles  Projective Clustering  Subspace Clustering  Dimensionality reduction  Optimization 1 ### INTRODUCTION Given a set of data objects as points in a multi  dimensional space  clustering aims to detect a number of homogeneous  well separated subsets  clusters  of data  in an unsupervised way  18   After more than four decades  a considerable corpus of methods and algorithms has been developed for data clustering  focusing on di erent aspects such as data types  algorithmic features  and application tar  gets  14   In the last few years  there has been an increased interest in developing advanced tools for data clustering  In this respect  clustering ensembles and projective clustering represent two of the most important directions of research  Clustering ensemble methods  28  13  36  29  17  aim to ex  tract a  consensus  clustering from a set  ensemble  of clus  tering solutions  The input ensemble is typically generated by varying one or more aspects of the clustering process  such as the clustering algorithm  the parameter setting  and the number of features  objects or clusters  The output con  sensus clustering is usually obtained using instance based  cluster based  or hybrid methods  Instance based methods require a notion of distance measure to directly compare the data objects in the ensemble solutions  cluster based meth  ods exploit a meta clustering approach  and hybrid methods attempt to combine the  rst two approaches based on hybrid bipartite graph clustering  Projective clustering  32  35  30  34  aims to discover clusters that correspond to subsets of the input data and have di erent  possibly overlapping  dimensional subspaces associated with them  Projected clusters tend to be less noisy because each group of data is represented in a sub  space that does not contain irrelevant dimensions and more understandable because the exploration of a cluster is eas  ier when few dimensions are involved  Clustering ensembles and projective clustering hence ad  dress two major issues in data clustering distinctly  projec  tive clustering deals with the high dimensionality of data  whereas clustering ensembles handle the lack of a priori knowledge on clustering targets  The  rst issue arises due to the sparsity that naturally occurs in data representation As such  it is unlikely that all features are equally relevant to form meaningful clusters  The second issue is related to the fact that there are usually many aspects that character  ize the targets of a clustering task  however  due to the al  gorithmic peculiarities of any particular clustering method  a single clustering solution may not be able to capture all facets of a given clustering problem  In  16   projective clustering and clustering ensembles are treated for the  rst time in a uni ed framework  The underlying motivation of that study is that the high  dimensionality and the lack of a priori knowledge problems usually co exist in real world applications  To address both issues simultaneously   16  hence formalizes the problem of projective clustering ensembles  PCE   the objective is to de ne methods that  by exploiting the information provided by an ensemble of projective clustering solutions  are able to compute a robust projective consensus clustering  PCE is formulated as an optimization problem  hence the sought projective consensus clustering is computed as a so  lution to that problem  Speci cally  two formulations of PCE have been proposed in  16   namely two objective PCE and single objective PCE  The two objective PCE formula  tion consists in the simultaneous optimization of two ob  jective functions  which separately consider the data object clustering and the feature to cluster assignment  A well  founded heuristic developed for this formulation of PCE  called MOEA PCE  has been found to be particularly ac  curate  although it has drawbacks concerning e ciency  pa  rameter setting  and interpretability of results  By contrast  the single objective PCE formulation embeds in one objec  tive function the object based and feature based representa  tions of candidate clusters  Apart from being a weaker for  mulation than two objective PCE  the developed heuristic for single objective PCE  called EM PCE  is outperformed by two objective PCE in terms of e ectiveness  while show  ing more e ciency  Both the early formulations of PCE have their own draw  backs and advantages  however none of them refers to any of the common approaches of clustering ensembles  i e   the aforementioned instance based  cluster based  and hybrid approaches  This may limit the versatility of such early formulations of PCE and  eventually  their comparability with existing ways of solving clustering ensemble problems at least in terms of experience gained in some real world scenarios  Besides this common shortcoming  an even more serious weakness concerns the inability of the two objective PCE of treating the object based and feature based cluster representations as interrelated  This fact in principle may lead to projective consensus clustering solutions that contain conceptual  aws in their cluster composition  In this work  we face all the above issues revisiting the PCE problem  For this purpose  we pursue a di erent ap  proach to the study of PCE  focusing on the development of methods that are closer to the standard clustering ensem  ble methods  By providing an insight into the theoretical foundations of the early two objective PCE formulation  we show its weaknesses and propose a new single objective for  mulation of PCE  The key idea underlying our proposal is to de ne a function that measures the distance of any pro  jective clustering solution from a given ensemble  where the object based and feature based cluster representations are considered as a whole  The new formulation enables the development of heuristic algorithms that are easy to de ne and  at the same time  are well founded as they can ex  ploit a corpus of research results obtained by the majority of existing clustering ensemble methods  Particularly  we investigate the opportunity of adapting each of the various approaches of clustering ensembles to the new PCE prob  lem  We de ne two heuristics that follow a cluster based approach  namely Cluster Based Projective Clustering En  sembles  CB PCE  and a step forward version called Fast Cluster Based Projective Clustering Ensembles  FCB PCE   We show not only the suitability of the proposed heuristics to the PCE context but also their advantages in terms of computational complexity w r t  the early formulations of PCE  Moreover  based on an extensive experimental evalua  tion  we assessed e ectiveness and e ciency of the proposed algorithms  and found that both outperform the early PCE methods in terms of accuracy of projective consensus clus  tering  In addition  FCB PCE reveals to be faster than the early two objective PCE and comparable or even faster than the early single objective PCE in the online phase  The rest of the paper is organized as follows  Section 2 provides background on clustering ensembles  projective clustering  and the PCE problem  Section 3 describes our new formulation of PCE and presents the two developed heuristics along with an analysis of their computational com  plexities  Section 4 contains experimental evaluation and results  Finally  Section 5 concludes the paper  2  BACKGROUND 2 1 Clustering Ensembles  CE  Given a set D of data objects  a clustering solution de ned over D is a partition of D into a number of groups  i e   clus  ters  A set of clustering solutions de ned over the same set D of data objects is called ensemble  Given an ensemble de ned over D  the goal of CE is to derive a consensus clus  tering  which is a  new  partition of D derived by suitably exploiting the information available from the ensemble  The earliest CE methods aim to explicitly solve the label correspondence problem to  nd a correspondence between the cluster labels across the clusterings of the ensemble  10  11  12   These approaches typically su er from e ciency is  sues  More re ned methods fall into instance based  cluster  based  and hybrid categories  2 1 1 Instance based CE Instance based CE methods perform a direct comparison between data objects  Typically  instance based methods operate on the co occurrence or co association matrix W  which resembles the pairwise object similarities according to the information available from the ensemble  For each pair of objects   o 0    o 00    the matrix W stores the number of solutions of the ensemble in which  o 0 and  o 00 are assigned to the same cluster divided by the size of the ensemble  Instance based methods derive the  nal consensus clustering by applying one of the following strategies   i  performing an additional clustering step based on W  using this matrix either as a new data matrix  20   or as a pairwise similarity matrix in  volved in a speci c clustering algorithm  13  22  15    ii  constructing a weighted graph based on W and partitioning the graph according to well established graph partitioning algorithms  28  3  29  2 1 2 Cluster based CE Cluster based CE lies on the principle  to cluster clus  ters   7  28  6   The key idea is to apply a clustering al  gorithm to the set of clusters that belong to the clustering solutions in the ensemble  in order to compute a set of meta  clusters  i e   sets of clusters   The consensus clustering is  nally computed by assigning each data object to the meta  cluster that maximizes a speci c criterion  such as the com  monly used majority voting  which assigns each data object  o to the metacluster that contains the maximum number of clusters which  o belongs to  2 1 3 Hybrid CE Hybrid CE methods combine ideas from instance based and cluster based approaches  The objective is to build a hybrid bipartite graph whose vertices belong to the set of data objects and the set of clusters  For each object  o and cluster C  the edge   o  C  of the bipartite graph usually as  sumes a unit weight  if the object  o belongs to the cluster C according to the clustering solution that includes C  and zero otherwise  36   Some methods use weights in the range  0  1   which express the probability that object  o belongs to cluster C  29   The consensus clustering of hybrid CE methods is obtained by partitioning the bipartite graph ac  cording to well established methods  e g   METIS  19    The nodes representing clusters are  ltered out from the graph partition  2 2 Projective Clustering  PC  Let D be a set of data objects  where each  o 2 D is de ned on a feature space F   f1          jFjg  A projective cluster C de ned over D is a pair hC   Ci such that   C denotes the object based representation of C  It is a jDj dimensional real valued vector whose component C  o 2  0  1   8 o 2 D  represents the object to cluster assignment of  o to C  i e   the probability Pr Cj o  that object  o belongs to C     C denotes the feature based representation of C  It is a jFj dimensional real valued vector whose component  C f 2  0  1   8f 2 F  represents the feature to cluster assignment of the feature f to C  i e   the probabil  ity Pr fjC  that feature f belongs to the subspace of features associated with C  Note that the above de nition addresses all possible types of projective clusters handled by existing PC algorithms  In fact  both soft and hard object to cluster assignments are taken into account the assignment is hard when C  o 2 f0  1g rather than  0  1   8 o 2 D  Similarly  feature to cluster assignments may be equally weighted  i e    C f   1 R  where R is the number of relevant features for C   if f is recognized as relevant   C f   0 otherwise  This repre  sentation is suited for dealing with the output of all those PC algorithms which only select the relevant features for each cluster  without specifying any feature to cluster as  signment probability distribution  Such algorithms fall into bottom up  34  25   top down  32  31  2  37  5   and hybrid ap  proaches  24  35  1   On the other hand  the methods de ned in  34  8  30  handle projective clusters having soft object  to cluster assignment and or feature to cluster assignment unequally weighted  The object based  C  and the feature based   C  repre  sentations of any projective cluster C are exploited to de ne the projective cluster representation matrix  for brevity  pro  jective matrix   XC of C  XC is a jDj jFj matrix that stores  8 o 2 D  f 2 F  the probability of the intersection of the events  object  o belongs to C  and  feature f belongs to the subspace associated with C   Under the assumption of inde  pendence between the two events  such a probability is equal to the product of Pr Cj o    C  o with Pr fjC     C f   Hence  given D   f o1           ojDjg and F   f1          jFjg  ma  trix XC can be formally de ned as  XC   0 B   C  o1   C 1       C  o1   C jFj             C  ojDj   C 1       C  ojDj   C jFj 1 C A  1  The goal of a PC method is to derive from an input set D of data objects a projective clustering solution denoted by C  which is de ned as a set of projective clusters that satisfy the following conditions  X C2C C  o   1  8 o 2 D and X f2F  C f   1  8C 2 C The semantics of any projective clustering C is that for each projective cluster C 2 C  the objects belonging to C are actually close to each other if  and only if  they are projected onto the subspace associated with C  2 3 Projective Clustering Ensembles  PCE  A projective ensemble E is de ned as a set of projective clustering solutions  No information about the ensemble generation strategy  algorithms and or setups   nor original feature values of the objects within D are provided along with E  Moreover  each projective clustering solution in E may contain in general a di erent number of clusters  The goal of PCE is to derive a projective consensus clus  tering by exploiting information on the projective solutions within the input projective ensemble  2 3 1 Two objective PCE In  16   PCE is formulated as a two objective optimiza  tion problem  whose objectives take into account the object  based  function  o  and the feature based  function  f   cluster representations of a given projective ensemble E  C     arg min C2E f o C  E    f  C  E g  2  where  o C  E    X C2E    o  C  C     f  C  E    X C2E    f  C  C    3  Functions  o and  f are de ned as  o  C 0   C 00       o C 0   C 00     o C 00   C 0    2 and  f  C 0   C 00       f  C 0   C 00      f  C 00   C 0     2  respectively  where  o C 0   C 00     1 jC 0 j X C02C0   1  max C002C00 J  C0   C00     f  C 0   C 00     1 jC 0 j X C02C0   1  max C002C00 J   C0    C00    J   u  v       u    v      k uk 2 2   k vk 2 2   u    v   2  0  1  denotes the extended Jaccard similarity coe cient  also known as Tani  moto coe cient  between any two real valued vectors  u and  v  26                       The problem de ned in  2  is solved by a well founded heuristic  in which a Pareto based Multi Objective Evolu  tionary Algorithm  called MOEA PCE  is used to avoid com  bining the two objective functions into a single one  2 3 2 Single objective PCE To overcome some issues of the two objective PCE for  mulation  such as those concerning e ciency  parameter setting  and interpretation of the results    16  proposes an alternative PCE formulation based on a single objective function  which aims to consider the object based and the feature based cluster representations in E as a whole  C     arg min C2E X C2C X  o2D    C  o X C2E   X C 2C  C  o   X f2F    C f   C f    2 where     1 is a positive integer that ensures non linearity of the objective function w r t  C  o  To solve the above problem  the EM based Projective Clus  tering Ensembles  EM PCE  heuristic is de ned  EM PCE iteratively looks for the optimal values of C  o  resp   C f   while keeping  C f  resp  C  o   xed  until convergence  3  CLUSTER BASED PCE 3 1 Problem Statement Experimental results have shown that the two objective PCE formulation is much more accurate than the single  objective counterpart  16   Nevertheless  two objective PCE su ers from an important conceptual issue that has not been discussed in  16   proving that the accuracy of two objective PCE can be further improved  We unveil this issue in the following example  Example  Let E be a projective ensemble de ned over a set D of data objects and a set F of features  Suppose that E contains only one projective clustering solution C and that C in turn contains two projective clusters C 0 and C 00   whose object  and feature based representations are di erent from one another  i e   9  o 2 D s t  C0   o    6 C00   o  and 9 f 2 F s t   C0  f     6 C00  f   Let us consider two candidate projective consensus clus  terings C1   fC 0 1  C 00 1 g and C2   fC 0 2  C 00 2 g  We assume that C1   C  whereas C2 is de ned as follows  Cluster C 0 2 has object  and feature based representations given by C0  i e   the object based representation of the  rst cluster C 0 within C  and  C00  i e   the feature based representation of the second cluster C 00 within C   respectively  cluster C 00 2 has object  and feature based representations given by C00  i e   the object based representation of the second cluster C 00 within C  and  C0  i e   the feature based representation of the  rst cluster C 0 within C   respectively  According to  3   it is easy to see that   o C1  E    o C2  E   0 and  f  C1  E    f  C2  E   0 Both the candidates C1 and C2 minimize the objectives of the early two objective PCE formulation reported in  2   and hence  they are both recognized as optimal solutions  This conclusion is conceptually wrong  because only C1 should be recognized as an optimal solution  since only C1 is exactly equal to the unique solution of the ensemble  Conversely  C2 is not well representative of the ensemble E  as the object  and feature based representations of its clusters are inversely associated to each other w r t  the associations present in C  Indeed  in C2  C 0 1   hC0    C00 i and C 00 1   hC00    C0 i  whereas  the solution C 2 E is such that C 0   hC0    C0 i and C 00   hC00    C00 i  The issue described in the above Example arises because the two objective PCE formulation ignores that the object  based and feature based representations of any projective cluster are strictly coupled to each other  and hence  need to be considered as a whole  In other words  in order to ef  fectively evaluate the quality of a candidate projective con  sensus clustering  the objective functions  o and  f cannot be kept separated from each other  We attempt to solve the above drawback by proposing the following alternative formulation of PCE  which is based on a single objective function  C     arg min C2E  of  C  E   4  where  of is a function designed to measure the  distance  of any well de ned projective clustering solution C from E in terms of both data clustering and feature to cluster assign  ment  To carefully take into account e ciency  we de ne  of based on an asymmetric function  which has been de  rived by adapting the measure de ned in  16  to our setting   of  C  E    X C2E    of  C  C    5  where  of  C 0   C 00     1 2    of  C 0   C 00      of  C 00   C 0      6  and  of  C 0   C 00     1 jC 0 j X C02C0   1 max C002C00 J   XC0   XC00     7  In  7   the similarity between any pair C 0   C 00 of projective clusters is computed in terms of their corresponding pro  jective matrices XC0 and XC00  cf   1   Sect  2 2   For this purpose  the Tanimoto similarity coe cient can easily be generalized to operate on real valued matrices  rather than vectors   J  X  X      Pjrows X j i 1 Xi   X  i kXk 2 2   kX  k 2 2  Pjrows X j i 1 Xi   X  i  8  where Xi   X  i denotes the scalar product between the i th rows of matrices X and X     From a dissimilarity viewpoint  as J  2  0  1   we adopt in this work the measure 1  J   We hereinafter refer to 1  J  as Tanimoto distance  It can be noted that the proposed formulation based on the function  of ful ls the requirement of measuring the quality of a candidate consensus clustering in terms of both data clustering and feature to cluster assignments as a whole  In particular  we remark that the issue described in the previous Example does not arise in the proposed formu  lation  Indeed  considering again the two candidate projec  tive consensus clusterings C1 and C2 of the Example  it is easy to see that   of  C1  E    0 and  of  C2  E    0 Thus  C1 is correctly recognized as an optimal solution  whereas C2 is not                    3 2 Heuristics Apart from solving the critical issue of two objective PCE previously explained  a major advantage of the proposed PCE formulation w r t  the early ones de ned in  16  is its close relationship to the classic formulations typically em  ployed by CE algorithms  Like standard CE  the problem de ned in  4  can be straightforwardly proved to be a special version of the median partition problem  4   which is de ned as follows  given a number of partitions  clusterings  de   ned over the same set of objects and a distance measure between partitions   nd a  new  clustering that minimizes the distance from all the input clusterings  The only di er  ence between  4  and any standard CE formulation is that the former deals with projective clustering solutions  and hence  it needs a new measure for comparing projective clus  terings   whereas the latter involves standard clustering so  lutions  The closeness to CE is a key point of our work  as it enables the development of heuristic algorithms for PCE following standard approaches to CE  The advantage in this respect is twofold  heuristics for PCE can be de ned by ex  ploiting the extensive and well established work so far given for standard CE  which enables the development of solutions that are simple and easy to understand  and e ective at the same time  Within this view  a reasonable choice for de ning proper heuristics for PCE is to adapt the standard CE approaches  i e   instance based  cluster based  and hybrid  cf  Sect  2 1   to the PCE context  However  it is arguable if all such CE approaches are well suited for PCE  In fact  de ning an instance based PCE method is intrinsically tricky  and this also holds for the hybrid approach  which is essentially a combination of the instance based and cluster based ones  We explain the issues on de ning instance based PCE in the following  First  as the focus of any hypothetical instance based PCE is primarily on data objects  performing the two PCE steps of data clustering and feature to cluster assignment alto  gether would be hard  Indeed  focusing on data objects may produce information about data clustering only  for instance  by exploiting a co occurrence matrix properly re  de ned for the PCE context   This would force the assign  ment of the features to the various clusters to be performed in a separate step  and only once the objects have been grouped in clusters  Unfortunately  performing the two PCE steps of data clustering and feature to cluster assignment distinctly may negatively a ect accuracy of the output con  sensus clustering  According to the de nition of projective clustering  the information about the various objects belong  ing to any projective cluster should not be interpreted as absolute  but always in relation to the subspace associated to that cluster and vice versa  Thus  data clustering and feature to cluster assignment should be interrelated  at each step of the heuristic algorithm to be de ned  A more crucial issue arises even accepting to perform data clustering and feature to cluster assignment separately  Given a set of data objects to be included in any projec  tive cluster  the feature to cluster assignment process should take into account that the notion of subspace of any given projective cluster makes sense only if it refers to the whole set of objects belonging to that cluster  In other words  say  ing that any set of data objects forms a cluster C having a subset S of features associated with it does not mean that each object within C is represented by S  but rather that Algorithm 1 CB PCE Input  a projective ensemble E  the number K of clusters in the output projective consensus clustering  Output  the projective consensus clustering C   1   E   S C2E   C  2  P   pairwiseClusterDistances  E   f 8 g 3  M   metaclusters  E   P  K  4  C      5  for all M 2 M do 6     M   object basedRepresentation  E  M  f 12 g 7     M   f eature basedRepresentation  E  M  f 22 g 8  C    C     fhM   Mig 9  end for the entire set C is represented by S  Unfortunately  perform  ing feature to cluster assignment apart from data clustering contrasts with the semantics of a subspace associated to a set of objects in a projective cluster  Indeed  the various fea  tures could be assigned to any given cluster C only by con  sidering the objects within C independently of one another  Let us consider  for example  the case where the assignment is performed by averaging over the objects within C and over the feature based representations of all the clusters within the ensemble E  i e    C f   avg  o2C C 2C  C2E   fC  o      C f   g  8f 2 F  This case clearly shows that each feature f is assigned to C by considering each object within C indepen  dently from the other ones belonging to C  Within this view  we discard instance based and hybrid approaches to embrace a cluster based approach  In the fol  lowing  we describe our cluster based proposal in detail and also show how this is particularly appropriate to the PCE context  3 2 1 The CB PCE algorithm The Cluster Based Projective Clustering Ensembles  CB  PCE  algorithm is proposed as a heuristic approach to the PCE formulation given in  4   In addition to the notation provided in Sect  2  CB PCE employs the following symbols  M denotes a set of metaclusters  i e   a set of sets of clusters   M 2 M denotes a metacluster  i e   a set of clusters   and M 2 M denotes a cluster  i e   a set of data objects   The outline of CB PCE is reported in Alg  1  Similarly to standard cluster based CE  the  rst step of CB PCE aims to group the set  E of clusters from each solution within the input ensemble E into metaclusters  Lines 1 2   A clustering step over the set  E is performed by the function metaclus  ters  This step exploits the matrix P of pairwise distances between the clusters within  E  Line 1   The distance be  tween any pair of clusters is computed by resorting to the Tanimoto similarity coe cient reported in  8   The set M of metaclusters is  nally exploited to derive the object  and feature based representations of each projective cluster to be included into the output consensus clustering C    Lines 3 8   Such representations are denoted by    M and    M  8M 2 M  respectively  more precisely     M  resp     M  denotes the object based  resp  feature based  representa  tion of the projective cluster within C   corresponding to the metacluster M     M and    M are derived by focusing on the optimization of a criterion easy to solve  which enables the  nding of reasonable and e ective approximations at the same time  In particular  we adapt the widely used majority voting to the context at hand  Let us  rst consider    M values  If       the projective clustering solutions within the ensemble are all hard at a clustering level  the majority voting criterion leads to the de nition of the following optimization problem  f   M j M 2 Mg   argmin fMjM2Mg X M2M X  o2D M  o jMj X M2M 1  M  o s t  X M2M M  o   1  8 o 2 D M  o 2 f0  1g  8M 2 M  8 o 2 D whose solution can be easily proved to be as follows  8M  8 o      M  o   8     1 if M   arg min M02M 1 jM0 j X M2M0 1  M  o 0 otherwise that is  each object  o is assigned to the metacluster M con  taining the maximum number of clusters to which  o belongs  i e   such that M  o   1   If the ensemble contains projective clusterings that are soft at clustering level  the following problem can be de ned  f   MjM2Mg   argmin fMjM2Mg Q  9  s t  X M2M M  o   1  8 o 2 D  10  M  o   0  8M 2 M  8 o 2 D  11  where Q  X M2M X  o2D    M  o AM  o   AM  o   1 jMj X M2M 1  M  o and     1 is an integer that guarantees the non linearity of the objective function Q w r t  M  o  needed to ensure    M  o 2  0  1   rather than f0  1g   1 The solution for such a problem however is not as straightforward as that of the traditional case  i e   hard data clustering   We derive the solution in the following  Theorem 1  The optimal solution of problem P de ned in  9   11  is given by  8M  8 o      M  o     X M02M   AM  o AM0   o   1  1  1  12  Proof  The optimal    M  o can be found by means of the conventional Lagrange multipliers method  To this end  we  rst consider the relaxed problem P 0 of P obtained by tem  porarily discarding the inequality constraints from the con  straint set of P  i e   the constraints de ned in  11    We de ne the new  unconstrained  objective function Q 0 for P 0 as follows  Q 0   Q   X  o2D   o   X M02M M0   o  1    13  The optimal    M  o are computed by  rst retrieving the stationary points of Q 0   i e   the points for which rQ 0       Q 0   M  o     Q 0     o     0 1 Alternatively  to obtain    M  o 2  0  1   properly de ned reg  ularization terms can be introduced  see  e g    21    Thus  we solve the following system of equations    Q 0   M  o     AM  o  M  o   1     o   0  14    Q 0     o   X M02M M0   o  1   0  15  Solving  14  w r t  M  o and substituting such a solution in  15   we obtain  X M02M     o   AM0   o   1  1   1  16  Solving  16  w r t    o and substituting such a solution in  14   we obtain    AM  o  M  o   1    X M2M   1   AM0   o   1  1    1    0  17  Finally  solving  17  w r t  M  o  we obtain a stationary point whose expression is exactly equal to that in  12      M  o     X M02M   AM  o AM0   o   1  1  1  18  As it holds that  i  the stationary points of the Lagrangian function Q 0 are also stationary points of the original objec  tive function Q   ii  the feasible region of P and hence  the feasible region of P 0 is a convex set  and  iii  Q is convex w r t  M  o  it follows that such a stationary point repre  sents a global minimum of Q  and  accordingly  the optimal solution of P 0   Moreover  as AM  o   0  8M  8 o  it is trivial to observe that    M  o   0  8M  8 o  Therefore  the solution in  18  satis es the inequality constraints that were tem  porarily discarded in order to de ne the relaxed problem P 0  cf   11    thus  it represents the optimal solution of the original problem P  which proves the theorem  An analogous reasoning can be carried out for    M f   In this case  the problem to be solved is the following  f    MjM2Mg  arg min f MjM2Mg X M2M X f2F     M f BM f  19  s t  X f2F  M f   1  8M 2 M  20   M f   0  8M 2 M  8f 2 F  21  where BM f   jMj 1 P M2M 1 M f and   plays the same role as   in function Q  The solution of such a problem is similar to that derived for    M  o   Theorem 2  The optimal solution of the problem de ned in  19   21  is given by the following  8M  8f       M f   2 4 X f02F   BM f BM f0   1  1 3 5 1  22  Proof  Analogous to Theorem 1                                                       Rationale of CB PCE  Let us now informally show that CB PCE is well suited for PCE  thus supporting one of the claim of this work  i e   cluster based approaches are particularly appropriate to the PCE context  unlike instance based and hybrid ones   Looking at the PCE formulation reported in  4   it is easy to see that function  of retrieves the consensus clustering C   so that each cluster within C   is ideally  assigned  to ex  actly one cluster of each projective clustering solution in the input ensemble E  where the  assignments  are performed by minimizing the Tanimoto distance 1  J   cf   8    Thus  considering all the solutions in the ensemble  any cluster C 2 C   is assigned to a set of clusters  metacluster  M that contains exactly one cluster of each solution in the ensem  ble  that is jMj   jEj  and M0 2 C   M00 2 C   M0   M00   8M0   M00 2M  8C 2 E  Clearly  if one would know in advance the optimal set of metaclusters to be assigned to the clusters within C     the problem in  4  would be optimally solved by computing  for each metacluster M  the cluster C   that minimizes the Tanimoto distance from all the clusters within M  that is  C     arg min C X M2M 1  J  XC  XM   23  However  it holds that   i  the metaclusters are not known in advance  as their computation is part of the optimization process   ii  the problem in  23  is hard to solve  it falls into the class of median problems in which the distance to be minimized is the Tanimoto distance  this kind of problems has been recently proved to be NP hard  9   The validity of CB PCE as a heuristic approach to the PCE formulation proposed in  4  lies in that it exactly fol  lows the scheme reported above  i e   it  rst recognizes meta  clusters and then assigns objects and features to metaclus  ters   following some approximations  These approximations are needed for solving two critical points  1  a sub optimal set of metaclusters is computed by clus  tering the overall set of projective clusters within the ensemble  where the distance measure used for com  paring clusters is the Tanimoto distance  which is the measure employed by the proposed formulation in  4   2     M and    M values  for each metacluster M  are com  puted by optimizing an easy to solve criterion that ef  fectively approximates the problem in  23   3 2 2 Speeding up CB PCE  FCB PCE Given a set D of data objects and a set F of features  the computational complexity of the measure J  reported in  8   used for computing the similarity between two projective clusters  is O jDj jFj   as it involves a comparison between two jDj jFj matrices  For e ciency purposes  we can lower the complexity by de ning an alternative measure working in O jDj   jFj   Given any two projective clusters C 0 and C 00   such a measure  called J  fast  exploits the object based  C0 and C00   and the feature based   C0 and  C00   rep  resentation vectors of C 0 and C 00   respectively  rather than their corresponding projective matrices  Formally  J  fast C 0   C 00     1 2   J  C0   C00     J   C0    C00      24  where J        denotes again the Tanimoto similarity coe   cient de ned in  8   which is in this case applied to real  valued vectors rather than matrices  It is easy to observe that  like J   J  fast 2  0  1   Taking into account J  fast  we de ne a version of the CB  PCE algorithm which is similar to that de ned in Sect  3 2 1  except for the measure involved for comparing the projec  tive clusters  which is  in this case  based on J  fast  We here  inafter refer to this alternative version of the algorithm as Fast Cluster Based Projective Clustering Ensembles  FCB  PCE  algorithm  Although clearly advantageous in terms of e ciency  a major drawback of FCB PCE concerns accuracy  In fact  a major weakness of the measure J  fast exploited by FCB  PCE is that it is less accurate than its slow counterpart J  exploited by CB PCE  This essentially depends on the fact that comparing any two projective clusters C 0 and C 00 by in  volving their projective matrices XC0 and XC00   respectively  is generally more e ective than involving their object  and feature based representation vectors C0   C00    C0   and  C00  23   2 Indeed  although it can be trivially proved that XC0   XC00   C0   C00    C0    C00   the vectors C0    C0   and C00    C00 are in general a factorization of the matrices XC0 and XC00   respectively  i e   XC0    T C0  C0 and XC00    T C00  C00    Thus  only matrices XC0 and XC00 provide the whole information about the representation of the corresponding projective clusters  Although J  fast is less accurate than J   it still allows the comparison of projective clusters by taking into account their object  and feature based representations altogether  Hence  the proposed FCB PCE heuristic based on J  fast still represents a valuable heuristic to the PCE formulation pro  posed in this work  as it overcomes the main issue of two  objective PCE explained in Sect  3 1  3 2 3 Computational Analysis Here we discuss the computational complexity of the pro  posed CB PCE and FCB PCE algorithms  We are given  a set D of data objects  each one de ned over a feature space F  a projective ensemble E de ned over D and F  and a positive integer K representing the number of clusters in the output projective consensus clustering  We also assume that the size jCj of each solution C in E is O K   For both the algorithms  we may distinguish three steps  1  pre processing  it concerns the computation of the pairwise distances between clusters  by involving mea  sures J   cf   8   for CB PCE and J  fast  cf   24   for FCB PCE  this step takes O K2 jEj 2 jDj jFj  and O K2 jEj 2  jDj   jFj   for CB PCE and FCB PCE  respectively  because computing J   resp  J  fast  is O jDj jFj   resp  O jDj   jFj    cf  Sect  3 2 2   and the clusters to be compared to each other are O K jEj   2  meta clustering  it concerns the clustering of the O K jEj  clusters of all the solutions in the ensemble  assuming to employ a clustering algorithm which is at most quadratic w r t  the size of the dataset to be par  titioned  this step takes O K2 jEj 2   for both CB PCE and FCB PCE  3  post processing  it concerns the assignment of objects and features to the metaclusters  and is exactly the 2  23  deals with hard projective clusters  however  the rea  soning therein involved can be easily extended to a soft case                Table 1  Computational complexities total online o ine MOEA PCE O ItK2 jEj jDj   jFj   O ItK2 jEj jDj   jFj     EM PCE O KjEjjDjjFj  O IKjDjjFj  O KjEjjDjjFj  CB PCE O K2 jEj 2 jDjjFj  O KjEj KjEj   jDj   jFj   O K2 jEj 2 jDjjFj  FCB PCE O K2 jEj 2  jDj   jFj   O KjEj KjEj   jDj   jFj   O K2 jEj 2  jDj   jFj   same for both CB PCE and FCB PCE  According to  12  and  22   both the object and the feature assign  ments need to look up all the clusters in each meta  cluster only once  thus  for each object and for each feature  the needed step costs O KjEj   Accordingly  performing this step for all objects and features leads to a total cost of O KjEj  jDj   jFj   for the entire post processing step  It can be noted that the  rst step is an o ine phase  i e   a phase to be performed only once in case of a multi run exe  cution  whereas the second and third are online steps  Thus  as summarized in Table 1  where we also report the com  plexities of the earlier MOEA PCE and EM PCE methods de ned in  16  3    we can  nally state that    the o ine  online  and total  i e   o ine   online  complexities of CB PCE are O K2 jEj 2 jDj jFj   O KjEj KjEj   jDj   jFj    and O K2 jEj 2 jDj jFj   respectively    the o ine  online  and total  i e   o ine   online  complexities of FCB PCE are O K2 jEj 2  jDj   jFj    O KjEj KjEj jDj jFj    and O K2 jEj 2  jDj jFj    respectively  Interpretation of the complexity results  Let us now provide an insight for the comparison between the  total  complexities derived above  For the sake of read  ability  we hereinafter omit the su x   PCE  from the names of the various PCE algorithms  We denote with r a1  a2  the ratio between the complexities of the PCE algorithms a1 and a2  Clearly  a ratio smaller  resp  greater  than 1 means that the complexity of a1 is smaller  resp  greater  than that of a2  Our main observations are summarized in the following    As expected  FCB PCE is always faster than CB PCE  as it holds that r FCB  CB     jDj jFj   jDjjFj    1  8 jDj  jFj   1    CB PCE    it holds that r CB EM    K jEj   1  thus  CB  PCE is always slower than EM PCE    the ratio r CB MOEA  is equal to  jEj jDj jFj   I t  jDj   jFj    This implies that r CB MOEA    1 if  2 jDj jFj   jDj   jFj    2 I t jEj  i e   as  jDj   jFj  2    2 jDj jFj   jDj   jFj   that r CB MOEA    1 if jDj   jFj   4 I t jEj  The latter condition is true only in a small number of real cases  as an example  considering the numerical values for I  t and jEj suggested in  16  3 In Table 1  I denotes the number of iterations to conver  gence  for MOEA PCE and EM PCE   whereas t is the pop  ulation size  for MOEA PCE only   16    i e   200  30 and 200  respectively   CB PCE is faster than MOEA PCE if jDj   jFj   120  i e   when the input dataset is very small and or low dimensional  For this purpose  CB PCE can be recognized as in practice always slower than MOEA PCE    FCB PCE    it holds that the ratio r FCB EM     K jEj  jDj   jFj    jDj jFj  is greater than 1 if  2 jDj jFj   jDj   jFj    2 K jEj  which essentially means that FCB PCE is slower than EM PCE if jDj   jFj   4 K jEj  as  jDj   jFj  2    2 jDj jFj   jDj   jFj   Thus  for large and or high dimensional datasets  i e   for datasets having jDj and jFj such that jDj   jFj   4 K jEj  FCB PCE may be faster than EM PCE  whereas for small and or low dimensional datasets may not    r FCB MOEA    jEj  I t   assuming to set t equal to 15  of the ensemble size jEj as suggested in  16   it holds that r FCB MOEA    20  3 I   Thus  as it typically holds that I   7  e g   in  16  I   200   r FCB MOEA  is always smaller than 1 and  therefore  FCB PCE is always faster than MOEA PCE  To summarize  we can state that CB PCE is the slowest method  FCB PCE is faster than MOEA PCE  whereas  compared to EM PCE  it is faster  resp  slower  for large  resp  small  and or high dimensional  resp  low  dimensional  datasets  4  EXPERIMENTAL EVALUATION We conducted an experimental evaluation to assess the ac  curacy and e ciency of the consensus clusterings obtained by the proposed CB PCE and FCB PCE  The comparison also involved the previous existing PCE algorithms  i e   MOEA PCE and EM PCE   16  as baseline methods  4 4 1 Evaluation methodology Following  16   we used eight benchmark datasets from the UCI Machine Learning Repository  27   namely Iris  Wine  Glass  Ecoli  Yeast  Segmentation  Abalone and Letter  and two time series datasets from the UCR Time Series Clas  si cation Clustering Page  33   namely Tracedata and Con  trolChart  Table 2 reports the main characteristics of the datasets  the interested reader is referred to  27  33  for a description of the datasets  4 Experiments were conducted on a quad core platform Intel Pentium IV 3GHz with 4GB memory and running Microsoft WinXP Pro Table 2  Datasets used in the experiments dataset objects attributes classes Iris 150 4 3 Wine 178 13 3 Glass 214 10 6 Ecoli 327 7 5 Yeast 1 484 8 10 Segmentation 2 310 19 7 Abalone 4 124 7 17 Letter 7 648 16 10 Tracedata 200 275 4 ControlChart 600 60 6 4 1 1 Ensemble generation We generated ensembles as suggested in  16   In particu  lar  for each set of experiments and dataset we considered 20 di erent ensembles  all results we present in the following refer to averages over these ensembles  Ensemble generation was carried out by running the LAC projective clustering al  gorithm  30   in which the diversity of the solutions was en  sured by randomly choosing the initial centroids and varying the parameter h  here we recall that this parameter controls the incentive for clustering on more features depending on the strength of the local correlation of data  To test the ability of the proposed algorithms to deal with soft clus  tering solutions and with solutions having equally weighted feature to cluster assignments  we generated each ensem  ble E as a composition of four equal sized subsets  denoted as E1  hard data clustering  feature to cluster assignments unequally weighted   E2  hard data clustering  feature to  cluster assignments equally weighted   E3  soft data clus  tering  feature to cluster assignments unequally weighted   and E4  soft data clustering  feature to cluster assignments equally weighted   4 1 2 Setting of the PCE algorithms We set the parameters of MOEA PCE and EM PCE as reported in  16   In particular  as far as MOEA PCE  the population size  t  was set equal to 15  of the ensemble size and the number I of maximum iterations equal to 200  The random noise needed for the mutation step was obtained via Monte Carlo sampling on a standard Gaussian distribution  Regarding EM PCE  the parameter   was set equal to 2  this value also represented the optimal value for the parameters   and   of our CB PCE and FCB PCE  4 1 3 Assessment criteria We assessed the quality of a consensus clustering C using both an external and an internal validity approach  specif  ically  we carried out two evaluation stages  the  rst based on the similarity of C w r t  a reference classi cation and the second based on the average similarity w r t  the solutions in the input ensemble E  Similarity w r t  the reference classi   cation  We denote with Ce a reference classi cation  where the object based representations Ce of each projective cluster Ce within Ce are provided along with D  the selected datasets are all available with a reference classi cation   whereas the feature based representations  C f e   8Ce 2 Ce  8f 2 F  are computed as suggested in  30    C f e   exp   U C  f e   h   P f02F exp   U C  f e 0   h   where the LAC s parameter h was set equal to 0 2 and  U C      f     X  o2D C  o    1 X  o2D C  o    c C      f   o f   2 c C      f     X  o2D C  o    1 X  o2D C  o     o f  with o f  denoting the   f th feature value of object  o  Similarity between C and Ce was computed in terms of the Normalized Mutual Information  by taking into account their object based  NMIo  representations  feature based representations  NMIf    or both  NMIof    and by adapting the original de nition given in  28  to handle soft solutions  Here we report the formal de nition of NMIof   NMIo and NMIf can be derived in a similar way  NMIof  C  Ce    X C2C X Ce2Ce a C Ce  T  C Ce    log   jDj 2 a C Ce  T  C Ce  b C  b Ce    q H C    H Ce  where a C 0   C 00     X  o2D X f2F C0   o  C0  f C00   o  C00  f b C     X  o2D X f2F C  o    C f   H C      X C 2C  b C   jDj log b C   jDj T C 0   C 00     X  o2D X f2F   X C02C0 C0   o  C0  f    X C002C00 C00   o  C00  f   We now explain the rationale of this evaluation stage  Let us consider NMIof   where analogous considerations hold for NMIo and NMIf   Since no additional information is provided along with any given input projective ensemble E the reference classi cations associated to the benchmark datasets are indeed exploited only for testing purposes  randomly extracting a projective solution from E is the only fair way to proceed in case no PCE method is used  Within this view  in order to establish the validity of a projective consensus C computed by any PCE algorithm  we compare the results achieved by C w r t  those obtained by any pro  jective clustering randomly chosen from E  Such a compari  son can be performed according to the following expression  which aims to compute the  expected di erence  between the results by C and those by E   of  C  E  Ce    X C2E     NMIof  C  Ce   NMIof  C   Ce    Pr C   where Pr C   is the probability of randomly choosing C  from E  Since no prior knowledge is provided along with E  we can assume a uniform distribution for the probabilities Pr C    i e   Pr C     jEj 1   8C 2 E     Computing  of hence becomes equal to computing the similarity between C and Ce minus                   Table 3  Evaluation w r t  the reference classi cation  of  o  f MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris   146   168   218   185   319   228   309   297   198   095   139   117 Wine   136   083   275   224   201   130   272   253   152   030   211   206 Glass   105   162   158   157   092   134   180   167   048   060   001   009 Ecoli   164   086   211   232   245   125   223   213   042   042   023   017 Yeast   049   021   092   095   090   066   113   110   006   090   102   010 Segmentation   137   144   148   141   102   206   194   185   075   079   098   150 Abalone   116   111   134   130   141   116   185   182   093   092   123   120 Letter   111   107   141   134   146   122   188   185   092   097   131   124 Trace   097   019   125   140   032   026   154   132   007   114   112   115 ControlChart   091   204   345   276   050   011   027   051   233   416   287   283 min   049   019   092   095   032   011   027   051   007   095   001   009 max   164   204   345   276   319   228   309   297   233   416   287   283 avg   115   110   185   171   142   116   185   178   093   093   123   122 the average similarity between Ce and the solutions within E  as proved by the following   of  C  E  Ce    X C2E     NMIof  C  Ce   NMIof  C   Ce    Pr C       NMIof  C  Ce   X C2E   NMIof  C   Ce    jEj 1     NMIof  C  Ce   avg C2E   NMIof  C   Ce   25   o and  f can be de ned analogously  The larger  of    o and  f   the better the quality of C  Similarity w r t  the ensemble solutions  The goal of this evaluation stage was to assess how well a consensus clustering complies with the solutions in the input ensemble  For this purpose  we evaluated the average similarity NMIof  C  E    avgC02ENMIof  C  C 0   between the consensus clustering C and the solutions in the ensemble E  NMIo and NMIf are de ned analogously   To improve the readability of the results  we normalize NMIof   NMIo and NMIf by dividing them by the average pairwise similarity of the solutions in the ensemble  Formally  we de ne the ratios  coe cients of variation   of    o  and  f    of  C  E    NMIof  C  E  avgC0  C002ENMIof  C 0   C 00    26   o and  f are de ned similarly  The larger these quantities are  the better the quality of C is  4 2 Results 4 2 1 Accuracy For each algorithm  dataset and ensemble  we performed 50 di erent runs  We reported average clustering results obtained by CB PCE and FCB PCE  as well as by the early MOEA PCE and EM PCE in Tables 3 and 4  Evaluation w r t  the reference classi   cation  Both CB PCE and FCB PCE achieved higher  of re  sults   rst 4 column groups in Table 3  than MOEA PCE on all datasets  In particular  CB PCE obtained an aver  age improvement of 0 070  with a maximum gain of 0 254  ControlChart   whereas FCB PCE obtained an average im  provement of 0 056  with a maximum of 0 185  ControlChart again   EM PCE was on average less accurate than MOEA  PCE  thus  the average gains of CB PCE and FCB PCE w r t  EM PCE were higher than those achieved w r t  MOEA PCE  0 075 and 0 061  respectively   Comparing the two proposed CB PCE and FCB PCE  the former achieved higher quality on nearly all datasets  all but Ecoli  Yeast and Trace   with an average gain of about 0 014 and peaks on ControlChart  0 069  and Wine  0 051   The higher perfor  mance of CB PCE vs  FCB PCE con rms one of the major claims of this work  cf  Sect  3 2 2   The superior performance of CB PCE and FCB PCE w r t  the early MOEA PCE and EM PCE was also con   rmed in terms of object based   o  and feature based   f   representations  In particular  CB PCE achieved average  o equal to 0 185 and average improvements w r t  MOEA  PCE and EM PCE of 0 043 and 0 069  respectively  Also  CB PCE outperformed MOEA PCE  resp  EM PCE  on seven  resp  eight  out of ten datasets  As far as FCB  PCE  the average  o was 0 178  with average gains w r t  MOEA PCE and EM PCE equal to 0 036 and 0 062  respec  tively  FCB PCE performed better than MOEA PCE and EM PCE on eight and nine out of ten datasets  respectively  In terms of  f   both CB PCE and FCB PCE were on average comparable to each other  in fact  they achieved average  f equal to 0 123 and 0 122  respectively  The av  erage improvements obtained by CB PCE  resp  FCB PCE  w r t  both MOEA PCE and EM PCE were equal to 0 030  resp  0 029   Like  of and  o  both the proposed CB PCE and FCB PCE performed better than MOEA PCE and EM  PCE on the majority of the datasets also in terms of  f   Evaluation w r t  the ensemble solutions  Concerning the coe cients of variation due to the consen  sus clustering w r t  the average pairwise similarity of the input ensemble  Table 4   CB PCE and FCB PCE led to average values respectively equal to 1 110 and 1 108   of    1 318 and 1 316   o   1 049 and 1 030   f    Particularly  in the case  of   CB PCE improved MOEA PCE and EM PCE by 0 062 and 0 114 on average  respectively  whereas the av  erage improvements obtained by FCB PCE w r t  MOEA  PCE and EM PCE were equal to 0 060 and 0 112  respec  tively  Also  CB PCE was able to obtain peaks of improve  ment up to 0 297  w r t  MOEA PCE  and 0 454  w r t  EM PCE   The maximum gains of FCB PCE were instead equal to 0 3 and 0 457 w r t  MOEA PCE and EM PCE  respectively  Both CB PCE and FCB PCE outperformed MOEA PCE and EM PCE on nearly all datasets  CB PCE results were better than those of MOEA PCE and EM PCE on seven and nine out of ten datasets  respectively  As far    Table 4  Evaluation w r t  the ensemble solutions  of  o  f MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris 1 019  914  984  989 1 025 1 004 1 044 1 039  953  906  986  977 Wine  993  960 1 074 1 072 1 060  991 1 057 1 056 1 018  952 1 001 1 001 Glass 1 023  918 1 1 003 1 114  971 1 064 1 066  979  915 1 004 1 004 Ecoli 1 074 1 052 1 058 1 015 1 034 1 023 1 027 1 028  975  924  986  992 Yeast 1 074 1 050 1 217 1 189 1 189 1 182 1 310 1 297  960 1 021 1 036 1 037 Segmentation 1 008  851 1 305 1 308 1 367 1 304 1 788 1 786  971  969 1 032 1 013 Abalone 1 044 1 001 1 068 1 071 1 121 1 102 1 208 1 208  982  902  980  986 Letter 1 040 1 001 1 045 1 088 1 118 1 099 1 277 1 274  981  891 1 169  998 Trace 1 170 1 207 1 196 1 196 1 325 1 501 1 503 1 503  949  927 1 062 1 062 ControlChart 1 034 1 006 1 152 1 152 1 162 1 237 1 903 1 903 1 085  577 1 234 1 234 min  993  851  98  989 1 025  971 1 027 1 028  949  577  980  977 max 1 170 1 207 1 305 1 308 1 367 1 501 1 903 1 903 1 085 1 021 1 234 1 234 avg 1 048  996 1 110 1 108 1 152 1 141 1 318 1 316  985  898 1 049 1 030 Table 5  Execution times  milliseconds  TOTAL ONLINE OFFLINE MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  MOEA  EM  CB  FCB  data PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE PCE Iris 17 223 55 13 235 906 17 223 53 343 372   2 12 892 534 Wine 21 098 184 50 672 993 21 098 153 306 323   31 50 366 670 Glass 61 700 281 110 583 3 847 61 700 239 1 713 1 713   42 108 870 2 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#dmp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_mining"/>
        <doc>Sampling Based Algorithms for Quantile Computation in Sensor Networks ### Zengfeng Huang Lu Wang Ke Yi Yunhao Liu Hong Kong University of Science and Technology  huangzf  luwang  yike  liu  cse ust hk ABSTRACT We study the problem of computing approximate quantiles in large scale sensor networks communication e   ciently  a problem previously studied by Greenwald and Khana  12  and Shrivastava et al   21   Their algorithms have a total communication cost of O k log 2 n     and O k log u      respectively  where k is the number of nodes in the network  n is the total size of the data sets held by all the nodes  u is the universe size  and    is the required approximation error  In this paper  we present a sampling based quantile computation algorithm with O      kh     total communication  h is the height of the routing tree   which grows sublinearly with the network size except in the pathological case h      k   In our experiments on both synthetic and real data sets  this improvement translates into a 10 to 100 fold communication reduction for achieving the same accuracy in the computed quantiles  Meanwhile  the maximum individual node communication of our algorithm is no higher than that of the previous two algorithms  Categories and Subject Descriptors F 2  Analysis of Algorithms and Problem Complexity   Nonnumerical Algorithms and Problems General Terms Algorithms Keywords Sensor networks  quantiles ### 1  INTRODUCTION Sensor networks are large ad hoc networks of interconnected  battery powered  wireless sensors  They are now being widely deployed to monitor diverse physical variables  such as temperature  sound  activities of wild life and so forth  15  17  27   As technologies mature  sensor networks Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  have reached the scale of thousands of nodes  1  2  and will get even larger in the near future  However  power consumption remains the biggest obstacle for large scale deployment for sensor networks as the on board battery is still the only power source for a sensor node  Since wireless transmission of data is the biggest cause of battery drain  20   in network aggregation techniques that prevent the nodes from forwarding all the data to the base station are extremely useful for energy conservation in sensor networks  The observation is that for many monitoring tasks  we do not actually need all the measurement data collected by all the sensors  often a succinct aggregate su   ces  Computing an aggregate could be much more communication e   cient than transmitting all the data to the base station  in terms of both the total communication  as well as the maximum individual node communication  Simple aggregates such as max  min  sum  count can be computed easily and e   ciently  14   The nodes    rst organize them into a spanning tree rooted at the base station  Then starting from the leaves  the aggregation propagates upwards to the root  When a node receives the aggregates from its children  it computes the aggregate of these aggregates and its own data  which equals the aggregate of all the data in the node   s subtree  and forwards it to its parent  Such a simple approach works due to the decomposable property of these aggregates  for any two disjoint sets S1 and S2  the aggregate of S1     S2 can be computed from the individual aggregates of S1 and S2  Some other aggregates such as average can also be computed this way since it is derived from two decomposable aggregates sum and count  though it is not decomposable itself  Letting k be the number of nodes in the sensor network  it is clear that a decomposable aggregate can be computed with O k  total communication and O 1  maximum node communication  assuming each node has O 1  children   However  these simple aggregates are not expressive enough  A quantile summary  which allows one to extract the     quantile for any 0        1 of the underlying data  much better characterizes the data distribution  Recall that the    quantile of a set S of n data values from a totally ordered universe is the value a with rank r a         n    in S  the rank of any value x  r x   is the number of values in S smaller than x  For ease of presentation we assume that all values in S are distinct  such an assumption can be easily removed by using any consistent tie breaker  The quantiles are also called the order statistics and equi depth histograms  and are very useful in a variety of data analytical tasks as they o   er a lot more insight into the underlying data than the simple single valued aggregates  Because a quantile summary that returns the accurate quantiles must contain the entire data set  an    approximation is usually allowed  an    approximate    quantile is a value with rank between          n and        n  This additive error de   nition is the one that has been mostly adopted in the literature  5  6  10   12  18  21  26   though multiplicative errors have also been considered  7  28   Note that since the error is additive  the    should be set small  usually on the order of 0 01  to 1   6   With an    approximation allowed  an  approximate  quantile summary could just retain the 1    values that rank at 0    n  2  n  3  n          respectively  Then for any     we return the value with the largest rank in the summary that is no larger than   n  One can also easily argue that     1     is the theoretical minimum size of such a summary  1 1 Previous results In 2004  two back to back  well cited papers by Greenwald and Khanna  12  and Shrivastava et al   21  studied the problem of computing quantile summaries in a communicatione   cient manner in sensor networks  Both of them still follow the    decomposable    approach described earlier  But unlike sum or max  a quantile by itself is not decomposable  for this reason it is also called a holistic aggregate in the literature  5    So the challenge was to design a decomposable summary that contains enough information so that the approximate quantiles can be extracted  Their solutions only differ in the decomposable quantile summaries used  The GK summary  12  has size O log 2 n     where n is the total number of data values in the network  while the q digest  21  has size O log u     where u is the size of the universe from which the data values are drawn  The two extra factors O log 2 n  and O log u  are not strictly comparable  but the GK summary is theoretically more general as it supports unbounded universes  assuming  of course  any value in the universe takes one unit of storage   The GK summary has two additional variants with size O h     and O log n log h          respectively  where h is the height of the routing tree  They can be better than the basic version when the routing tree is well balanced  Speci   cally  both algorithms  12  21  start from the leaves and compute the GK summary  resp  q digest  upwards  Each node    rst computes a summary of its own data  and then combines it with all the summaries it receives from its children  This produces an aggregated summary that incorporates all the data in the node   s subtree  which is then forwarded to its parent  It is clear that the individual node communication is equal to the summary size  while the total communication is k times that  1 2 Our results In this paper  we present a new algorithm for computing quantile summaries in sensor networks  with an expected total communication of O      kh      The computed quantile summary allows one to extract an    approximate    quantile with constant probability for any 0        1  Depending on various physical situations  the height of the tree  h  could range from log k to k  but usually does not exceed     k  which happens when the sensor nodes form a grid structure   Thus the communication is always less than that of the previous algorithms  More importantly  except in the pathological case h      k   the cost of our algorithm grows sublinearly in the size of the network  leading to excellent scalability  total comm  max node comm  q digest  21  k log u    log u    GK algorithm  12  k log 2 n    log 2 n    GK algorithm v2 k log n log h        log n log h        GK algorithm v3 kh    h    new     kh    log k h     Table 1  The asymptotic communication costs of the algorithms  where n is the total number of data values  u is the universe size  k is the network size  h is the height of the routing tree  and    is the error parameter  Meanwhile  the maximum individual node communication of our algorithm is O log k h       which is also strictly better than any of the previous algorithms  as log k   h   k   n   u  A comparison of the communication costs of these algorithms is given in Table 1  Note that for relatively large     say a constant  O      kh     could be much smaller than k  which means that we can compute the quantiles without contacting all nodes  This may seem surprising  as even computing or approximating a simple aggregate  like sum or count  needs    k  communication  This    surprise    comes with the assumption that our algorithm knows the values of n and k  otherwise  we need O k  communication to compute them    rst  We separated the cost of computing n and k from that of the quantile problem itself for the following reasons   1  It simpli   es the bound and makes the core problem of quantile computation stand out  otherwise we should always include an additive O k  term   2  Applications usually use quantiles to track the distribution of the underlying data  so will compute quantile summaries periodically  The values of n and k from period to period are unlikely to change much  while our algorithm actually only needs to know n and k within a constant factor  So we can usually repeatedly execute it without refreshing n and k  Anyways  in practice we typically have k         kh     and the O k  term can be neglected  Note that for any algorithm using decomposable summaries  the total communication is always at least     k      as any such summary must have size     1      To break this barrier  we deviate from the decomposable framework  The message a node sends to its parent does not necessarily contain a quantile summary of the data in its subtree  Nevertheless  we will make sure that the base station in the end will have a valid quantile summary for the entire data set  In particular  our algorithm uses larger messages  but of size at most O log k h       for nodes near the base station  while nodes far away send small or even no messages  This is in contrast with the previous approaches where all nodes use essentially the same message size  ignoring polylog factors   The improvement of our algorithm for the maximum individual node communication is not as drastic as that for the total communication  In fact  it is not di   cult to show an     1     lower bound on the maximum node communication for any quantile algorithm  decomposable or not  and all the algorithms already come close to this theoretical limit within polylog factors  with our log factor being the smallest  Some may say that the maximum node cost is more important  12   but we would argue the other way round  for the following reasons  1  The network will not be disconnected due to a single node exhausting its battery  A sensor network usually has enough redundancy so that rerouting is possible fora permissible number of node failures  2  We can install larger batteries for sensor nodes that are expected to have larger power consumptions  e g  those near the base station  or simply deploy more sensors there to increase redundancy  When we have good provisioning  the total communication cost  rather than the maximum individual node cost  will become the determining factor for the longevity of the network  In fact  most previous papers on data aggregation indeed used total communication as the primary measure of energy e   ciency  e g   4  16  22   The previous work on the quantile problem  12  21  did not emphasize it as much because for those algorithms  the total communication is just k times the     xed  message size  Our algorithm is based on sampling  However  simply taking a random sample of the data and computing the quantiles on the sample is not accurate enough  as it is well known that to achieve an    error with constant probability  a random sample of size    1    2   needs to be drawn  23   This results in O h    2   total communication and O 1    2   maximum node communication  To improve accuracy  or equivalently  to reduce size   we augment the random sample with additional information about the data  together with several new ideas that we brie   y outline in Section 1 4 and develop in stages in later sections  In fact  the total communication of our algorithm is O min      kh     h    2     i e   it is always no worse than simple random sampling  but we will avoid carrying along with the    min    throughout the paper to simplify exposition  Since our algorithm is based on random sampling  it provides a probabilistic guarantee  that any    quantile can be extracted within error    with a constant probability which can be made arbitrarily close to 1  While the GK algorithm and the q digest provide a worst case    error guarantee  However  we would be happy with a probabilistic guarantee  since transmission errors and link failures are common in sensor networks  a theoretical worst case guarantee becomes a probabilistic one in practice anyway  Nevertheless  to ensure a fair comparison  in the experiments  Section 5  we did not simply set the required    and compare the communication costs  Instead  we measure the actual average and maximum error of 99 quantiles for      1   2           99   and compare all the algorithms in terms of the communication error trade o    curve  1 3 Related work Data aggregation in sensor networks has been a topic of intensive studies for the past years  Below we only review the most relevant work  please refer to the surveys  9  24  for more comprehensive results in this area  Data aggregation techniques can be broadly classi   ed into two categories  tree based approaches and multi path approaches  In a tree based approach  the nodes organize themselves into a routing tree  Typically the tree is built from the base station in a breadth    rst manner  All nodes within communication range with the base station become level one nodes  Then a node u that can reach a level one node v becomes a level two node  with v being the parent of u  and so on so forth  When a link fails  the child will try to    nd a new parent  but at any time  a node has only one parent and the aggregation is performed along a tree  Most data aggregation techniques  including all the quantile algorithms  are tree based  In a multi path approach  a node has multiple parents and will broadcast its message to all of them  Such an approach is more robust against link failures  but also requires more communication  as now the algorithm has to be designed to be insensitive to the duplication of messages  With a multi path approach  even a simple aggregate like count or sum cannot be computed exactly  To obtain an    approximation of count or sum  each node will need to send a message of size O 1    2    4  19   Currently there are no multi path algorithms for the quantile problem  There is also a hybrid approach  16  that combines the bene   ts of tree based and multi path approaches  where it uses tree aggregation when the link failure rate is low to gain better communication e   ciency  while adopts a multi path strategy when the failure rate is high  The closely related heavy hitters problem has also received a lot of attention  where the goal is to compute a summary of a multiset of size n from which we can estimate the frequency of any item up to an additive error of   n  It is well known  6  that this problem can be reduced to the         approximate quantile problem for some                  by simply using some tie breaker to convert the multiset into a set  say  padding di   erent lower order bits   and then asking quantile queries with               2                 Thus the previous quantile algorithms  12  21   as well as our new algorithm  also solve the heavy hitters problem  But of interest is whether the heavy hitters problem can be solved more e   ciently  as in the case of the streaming model  where the heavy hitters problem can be solved in O 1     space  6  while the best quantile algorithm needs O log n     space  11   Manjhi et al   16  proposed a deterministic algorithm for computing the heavy hitters with total communication O k      but the bound only holds for a class of    nice    routing trees that they de   ne  It is still an open question whether there are better deterministic heavy hitter algorithms for arbitrary routing trees  For randomized algorithms  one can use the count min sketch  8  in the decomposable framework  leading to total communication O k     for any routing tree  Our algorithm improves this to O      kh      Quantile summaries  as fundamental statistics and a useful data analytical tool  have been extensively studied in several other settings  Munro and Paterson  18  studied how to compute an exact quantile with limited memory and multiple passes  and also showed that approximation is necessary if only one pass is allowed  The best one pass  i e   streaming  algorithm for computing approximate quantile summaries is due to Greenwald and Khana  11   whose algorithm uses O log n     space  Gilbert et al   10  and Cormode and Muthukrishnan  8  studied how to maintain a quantile summary under both insertions and deletions using small space  Finally  Cormode et al   5  and Yi and Zhang  26  studied how to track the quantile summary over distributed data sets as they evolve  1 4 Roadmap We develop our algorithm in three stages  In Section 2 we    rst present the algorithm in the    at model  in which all nodes are directly connected to the base station  This algorithm has O      k     total communication and O 1     maximum individual node communication  Simply running the algorithm on a spanning tree of height h would result in O h     k     total communication  even worse  the maximum individual node communication could be as high as O      k     since all the tra   c might have to go through a single node  In Section 3 we develop techniques that combinethe messages a node receives before it forwards its message to its parent  This results in an O log k     maximum message size  Finally in Section 4  we use a tree partitioning technique to improve the total communication cost to the claimed O      kh     bound  2  THE FLAT MODEL In this section we    rst describe our algorithm in the    at model  in which each node is directly connected to the base station  Let the set of data values at node i be Si  i   1          k  and let n be the total number of data values  The algorithm  The algorithm is very simple  Each node    rst independently samples each of its data values with some probability p  to be determined later   For each sampled value a  it computes its local rank r a  i  at node i  i e   the rank of a in set Si  Then it simply sends all the sampled values and their local ranks to the base station  We    rst show how a value to rank query can be answered at the base station from the information it receives  namely  given any value x  we need to estimate r x   the rank of x in S i Si  After this  quantile  rank to value  queries can be easily answered  Let pred x  i  be the predecessor of x in the sampled values sent from node i  namely  the largest value no larger than x  note that pred x  i  may not exist  We show below that r   x  i        r pred x  i   i    1 p  if pred x  i  exists  0  else is an unbiased estimator of r x  i   the local rank of x in Si  Then  we can estimate the global rank of x as r   x    X i r   x  i   Node 1 10 33 42 68 101 132 Node 2 52 97 125 Node 3 21 74 111 Figure 1  There are 3 nodes  each of which holds a set of integers  and the shaded integers are sampled  the sample rate here is 1 2   See Figure 1 for an example  Suppose we want to query for the rank of 80  In this example    r 80  1    2 2   4 since in the sample from Node 1  the local rank of 80   s predecessor  42  is 2 and the sample probability is 1 2  Similarly  r   80  2    0 and   r 80  3    0   2   2  so the estimated global rank of 80 is   r 8    4   0   2   6  whereas its actual global rank is 7  Analysis  The key to showing that   r x  estimates the global rank of x accurately is the following lemma  Lemma 1  For any x  r   x  i  is an unbiased estimator for r x  i   with variance Var   r x  i       1      1     p  r x i  p 2   Proof  Consider the random variable X       r x  i      r pred x  i   i   if pred x  i  exists  r x  i    1 p  else  Note that   r x  i    r x  i      X   1 p  So we just need to show that E X    1 p and bound Var X   Starting from x and walking to smaller values  we observe that X represents the number of values we see until the    rst sampled value pred x  i  in set Si  when it exists  When all the r x  i  values smaller than x are not sampled  X is set to r x  i    1 p  Therefore  we have  detailed derivations are omitted  r x  i  is shorthanded as r  E X    Xr     1    p 1     p        1    1     p  r  r   1 p    1 p  The variance is Var X    E X 2       E X  2   Xr     1     2 p 1     p        1    1     p  r  r   1 p  2     1 p 2    1      1     p  r   1     p  p 2     1      1     p  r x i  p 2   Since the global rank of x is the sum of the local ranks    r x  is an unbiased estimator of r x  with variance Pk i 1 Var   r x  i    We bound Var   r x  i   in two ways  First it is clear that Var   r x  i       1 p 2   and hence P i Var   r x  i       k p 2   Thus  by setting p           k   n   the variance becomes O    n  2    By Chebyshev   s inequality  this means that   r x  approximates r x  within an additive error of   n with constant probability  This constant probability can be made arbitrarily close to 1 by enlarging p by appropriate constant factors  In this case the total communication is O pn    O      k      Alternatively we can bound Var   r x  i   as Var   r x  i       1      1     pr x  i   p 2   r x  i  p   so Var   r x     X i Var   r x  i       1 p X i r x  i      n p   This means that when p is set to    1    2 n   Var   r x   is also O    n  2    In this case the total communication is O 1    2    namely the same as simple random sampling  Therefore  the algorithm has a total communication of O min      k     1    2     In the rest of the paper  we will only consider the interesting and typical case when     k      1    2   i e   k   1    2   to avoid carrying the    min    around  Reducing individual node communication  The algorithm described above has the desired total communication O      k      but all this tra   c could be from one node  if it dominates the entire data set  Below we show how to limit the individual node communication to O 1      We classify the nodes into those with more than n      k data values and those with at most that  If a node i has  Si      n      k values  it still uses the previously determined sample probability pi   p  if  Si    n      k  it will use a smaller sample probability pi   1    Si   The estimator correspondingly becomes r   x  i        r pred x  i   i    1 pi  if pred x  i  exists  0  else  It is clear that now a node samples at most O 1     values in expectation  To see that the estimator is still accurate by Lemma 1    r x  is still an unbiased estimator of r x  with variance Var   r x       Xk i 1 1 p 2 i   X i    Si      n k    ni  2   X i    Si        n k    n  2 k     0 B   X  i    Si      n k      ni  1 C A 2      n  2     2   n  2   Thus   r x  is still an    approximation of r x  with constant probability  Quantile queries  We have shown how to answer value torank queries using the summary structure at the base station  Quantile  rank to value  queries can also be answered easily  as follows  For each sampled data value a received by the base station from node i  we    rst estimate its global rank   r a  as r   a    r a  i    X j 6 i r   a  j   Now  given a required rank r  we simply return the sampled value x that has the closest estimated rank   r x  to r  Below we argue that its true rank  r x   is away from r by at most   n with constant probability  Assume that r is between the estimated ranks of two consecutive values x and y in the sample  i e     r x      r     r   y   Consider the following three events  1    r x      1 2   n     r x      r   x    1 2   n  2    r y      1 2   n     r y      r   y    1 2   n  3  r y      r x        n  When all three events happen  one can verify that x or y  whoever has the closest estimated rank to r  must have its true rank within   n to r  By appropriately adjusting the constants  we can make sure that events 1  and 2  each happen with probability 8 9  say   Since the sample probability is at least 1    Si       1   n  the number of missed values between x and y is no more than   n with constant probability  Again this constant can be boosted to 8 9  Then by a union bound  all three events happen together with probability at least 2 3  Theorem 1  Our algorithm in the    at model has O      k     total communication and O 1     maximum individual node communication  and answers an    approximate quantile query with constant probability  3  THE TREE MODEL There are two challenges in extending the    at model algorithm to a general routing tree  First  if each node simply sends its message through its ancestors in the routing tree to the base station without any data reduction  an intermediate node might see too much tra   c going through  This could result in an O      k     maximum individual node communication  Second  in terms of total communication  simply running the    at model algorithm in the tree model would result in O h     k     communication as a message needs O h  hops to reach the base station  This section will resolve the    rst issue while Section 4 the second  3 1 Basic ideas From Theorem 1 we know that each node   s own message has size at most O 1      Problems arise when a node has too many descendants whose messages need to be forwarded  Our idea is to merge these messages in a systematic way so as to reduce their size  The unit of our merge operation is a sample s taken from a ground set D s   Let n s  denote the size of the ground set D s   and we store n s  together with s  We say s is a small sample if n s    n      k and a large sample if n s      n      k  Initially  each such sample s is generated by a node i from its own data set D s    Si  Recall from the previous section that the initial samples have the following properties   P1  The ground sets of the samples are disjoint  and their union is the entire data set   P2  Each value in D s  has been sampled to s with some equal probability p s   in particular  p s        k   n if s is a small sample and p s    1   n s  if it is a large sample   P3  Each sampled value a in s is associated with r a  D s    the local rank of a in D s   Recall that an immediate consequence of  P2  is that each sample has size at most O 1      For a large sample s  we de   ne its class number as c s       log n s      k n      It is clear that 0     c s      log     k  When a node has received a number of samples from its children  together with one of its own  it will    rst check if the ground sets of all the small samples have a combined size of at least n      k  If so it will merge all the small samples into a large sample  Next it will repeatedly merge two large samples of the same class into one in the next class  until no two large samples are in the same class  As a result  there will be at most one large sample per class left  During the merge operation  we will also ensure that the three properties above are maintained  As a result  since all the small samples  if there are any  have their combined ground set smaller than n      k  the total size of all the small samples is O 1      and because each large sample has size at most O 1      the node will eventually send out a message of size O log k      When merging two samples s1 and s2 and producing a merged sample on the ground set D s1      D s2   properties  P1  and  P2  are relatively easy to maintain  by appropriately subsampling the values in s1 and s2 based on p s1  and p s2   However   P3  is di   cult to guarantee  In fact  because we only have a sample s2 from D s2   for any value a subsampled to the merged sample from s1  we cannot really compute its exact rank in D s2   and vice versa  So we will instead estimate its rank in D s2  based on s2  Thus  we will relax property 3  to the following   P3       Each sampled value a in s is associated with   r a  D s    which is an unbiased estimator of r a  D s    the local rank of a in D s   Now we need to be careful since the errors in these estimated ranks will propagate as more merges are performed  and we need to make sure that when the samples reach the base station  the accumulated error should not exceed   n  Below we    rst present the relatively simple merging algorithm  and defer the more complicated analysis to later 3 2 The merging algorithm As described above  if all the small samples have their combined ground set smaller than n      k  we will not do any merges since their total sample size is O 1      Otherwise  we use the following merge small operation to merge them into a large sample  merge small  Let s1  s2          sm be all the small samples  such that P i n s      n      k  Let s be the merged sample  and let n s    P i n si  be the size of the combined ground set  Note that s will be a large sample  so its sample probability should be p s    1   n s   To form the sample  we can simply subsample each data value in all the si   s with probability p s  p si    n n s      k   For each a thus sampled  if it is from si  we estimate its local rank in the combined ground set as r   a  D s     r a  i    X j 6 i r   a  D sj     where r   a  D sj      8     r pred a  sj    D sj      1 p sj    if pred a  sj   exists  0  else  As before  here pred a  sj   denotes the predecessor of a in the sample sj   It is easy to see that merge small maintains properties  P1    P2   and  P3       since by Lemma 1    r a  D sj    is an unbiased estimator of r a  D sj     After executing merge small  we will repeatedly execute merge large to merge the large samples  Unlike mergesmall  we apply merge large only on pairs of large samples of the same class  one pair at a time  progressively from the low classes to high  More precisely  starting from class c   0  as long as there are at least two large samples in this class  we merge them with merge large  to form a sample in class c   1  When there is one or no sample left in class c  we move on to class c   1  This idea is similar to that in  12   but because of our way of sampling and the fact that we deviate from the decomposable framework  the total size of our merged samples is smaller than that of  12  by a logarithmic factor  merge large  Let s1 and s2 be two large samples to be merged  Let s be the merged sample  and set n s    n s1   n s2   As s has a sample probability p s    1   n s   we subsample each data value in s1 with probability p s  p s1    n s1  n s  and subsample each data value in s2 with probability p s  p s2    n s2  n s   For each subsampled value a  if it is from s1  we estimate its rank in D s  as r   a  D s       r a  D s1       r a  D s2    where   r a  D s1   is the rank  either exact or approximate  that a carries from s1  and   r a  D s2   is computed from s2 similarly as before r   a  D s2     8     r   pred a  s2   D s2     1 p s2   if pred a  s2  exists  0  else  except that   r pred a  s2   D S2   could now also be an estimate rather than the exact rank of pred a  s2  in D S2   The case where a is from s2 is handled symmetrically  Let us see an example of merging two messages  Suppose n      k   100  and the two messages contain summaries  s1  s2  s3  and  t1  t2  t3   with ground set sizes 80  400  800  and 60  400  3200 respectively  So s1 and t1 are small samples  and we merge them into s     1 using merge small  The resulting merged sample has class number 0  We then    nd that s2 and t2 are both in class 2  and merge them together into a new summary with class number 3  It is further merged with s3  getting a summary s     2 with class number 4  Now we are done  since the summaries left are  s     1  s     2  t3   all of which have di   erent class numbers  When all the samples have been sent to the base station  we can use exactly the same query algorithms as in the    at model to answer value to rank and rank to value queries using these samples  just that now the local ranks for the sampled values are estimates of the actual local ranks in the respective ground sets  3 3 Error analysis It easily follows from the merging algorithm that properties  P1    P2   and  P3       are all maintained  but it remains to show that when the base station has received all the samples  an    approximate quantile query can still be answered with constant probability  Lemma 2  For any large sample s resulted from mergesmall  the estimated local rank r   a  D s   has variance at most m   n  2  k  where m is the number of merged small samples  Proof  Because each small sample is an initial sample with sample probability p       k   n  the lemma directly follows from Lemma 1  Lemma 3  Let s be any large sample of class c s   For any data value a     s  its estimated rank r   a  D s   has variance at most m   n  2  k      2 c s  1 n  2  k  where m is the number of small samples whose ground sets are included in D s   Proof  We will prove by induction on c s   The base case c s    0 is easy to verify  For any large sample of class 0  it is either an initial sample or one produced from mergesmall  The variance of   r a  D s   is 0 in    rst case  and at most m   n  2  k in the second case by Lemma 2  Now we assume the lemma holds for all large samples of class i and proceed to prove it for class i   1  A large sample s with c s    i   1 could be produced from mergesmall  or merged from two large samples s1 and s2 with c s1    c s2    i  In the    rst case  again by Lemma 2  the variance is at most m   n  2  k  in the second case  by the induction hypothesis  we have Var   r a  D sj         mj    n  2  k      2 i 1 n  2  k  for any a     sj   where mj is the number of small samples whose ground sets are included in D sj    j   1  2  Consider a data value a     s  W l o g   assume that it is subsampled from s1  The rank of a in D s  is estimated as r   a  D s       r a  D s1       r a  D s2    The variance of   r a  D s1    by the induction hypothesis  is at most m1   n  2  k      2 i 1 n  2  k   1 The variance of   r a  D s2    if the local ranks in s2 were accurate  by Lemma 1 is at most 1 p s2  2      n s2   2        2 i 1 n  2  k   2  Since now we only have an estimate for the rank of pred a  s2  with variance m2   n  2  k      2 i 1 n  2  k   3  by the law of total variance  Var   r a  s   is the sum of  1    2  and  3   Thus Var   r a  s        m1   m2    n  2  k   3   2 i 1 n  2  k     m   n  2  k      2 i 2 n  2  k  which completes the induction  Theorem 2  Our algorithm in the tree model has O h     k     total communication and O log k     maximum individual node communication  and answers an    approximate quantile query with constant probability  Proof  The communication bounds follow directly from the algorithm description  so we only prove correctness here  Below we only focus on a value to rank query  i e   estimating the rank r x  of any given value x within error   n  after that  a quantile query can be answered in the same way as in Section 2  Recall that we use the same algorithm as in the    at model to estimate r x  from a number of small samples and at most one large sample per class  The total variance from all the small samples is at most O    n  2   according to the analysis in Section 2  since they are the initial samples without merging  Let s0  s1          s log     k be the large samples for each of the classes  The estimated local ranks of x in these samples have two sources of variance  the variance due to the sampling  which is 1 p si  2 as in Lemma 1  and the variance of the estimated local rank   r pred x  s   D s    which can be bounded by Lemma 3  The total variance from the    rst source is at most log     Xk i 0 1 p si  2     log     Xk i 0    2 i 1 n  2  k   O    n  2    The total variance from the second source  by Lemma 3  is at most log     Xk i 0  mi   n  2  k      2 i 1 n  2  k      k   n  2  k   log     Xk i 0    2 i 1 n  2  k   O    n  2    Again  the constant in the big Oh can be made arbitrarily small by enlarging the sample probabilities by constant factors  This means that we can estimate r x  within   n error with a constant probability  4  TREE PARTITIONING In this section we describe our    nal improvement of the algorithm  reducing the total communication by another O      h  factor to O      kh      which is sublinear in k for all h   o k   The idea is to partition the routing tree into t connected components  each of which contains O k t  nodes  Then we conceptually shrink each component into a    super node     These t super nodes form a tree of size t but whose height still could be h  Now  if we apply our algorithm of Section 3 on these super nodes  the total communication would be O h     t      This seems to suggest a t that is as small as possible  However  since a super node is not really one node  but O k t  nodes that are connected  To produce an initial sample for a super node and compute the local ranks for the sampled values within the super node  we have to send messages among the O k t  nodes  It turns out preparing the initial samples now takes communication O k        t   Thus  setting t   k h balances these two terms and yields the desired O      kh     bound  We next elaborate on this idea in the rest of this section  4 1 Tree partitioning We    rst partition the routing tree into O t    O k h  components  each of which has O h  nodes  To ensure that each component is connected  we may introduce a few virtual nodes  by cloning the actual nodes  A virtual node has no data  It sits inside the actual node but logically operates on its own  Note that the tree partitioning phase depends only on the topology of the routing tree  so we can separate it from the actual quantile algorithm and only run it when the tree topology changes  Below we present a distributed algorithm that does the partitioning in O k  total communication  We assume that each node is aware of its subtree size  if not this information can be obtained easily using a bottom up computation with O k  communication  Each node u maintains a weight w u  which is initially set to u   s subtree size  during the algorithm w u  will represent the number of unpartitioned nodes in u   s subtree  The partitioning algorithm starts by calling partition root of the tree   and proceeds recursively  as outlined in Algorithm 1  Algorithm 1  partition u  1 foreach child v of u do 2 if w v      h then partition v   set w u     P 3 v w v    1 for all children v of u  4 while w u    h do if u has children v1          vl s t  h 2     P 5 i w vi      h then 6 put all the unpartitioned nodes in the subtrees of v1          vl into one component  7 mark all these nodes as    partitioned     8 if l     2  create a virtual node at u as the root of this component  9 set w u     w u      w v1                   w vl   10 set w vi     0 for i   1          l  Note that when partition root  returns  the root might still have at most h unpartitioned nodes below  Then we simply allocate these nodes into the last component  An example of the partitioning obtained by this algorithm is shown in Figure 2  Lemma 4  The partitioning algorithm partitions an arbitrary routing tree into O k h  connected components  each of size O h   At most one virtual node is created for each component a b d i j e f k l c g h a b     d i j e b f k l c g h Figure 2  An example of the tree partition algorithm when k   12 and h   4  The whole tree is partitioned into 3 components  Node b     is a virtual node added by the algorithm  and in real life its role can be played by node b  Proof  It is clear that the algorithm only produces components of size between h 2 and h  and at most one component  the last one  of size between 1 and h   1  But we still need to argue that all nodes must have been partitioned eventually  To do so  we show that when partition u     nishes  u has at most h unpartitioned nodes in its subtree  We prove it by induction  When u is a leaf  partition u  does nothing and the claim is certainly correct  Now consider an internal node u  By the induction hypothesis when lines 1   2 are done  each of u   s children v has at most h unpartitioned nodes below  namely  w v      h  Thus  as long as the sum of their weights is at least h  line 5 will always evaluates to true  In fact  we can    rst check if there is any v with h 2     w v      h  If there is one that already satis   es the condition  Otherwise all of them have w v    h 2  We can then simply collect them one by one until the sum falls between h 2 and h  Therefore  we can continue producing components of size between h 2 and h until w u      h  This    nishes the induction and hence the proof  4 2 Quantile algorithm on the partitioned tree On the partitioned tree  we run our tree model algorithm of Section 3 by treating each component as a super node  Recall that there are only t   O k h  super nodes  Let Si be the set of data values in the i th super node  The previous analysis suggests a sampling rate of p       t   n for a super node with less than n      t data values  and p   1    Si  if  Si      n      t  After the sample is drawn  we also needed to compute the local ranks of the sampled values in Si  However  now Si does not reside on one single node  but distributed among O h  nodes in the component  so we have to pay communication to compute the local ranks  Speci     cally  each node in a super node    rst samples its own data  and then the sampled values to the root of the component  The root of the component  after receiving all the samples  broadcasts them to everyone in the component  Now every node in the components has a copy of the sample drawn from the whole component  and thus can compute their ranks within its own data set  Finally we aggregate these local ranks in a bottom up fashion to the root of the component  which is actually the same as performing multiple sum aggregations within the component  one per sampled value  After the root of each component has prepared its initial sample and the associated local ranks  we can simply run our previous tree model algorithm on these initial samples  More precisely  starting from these component roots  we send the samples hop by hop to the base station  As before  when an intermediate node has received multiple samples  it tries to merge them before propagating them upwards  Theorem 3  Our quantile algorithm  when running on a partitioned tree  has O      kh     total communication and O log k h      maximum individual node communication  Proof  Since we sample the data values with probability at most     t   n  where t   O k h   the total sample size is O  p k h      in expectation   In the    rst phase of the algorithm  all the sampled data values are sent to the component roots  then broadcast to all nodes in the component  and      nally aggregated back to the component roots to compute their local ranks  Thus each sampled data value could travel to all the O h  nodes in a component in the worst case  This results in O  p k h       h    O      kh     communication in total  In the second phase  the component roots send their initial samples to the base station  Even if we do not do any merging of the samples  the cost would be at most O  p k h     h    O      kh      since each sample takes at most h hops to reach the base station  Thus the total communication cost is O      kh      In terms of maximum individual node communication  we still use the same algorithm in Section 3  except that a sample is said to be large if its ground set is of size at least n      t  This results in O log     t    O log k h   classes  So the maximum individual node communication is O log k h       expected  according to the analysis in Section 3  Remarks  The partitioning approach well    ts the case where the sensor network already uses a clustered structure  as in LEACH  13  and COUGAR  25   In this case a cluster naturally corresponds to a connected component  Note that we set the size of the component to O h  and the number of component to O k h  just for optimizing the communication cost  The correctness of the algorithm and its probabilistic guarantees on the returned quantiles do not depend on these parameters  When the cluster sizes deviate from O h   the total communication cost might be a   ected slightly but not the quality of the computed quantile summary  However  when the sensor network does not deploy a clustered structure  the algorithm on the partitioned tree might introduce additional overhead since it is no longer a oneround algorithm as in  12  14  21   It needs 3 rounds of communication within each component plus one    nal round from the component roots to the base station  5  EXPERIMENTS In this section we evaluate our algorithm experimentally  comparing with the two previous algorithms  the q digest  21  and the GK algorithm v2  12   See Table 1   We denote our Sampling Based algorithms as SB 1  for the one round version in Section 3  and SB p  for the improved version based on tree partitioning in Section 4   5 1 Experiment setup We built a simulator which simulates a sensor network and implemented all four algorithms on top of the same platform  The network topology is generated in the same way as in  21   i e   sensors are distributed over a certain area uniformly at random  Sensors are assumed to have a    xed radio range  and two sensors may communicate with each other if and only if they are within range of each other  The root of the network is chosen from the sensors randomly  after that a routing tree is generated by a breadth    rst search10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7  1e 05 0 0001 0 001 0 01 0 1 1 Maximum node communication Maximum error of queries q digest GK SB p SB 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10  1e 05 0 0001 0 001 0 01 0 1 1 Total communication Maximum error of queries q digest GK SB p SB 1 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7  1e 05 0 0001 0 001 0 01 0 1 1 Maximum node communication Average error of queries q digest GK SB p SB 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10  1e 05 0 0001 0 001 0 01 0 1 1 Total communication Average error of queries q digest GK SB p SB 1 Figure 3  Error Communication trade o   s on the synthetic data set with k   16384  starting from the root  Since our main goal is to improve the scalability of the algorithms in terms of network size  the experiments are done on relatively large networks  with k   1024  2048  4096  8192  16384 nodes  respectively  We used both synthetic and real data to test the performance of the algorithms  For the synthetic data sets  we    rst generated a total of n   1 billion values from a Gaussian distribution with mean 0 and variance 1  and then scaled them to the range  0  2 32     1  and round them to integers  since q digest cannot handle    oating point numbers  though the other algorithms can  Next  we deployed the sensors over a unit square area  and randomly distributed these n integers to the sensors  For the real data set  we used a terrain data for the Neuse River Basin  available at  3   This data set contains roughly 0 5 billion LIDAR points which measure the elevation of the terrain  In this case we randomly deploy the sensors on the terrain  and assume that each sensor collects the elevation data nearby  We adjusted the radius of the area from which a sensor collects data such that the total size of data set  i e   n  is around 1 billion  Note that this data set is highly correlated  since close sensors will observe similar elevations  To perform a quantile computation  we set some    and run all four algorithms  However  as mentioned earlier this may not be a fair comparison  since our algorithms give an    error with a constant</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis">
        <doc>Data Serving in the Cloud ### Raghu Ramakrishnan Chief Scientist  Audience and Cloud Computing Brian Cooper Adam Silberstein Utkarsh Srivastava Yahoo  Research Joint work with the Sherpa team in Cloud Computing2 Outline     Clouds     Scalable serving   the new landscape     Very Large Scale Distributed systems  VLSD      Yahoo    s PNUTS Sherpa     Comparison of several systems     Preview of upcoming Y  Cloud Serving  YCS   benchmark3 Types of Cloud Services     Two kinds of cloud services      Horizontal     Platform     Cloud Services     Functionality enabling tenants to build applications or new  services on top of the cloud     Functional Cloud Services      Functionality that is useful in and of itself to tenants  ### E g    various SaaS instances  such as Saleforce com  Google  Analytics and Yahoo    s IndexTools  Yahoo  properties aimed  at end users and small businesses  e g   flickr  Groups  Mail   News  Shopping      Could be built on top of horizontal cloud services or from  scratch     Yahoo  has been offering these for a long while  e g   Mail for  SMB  Groups  Flickr  BOSS  Ad exchanges  YQL 5 Yahoo  Horizontal Cloud Stack Provisioning   Self serve  YCS Horizontal YCPI  Cloud ServicesBrooklyn     EDGE Monitoring Metering Security Hadoop Horizontal   Cloud Services BATCH STORAGE PNUTS SherpaHorizontal MOBStor Cloud Services    OPERATIONAL STORAGE VM OS Horizontal Cloud Services     APP VM OS Horizontal Cloud Services yApache WEB Data Highway Serving Grid PHP App Engine6 Cloud Power   Yahoo  Ads  Optimization Content  Optimization Search  Index Image Video  Storage   Delivery Machine  Learning   e g  Spam  filters  Attachment Storage7 Yahoo    s Cloud   Massive Scale  Geo Footprint     Massive user base and engagement     500M  unique users per month     Hundreds of petabyte of storage      Hundreds of billions of objects     Hundred of thousands of requests sec     Global     Tens of globally distributed data centers     Serving each region at low latencies     Challenging Users     Downtime is not an option  outages cost  millions      Very variable usage patterns8 New in 2010      SIGMOD and SIGOPS are starting a new annual  conference  co located with SIGMOD in 2010   ACM Symposium on Cloud Computing  SoCC   PC Chairs  Surajit Chaudhuri   Mendel Rosenblum GC  Joe Hellerstein Treasurer  Brian Cooper      Steering committee  Phil Bernstein  Ken Birman   Joe Hellerstein  John Ousterhout  Raghu  Ramakrishnan  Doug Terry  John Wilkes9 VERY LARGE SCALE   DISTRIBUTED  VLSD   DATA SERVING ACID or BASE  Litmus tests are colorful  but the picture is cloudy10 Databases and Key Value Stores http   browsertoolkit com fault tolerance png11 Web Data Management Large data analysis  Hadoop  Structured record  storage  PNUTS Sherpa  Blob storage  MObStor     Warehousing    Scan  oriented  workloads    Focus on  sequential  disk I O      per cpu  cycle    CRUD     Point lookups  and short  scans    Index  organized  table and  random I Os      per latency    Object  retrieval and  streaming    Scalable file  storage      per GB  storage    bandwidth12 The World Has Changed     Web serving applications need      Scalability      Preferably elastic     Flexible schemas     Geographic distribution     High availability     Reliable storage     Web serving applications can do without      Complicated queries     Strong transactions     But some form of consistency is still desirable13 Typical Applications     User logins and profiles     Including changes that must not be lost      But single record    transactions    suffice     Events     Alerts  e g   news  price changes      Social network activity  e g   user goes offline      Ad clicks  article clicks     Application specific data     Postings in message board     Uploaded photos  tags     Shopping carts14 Data Serving in the Y  Cloud Simple Web Service API   s Database PNUTS   SHERPA Search Vespa Messaging Tribble Storage MObStor Foreign key photo     listing FredsList com application ALTER Listings MAKE CACHEABLE Compute Grid Batch export Caching memcached 1234323   transportation   For sale  one  bicycle  barely  used 5523442   childcare   Nanny  available in  San Jose DECLARE DATASET Listings AS   ID String PRIMARY KEY  Category String  Description Text   32138   camera   Nikon  D40  USD 30015 VLSD Data Serving Stores     Must partition data across machines     How are partitions determined      Can partitions be changed easily   Affects elasticity      How are read update requests routed      Range selections  Can requests span machines      Availability  What failures are handled      With what semantic guarantees on data access       How  Is data replicated      Sync or async  Consistency model  Local or geo      How are updates made durable      How is data stored on a single machine 16 The CAP Theorem     You have to give up one of the following in  a distributed system  Brewer  PODC 2000   Gilbert Lynch  SIGACT News 2002       Consistency of data      Think serializability     Availability     Pinging a live node should produce results     Partition tolerance     Live nodes should not be blocked by partitions17 Approaches to CAP        BASE        No ACID  use a single version of DB  reconcile later     Defer transaction commit      Until partitions fixed and distr xact can run     Eventual consistency  e g   Amazon Dynamo      Eventually  all copies of an object converge     Restrict transactions  e g   Sharded MySQL      1 M c Xacts  Objects in xact are on the same machine      1 Object Xacts   Xact can only read write 1 object     Object timelines  PNUTS  http   www julianbrowne com article viewer brewers cap theorem18 18    I want a big  virtual database       What I want is a robust  high performance virtual  relational database that runs transparently over a  cluster  nodes dropping in and out of service at will   read write replication and data migration all done  automatically  I want to be able to install a database on a server  cloud and use it like it was all running on one  machine        Greg Linden   s blog19 PNUTS   SHERPA To Help You Scale Your Mountains of Data Y  CCDI20 Yahoo  Serving Storage Problem     Small records     100KB or less     Structured records     Lots of fields  evolving     Extreme data scale   Tens of TB     Extreme request scale   Tens of thousands of requests sec     Low latency globally   20  datacenters worldwide     High Availability   Outages cost  millions     Variable usage patterns   Applications and users change 2021 E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E What is PNUTS Sherpa  E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E CREATE TABLE Parts   ID VARCHAR  StockNumber INT  Status VARCHAR       Parallel database Geographic replication Structured  flexible schema Hosted  managed infrastructure A     42342               E B     42521               W C     66354               W D     12352               E E     75656               C F     15677               E 2122 What Will It Become   E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E Indexes and views23 Scalability     Thousands of machines     Easy to add capacity     Restrict query language to avoid costly queries Geographic replication     Asynchronous replication around the globe     Low latency local access High availability and fault tolerance     Automatically recover from failures     Serve reads and writes despite failures Design Goals 23 Consistency     Per record guarantees     Timeline model      Option to relax if needed Multiple access paths     Hash table  ordered table     Primary  secondary access Hosted service     Applications plug and play     Share operational cost24 Technology Elements PNUTS      Query planning and execution     Index maintenance Distributed infrastructure for tabular data     Data partitioning      Update consistency     Replication YDOT FS     Ordered tables Applications Tribble     Pub sub messaging YDHT FS      Hash tables Zookeeper     Consistency service YCA  Authorization PNUTS API Tabular API 2425 PNUTS  Key Components     Maintains map from  database table key totablet to SU     Provides load balancing     Caches the maps from the TC     Routes client requests to  correct SU     Stores records     Services get set delete  requests 2526 Storage units Routers Tablet Controller REST API Clients Local region Remote regions Tribble Detailed Architecture 2627 DATA MODEL 2728 Data Manipulation     Per record operations     Get     Set     Delete     Multi record operations     Multiget     Scan     Getrange     Web service  RESTful  API 2829 Tablets   Hash  Table Apple Lemon Grape Orange Lime Strawberry Kiwi Avocado Tomato Banana Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  How much did you pay for this lemon  Is this a vegetable  New Zealand The perfect fruit Name Description Price  12  9  1  900  2  3  1  14  2  8 0x0000 0xFFFF 0x911F 0x2AF3 2930 Tablets   Ordered Table 30 Apple Banana Grape Orange Lime Strawberry Kiwi Avocado Tomato Lemon Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  The perfect fruit Is this a vegetable  How much did you pay for this lemon  New Zealand  1  3  2  12  8  1  9  2  900  14 Name Description Price A Z Q H31 Flexible Schema Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Color Red Condition Good Fair32 32 Primary vs  Secondary Access Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Price Posted date Listing id 15 6 5 07 421133 86 6 1 07 763245 570 6 1 07 424252 1123 6 3 07 211242 Primary table Secondary index Planned functionality33 Index Maintenance     How to have lots of interesting indexes  and views  without killing performance      Solution  Asynchrony      Indexes views updated asynchronously when  base table updated34 PROCESSING READS   UPDATES 3435 Updates 1 Write key k 2 Write key k 7 Sequence   for key k 8 Sequence   for key k SU SU SU 3 Write key k 4 5 SUCCESS 6 Write key k Routers Message brokers 3536 Accessing Data 36 SU SU SU 1 Get key k 2 3 Record for key k Get key k 4 Record for key k37 Bulk Read 37 SU Scatter  gather  server SU SU 1  k1  k2      kn  Get k 2 1 Get k2 Get k338 Storage unit 1 Storage unit 2 Storage unit 3 Range Queries in YDOT     Clustered  ordered retrieval of records Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 1 Router Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Grapefruit   Pear  Grapefruit   Lime  Lime   Pear  Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 139 Bulk Load in YDOT     YDOT bulk inserts can cause performance  hotspots     Solution  preallocate tablets40 ASYNCHRONOUS REPLICATION  AND CONSISTENCY 4041 Asynchronous Replication 4142 Consistency Model     If copies are asynchronously updated   what can we say about stale copies      ACID guarantees require synchronous updts     Eventual consistency  Copies can drift apart   but will eventually converge if the system is  allowed to quiesce     To what value will copies converge       Do systems ever    quiesce         Is there any middle ground 43 Example  Social Alice User Status Alice Busy West East User Status Alice Free User Status Alice     User Status Alice     User Status Alice Busy User Status Alice         Busy Free Free Record Timeline  Network fault   updt goes to East   Alice logs on 44     Goal  Make it easier for applications to reason about updates  and cope with asynchrony     What happens to a record with primary key    Alice     PNUTS Consistency Model 44 Time Record  inserted Update Update Update Update Update Delete Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Update Update As the record is updated  copies may get out of sync 45 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write Current  version Stale version Stale version PNUTS Consistency Model 45 Achieved via per record primary copy protocol  To maximize availability  record masterships automaticlly  transferred if site fails  Can be selectively weakened to eventual consistency   local writes that are reconciled using version vectors 46 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write if   v 7 ERROR Current  version Stale version Stale version PNUTS Consistency Model 46 Te s t and set writes facilitate per record transactions47 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Current  version Stale version Stale version Read PNUTS Consistency Model 47 In general  reads are served using a local copy48 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read up to date Current  version Stale version Stale version PNUTS Consistency Model 48 But application can request and get current version49 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read     v 6 Current  version Stale version Stale version PNUTS Consistency Model 49 Or variations such as    read forward      while copies may lag the master record  every copy goes through the same sequence of changes50 OPERABILITY 5051 51 Server 1 Server 2 Server 3 Server 4 6 2 07 636353 Bike  86 6 5 07 662113 Chair  10 Distribution 6 1 07 424252 Couch  570 6 1 07 256623 Car  1123 6 7 07 121113 Lamp  19 6 9 07 887734 Bike  56 6 11 07 252111 Scooter  18 6 11 07 116458 Hammer  8000 Data shuffling for load balancing Distribution for parallelism52 Tablet Splitting and Balancing 52 Each storage unit has many tablets  horizontal partitions of the table  Overfull tablets split Tablets may grow over time Storage unit may become a hotspot Shed load by moving tablets to other servers Storage unit Tablet53 Consistency Techniques     Per record mastering     Each record is assigned a    master region        May differ between records     Updates to the record forwarded to the master region     Ensures consistent ordering of updates     Tablet level mastering     Each tablet is assigned a    master region        Inserts and deletes of records forwarded to the master region     Master region decides tablet splits     These details are hidden from the application     Except for the latency impact 54 54 Mastering A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                 E A     42342                E B     42521                E C     66354                W D     12352                E E     75656                C F     15677                 E C     66354                W B     42521                E A     42342                E D     12352                E E     75656                C F     15677                E55 55 Record vs  Tablet Master A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#p1">
        <doc>A Latency and Fault Tolerance Optimizerfor Online Parallel Query ### PlansPrasang UpadhyayaUniversity of Washingtonprasang cs uw eduYongChul KwonUniversity of Washingtonyongchul cs uw eduMagdalena BalazinskaUniversity of Washingtonmagda cs uw eduABSTRACTWe address the problem of making online  parallel queryplans fault tolerant  i e   provide intra query fault tolerancewithout blocking  We develop an approach that not onlyachieves this goal but does so through the use of di er ent fault tolerance techniques at di erent operators within aquery plan  Enabling each operator to use a di erent fault tolerance strategy leads to a space of fault tolerance plansamenable to cost based optimization  We develop FTOpt  acost based fault tolerance optimizer that automatically se lects the best strategy for each operator in a query planin a manner that minimizes the expected processing timewith failures for the entire query  We implement our ap proach in a prototype parallel query processing engine  Ourexperiments demonstrate that  1  there is no single bestfault tolerance strategy for all query plans   2  often hybridstrategies that mix and match recovery techniques outper form any uniform strategy  and  3  our optimizer correctlyidenti es winning fault tolerance con gurations Categories and Subject DescriptorsC 4  Performance of Systems   Fault tolerance  modelingtechniques  H 2 4  Database Management   Systems Parallel databases Query processingGeneral TermsPerformance1 ### INTRODUCTIONThe ability to analyze large scale datasets has become acritical requirement for modern business and science  Tocarry out their analyses  users are increasingly turning to ward parallel database management systems  DBMSs   14 41  45  and other parallel data processing engines  10  15  20 deployed in shared nothing clusters of commodity servers In many systems  users can express their data processingneeds using SQL or other specialized languages  e g   PigPermission to make digital or hard copies of all or part of this work forpersonal or classroom use is granted without fee provided that copies arenot made or distributed for pro   t or commercial advantage and that copiesbear this notice and the full citation on the    rst page  To copy otherwise  torepublish  to post on servers or to redistribute to lists  requires prior speci   cpermission and or a fee SIGMOD   11  June 12   16  2011  Athens  Greece Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00 Latin  30   DryadLINQ  48    The resulting queries or scriptsare then translated into a directed acyclic graph  DAG of operators  e g   relational operators  maps  reduces  orother  20   that execute in the cluster An important challenge faced by these systems is fault tolerance  When running a parallel query at large scale some form of failure is likely to occur during execution  9  Existing systems take two radically di erent strategies tohandle failures  parallel DBMSs restart queries if failures oc cur during their execution  The limitation of this approachis that a single failure can cause the system to reprocess aquery in its entirety  While this is not a problem for queriesrunning across a small number of servers and for a shortperiod of time  it becomes undesirable for long queries us ing large numbers of servers  In contrast  MapReduce  10 and similar systems  15  materialize the output of each op erator and restart individual operators when failures occur This approach limits the amount of work repeated in theface of failures  but comes at the cost of materializing allintermediate data  which adds signi cant overhead even inthe absence of failures  Furthermore  because MapReducematerializes data in a blocking fashion  this approach pre vents users from seeing results incrementally  Partial resultsare a desirable feature during interactive data analysis nowcommonly performed with these systems  43  In this paper  we study the problem of providing usersboth the ability to see early results as motivated by onlinequery processing  17  18  and achieve a low expected totalruntime  We thus seek to enable intra query fault tolerancewithout blocking and we want to do so in a manner that min imizes the expected total runtime in the presence of failures Other objective functions could also be useful  e g   minimizeruntime without failures subject to a constraint on recoverytime   We choose to minimize the sum of time under normalprocessing and time in failure recovery  This function com bines high performance at runtime with fast failure recoveryinto a single objective  We want to minimize this functionwhile preserving pipelining Recent work  43  has also looked at the problem of com bining pipelining and fault tolerance  they developed tech niques for increased data pipelining in MapReduce  Thissystem both pipelines and materializes data between opera tors  We observe  however  that data materialization is onlyone of several strategies for achieving fault tolerance in apipelined query plan  Other strategies are possible includingrestarting a query or operator but skipping over previouslyprocessed data  21  24  or checkpointing operator states andrestarting from these checkpoints  11  21   Additionally  themost appropriate fault tolerance method may depend on theavailable resources  failure rates  and query plan properties For example  an expensive join operator may need to check point its state while an inexpensive  lter may simply skipover previously processed data after a failure Given these observations  we develop  1  a framework thatenables mixing and matching of fault tolerance techniques ina single  pipelined query plan and  2  FTOpt  a cost basedfault tolerance optimizer for this framework  Our frame work enables intra query fault tolerance without blocking thus preserving pipelining  Given a query plan and infor mation about the cluster and expected failure rates  FTOptautomatically selects the fault tolerance strategy for eachoperator in a query plan such that the overall query runtimewith failures is minimized  We call the resulting con gura tion a fault tolerance plan  In our fault tolerance plans  eachoperator can individually recover after failure and it can re cover using a di erent strategy than other operators in thesame plan  In summary  we make the following contribu tions 1  Extensible  heterogeneous fault tolerance framework We propose a framework that enables the mixing andmatching of di erent fault tolerance techniques in asingle distributed  parallel  and pipelined query plan Our framework is extensible in that it is agnostic of thespeci c operators and fault tolerance strategies used We also describe how three well known strategies canbe integrated into our framework  Section 4  2  Fault tolerance optimizer  We develop a cost basedfault tolerance optimizer  Given a query plan and afailure model  the optimizer selects the fault tolerancestrategy for each operator that minimizes the totaltime to complete the query given an expected num ber of failures  Section 5  3  Operator models for pipelined plans  We model theprocessing and recovery times for a small set of repre sentative operators  Our models capture operator per formance within a pipelined query plan rather than inisolation  They are su ciently accurate for the fault tolerance optimizer to select good plans yet su cientlysimple for global optimization using a Geometric Pro gram Solver  3   We also develop an approach thatsimpli es the modeling of other operators within ourframework thus simplifying extensibility  Section 5 3  We implemented our approach in a prototype parallelquery processing engine  The implementation includes ournew fault tolerance framework  speci c per operator fault tolerance strategies for a small set of representative opera tors  select  join  and aggregate1   and a MATLAB mod ule for the FTOpt optimizer  Our experiments demon strate that di erent fault tolerance strategies  often hybridones  lead to the best performance in di erent settings  forthe con gurations tested  total runtimes with as little asone failure di ered by up to 70  depending on the fault tolerance method selected  These results show that fault tolerance can signi cantly a ect performance  Additionally our optimizer is able to correctly identify the winning fault tolerance strategy for a given query plan  Overall  FTOpt1As in online aggregation  18   aggregates can occur at topof plans  Our prototype uses a standard aggregate operatorbut it could be replaced with an online one Data part  1O11RepartitionO21Input dataon diskCompute nodePartition 1 of operator 1 O31Data part  2O12Data part  NO1N   O22O2XFault tolerance strategy 1RepartitionO31O3YPartition 1 of operator 2 strategy 2 strategy 3       Figure 1  Parallel query plan comprising three op erators  O1  O2  O3  and one input from disk  Eachoperator is partitioned across a possibly di erentnumber of nodes  Data can be re partitioned be tween operators  Fault tolerance strategies are se lected at the granularity of operators is thus an important component of parallel data processing enabling performance gains similar in magnitude to severalother recently proposed MapReduce optimizations  22  26  2  MODEL AND ASSUMPTIONSQuery Model and Resource Allocation  In paral lel data processing systems  queries take the form of di rected acyclic graphs  DAGs  of operators that are dis tributed across servers in a cluster as illustrated in Figure 1 Servers are also referred to as nodes  Each operator canbe partitioned and these partitions then execute in parallelon the same or on di erent nodes  Multiple operators canalso share the same nodes  In this paper  we focus on non blocking query plans  which take the form of trees  ratherthan DAGs   where operators are scheduled and executed atthe same time  and where data is pipelined from one opera tor to the next  producing results incrementally  We assumethat aggregation operators  if any  appear only at the topof a plan  Since input data comes from disk  it can be readand consumed at a steady pace  there are no unexpectedbursts as in a streaming system for example   If a queryplan is too large for all operators to run simultaneously  ourapproach will optimize fault tolerance for pipelined subsetsof the plan  materializing results at the end Fault tolerance choices and resource allocation are in tertwined  an operator can perform more complex fault tolerance if it is allocated a greater fraction of the computeresources  In addition to fault tolerance strategies  our op timizer computes the appropriate allocation of resources tooperators  Due to space constraints  however  in this paper we assume that each operator is partitioned across a givennumber of compute nodes and is allocated its own core s and disk on that node  For the resource allocation details we refer the reader to Appendix A 5 Failure Model  In a shared nothing cluster  di erenttypes of failures occur  Our approach handles a variety offailures from process failures to network failures  To simplifythe presentation  we  rst focus on process failures  i e   weassume that each operator partition runs in its own processand that these processes crash and are then restarted  withan empty state  independently of one another  We comeback to more complex failures in Section 5 5 To make fault tolerance choices  our optimizer must knowthe likelihood for di erent types of failures  If ni is the totalnumber of processes allocated to operator i  we assume thatthe expected number of failures during query execution forthat operator is given by  zi  nin Z where n  Pj2O nj   Ois the set of all operators in the plan  and Z is the expectednumber of process failures for the query  Z can be estimatedfrom the observed failure rates for previous queries and ad ministrators typically know this number  9   We assumeZ to be independent of the chosen fault tolerance plan  Zdepends on the query runtime  whose order of magnitudecan be estimated by FTOpt as the total runtime withoutfault tolerance and without failures  we show that resultsare robust to small errors in Z s value in Section 6 6  Operator Determinism  We assume that individualoperator partitions are deterministic  i e   an operator par tition produces an identical output when it processes thesame input tuples in the same order  This is a commonassumption  20  47  37  2  36  23  and most relational opera tors are deterministic  In a distributed system  however  theorder in which input tuples reach an operator partition maynot be deterministic  Our approach handles this scenario 3  RELATED WORKFault Tolerance in Relational DBMSs  Commercialrelational DBMSs provide fault tolerance through replica tion  6  34  40   Similarly  parallel DBMSs  14  41  45  usereplication to handle various types of failures  Neither  how ever  provides intra query fault tolerance  32  Main memory DBMSs  25  35  28  use a variety of check pointing strategies to preserve the in memory state of theirdatabases  In contrast  our approach preserves and recoversthe state of ongoing computations Fault Tolerance in MapReduce type systems  TheMapReduce framework  10  provides intra query fault tolerance by materializing results between operators andre processing these results upon operator failures  Thisapproach  however  imposes a high runtime overhead andprevents users from seeing any output until the job com pletes  In Dryad  20   data between operators can either bepipelined or materialized  In contrast  we strive to achieveboth pipelining and fault tolerance at the same time  Wealso study how to decide when to materialize or check point data  Recent work  47  applies MapReduce style fault tolerance to distributed databases by breaking long runningqueries into small ones that execute and can be restarted in dependently  This approach  however  supports only a spe ci c type of queries over a star schema  In contrast  weexplore techniques that are more generally applicable  Re cent work also introduced the ability to partly pipeline datain Hadoop  43   a MapReduce type platform  This work iscomplementary to ours as it retains the use of materializa tion throughout the query plan for fault tolerance purposes Other Fault Tolerance Strategies  In the distributedsystems and stream processing literatures  several additionalfault tolerance strategies have been proposed  11  21  37  All these strategies involve replication  One set of tech niques is based on the state machine approach  Here  thesame computation is performed in parallel by two process ing nodes  2  36  37   We do not consider such techniques inthis paper because of their overhead  to tolerate even a sin gle failure  they require twice the resources  The second setof techniques uses rollback recovery methods  11  21   wherethe system takes periodic snapshots of its state that it copiesonto stable storage  i e   into memory of other nodes or ontodisk   We show how to integrate the latter techniques intoour fault tolerance optimization framework  Section 4 2  Recently  Simitsis et  al   38  studied the problem ofselecting fault tolerance strategies and recovery points forETL ows  Similar to us they consider using di erent fault tolerance strategies within a single ow  In contrast to ourwork  they do not propose a general heterogeneous fault tolerance framework  do not have individually recoverableoperators  and do not optimize for overall latency nor showhow fault tolerance choices a ect processing latencies Additional Related Work  Hwang et al   19  studiedself con guring high availability methods  Their approachis orthogonal to our work as it is based on a uniform check pointing strategy and optimizes the time when checkpointsare taken and the backup nodes where they are saved Techniques for query suspend and resume  4  5  use roll back recovery but are otherwise orthogonal to our work Phoenix App  27  explores the problem of heterogeneousfault tolerance in the context of web enterprise applications This approach identi es three types of software compo nents  Persistent  Transactional  and External depending onthe fault tolerance strategy that each uses  message loggingwith checkpointing  transactions  or nothing respectively  Phoenix App then de nes di erent  interaction contracts for each combination of component types  Each contractimplements a di erent protocol with di erent guarantees Thus in Phoenix App  the protocol depends on the fault tolerance capabilities of the communicating components  Incontrast  our approach enables the mixing and matching offault tolerance strategies without changes to the protocol 4  FRAMEWORK FOR HETEROGENEOUS FAULT TOLERANCEWe present a framework for mixing and matching fault tolerance techniques  Our framework relies on conceptsfrom the literature including logging  acknowledging  and re playing tuples as previously done in uniform fault tolerancesettings  21  37  and  contract based  methods for querysuspend resume  4   Our contribution lies in articulatinghow these strategies can be used to enable fault tolerance het erogeneity  We also discuss how three fault tolerance tech niques from the literature can be used within our framework 4 1 ProtocolTo enable heterogeneous fault tolerance between consec utive operators in a query plan  we isolate these operatorsby  xing the semantics of their interactions through a setof four rules  These rules enable each operator to be indi vidually restartable without requiring any blocking materi alization as in MapReduce and also without requiring thatall operators use the same fault tolerance strategy In our framework  as in any parallel data processing sys tem  operators receive input tuples from their upstreamneighbors  they process these tuples and send results down stream  For example  in Figure 1  each partition of operatorO2 receives data from each O1 partition and sends data toall O3 partitions  If an operator partition such as O21 fails a new instance of the operator partition is started with anempty state  To recover the failed state  in our framework the new instance can read any state persistently captured bythe operator s fault tolerance strategy  It can also ask up stream operators to resend  a subset  of their data  To en able such replays  tuples must have unique identi ers  whichmay or may not be visible to applications  and operatorsmust remember the output they produced  For this  we de  ne the following two rules Rule 4 1  Each relation must have a key Rule 4 2  Producer replay guarantee  Upon request  anoperator  must regenerate and resend in order and withoutduplicates any subset of unacknowledged output tuples Acknowledgments mentioned in this rule help reduce thepotential overhead of storing old output tuples by bound ing how much history must be retained  21  37   In ourframework  acknowledgments are optional and are sent fromdownstream operators to upstream ones  For example  onceall operator partitions O21 through O2X that have receivedan input tuple t from operator partition O11 acknowledgethis tuple  the tuple need no longer be retained by O11 Upon sending an acknowledgment  an operator promisesnever to ask for the corresponding tuple again  Formally Rule 4 3  Consumer progress guarantee  If an operatoracknowledges a tuple rx  it guarantees that  even in case offailure  it will never ask for rx again Most parallel data processing systems use in order com munication  e g   TCP  between operators  In that case  anoperator can send a single message with the identi er of atuple rx to acknowledge all tuples up to and including rx When a failure occurs and an operator restarts with anempty state  most fault tolerance techniques will cause theoperator to produce duplicate tuples during recovery  To en sure that an operator can eliminate duplicates before send ing them downstream  we add a last requirement Rule 4 4  Consumer Durability Guarantee  Upon re quest  an operator Od must produce the identi er of themost recent input tuple that it has received from an upstreamneighbor Ou Together  these four rules enable a parallel system to en sure that it produces the same output tuples in the sameorder with and without failure  the tuples may still be de layed due to failure recovery   They also enable operatorsto be individually restartable and the query plan to be bothpipelined and fault tolerant  since data can be transmittedat anytime between operators  Finally  the framework isagnostic of the fault tolerance method used as long as themethod works within the pre de ned types of interactions From the above four rules  only the  Producer replay guar antee  rule potentially adds a visible overhead to the systemsince it requires that a producer be able to re generate  partof  its output 2A no cost solution to satisfy this rule isfor an operator to restart itself upon receiving a replay re quest  With this strategy  an operator failure can cause acascading rollback e ect  where all preceding operators inthe plan get also restarted  This approach is equivalent torestarting a subset of the query plan after a failure occursand is no worse than what parallel DBMSs do today  Alter natively  an operator could write its output to disk  Finally 2Our framework also requires unique identi ers for tuples In our implementation  we create unique identi ers consist ing of 3 integers  44   for the 512 byte tuples used in ourexperiments the space overhead is less than 2 5  some operators  such as joins  can also easily re generatetheir output from their state without the need to log theiroutput  Each of these solutions leads to di erent expectedquery runtimes with and without failures  Our optimizeris precisely designed to select the correct strategy for eachoperator  from a pre de ned set of strategies  in a way thatminimizes the total runtime with failures for a given queryplan as we discuss further below 4 2 Concrete Framework InstanceWe now discuss how three fault tolerance strategies fromthe literature can be integrated into our framework Even though the operators in our framework are deter ministic  see Section 2   in a distributed setting tuples mayarrive in di erent interleaved order on di erent inputs  Wedevelop a low overhead method  based on lightweight log ging of information about input tuple processing order  toensure determinism in this case  but we omit it due to spaceconstraints and refer the reader to Appendix A 4 Strategy NONE  Within our framework  an operatorcan choose to do nothing to make itself fault tolerant  Wecall this strategy NONE  To ensure that it can recover froma failure  such an operator can simply avoid sending any ac knowledgments upstream  Upon a failure  that operator canthen request that its upstream neighbors replay their entireoutput  This strategy is analogous to the upstream backupapproach developed for stream processing engines  21  As in upstream backup  operators such as select or projectthat do not maintain state between consecutive tuples  i e   stateless operators   can send acknowledgments in somecases  e g   if an input tuple r makes it through a selection togenerate the output q and is acknowledged by all operatorsdownstream  then r can be acknowledged  Unlike upstreambackup  which uses di erent types of acknowledgments  21  our approach uses only one type of acknowledgments facili tating heterogeneous fault tolerance  This approach of skip ping over input data during recovery has also been used forresumptions of interrupted warehouse loads  24  To handle a request for output tuples  a stateless oper ator can fail and restart itself to reproduce the requesteddata  For stateful operators  i e   operators such as joinsthat maintain state between consecutive tuples   a more ef  cient strategy is to maintain an output queue and replaythe requested data  21   Such a queue  however  can stillimpose a signi cant memory overhead and an I O overheadif the queue is written to disk  We observe  however  thatstateful relational operators need not keep such output queuebut  instead  can re generate the data from their state  Weimplement this strategy and use it in our evaluation Strategy MATERIALIZE  An alternate rollback re covery approach consists in logging intermediate results be tween operators as in MapReduce  10   While CHCKPTspeeds up recovery for the checkpointed operator itself  MA TERIALIZE potentially speeds up recovery for downstreamoperators  to satisfy a replay request  an operator can sim ply re read the materialized data  Since materialized outputtuples need never be generated again  an operator can usethe same acknowledgement and recovery policy as in NONE Strategy CHCKPT  This strategy is a type of rollbackrecovery strategy where operators  state is periodically savedto stable storage  Because our framework recovers operatorsindividually  it requires what is called uncoordinated check pointing with logging  11   One approach that can directly beapplied is passive standby  21   where operators take periodiccheckpoints of their state  independently of other operators Our framework requires that an operator save su cientinformation to guarantee the consumer progress  consumerdurability  and producer replay guarantees  For this  the op erator must log its state  e g   partial aggregates  join hashtables  and  when applicable  its output queue  The opera tor can acknowledge checkpointed input tuples  Upon fail ures  the operator restarts from the last checkpoint  As anoptimization  operators can checkpoint only delta changesof their state  11   Other optimizations are also possible  11 19  23  and can be used with our framework Unlike NONE and MATERIALIZE  with CHCKPTblocking operators also bene t from fault tolerance provi sioning as they can checkpoint their state periodically andrestart from the latest checkpoint after a failure 3In summary  while our framework imposes constraintson operator interactions  all three of these common fault tolerance strategies can easily be incorporated into it 5  FTOptFTOpt is an optimizer for our heterogeneous fault tolerance framework  FTOpt runs as a post processing step it takes as input  a  a query plan selected by the queryoptimizer and annotates it with the fault tolerance strate gies to use  The optimizer also takes as input  b  informa tion about the cluster resources and cluster failure model and  c  models for the operators in the plan under di er ent fault tolerance strategies  FTOpt produces as outputa fault tolerance plan that minimizes an objective function i e   the expected runtime with failures  given a set of con straints  that model the plan  FTOpt s fault tolerance plans have three parts   1  afault tolerance strategy for each operator   2  checkpoint fre quencies for all operators that should checkpoint their states and  3  an allocation of resources to operators  As indicatedin Section 2  however  we do not discuss resource allocationin this paper due to space constraints  We assume a givenresource allocation to operators For a given query plan  the optimizer s search space thusconsists of all combinations of fault tolerance strategies  Inthis paper  we use a brute force technique to enumeratethrough that search space and leave more e cient enumera tion algorithms for future work  For each such combination FTOpt estimates the expected total runtime with failures the optimal checkpoint frequencies and  optionally  an allo cation of resources to operators  44   It then chooses theplan with the minimum total runtime with failures 5 1 Geometric ModelAs it enumerates through the search space  given a po tential fault tolerance plan  in order to select optimal check point frequencies and estimate the total runtime with fail ures for the plan  FTOpt uses a geometric programming GP  framework  GP allows expressions that model resourcescaling  for resource allocation  and non linear operator be havior  but still  nds a global minima for the model  3  In a geometric optimization problem  the goal is to mini mize a function f0  x   where  x is the optimization variable3FTOpt works irrespective of a blocking operator s place ment in a query plan  We focus on online query processingsince FTOpt is especially useful for such plans  it enablesfault tolerance without creating unnecessary blocking vector  The optimization is subject to constraints on otherfunctions fi  x  and gi  x   All of  x  g  x   and f  x  are con strained to take the following speci c forms    x    x1          xn  such that 8i xi   0  xi 2 R   g  x  must be a monomial of the form cxa11 xa22      xannwith c   0 and ai 2 R   f  x  must be a posynomial de ned as a sum of one ormore monomials  Speci cally  with ck   0 and aik 2R  f  x   Pk Kk 1ckxa1k1 xa2k2      xankn  The optimization is then expressed as follows minimize f0  x subject to fi  x    1  i   1          mgi  x    1  i   1          pIn our case   x    i2O ci  Ni  xNi  xRDi  xRSi   where O isthe set of all operators in the query plan and   denotesconcatenation  Each operator has a vector of variables thatincludes  ci  its checkpoint frequency  Ni  the number ofnodes assigned to it  we assume that it is  xed in this pa per   and xNi  xRDi  and xRSi  which capture the average timebetween two consecutive output tuples  requested replay tu ples  and  units of recovery   a measure of recovery speed  respectively  Tables 1 and 2 summarize these parameters we come back to the tables shortly  Our objective function  f0  x    Ttotal  is the total timeto execute the query including time spent recovering fromfailures  We de ne it more precisely in Sections 5 2 and 5 3 Our constraints comprise framework and operator con straints  The former constrain how operator models arecomposed   a  the average input and output rates of con secutive operators must be equal since the query plan ispipelined   b  aggregate input and output rates for opera tors cannot exceed the network and processing limits  and c  if an operator uses an output queue  it must either check point its output queue to disk frequently enough  or mustreceive acknowledgements from downstream operators fre quently enough to never run out of memory  Individual op erators can add further constraints  see Section 5 3  5 2 Objective FunctionFTOpt minimizes the following cost function  that cap tures the expected runtime of a query plan Ttotal   maxp2P Tpd  i Xd1i 1Dpi  Xi2Ozi   Ri  1 The  rst term is the total time needed to completely pro cess the query including the overhead of fault tolerance if nofailures occur The second term is the expected time spentin recovery from failures  Failure recovery can be added ontop of normal processing because  with our approach  whena failure occurs  it blocks the entire pipeline  Indeed  evenif one operator partition fails  operators upstream from thatpartition stop executing normally and take part in the re covery  A side e ect of this approach is that recovering asingle operator partition or recovering all partitions yieldapproximately the same recovery time In more detail  for the  rst term  P is the set of all pathsfrom the root of the query tree to the leaves  For a givenpath p 2 P of length d  the root is labeled with p1 andthe leaf with pd  Dpiis the delay introduced by operator pi Table 1  Functions capturing operator behavior Delay to produce the  rst tupleDN    Average delay to output  rst tuple during normal pro cessing  with fault tolerance overheads  DRD    Average delay to produce  rst tuple requested by a down stream operator during a replay DRS    Average delay to the start of state recovery on failure Average processing timexN    Average time interval between successive output tuplesduring normal runtime  with fault tolerance overheads  xRD    Average time interval between successive output tuplesrequested by a downstream operator xRS    Average time interval between strategy speci c  units ofrecovery   e g   checkpointed tuples read from disk  Acknowledgement interval  a     sent to upstream nodes Table 2  Operator behavior parameters    Query parametersjIuj Number of input tuples received from upstream operator u jIj Number of tuples produced by current operator Operator parameterstcpu Operator cost in terms of time to process one tuple tioThe time taken to write a tuple to disk Runtime parametersxNu Average inter tuple arrival time from upstream operator u innormal processing F Fault tolerance strategy c Number of tuples processed between consecutive checkpoints N The number of nodes assigned to the operator Surrounding fault tolerance contextad Maximum number of unacknowledged output tuples xRDu Average inter tuple arrival time from upstream operator uduring a replay where the delay is de ned as the time taken to produce its rst output tuple from the moment it receives its  rst inputtuple  and  Tpdis the time taken by the leaf operator tocomplete all processing after receiving its  rst input tuple Tpd   Dpd  xNpdjIj  where jIj is the number of output tuplesproduced by the leaf operator  Dpi and Tpd depend on theinput tuple arrival rate at an operator  which depend on howfast previous operators are in the pipeline  We capture thesedependencies with constraints as mentioned in Section 5 1 For the second term  O is the set of all operators in thetree  For operator i 2 O  zi is the expected number of fail ures during query execution  and Ri is the expected recoverytime from a failure  We estimate zi using an administrator provided failure model as described in Section 2 To adapt Ttotal to be a posynomial  we need to eliminatethe max operator  For this  we introduce a variable T forTtotal and decompose the objective to be  minimize T witha constraint for each path such that   T 1  expected totaltime for the path     1   Since the expected total time  fora single path  is a posynomial  the constraints are posyno mials  and the entire program is a GP 5 3 Operator ModelingTo compute the objective function  FTOpt thus requiresthat each operator provide expressions that characterizeits delay and processing time during normal operation andwhen failures occur  These expressions must be providedfor each fault tolerance strategy that the operator supports Formally  FTOpt needs to be given the functions in Table 1expressed in terms of the parameters  represented by   inTable 2  these parameters capture the inter dependenciesbetween operators   FTOpt combines these functions to gether to derive the overall processing time Ttotal In this section  we show how to express such functions inActual runtimeNumber of output  tuplesTimeTotal outputtacpuPoint of change  tangent NBout nin Figure 2  Data output curve  comprised of NBout   curve till the  Point of change  and the dashed lineafter it  for a symmetric hash join operator our framework  We proceed through an example  we derivethe constraint equations for join  symmetric hash join   Themodels for select and aggregate are similarly derived  44  5 3 1 Modeling Basic Operator RuntimeTo model our problem as a GP  we must  a  derive theoperator output rate  given by the inter output tuple delay xN  in the absence of failures  and  b  derive the delay  DN The delay  however  is simply either negligible for selects andthe symmetric hash join that we model or equal to the totalprocessing time for aggregates 4The challenge in expressing an operator s output rate isthat xNcan follow a complex curve for some operators suchas certain non blocking join algorithms as illustrated in Fig ure 2  The  gure shows the data output curve for a sym metric hash join operator  For this operator  the more tu ples that it has already processed  the more likely the joinis to  nd matching tuples  and thus it outputs tuples at anincreasingly faster rate  As a result  at the beginning ofthe computation  the bottleneck is the input data rate  theNBout nin  curve  and the operator produces increasinglymore output tuples for each input tuple  Eventually  theCPU at the join becomes the bottleneck  tcpua curve  andthe output rate attens We found that ignoring such e ects and assuming a con stant output signi cantly underestimated the total runtimefor the operator  Alternatively  modeling these e ects andexposing them to downstream operators signi cantly com plicated the overall optimization problem  We thus optedfor the following middle ground  we model the non uniformoutput rate of an operator to derive its total runtime  Giventhe total runtime  we compute the equivalent average out put rate that we use as a constant input arrival rate for thenext operator  The GP framework is helpful here to expressthese non linear behaviors Interestingly  we  nd that we can automatically derive theabove curve from the following operator properties   tcpua   Average time to generate one output tuple if allinput is available with no delay   NBout nin   This function provides the total numberof output tuples produced for a given number of tuples nin  received across all input streams The above functions can easily be derived  hence simpli fying optimizer extensibility   Both these functions are ex tensions of parameters of standard query optimizers   a tcpua corresponds to the standard query optimizer functionfor computing an operator s cost  except that we then di vide this cost by the operator output cardinality  and  b 4To compute total query times  we ignore any partial resultsthat an online aggregate may produce  NBout nin  is similar to computing the cardinality of anoperator output  except that it also captures how that out put is produced as the input data arrives  Simple operatorslike select or merge join have NBout    nin  where   isthe operator selectivity  For blocking operators such as ag gregates  after the delay DN    all the output tuples areproduced at once and hence NBout   jIj  For other non blocking operators the relationship can be more complex aswe discuss next using our symmetric hash join as example For the symmetric hash join operator  de ne Iutot to bethe set of all tuples received from both upstream input chan nels  Hence  jIutotj   jI1j   jI2j  For this operator tcpua   jIj1 jIutotj   jIj tcpuThe expression is a product of the average time takento process either an input or output tuple  tcpu  obtainedthrough micro benchmarks  and the total number of tuplesseen by the operator  including the input tuples  Iutot  andthe output join tuples  I   This number is then divided bythe total number of output tuples  jIj  to get the averagetime per output tuple To get the NBout function for a symmetric hash join weassume that the input tuples from the two input channelscan arrive in any order  each order being equally likely  Let    a function of the join selectivity    jI1j and jI2j  44   be theprobability that two tuples from di erent channels join andpi be the probability that a tuple belongs to the ithchannel In this case  the function NBout nin  is de ned as follows NBout nin      p1p2nin nin  1     p  1p2n2inIntuitively  nin nin1  is the count of pairs of distinct tuplesto join  p1p2 is the probability that they come from di erentchannels  and    is the probability that they join We now show how our optimizer translates these functionsinto a set of inequalities that characterize the average timeinterval between successive output tuples produced by anoperator  For this  we require that the NBout nin  functiontake the form  NBout nin    nkin  in order to  t into theGP framework  Thus  for our join operator       p1p2 andk   2  Informally  as the operator sees more input tuples the number of the output tuples produced after processinga single new input tuple should never decrease Given the above  the average time interval between con secutive output tuples  xN  is given by the following inequal ities me    xIN kktk1fme    tcpua  1me   1k  xIN 1kjIj1 1k 1  k1 tf   jIjm1e   xNjIjThe above inequalities take xINas input  which is the timeinterval at which input tuples are arriving  xINdepends onthe current execution context  If we are operating normally it is the average time interval between tuples produced bythe upstream operators  if we are recovering from a failure we might read the input tuples from disk at the maximumbandwidth possible for the disk For the exact derivation of this mode  we refer the readerto Appendix B  Here  we only provide the intuition behindit In the above equations  jIj is the output cardinality  and k come from the NBout nin  function  me is the num ber of output tuples produced per second at the instant theprocessing ends and tf is the  rst time at which the outputproduces tuples at the rate me  The  rst equation realizesthis relationship between me and tf   The following inequal ity states that the operator can not take less than tcpua timeto produce an output tuple  since this is the least amountof time the processor needs per tuple  given the resourcesit has  For the second inequality  its right hand side is themaximum rate at which output could be produced if theonly bottleneck was the rate of arrival of input tuples  Notethat  since we require the NBout    function to have a non negative rate of change  the fastest output production ratewill be at the end of the computation and the derivative ofthe function NBout     with respect to xIN  at the end givesus this value  Since  in a real computation the processingcost is positive  the actual observed rate has to be less thanthe derivative  the right hand side in the second inequality  The third inequality states that the total time to process alltuples  which is equal to the average output rate times thenumber of output produced  must be higher than the actualprocessing time  which is its left hand side To model a di erent operator  the functions for tcpua andNBout nin  would change  while the form of the inequalitiesand equalities used by the optimizer would remain the same They simply use the above as parameters We model a partitioned operator as a single operator thatscales linearly with allocated resources  This approach suf  ces to show the feasibility and impact of fault toleranceoptimization  We leave extensions to more complex models including data skew between partitions  for future work 5 3 2 Modeling Overhead of Fault toleranceFault tolerance overhead only a ects tcpua   the time an op erator needs to produce an output  The model depends onthe operator implementation  For MATERIALIZE  our joinwrites all output tuples to disk  For CHCKPT  it logs theincoming tuples to disk incrementally5  The join does notmaintain any output queue For brevity  we use the notation that IN  IMand ICare 1 ifNONE  MATERIALIZE or CHCKPT is the chosen fault tol erance option  respectively  and are 0 otherwise  Althoughwe need one equation per fault tolerance strategy we repre sent them as a single one tcpua   jIj1 tcpu jIutotj   jIj    ICtiojIutotj   IMtiojIj Here tiois the time to write a tuple to disk and is alsoobtained through micro benchmarks 5 3 3 Modeling Replay Request TimesFTOpt also needs to know the average rate at which out put tuples are produced to satisfy a replay request and thedelay in generating the  rst requested tuple  The replayrate may depend on when  during the course of the query the downstream fails  For example  if the replay behavesas during normal operations for the symmetric hash join  itmight be slower if the downstream fails early on and be fasterlater  To approximate the recovery rate we  nd the time ittakes to replay all output tuples and divide that number by5Out of simplicity  our join checkpoints input tuples as theyarrive rather than checkpointing the hash table  When itrebuilds the hash table from a checkpoint  the operator doesnot redo the join             the total number of output tuples  During this replay phase the operator has no fault tolerance overheads As before  the exact model depends on the implementa tion details  Our join implementation uses its in memoryhash table to regenerate outputs and hence the delay is neg ligible  But it could be signi cant for a join that can not useeither its state or its output to answer tuple requests To get the average output rate  we reuse the frameworkwe developed in the previous section  Thus we only need tospecify tcpua and NBout nin  for the replay mode Since  during replay  we only reprocess the inputs withoutany fault tolerance overhead  tcpua   jIj1 jIutotj   jIj tcpu The form of the NBout    remains the same as for the nor mal processing  Also  during reprocessing the input tuplesare already in memory  hence the inter tuple arrival time ofinputs xINis at least tcpuand we take xIN  tcpu 5 3 4 Modeling Recovery TimeTo compute the total time to recover from a failure  weneed to know the average rate at which recovery proceeds As before  the exact recovery model depends on the imple mentation  For our join  upon failure the MATERIALIZEand the NONE options have to request all the input fromthe upstream nodes and rebuild the hash table exactly as itwas before  using Rule 4 2 and operator determinism  44   while CHCKPT rebuilds it from the input tuples logged todisk In all cases  during recovery  no output is produced whenthe input tuples are processed to remake the hash table Thus  tcpua   tcpusince we look at each input tuple once To de ne the function NBout nin  we think of the hashtable being rebuilt as the desired  output  and the inputtuples as the inputs  Since all the input tuples are usedto generate the  output  hash table  NBout nin    nin For MATERIALIZE and NONE  xINis the average timeinterval in which requested tuples from the upstream nodesarrive  For CHCKPT  since we directly read tuples from thedisk  xIN  tio The delay in getting the  rst input is negligible if we useCHCKPT and is equal to the delay of the upstream tuplesin the case of NONE and MATERIALIZE We approximate the expected hash table size to recoverto be12jIutotj  Thus  the expected time to recover is thesum of  1  the delay to receive the  rst input tuple  and  2 the product of the expected hash table size and the averagetime per tuple spent in adding a tuple to that hash table In summary  compared to existing cost models for paral lel query runtime estimation  13  12  and fault tolerance instreaming engines  21   our models capture the dynamic op erator interactions in pipelined queries  which we observedto a ect runtime predictions and fault tolerance optimiza tion  For example  a fast operator following a slow one in apipeline will produce its output slowly  At the same time we do not require that an operator s output tuples be uni formly spread across the entire execution time of the oper ator  16  49   Indeed  because we use a GP framework  wesupport simple types of non uniform outputs such as thatof asymmetric hash join  Of course  our GP framework maynot cover all cases  In particular  for multi phase operators e g   a symmetric hash join that spills state to disk   we maystill need to split the operator into multiple sub operatorsfor more accurate modeling of each phase 5 4 Approach ImplementabilityOur approach consists of  1  a protocol that enables het erogeneous fault tolerance in a parallel query plan and  2 an optimizer that automatically selects the fault tolerancestrategy that each operator should use  We now discuss thedi culty of implementing this approach in a parallel dataprocessing system To implement our approach  developers need to  a  imple ment desired fault tolerance strategies for their operators ina manner that follows our protocol  In Section 4 2  however we showed  how to e ciently implement three well knownfault tolerance strategies for generic stateless and statefuloperators  Existing libraries can also help with such imple mentation  e g    23    Developers must also  b  model theiroperator costs within a pipelined query plan  To simplifythis latter task  we develop an approach that requires onlythat developers specify well known functions under di erentfault tolerance strategies and during recovery  an operatorcost function and a function that computes how the outputsize of an operator grows with the input size  Our opti mizer derives the resulting operator dynamics automatically For parallel database systems  41  14  and MapReduce typesystems such as Hive  1  or Pig  30   which come with pre de ned operators  the above overhead needs only be paidonce and we thus posit that it is a reasonable requirement For user de ned operators  UDOs   the above may stillbe too much to ask  In that case  the simplest strategyis to treat UDOs as if they could only support the NONEor MATERIALIZE strategies  depending on the underlyingplatform  without ever producing acknowledgments  Withthis approach  UDO writers need not do any extra workat all  yet the overall query plan can still be optimized andachieve higher performance than without fault tolerance op timization as we show in Section 6 4 Finally  our approach relies on a set of parameters includ ing IO cost  expressed as the time tiospent in a byte sizeddisk IO   per operator CPU cost  expressed as the time tcpuspent processing each tuple   and total network bandwidth Commercial database systems already automate the collec tion of such statistics  e g    31    though tcpuis typicallyexpanded into a more detailed formula Other necessary information includes the expected num ber of failures for the query  see Section 2   operator selectiv ities  standard optimizer provided metric   and an estimateof the total checkpointable state  As shown in Section 6 6 our optimizer is insensitive to small errors in these estimates Overall  the requirements of our fault tolerance optimiza tion framework are thus similar to those of existing cost based query optimizers 5 5 Handling Complex Failure ScenariosSo far  we have focused on process failures  However  ourapproach also handles other types of failures Our approach still works when entire physical machinesfail  e g   due to a disk failure  a power failure  or a net work failure   To support such failures  checkpoints must bewritten to remote nodes instead of locally  19   which addsnetwork and CPU costs that must be taken into account bythe optimizer  Given that the optimizer knows the size ofthese checkpoints  it can take that cost into account  Sec ond  when a physical machine fails or becomes disconnected the number of nodes in the cluster is reduced by one  whichmust also be taken into account by the optimizer  22 2  2  2  88  16 a  Query 11608 8  8  8  88  8 b  Query 21608 16  8  16  160  0 008 c  Query 31608 40  8  40  80  0 008 d  Query 41608 8  8  8  88  328 96 20 1  8 96  20 1  79 4  0 008 e  Query 5Figure 3  Query plans used in experiments         denote Select  Join  and Aggregation  respectively All numbers are in millions Our approach does not currently handle failures that af fect a large number of machines  Indeed  such failures cancause the temporary loss of input data or checkpointed data In such cases  the query needs to be restarted in its entiretyonce the input data becomes available again  In general however  large scale rack and network failures are infrequent while single machine failures are common  For example Google reports 5 average worker deaths per MapReduce jobin March 2006  9   but only approximately 20 rack failuresper year  and similarly few network failures   8  Even though our approach does not handle large scale fail ures that cause the loss of input or checkpointed data  it doeshandle multiple operators failing at the same time  The onlyrequirement in such cases is that operators be restarted fromdownstream to upstream  ensuring that each operator knowswhere to start recovering from before asking upstream neigh bors to replay data 6  EVALUATIONWe evaluate FTOpt by answering the following questions  1  Does the choice of fault tolerance strategy for a parallelquery matter   2  Are there con gurations where a hybridplan  where di erent operators use di erent fault tolerancetechniques  outperforms uniform plans   3  Is our optimizerable to  nd good fault tolerance plans automatically   4 How do user de ned operators a ect FTOpt   5  What isthe scalability of our approach   6  How sensitive is FTOptto estimation errors in its various parameters We answer these questions through experiments with a va riety of queries in a 17 node cluster  Each node has dual 2 5GHz Quad Core E5420 processors and 16 GB RAM runningLinux kernel 2 6 18 with two 7 2K RPM 750 GB SATA harddisks  The cluster runs a simple parallel data processing en gine that we wrote in Java  The implementation includesour new fault tolerance framework and speci c per operatorfault tolerance strategies for a small set of representative op erators  All fault tolerance strategies were moderately opti mized  see Section 4 2   We implemented the optimizer inMATLAB using the cvx package  7  The query plans that we use in the experiments are shownin Figure 3  They include an SJJJ and SJJA query  we alsotest a more complex query later in this section   For bothqueries we have 8 partitions per operator with 2 cores and1 disk per partition  Partitions of the same operator run ondi erent machines  The input data is synthetic and withoutskew  Tuples are 0 5 KB in size  The schema consists of 40 50 100 150 200 250 NM  NN  MM  CC  cc  MM  cc Select Join  Select Average  Select Join Runtime  s  Configurations Real  Predicted Figure 4  Runtime without failures for various two operator queries  X axis labels show the fault tolerance strategy chosen  N for NONE  M for MA TERIALIZE  C for CHCKPT with a total of 10checkpoints  and c for CHCKPT with 1K check points attributes used to hash partition tuples for each operator  a5th attribute for grouping the aggregates  and a 6th one forthe join predicates  A separate producer process generatesinput tuples  For a given plan  we get the expected recoverytime by injecting a failure midway through the time theplan takes to execute with no failures  We inject exactly onefailure per run and show the recovery time averaged over alldistinct operators in the plan 6 1 Model Validation ExperimentsFTOpt requires the tcpuand the tiovalues for each opera tor  It also requires the network bandwidth for each machinein the cluster  Through micro benchmarks  we  nd that theaverage time to read a tuple from disk  sequential read  istio  13 0  s for a 0 5 KB tuple  This number is equivalentto a disk throughput of 37 MBps  For select and aggregateoperators  we measure tcputo be 1 82 s  The join operator internally  works in two parts   1  hashing the input tupleand storing it in one of the tables for a cost of t1   8 sand  2  joining the hashed input tuple to the correspondingtuples from the other table for a cost of t2   1 s  We uset1  t2  and the operator s selectivity to estimate its tcpu  Fi nally  we measure the network I O time per 0 5 KB tupleto be 4 7 s  which is equivalent to a network bandwidth of109 4 MBps and is close to the theoretical maximum of 1Gbps network bandwidth for each machine in the cluster These parameters along with our operator models enableFTOpt to predict the runtime for an entire query plan Figure 4 shows the runtime without failure for a few two operator queries  While the median percentage di erencebetween real and predicted runtime is 9 5   this error issmall given the overall di erences in runtime between var ious con gurations  We measure the sensitivity of our ap proach to the benchmarked parameter values in Section 6 6 6 2 Impact of Fault Tolerance StrategyThe  rst question that we ask is whether a fault toleranceoptimizer is useful  how much does it really matter whatfault tolerance strategy is used for a query plan Figures 5 through 7 show the actual and predicted run times for Queries 1 through 3 from Figure 3 with 8 partitionsper operator  Note that  each join receives input from twosources  its upstream operator in the plan and a producerprocess  In all our experiments  an equal number of tupleswas received from each source  Whenever FTOpt selectsCHCKPT as a strategy  it also chooses the checkpoint fre quency  Query 3   In other cases  we use 100 checkpoints  aCKPT MAT NONE OPT  RESTART010203040506070Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed Normal250 CKPTFigure 5  Query 1  SJJJ  MAT NONE OPT  RESTARTCKPT MAT OPT  NONE RESTART050100150200250Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 6  Query 2  SJJJ with lower selectivities manually selected value that we found to give high perfor mance in these experiments The most important result from these experiments is that while these queries are all similar to each other  each one re quires a di erent fault tolerance plan to achieve best perfor mance  For Query 1  a uniform NONE strategy is best  ForQuery 2  uniform MATERIALIZE wins  Finally  for Query3  uniform CHCKPT outperforms the other options Second  restarting a query is at most 50  slower than astrategy with more  ne grained fault tolerance  The  ne grained strategy gains the most when it reduces recoverytimes with minimal impact on runtime without failures For some queries  the appropriate choice of fault tolerancegets close to this theoretical upper bound  For Query 2 RESTART is 31  worse than the best strategy while forQuery 3  restarting is 44  slower than the best strategy Achieving such gains  however  requires fault tolerance op timization  Indeed  di erent strategies win for di erentqueries and a wrong fault tolerance strategy choice leads tomuch worse performance than restarting a query  Overall the di erences between the best and worst plan are high 58  for Query 1  31  for Query 2  and 72  for Query 3 Finally  in all cases  FTOpt is able to identify the winningstrategy  Predicted runtimes do not always match the ob served ones exactly  Most of the di erence is attributable toour simple model for the network and FTOpt s predictionsare thus more accurate when either CPU or disk IO is thebottleneck in a query plan and less accurate when it is thenetwork  While we could further re ne our models  to pickthe optimal strategy  we only need to have correct relativeorder of predicted runtimes for di erent plans  As shownin Figures 4 through 8  FTOpt preserves that order whenruntime di erences are large  When two con gurations leadto very similar runtimes  FTOpt may not  nd the best ofthese plans but the choice of plan matters less in such a caseand FTOpt always suggests one of the good plans In summary  the correct choice of fault tolerance strategycan signi cantly impact query runtime and that choice isnot obvious as similar query plans may require very di erentstrategies  FTOpt can automatically select a good plan 6 3 Bene   ts of Hybrid Con   gurationsWe now consider a query  Query 4   similar to QueryCKPT OPT  MAT NONE RESTART050100150200250300350400450500Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 7  Query 3  SJJA query CKPT MAT NONE HYBRID OPT  RESTART050100150200250300350400450Runtime  s Predicted Recovery Predicted NormalObserved Recovery Observed NormalFigure 8  Query 4  SJJA with more expensive joins  The hybrid strategy is to materialize after select  donothing for joins  and checkpoint the aggregate3  but with the joins processing and producing much moredata  making checkpointing expensive  Figure 8 shows thata hybrid strategy that materializes the select s output  doesnothing for the joins  and checkpoints the aggregate s statefor a total of 40 checkpoints  value selected by the opti mizer   yields the best performance  The uniform strate gies are 15  slower at best and 21  slower at worst whileRESTART is 35  slower We observe similar gains for a longer query  Query 5 with eight operators  Figure 9 shows that the hybrid plan chosen by the optimizer  materializes both selects  outputs does nothing for joins and takes 20 checkpoints of the aggre gate  The best and worst uniform strategies and RESTARTare 16   23  and 36  slower  respectively  Manually  wefound that checkpointing the  rst two joins in the hybridplan led to another hybrid plan that was 2  faster  Whilethe optimizer did not choose this better plan  the plan itchose performs similarly  Further  both the observed andpredicted best plans are hybrid The experiments thus show that hybrid fault tolerancestrategies can be advantageous and the best strategy for anoperator depends not only on the operator but on the wholequery plan  the same operator can use di erent strategiesin di erent query plans  e g   select in Queries 3 and 4 Note that we inject only one failure per experiment  Thus our graphs show the minimum guaranteed gains  Additionalfailures amplify di erences between strategies 6 4 Performance in Presence of UDOsWe look at the applicability of heterogeneous fault tolerance when an operator is a user de ned function withlimited fault tolerance capabilities  We experiment withQuery 3  but treat its last operator  the aggregate  as aUDO that can only restart from scratch if it fails  Note thatRule 4 2 and operator determinism  44  allow restarting aUDO in isolation without restarting the entire query Figure 10 shows the results  Previously  the best fault tolerance strategy  with a single failure  was to checkpointevery operator   With CKPT   and checkpointing aggregateprovided signi cant savings in recovery time  Now that theaggregate can use NONE as sole strategy  we  nd that ma 0200400600OPT NONE MAT CKPT RESTARTRuntime  in s Observed Normal Observed RecoveryFigure 9  Query 5  SJJJSJJA Query   The opti mal hybrid strategy is MNNNMNNC where M de notes MATERIALIZE and N denotes NONE and Cdenotes CHCKPT  In the optimal con guration 20checkpoints are taken 0100200300400500With CKPT UDO OPT MAT NONE RESTARTRuntimes  in s Observed Normal Observed RecoveryFigure 10  Impact of aggregate becoming a UDOswithout fault tolerance capabilities on Query 3  Theoptimal strategy is to materialize after select and donothing elsewhere terializing the  rst operator s output and using NONE forthe remaining operators outperforms uniformly materializ ing  none and RESTART by 48   12   and 24   respec tively  The hybrid strategy is itself 16  slower than theoptimal strategy for Query 3   With CKPT   Hence even in the presence of fault tolerance agnosticUDOs  FTOpt can generate signi cant runtime savings 6 5 ScalabilityFTOpt currently uses a brute force search algorithm  butwe  nd that simple heuristics can signi cantly prune thesearch space  Indeed  we observe that the best hybrid plansuse the NONE strategy for many operators and using an other strategy in place of NONE will always increase theruntime without failures  Thus  if the runtime without fail ure for a plan exceeds the runtime with failures for anotherplan  we can prune the former plan  Hence  evaluating plansin the decreasing order of the number of operators that usethe NONE strategy can prune signi cant fractions of thesearch space  For example  with this heuristic  the optimizerexamines only 28 out of 81 con gurations for Query 4  Inaddition  the search essentially computes the least costly ofa set of independent optimization problems and all of theseproblems can be optimized in parallel FTOpt s MATLAB implementation uses the cvx pack age  which o ers a successive approximation solver usingSDPT3  42   In our prototype  the average time to solve theoptimization problem per plan is around 25s for the 4 oper ator plans in the previous sections  However  an optimizedsolver can solve a larger problem in a sub millisecond  29  The behavior of an operator for a fault tolerance strategyis modeled using at most 12 inequality and 4 equality con straints of 11 variables  Thus  a query with n operators canbe modeled using 11n variables  13n   1 inequality and 4nequality constraints  Further  all but one of the constraintsare sparse  they depend on just a few variables indepen dent of n  For example  with 4 operators  our models use44 variables  16 equalities  and 53 inequalities  The existing01002003004005001 11 21 31 41 51 61 71 81Runtime  in s RankPredicted ObservedFigure 11  Observed and predicted runtimes forQuery 3  sorted on the predicted runtime  for all81 fault tolerance plans for the query Table 3  Real rankings of top 5 plans from perturbedcon gurations Perturbation RankingsFailing thrice instead of once 1 2 3 4 5IO cost 2 0x of true value 1 6 8 9 18IO cost 0 5x of true value 2 1 3 4 5IO cost 10x of true value 6 18 20 21 24IO cost 0 1x of true value 2 28 31 30 29Selectivity of all operators 1 1x 1 2 3 4 5Selectivity of all operators 0 9x 1 2 3 4 5Selectivity of all operators 2 0x 1 2 3 4 5Selectivity of all operators 0 5x 56 1 66 67 10optimized solvers can solve a problem of 140 variables  120equalities  and 60 inequalities in 0 425 ms on average  29  To sum up  with an optimized solver  and a parallelizedheuristic search algorithm  FTOpt could be scalable enoughto handle larger query plans within a few seconds 6 6 Optimizer SensitivityWe evaluate FTOpt s sensitivity to inaccuracies in param eter estimates  We experiment with Query 3 since it is mostsensitive to wrong choices  Figure 11 shows that runtimesvary from about 250s to 400s depending on the chosen plan To evaluate the sensitivity for a given parameter  we re run FTOpt  feeding it a perturbed parameter value  Weonly perturb a single parameter at a time while keeping theother parameters at their true values  We then compute thetop 5 plans with the perturbed value and report the ranks ofthese plans in FTOpt s original ranking  Figure 11   Table 3shows the results  As an example  in this table  when IO costincreases to 2X its true value  the second best plan identi edby FTOpt was ranked 6th with the real IO costs Table 3 shows that FTOpt is very robust to small errors inthe number of failures and it is fairly robust to even large er rors in IO cost  a 10x change still leads to a good plan  withtrue rank 6  being chosen  though the subsequent plans havepoor true rankings  FTOpt is least robust to cardinality esti mation errors  In our experiments  we varied the selectivitiesof all the operators in tandem  and with the join always pro cessing the same number of tuples from both streams   Inthis scenario  our predictions were unchanged for changes of1 1x  2x and 0 9x in selectivity but for a 0 5x change  the topchoice s true rank was 56 with an observed runtime about70  worse than that of the best con guration possible The robustness to I O cost errors and failure errors can beexplained by the fact that the e ect of these errors is mostlylinear on the optimizer  However  imprecise selectivity es timates have an exponential e ect  the further an operatoris from the beginning  the less data it processes and it pro duces even less output  on FTOpt  Thus  the optimizer ismore sensitive to perturbations in selectivity estimates 7  CONCLUSIONIn this paper  we presented a framework for heterogeneousfault tolerance  a concrete instance of that framework  andFTOpt  a latency and fault tolerance optimizer for paralleldata processing systems  Given a pipelined query plan  ashared nothing cluster  and a failure model  FTOpt selectsthe fault tolerance strategy for each operator in a query planto minimize the time to complete the query with failures  Weimplemented our approach in a prototype parallel query pro cessing engine  Our experimental results show that di erentfault tolerance strategies  often hybrid ones  lead to the bestperformance in di erent settings and that our optimizer isable to correctly identify a winning strategy AcknowledgmentsWe thank Phil Bernstein  Bill Howe  Julie Letchner  DanSuciu  and the anonymous reviewers for helpful commentson the paper s early drafts  This work is supported in partby the National Science Foundation grants NSF CAREERIIS 0845397 and IIS 0713123  gifts from Microsoft Research and Balazinska s Microsoft Research Faculty Fellowship 8  REFERENCES 1  Ashish Thusoo et  al  Hive   a petabyte scale data warehouseusing hadoop  In Proc  of the 26th ICDE Conf   2010  2  M  Balazinska  H  Balakrishnan  S  Madden  andM  Stonebraker  Fault tolerance in the Borealis distributedstream processing system  In Proc  of the SIGMOD Conf  June 2005  3  S  P  Boyd  S  J  Kim  L  Vandenberghe  and A  Hassibi  Atutorial on geometric programming  Technical report  StanfordUniversity  Info  Systems Laboratory  Dept  Elect  Eng   2004  4  B  Chandramouli  C  N  Bond  S  Babu  and J  Yang  Querysuspend and resume  In Proc  of the SIGMOD Conf   2007  5  S  Chaudhuri  R  Kaushik  A  Pol  and R  Ramamurthy Stop and restart style execution for long running decisionsupport queries  In Proc  of the 33rd VLDB Conf   2007  6  Chen et  al  High availability and scalability guide for DB2 onlinux  unix  and windows  IBM Redbookshttp   www redbooks ibm com redbooks pdfs sg247363 pdf Sept  2007  7  Cvx  http   www stanford edu  boyd cvx   8  J  Dean  Software engineering advice from building large scaledistributed systems  http   research google com people jeff stanford 295 talk pdf  9  J  Dean  Experiences with MapReduce  an abstraction forlarge scale computation  Keynote I  PACT  2006  10  J  Dean and S  Ghemawat  MapReduce  simpli ed dataprocessing on large clusters  In Proc  of the 6th OSDI Symp  2004  11  E  N  M  Elnozahy  L  Alvisi  Y  M  Wang  and D  B  Johnson A survey of rollback recovery protocols in message passingsystems  ACM Computing Surveys  34 3   2002  12  S  Ganguly  A  Goel  and A  Silberschatz  E cient andaccurate cost models for parallel query optimization  extendedabstract   In Proc  of the 15rd PODS Symp   1996  13  S  Ganguly  W  Hasan  and R  Krishnamurthy  Queryoptimization for parallel execution  In Proc  of the SIGMODConf   pages 9 18  1992  14  Greenplum database  http   www greenplum com   15  Hadoop  http   hadoop apache org   16  W  Hasan and R  Motwani  Optimization algorithms forexploiting the parallelism communication tradeo  in pipelinedparallelism  In Proc  of the 20th VLDB Conf   1994  17  J  M  Hellerstein  R  Avnur  and V  Raman  Informix underCONTROL  Online query processing  Data Mining andKnowledge Discovery  4 4  281 314  2000  18  J  M  Hellerstein  P  J  Haas  and H  J  Wang  Onlineaggregation  In Proc  of the SIGMOD Conf   1997  19  J  H  Hwang  Y  Xing  U   Cetintemel  and S  Zdonik  Acooperative  self con guring high availability solution forstream processing  In Proc  of ICDE Conf   Apr  2007  20  M  Isard  M  Budiu  Y  Yu  A  Birrell  and D  Fetterly  Dryad Distributed data parallel programs from sequential buildingblocks  In Proc  of the EuroSys Conf   pages 59 72  2007  21  Jeong Hyon Hwang et  al  High availability algorithms fordistributed stream processing  In Proc  of the 21st ICDEConf   Apr  2005  22  S  Y  Ko  I  Hoque  B  Cho  and I  Gupta  Making cloudintermediate data fault tolerant  In Proc  of the 1st ACMsymposium on Cloud computing  SOCC   pages 181 192  2010  23  Y  Kwon  M  Balazinska  and A  Greenberg  Fault tolerantstream processing using a distributed  replicated  le system  InProc  of the 34th VLDB Conf   2008  24  W  J  Labio  J  L  Wiener  H  Garcia Molina  and V  Gorelik E cient resumption of interrupted warehouse loads  SIGMODRecord  29 2  46 57  2000  25  A  P  Liedes and A  Wolski  Siren  A memory conserving snapshot consistent checkpoint algorithm for in memorydatabases  In Proc  of the 22nd ICDE Conf   page 99  2006  26  D  Logothetis  C  Olston  B  Reed  K  C  Webb  and K  Yocum Stateful bulk processing for incremental analytics  In Proc  ofthe 1st ACM symposium on Cloud computing  SOCC   2010  27  D  Lomet  Dependability  abstraction  and programming  InDASFAA  09  Proc  of the 14th Int  Conf  on DatabaseSystems for Advanced Applications  pages 1 21  2009  28  Marcos Vaz Salles et  al  An evaluation of checkpoint recoveryfor massively multiplayer online games  In Proc  of the 35thVLDB Conf   2009  29  J  Mattingley and S  Boyd  Automatic code generation forreal time convex optimization  In Convex Optimization inSignal Processing Optimization  Cambridge U  Press  2009  30  C  Olston  B  Reed  U  Srivastava  R  Kumar  and A  Tomkins Pig latin  a not so foreign language for data processing  InProc  of the SIGMOD Conf   pages 1099 1110  2008  31  Oracle database  http   www oracle com   32  A  Pavlo et  al  A comparison of approaches to large scale dataanalysis  In Proc  of the SIGMOD Conf   2009  33  L  Raschid and S  Y  W  Su  A parallel processing strategy forevaluating recursive queries  In W  W  Chu  G  Gardarin S  Ohsuga  and Y  Kambayashi  editors  VLDB 86 TwelfthInternational Conference on Very Large Data Bases  August25 28  1986  Kyoto  Japan  Proceedings  pages 412 419 Morgan Kaufmann  1986  34  A  Ray  Oracle data guard  Ensuring disaster recovery for theenterprise  An Oracle white paper  Mar  2002  35  K  Salem and H  Garcia Molina  Checkpointingmemory resident databases  In Proc  of the 5th ICDE Conf  pages 452 462  1989  36  F  B  Schneider  Implementing fault tolerant services using thestate machine approach  a tutorial  ACM Computing Surveys 22 4  299 319  1990  37  M  Shah  J  Hellerstein  and E  Brewer  Highly available fault tolerant  parallel dataows  In Proc  of the SIGMODConf   June 2004  38  A  Simitsis  K  Wilkinson  U  Dayal  and M  Castellanos Optimizing etl workows for fault tolerance  In Proc  of the26th ICDE Conf   2010  39  U  Srivastava and J  Widom  Flexible time management in datastream systems  In Proc  of the 23rd PODS Symp   June 2004  40  R  Talmage  Database mirroring in SQL Server 2005 http   www microsoft com technet prodtechnol sql 2005 dbmirror mspx  Apr  2005  41  Teradata  http   www teradata com   42  R  H  Tut   unc   u  K  C  Toh  and M  J  Todd  Solving  semide nite quadratic linear programs using SDPT3 Mathematical programming  95 2  189 217  2003  43  Tyson Condie et  al  MapReduce online  In Proc  of the 7thNSDI Symp   2010  44  P  Upadhyaya  Y  Kwon  and M  Balazinska  A latency andfault tolerance optimizer for online parallel query plans Technical report  Department of Computer Science andEngineering  Univ  of Washington  2010  45  Vertica  inc  http   www vertica com   46  A  N  Wilschut and P  M  G  Apers  Dataow query executionin a parallel main memory environment  In Proceedings of theFirst International Conference on Parallel and DistributedInformation Systems  PDIS 1991   Fontainebleu HiltonResort  Miami Beach  Florida  December 4 6  1991  pages68 77  IEEE Computer Society  1991  47  C  Yang  C  Yen  C  Tan  and S  R  Madden  Osprey Implementing MapReduce style fault tolerance in ashared nothing distributed database  In Proc  of the 26thICDE Conf   2010  48  Yuan Yu et  al  DryadLINQ  A system for general purposedistributed data parallel computing using a high level language In Proc  of the 8th OSDI Symp   2008  49  M  Za  t  D  Florescu  and P  Valduriez  Benchmarking theDBS3 parallel query optimizer  IEEE Parallel Distrib Technol   4 2  26 40  1996 Input BatchLogCheckpoint Materialized outputUpstreamhandlerDown streamhandlerOutputqueueInputbufferInput Batch ProcessorInputbatchOperatorFigure 12  Architecture of the operator frame work  The operator processes the incoming datain a pipelined manner  Threads are assigned toeach stage of the pipeline and thus each stage runsconcurrently  Network IO is handled by a poolof threads  The dashed components represent in memory data structures and the implementation of user supplied  operator logic APPENDIXIn this Appendix  we provide additional information aboutvarious aspects of our framework and the FTOpt optimizer A  IMPLEMENTATIONThe prototype is written in Java and built on top of theApache MINA framework  http   mina apache org   toimplement e cient network IO  The current implementa tion can run a directed acyclic graph  DAG  of operators At runtime  each operator in the DAG is replicated acrossmultiple machines and executed in parallel  The data com munication between upstream and downstream operators isdone using all to all TCP connections A 1 Operator Framework ArchitectureFigure 12 illustrates the framework of an operator  Theframework has three concurrently executing components Upstream Handler  Batch Processor  and Downstream Han dler  The three components make up a data processingpipeline connected by queues  We now walk through howthe incoming data is processed by this pipeline First  for each upstream partition  the Upstream Handlerbu ers the input tuples and creates a batch of input tu ples whenever there are enough tuples or when the streamis stalled  which is detected by a timeout   Both the sizeof an input tuple batch and the timeout are con gurableparameters Next  the input batch is handed to the Input Batch Pro cessor  IBP   Before running the core operator algorithm the batch processor logs the summary information for thecurrent batch for deterministic replay of the input stream Because the tuples in an input tuple batch are all from thesame upstream partition  in the batch summary  we onlyneed to record the upstream partition identi er  the  rsttuple identi er in the batch  and the number of tuples inthe batch for deterministic replay  In Appendix A 4  we dis cuss the details of logging and show that logging imposes aminimal overhead  The output of core operator algorithmis also collected in a batch and handed to the DownstreamHandler Finally  the Downstream Handler streams the output tu ples to the downstream operators and completes the pro cessing of a batch of input tuples  The output tuples arerouted to designated downstream operators according to apartition function  The downstream handler also takes therequired fault tolerance action such as materializing outputbefore writing to the network or triggering checkpoints tocapture the current state of the operator at the end of pro cessing the current output batch Our prototype supports the three fault tolerance strate gies we mention in Section 4 2  NONE  MATERIALIZE and CHCKPT  CHCKPT is supported only when the op erator algorithm implements necessary hooks  serialize andde serialize state   The other two strategies are supportedautomatically by the framework  For stateless operator suchas Select and Project  when the strategy NONE is chosen  theprototype supports skipping over the previously processedinput during recovery and replay A 2 Operator ImplementationThe current prototype implements three representative re lational operators  Select  Aggregate  and Join  For fault tolerance strategy  we only describe the detail of CHCKPTbecause NONE and MATERIALIZE strategies are automat ically supported by the framework Select  This operator evaluates a given predicate on eachinput tuple  We did not implement checkpoint hooks for itsince it is stateless Aggregate  This operator computes the average of a spe ci c column in a group  It keeps track of the partially ag gregated states using an in memory hash table  We imple mented checkpoint hooks to store the in memory hash tableinto a checkpoint and load it from a checkpoint Join  We implemented a binary symmetric hash join op erator using two in memory hash tables  33  46   We imple mented incremental checkpoints  input tuples are bu eredin memory  written to disk when a checkpoint occurs  andthen deleted from memory  During recovery  the operator re builds its hash tables by reading input tuples from the disk No joins need to be performed at this point  During replay the operator  rst locates the oldest input tuple to replay then joins the following input tuples with the in memorystate A 3 Synthetic Benchmark SetupWe implemented the synthetic workload in Section 6 asfollows   Data  All  elds are randomly generated integers orstrings   Partition  We hash partition the output of each op erator on a di erent attribute  We use the randomlygenerated value for that attribute to determine whereto route each tuple   Select  We send a tuple to the output if the randomvalue of the select  eld  of type double and taking val ues from 0 through 1  is less than the given selectivity   Aggregate  We vary the state size by controlling thenumber of groups to which the input tuples are aggre gated   Join  Given the join selectivity    we join two inputtuples when the join attribute  for the two tuples  iscongruent modulo d 1e A 4 Ensuring Operator DeterminismOur framework requires that the operator partitions bedeterministic  In particular  rule 4 2 requires that  in re sponse to a valid request  a partition must always return the same sequence of tuples  irrespective of any failures itexperiences Most relational operators  and hence their partitions  canbe made deterministic as long as when they restart  theyprocess the same tuples in the same order across all their in puts  The challenge is that these inputs come from di erentmachines in the cluster and may thus arrive with di erentlatencies when they are replayed  One approach to ensurea deterministic input data order is to bu er and interleavetuples using a pre de ned rule  2  39   These techniques however  can impose signi cant memory and latency over heads due to tuple bu ering Instead  we adopt the approach of logging determi nants  11   As the operator receives input tuples  it accu mulates them into small batches  with one batch per inputrelation partition  For example  consider an operator withtwo inputs coming from parent operators p1and p2  Tuplesarrive on these inputs starting with tuple id p11 from p1andp21 from p2  Tuples arrive in interleaved order and the op erator accumulates them into batches  bu ering these twobatches separately in memory while maintaining the tuplearrival order within a batch  Whenever a particular batchexceeds a pre de ned size or receives an end of stream sig nal  the operator writes a log entry to disk that contains the identi er of the stream for this batch  the identi er ofthe  rst tuple in the batch  and the number of tuples inthe batch  Each log entry also has an implicit log sequencenumber  lsn  that is not written to disk  The logging isdone before processing a batch  The operator processes thebatches in the same order in which it writes their log en tries to disk  In our example  if we use a batch size of2500  and the operator receives 3500 tuples from p1and4000 tuples from p2  the logged entries might look as follows h2  p21  2500i h1  p11  2500i h1  p12501  1000i h2  p22501  1500i Log entries are force written to stable storage but  as weshow below  this logging overhead is negligible even for batchsizes as small as 512 tuples per batch  If the operator needsto reprocess its input  it uses the log to ensure the reprocess ing occurs in the same order as before  To avoid expensivedisk IOs when possible  i e   when the operator itself doesnot fail but its downstream neighbor fails   recent determi nants are cached in memory Before processing an input tuple  the operator tags it withhlsn  psni  where lsn corresponds to the log entry sequencenumber of the corresponding batch and psn is the tupleorder within that batch  This information is used to assignunique tuple identi ers to output tuples  Note that all logentries are of a constant size and a lsn is enough to index alog entry Output tuple identi ers consist of three integer  elds hlsn  psn  seqi  The  rst two  elds identify one of the inputtuples that contributed to this output tuple  A sequencenumber  seq  is added since one input tuple can contributeto many output tuples  as in the case of joins  As an example  we show how we use this mechanism togenerate unique identi ers for tuples produced by the fol lowing operators   Select  Our select always has a selectivity less than orequal to one and can thus propagate the input tupleidenti er onto the output tuple  setting seq to zero   Join  The latest tuple that led to the creation of thistuple is used to populate the  rst two  elds  The third01020304050601 3 6 12Runtime  s Number of ProducersWith input log Without input logFigure 13  Each pair of bars represents the time tocomplete processing  with and without logs  with adi erent number of upstream producers for a selectoperator  There is virtually no overhead even for 12input streams 0510152025512 1024 2048 4096 8192Runtime  s Batch SizeWith input log Without input logFigure 14  Each pair of bars represents the timeto complete processing  with and without logs  withdi erent batch sizes for a join operator  The mini mum overhead occurs with a batch size of 2048  eld is a count of the number of matches for any giventuple   Aggregate  Since aggregates are blocking operators they do not need a log  In case we use CHCKPT  wecan store the last tuple identi ers received from each ofthe upstream partitions when we make the checkpoint To validate that logging overhead is negligible  we executea select operator on a single machine with an input of size2 5   106tuples  or 1 19 GB  and we vary the number ofupstream producers while keeping the batch size  xed at512 tuples  Figure 13 shows the time to process all tupleswith and without logging enabled  The results show that thelogging mechanism scales well with the number of upstreamproducers  The average runtimes of three runs rounded tothe nearest second are identical To select the optimal log batch size we execute a joinoperator that processes 1 million tuples from each of its twoinputs  It is a 1x1 foreign key join and produces 1 millionoutput tuples  We have a total of four producers generatingall the data and we vary the log batch size from 512 to 8192 As Figure 14 shows  the smallest runtime overhead was 3 for a batch size of 2048 tuples  As expected the runtime withno logs for smaller batch sizes remains the same as that for2048 while the runtime with logging increases since we writemore log entries if batch sizes are smaller and more cputime is spent in writing the log entries to disk  It should benoted that the runtime with and without logs increases forbatch sizes of 4096 and 8192  This is because of an increasedbu ering delay for each input batch  In all our experiments we use a batch size of 2048 and a tuple size of 0 5 KB A 5 Resource Allocation in FTOptIn addition to fault tolerance strategies  FTOpt also pro duces an allocation of resources to operators because re source allocation and choice of fault tolerance strategy aretightly interconnected  Resource allocation is expressed asa fraction of all available CPU and network bandwidth re sources  Bandwidth is further constrained by network topol ogy In this paper  we make several simplifying assumptionsto implement and test our proof of concept optimizer  Weassume a simple setting where the set of compute nodesare connected through a single switch  The current versionof our optimizer abstracts out the resource allocation byassuming that the time to process each tuple and the disk IOcosts scale linearly with the amount of resources allocated toan operator  Thus  if a single operator partition of operatori takes tcputime to process a tuple  then with ni partitionseach assigned exclusively to a machine  the e ective time theoperator  i e   all the operator partitions together  takes toprocess a single tuple istcpunitime  Similarly the time takento write a tuple to disk is taken to betioni  Our optimizerhandles fractional resource assignments Given a resource allocation  operators can either be co scheduled on the same physical nodes  i e   all nodes exe cute all operators  or separated onto di erent nodes  i e  each node executes a subset of all operators   In the lattercase  resource allocation must be rounded o  to the gran ularity of machines  which can lead to lower performance In the former case  operators may end up writing their logsand checkpoints to the same disks for a more complex per formance curve for these interleaved IO operations  Whileour optimizer handles both strategies and computes frac tional resource assignments  in our experiments  we pinnedeach operator partition to its own core and its own disk oneach node to keep our models simple B  OPERATOR MODELING STRATEGYWe refer the reader to Section 5 3 1 for an overview ofwhat information about each operator in the pipeline we re quire so as to automatically infer the runtimes for the entirepipeline  To recap  We only require  a  the expected num ber of output tuples produced given the number of inputtuples received  and  b  the average cpu time required toproduce each output tuple We remind the reader that we have abstracted away thedi erent number of partitions of each operator by dividingthe time to process each tuple and the time to read or writeeach tuple by the number of machines allocated to the op erator We now restate the equations and explain how the equa tions are derived me    xIN kktk1f 2 me    tcpua  1 3 me   1k  xIN 1kjIj1 1k  4  1  k1 tf   jIjm1e   xNjIj  5 In the above equations  jIj is the output cardinality  and k come from the NBout nin  function  me is the num ber of output tuples produced per second at the instant theprocessing ends and tf is the  rst time at which the outputproduces tuples at the rate me Equality 2 realizes this relationship between me and tf  Speci cally  the rate at which output tuples are producedby the operator after time tf isme  ddtNBout nin t tf ddtnkint tf ddt txIN kt tf   xIN kktk1fInequality 3  me    tcpua  1  states that the operator cannot take less than tcpua time to produce an output tuple since this is the least amount of time the processor needs pertuple  given the resources it has  The inequality becomes anequality when the operator reaches a stage in its processingwhen the processor is working at its full capacity and cannot keep up with the theoretically maximum rate at whichoutput tuples could be produced given the rate at which theinput tuples are being received For inequality 4  me   1k  xIN 1kjIj1 1k   its right handside is the maximum rate at which output could be pro duced if the only bottleneck was the rate of arrival of inputtuples  Note that  since we require the NBout    function tohave a non negative rate of change  i e   the fastest outputproduction rate w</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory"/>
        <doc>On Finding Skylines in External Memory ### Cheng Sheng Yufei Tao CUHK CUHK Hong Kong Hong Kong csheng cse cuhk edu hk taoyf cse cuhk edu hk ABSTRACT We consider the skyline problem  a k a  the maxima problem   which has been extensively studied in the database community  The input is a set P of d dimensional points  A point dominates another if the former has a lower coordinate than the latter on every dimension  The goal is to    nd the skyline  which is the set of points p     P such that p is not dominated by any other data point  In the external memory model  the 2 d version of the problem is known to be solvable in O  N B  logM B N B   I Os  where N is the cardinality of P  B the size of a disk block  and M the capacity of main memory  For    xed d     3  we present an algorithm with I O complexity O  N B  log d   2 M B  N B    Previously  the best solution was adapted from an in memory algorithm  and requires O  N B  log d   2 2  N M   I Os  Categories and Subject Descriptors F2 2  Analysis of algorithms and problem complexity   Nonnumerical algorithms and problems   geometric problems and computations General Terms Algorithms  theory Keywords Skyline  admission point  pareto set  maxima set ### 1  INTRODUCTION This paper studies the skyline problem  The input is a set P of d dimensional points in general position  i e   no two points of P share the same coordinate on any dimension  Given a point p     R d   denote its i th  1     i     d  coordinate as p i   A point p1 is said to dominate another point p2  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  PODS   11  June 13   15  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0660 7 11 06     10 00  represented as p1     p2  if p1 has a smaller coordinate on all d dimensions  namely     i   1       d  p1 i    p2 i   The goal is to compute the skyline of P  denoted as SK Y  P   which includes all the points in P that are not dominated by any other point  namely  SK Y  P     p     P    p       P s t  p       p    1  The skyline is also known under other names such as the pareto set  the set of admission points  and the set of maximal vectors  see  24    price rating 1 2 3 4 5 6 7 8 Figure 1  The skyline is  1  5  7  Ever since its debut in the database literature a decade ago  7   skyline computation has generated considerable interests in the database area  see  24  for a brief survey   This is  at least in part  due to the relevance of skylines to multicriteria optimization  Consider  for example  a hotel recommendation scenario  where each hotel has two attributes price and rating  a smaller rating means better quality   Figure 1 illustrates an example with 8 hotels  of which the skyline is  1  5  7   Every hotel not in the skyline is worse than at least one hotel in the skyline on both dimensions  i e   more expensive and rated worse at the same time  In general  for any scoring function that is monotone on all dimensions  the skyline always contains the best  top 1  point that minimizes the function  This property is useful when it is di   cult  if not impossible  for a user to specify a suitable scoring function that accurately re   ects her his preferences on the relative importance of various dimensions  In Figure 1  for instance   1  5  7  de   nitely includes the besthotel  no matter the user emphasizes more  and how much more  on price or rating  1 1 Computation model Our complexity analysis is under the standard external memory  EM  model  2   which has been successful in capturing the characteristics of algorithms dealing with massive data that do not    t in memory  see  27  for a broad summary of results in this model   Speci   cally  in this model  a computer has a main memory that is able to accommodate M words  and a disk of unbounded size  The disk is formatted into disjoint blocks  each of which contains B consecutive words  The memory should have at least two blocks  i e   M     2B  An I O operation reads a block of data from the disk into memory  or conversely  writes a block of memory information into the disk  The time complexity is measured in the number of I Os performed by an algorithm  CPU calculation is free  In EM  linear cost should be interpreted as O N B  for a dataset of size N  as opposed to O N  in a memoryresident model such as RAM  In this paper  poly logarithmic should be understood as O polylogM B N B    instead of O polylog N   namely  it is important to achieve base M B  In this paper  a function is said to be near linear if it is in O  N B  polylogM B N B   but not in O N B   1 2 Previous results In internal memory  Matousek  21  showed how to    nd the skyline in O N 2 688   time when the dimensionality d of the dataset P is as high as the number N of points in P  In 2 d space  Kung et al   19  proposed an O N log N  time algorithm  For any    xed dimensionality d     3  they also gave an algorithm with time complexity O N log d   2 N   Bentley  4  developed an alternative algorithm achieving the same bounds as those of  19   Kirkpatrick and Seidel  16  presented algorithms whose running time is sensitive to the result size  and has the same complexity as the algorithms in  4  19  when the skyline has     N  points   It can be shown that any algorithm in the comparison class 1 must incur     N log N  time  implying that the solutions of  4  19  are already optimal in this class for d   2 and 3  see also some recent results on instance optimality due to Afshani et al   1    For d     4  Gabow et al   11  discovered an algorithm terminating in O N log d   3 N log log N  time  which still remains the best result up to this day  Note  however  that the solution of  11  does not belong to the comparison class  due to its reliance on the van Emde Boas structure  26  that uses features of the RAM model  Faster algorithms have been developed in some special circumstances where  for example  the data follow special distributions  5  6  10   or each dimension has a small domain  22   All the RAM algorithms can be applied in the EM model directly by treating the disk as virtual memory  Such a brute force approach  however  can be expensive in practice because it fails to take into account the e   ects of blocking  which do not exist in RAM but are inherent in external memory  For example  running the solution of  11  in EM naively 1 A skyline algorithm is comparison based if it can infer the dominance relation by only comparing pairs of points  The comparison class includes all such algorithms  would entail O N log d   3 N log log N  I Os   which amounts to reading the entire dataset O B log d   3 N log log N  times  B is at the order of thousands in practice   Hence  there is a genuine need to design I O oriented algorithms  For d   2  such an algorithm can be easily found  as Kung et al   19  showed that the problem can be settled by sorting the data followed by a single scan  we will come back to this in Section 2   which takes O  N B  logM B N B   I Os in total  To our knowledge  for general d  the RAM algorithm that can be most e   ciently adapted to EM is the one by Bentley  4   which performs O  N B  log d   2 2  N M   I Os     note that the base of the log is 2  instead of M B  We are not aware of any algorithm that can achieve near linear  or better  running time  For d     3   the skyline of  a  dataset P can be trivially obtained by computing the cartesian product P    P  i e   by comparing all pairs of points in P  which  in turn  can be produced by a blocked nested loop  BNL  in    N 2   M B   I Os  It has been observed  7  that such a quadratic complexity is too slow in practice for large N  In the past decade  several algorithms  as we survey below  have been designed to alleviate the de   ciency  typically by leveraging the transitivity of the dominance relation  i e   p1     p2 and p2     p3 imply p1     p3   Although empirical evaluation has con     rmed their e   ectiveness on selected datasets  none of those algorithms has been proved to be asymptotically faster than BNL in the worst case  We say that they are captured by the quadratic trap  Borzsonyi et al   7  presented a divide and conquer  DC  method that partitions P into disjoint groups P1       Ps where the number s of groups is large enough so that each Pi  i     s     ts in memory  DC proceeds by invoking an in memory algorithm to    nd the skyline SK Y  Pi  of each Pi  and then  deleting those points of SK Y  Pi  dominated by some point in the skyline of another group  Although divide and conquer is a promising paradigm for attacking the skyline problem  it is also employed in our solutions   its application in DC is heuristic and does not lead to any interesting performance bound  The sort    rst skyline  SFS  algorithm by Chomicki et al   9  works by sorting the points p     P in ascending order of score p   where score can be any function R d     R that is monotonically increasing on all dimensions  The monotonicity ensures that  p1     p2 implies score p1    score p2   but the opposite is not true   As a result  a point p     P cannot be dominated by any point that ranks behind it in the ordering  Following this rationale  SFS scans P in its sorted order  and maintains the skyline    of the points already seen so far  note that        SK Y  P  at any time   As expected  the choice of score is crucial to the e   ciency of the algorithm  No choice  unfortunately  is known to be able to escape the quadratic trap in the worst case  In SFS  sorting is carried out with the standard external sort  Intuitively  if mutual comparisons are carried out among the data that ever co exist in memory  during the external sort   many points may get discarded right away once con   rmed to be dominated  at no extra I O cost at all  Based on this idea  Godfrey et al   12  developed the linear elimination sort for skyline  LESS  algorithm  LESS has the property that  it terminates in linear expected I Os under the independent and uniform assumption  i e   all di method I O complexity remark Kung et al   19  O N log d   2 N  Gabow et al   11  O N log d   3 N log log N  not in the comparison class  Bentley  4  O  N B  log d   2 2  N M   adapted from Bentley   s O N log d   2 N  algorithm in RAM BNL    N 2   M B   also applies to the BNL variant of Borzsonyi et al   7  DC  7      N 2   M B   SFS  9  O N 2   M B   LESS  12  O N 2   M B   RAND  24  O   N  M B   expected    is the number of points in the skyline  which can be     N   this paper O  N B  log d   2 M B N B   optimal for d   3 in the comparison class Table 1  Comparison of skyline algorithms for    xed d     3 mensions are independent  and the points of P distribute uniformly in the data space   provided that the memory size M is not too small  12   When the assumption does not hold  however  it remains unknown whether the cost of LESS is o N 2   M B    Sarma et al   24  described an output sensitive randomized algorithm RAND  which continuously shrinks P with repetitive iterations  each of which performs a three time scan on the current P as follows  The    rst scan takes a random sample set S     P with size    M   The second pass replaces  if possible  some samples in S with other points that have stronger pruning power  All samples at the end of this scan are guaranteed to be in the skyline  and thus removed from P  The last scan further reduces  P   by eliminating all the points that are dominated by some sample  At this point  another iteration sets o    as long as P         RAND is e   cient when the result has a small size  Speci   cally  if the skyline has    points  RAND entails O   N  M B   I Os in expectation  When          N   however  the time complexity falls back in the quadratic trap  There is another line of research that concerns preprocessing a dataset P into a structure that supports fast retrieval of the skyline  as well as insertions and deletions on P  see  14  15  18  20  23  and the references therein   In our context  such pre computation based methods do not have a notable advantage over the algorithms mentioned earlier  1 3 Our results Our main result is  Theorem 1 1  The skyline of N points in R d can be computed in O  N B  log d   2 M B  N B   I Os for any    xed d     3  The theorem concerns only d     3 because  as mentioned before  the skyline problem is known to be solvable in O  N B  logM B N B   I Os in 2 d space  Unlike the result of Godfrey et al   12   we make no assumption on the data distribution  Our algorithm is the    rst one that beats the quadratic trap and  at the same time  achieves near linear running time  In 3 d space  our I O complexity O  N B  logM B N B   is asymptotically optimal in the class of comparison based algorithms  For any    xed d  Theorem 1 1 shows that the skyline problem can be settled in O N B  I Os  when N B    M B  c for some constant c  a situation that is likely to happen in practice   No previous algorithm is known to have such a property  See Table 1 for a comparison of our and existing results  The core of our technique is a distribution sweep 2 algorithm for solving the skyline merge problem   where we are given s skylines   1         s that are separated by s   1 hyperplanes orthogonal to a dimension  and the goal is to return the skyline of the union of all the skylines  namely  SK Y    1               s   It is not hard to imagine that this problem lies at the heart of computing the skyline using a divide and conquer approach  Indeed  the lack of a fast solution to skyline merging has been the obstacle in breaking the curse of quadratic trap  as can be seen from the divide andconquer attempt of Borzsonyi et al   7   We overcome the obstacle by lowering the dimensionality to 3 gradually  and then settling the resulting 3 d problem in linear I Os  Our solution can also be regarded as the counterpart of Bentley   s algorithm  4  in external memory  Remark  We have de   ned our problem by assuming that P is in general position  The skyline SK Y  P  is still well de   ned without this assumption  Speci   cally  let P be a set of points in R d  implying that P has no duplicates   Given two points p  p   in P  we have p     p   if p i      p    i  for all 1     i     d  Note that since p    p     the equality does not hold for at least one i  Then  SK Y  P  is still given by Equation 1  Our algorithms can be extended to solve this version of the skyline problem  retaining the same performance guarantee as in Theorem 1 1  Details can be found in Section 3 3  2  PRELIMINARIES In this section  we review some skyline algorithms designed for memory resident data  The purposes of the review are three fold  First  we will show that the 2 d solution of Kung et al   19  can be easily adapted to work in the EM model  Second  our discussion of their algorithm and Bentley   s algorithm  4  for d     3 not only clari   es some characteristics of the underlying problems  but also sheds light on some obstacles preventing a direct extension to achieve near linear time complexity in external memory  Finally  we brie   y explain the cost lower bound established in  19  and why a similar bound also holds in the I O context  Let us    rst agree on some terminologies  We refer to the 2 An algorithm paradigm proposed by Goodrich et al   13  that can be regarded as the counterpart of plane sweep in external memory p ymin x y 1 2 3 y z 1 2 3 4 p 5  a   b  Figure 2  Illustration of algorithms by Kung et al   19    a  2 d   b  3 d    rst  second  and third coordinate of a point as its x   y   and z coordinate  respectively  Sometimes  it will be convenient to extend the de   nition of dominance to subspaces in a natural manner  For example  in case p1 has smaller x  and y coordinates than p2  we say that p1 dominates p2 in the x y plane  No ambiguity can arise as long as the subspace concerning the dominance is always mentioned  2 d  The skyline SK Y  P  of  a  set P of 2 d points can be extracted by a single scan  provided that the points of P have been sorted in ascending order of their x coordinates  To understand the rationale  consider any point p     P  and  let P   be the set of points of P that rank before p in the sorted order  Apparently  p cannot be dominated by any point that ranks after p  because p has a smaller x coordinate than any of those points  On the other hand  p is dominated by some point in P   if and only if the y coordinate of p is greater than ymin  where ymin is the smallest y coordinate of all the points in P     See Figure 2a where P   includes points 1  2  3  and that no point in P   dominates p can be inferred from the fact that p has a lower y coordinate than ymin  Therefore  to    nd SK Y  P   it su   ces to read P in its sorted order  and at any time  keep the smallest y coordinate ymin of all the points already seen  The next point p scanned is added to SK Y  P  if its y coordinate is below ymin  in which case ymin is updated accordingly  In the EM model  this algorithm performs O  N B  logM B N B   I Os  which is the time complexity of sorting N elements in external memory  3 d  Suppose that we have sorted P in ascending order of their x coordinates  Similar to the 2 d case  consider any point p     P  with P   being the set of points before p  It is clear that p cannot be dominated by any point that ranks after p  Judging whether p is dominated by a point of P     however  is more complex than the 2 d scenario  The general observation is that  since all points of P   have smaller x coordinates than p  we only have to check whether p is dominated by some point of P   in the y z plane  Imagine that we project all the points of P   onto the y z plane  which yields a 2 d point set P      Let    be the  2d  skyline of P      It is su   cient to decide whether a point in    dominates p in the y z plane  It turns out that such a dominance check can be done e   ciently  In general  a 2 d skyline is a    staircase     In the y z plane  if we walk along the skyline in the direction of growing y coordinates  the points encountered must have descending z coordinates  Figure 2b illustrates this with a    that consists of points 1       5  To    nd out whether p is dominated by any point of    in the y z plane  we only need to    nd the predecessor o of p along the y dimension among the points of     and give a    yes    answer if and only if o has a lower z coordinate than p  In Figure 2b  the answer is    no    because the predecessor of p  i e   point 2  actually has a greater z coordinate than p  Returning to the earlier context with P     a    no    indicates that p is not dominated by any point in P     and therefore  p belongs to the skyline SK Y  P   Based on the above reasoning  the algorithm of  19  maintains    while scanning P in its sorted order  To    nd predecessors quickly  the points of    are indexed by a binary tree T on their y coordinates  The next point p is added to SK Y  P  upon a    no    answer as explained before  which takes O log N  time with the aid of T   Furthermore  a    no    also necessitates the deletion from    of all the points that are dominated by p in the y z plane  e g   points 3  4 in Figure 2b   Using T   this requires only O log N  time per point removed  As each point of P is deleted at most once  the entire algorithm    nishes in O N log N  time  A straightforward attempt of externalizing the algorithm is to implement T as a B tree  This will result in the total execution time of    N logB N   which is higher than the cost O  N B  logM B N B   of our solution by a factor of     B logB M   The de   ciency is due to the fact  see  13   that plane sweep  which is the methodology behind the above algorithm  is ill    tted in external memory  because it issues a large number of queries  often     N    rendering it di   cult to control the overall cost to be at the order of N B  Following a di   erent rationale  Bentley  4  gave another algorithm of O N log N  time  We will not elaborate his solution here because our algorithm in the next section degenerates into Bentley   s  when M and B are set to constants satisfying M B   2  Dimensionalities at least 4  Kung et al   19  and Bentley  4  deal with a general d dimensional  d     4  dataset P by divide and conquer  More speci   cally  their algorithms divide P into P1 and P2 of roughly the same size by a hyperplane perpendicular to dimension 1  Assume that the points of P1 have smaller coordinates on dimension 1 than those of P2  Let   1 be the skyline SK Y  P1  of P1  and similarly    2   SK Y  P2   All points of   1 immediately belong to SK Y  P   but a point p       2 is in SK Y  P  if and only if no point in   1 dominates p  Hence  after obtaining   1 and   2 from recursion  a skyline merge is carried out to evict such points as p z x y    2     3  1 2 3 4 5 6 8    1  7 Figure 3  Illustration of 3 d skyline merge  The value of s is 3  Only the points already encountered are shown  Points are labeled in ascending order of their y coordinates  which is also the order they are fetched   Point 8 is the last one seen  Each cross is the projection of a point in the x y plane     1  contains points 2  3  7     2  includes 4  6  8  and    3  has 5  1     1      2      3  equal the z coordinate of point 2  8  5  respectively  Point 8 does not belong to SK Y  P   because its z coordinate is larger than    1   i e   it violates Inequality 2 on j   1   Externalization of the algorithms of Kung et al   19  and Bentley  4  is made di   cult by a common obstacle  That is  the partitioning in the divide and conquer is binary  causing a recursion depth of     polylog N M    To obtain the performance claimed in Theorem 1 1  we must limit the depth to O polylogM B N B    This cannot be achieved by simply dividing P into a greater number s   2 of partitions P1       Ps   because  doing so may compromise the e   ciency of merging skylines  To illustrate  let   i   SK Y  Pi  for each 1     i     s  A point p     Si must be compared to SK Y  Pj   for all j   i  Applying the skyline merge strategy of  4  or  19  would blow up the cost by a factor of     s 2    which would o   set all the gains of a large s  Remedying the drawback calls for a new skyline merge algorithm  which we give in the next section  Cost lower bound  Kung et al   19  proved that any 2 d skyline algorithm in the comparison class must incur     N log N  execution time  To describe the core of their argument  let us de   ne the rank permutation of a sequence S of distinct numbers  x1       xN   as the sequence  r1       rN  where ri  1     i     N  is the number of values of S that are at most xi  For example  the rank permutation of  9  20  3  is  2  3  1   Kung et al   19  identi   ed a series of hard datasets  where each dataset P has N points p1       pN whose x coordinates can be any integers  They showed that  any algorithm that correctly    nds the skyline of P must have determined the rank permutation of the sequence formed by the x coordinates of p1       pN  In the EM model  it is known  2  that deciding the rank permutation of a set of N integers demands      N B  logM B N B   I Os in the worst case for any comparison based algorithm  It thus follows that this is also a lower bound for computing 2 d skylines in external memory  Note that the same bound also holds in higherdimensional space where the problem is no easier than in the 2 d space  It is worth mentioning that the I O lower bound      N B  logM B N B   is also a direct corollary of a result due to Arge et al   3   3  OUR SKYLINE ALGORITHM We will present the proposed solution in a step by step manner  Section 3 1    rst explains the overall divide andconquer framework underpinning the algorithm by clarifying how it works in 3 d space  To tackle higher dimensionalities d  there is another layer of divide and conquer inside the framework  as elaborated in Section 3 2 for d   4   The  4 d description of our algorithm can then be easily extended to general d  which is the topic of Section 3 3  3 1 3 d Our algorithm accepts as input a dataset P whose points are sorted in ascending order of x coordinates  If the size N of P is at most M  i e   the memory capacity   we simply    nd the skyline of P using a memory resident algorithm  The I O cost incurred is O N B   In case N   M  we divide P into s      M B  partitions P 1        P s  with roughly the same size  such that each point in P i  has a smaller x coordinate than all points in P j  for any 1     i    j     s  As P is already sorted on the xdimension  the partitioning can be done in linear cost  while leaving the points of each P i  sorted  in the same  way  The next step is to obtain the skyline    i  of each P i   i e      i    SK Y  P i    Since this is identical to solving the original problem  only on a smaller dataset   we recursively invoke our algorithm on P i   Now consider the moment when all    i  have been returned from recursion  Our algorithm proceeds by performing a skyline merge  which    nds the skyline of the union of all    i   that is  SK Y     1                 s    which is exactly SK Y  P   We enforce an invariant that  SK Y  P  be returned in a disk    le where the points are sorted in ascending order of y coordinates  to be used by the upper level of recursion  if any   Due to recursion  the invariant implies that  the same ordering has been applied to all the    i  at hand  We now elaborate the details of the skyline merge  SK Y  P  is empty in the outset     1           s  are scanned synchronously in ascending order of y coordinates  In other words  the next point fetched is guaranteed to have the lowest y coordinate among the points of all    i  that have not been encountered yet  As s      M B   the synchronization can be achieved by assigning a block of memory as the input bu   er to each    i   We maintain a value    i   which equals the minimum z coordinate of all the points that have already been seen from    i   If no point from    i  has been read     i         We decide whether to include a point p in SK Y  P  when p is fetched by the synchronous scan  Suppose that p is from    i  for  some i  We add p to SK Y  P  if p 3       j      j   i  2  where p 3  denotes the z coordinate of p  See Figure 3 for an illustration  The lemma below shows the correctness of this rule  Lemma 3 1  p     SK Y  P  if and only if Inequality 2 holds  Proof  Clearly  p cannot be dominated by any point in    i   1           s  because p has a smaller x coordinate than all those points  Let S be the set of points in    j  already scanned before p  for any j   i  No point p          j    S can possibly dominate p  as p has a lower y coordinate than p     On the other hand  all points in S dominate p in the x y plane  Thus  some point in S dominates p in R 3 if and only if Inequality 2 holds  We complete the algorithm description with a note that a single memory block can be used as an output bu   er  so that the points of SK Y  P  can be written to the disk in linear I Os  by the same order they entered SK Y  P   namely  in ascending order of their y coordinates  Overall  the skyline merge    nishes in O N B  I Os  Running time  Denote by F N  the I O cost of our algorithm on a dataset with cardinality N  It is clear from the above discussion that F N      O N B  if N     M s    F N s    G N  otherwise  3  where G N    O N B  is the cost of a skyline merge  Solving the recurrence gives F N    O  N B  logM B N B    3 2 4 d To    nd the skyline of a 4 d dataset P  we proceed as in the 3 d algorithm by using a possibly smaller s      min      M    M  B    The only di   erence lies in the way that a skyline merge is performed  Formally  the problem we face in a skyline merge can be described as follows  Consider a partition P 1        P s  of P such that each point in P i  has a smaller coordinate on dimension 1 than all points in P j   for any 1     i   j     s  Equivalently  the data space is divided into s slabs   1 1          1 s  by s     1 hyper planes orthogonal to dimension 1 such that P i    P       1 i  for all 1     i     s  We are given the skyline   1 i  of each P i   where the points of   1 i  are sorted in ascending order of their 2nd coordinates  The goal is to compute SK Y    1 1                1 s    which is equivalent to SK Y  P   Further  the output order is important  for backtracking to the upper level of recursion   we want the points of SK Y  P  to be returned in ascending order of their 2nd coordinates  The previous subsection solved the problem in 3 d space with O N B  I Os where N    P   In 4 d space  our objective is to pay only an extra factor of O logM B N B   in the cost  We ful   ll the purpose with an algorithm called preMerge 4d  the input of which includes     slabs   1 1          1 s      a set    of points sorted in ascending order of their 2nd coordinates     has the property that  if two points p1  p2        fall in the same slab  they do not dominate each other  preMerge 4d returns the points of SK Y      in ascending order of their 3rd coordinates  Although stated somewhat di   erently  the problem settled by preMerge 4d is  almost  the same as skyline merge  Notice that    can be obtained by merging   1 1          1 s  in O N B  I Os  Moreover  we can sort the points of SK Y       output by preMerge 4d  ascendingly on dimension 2 to ful     ll the order requirement of skyline merge  which demands another O  N B  logM B N B   I Os  Algorithm preMerge 4d  In case    has at most M points  preMerge 4d solves the problem in memory  Otherwise  in O      B  I Os the algorithm divides    into s partitions    1           s  of roughly the same size  with the points of    i  having smaller 2nd coordinates than those of    j  for any 1     i    j     s  We then invoke preMerge 4d recursively on each    i   feeding the same    1 1          1 s    to calculate   2 i    SK Y     i    Apparently  SK Y      is equivalent to the skyline of the union of all   2 i   namely  SK Y        SK Y    2 1                2 s    It may appear that we are back to where we started     this is another skyline merge  The crucial di   erence  however  is that only two dimensions remain    unprocessed     i e   dimensions 3 and 4   In this case  the problem can be solved directly in linear I Os  by a synchronous scan similar to the one in Section 3 1  By recursion  the points of each   2 i  have been sorted ascendingly on dimension 3  This allows us to enumerate the points of   2 1                2 s  in ascending order of their 3rd coordinates  by synchronously reading the   2 i  of all i      1  s   In the meantime  we keep track of s 2 values    i1  i2  for every pair of i1  i2      1  s   Speci   cally     i1  i2  equals the lowest 4th coordinate of all the points in   1 i1        2 i2  that have been scanned so far  or    i1  i2        if no such point exists  Note that the choice of s makes it possible to maintain all    i1  i2  in memory  and meanwhile  allocate an input bu   er to each   2 i  so that the synchronous scan can be completed in linear I Os  SK Y      is empty at the beginning of the synchronous scan  Let p be the next point fetched  Suppose that p fallsin   1 i1   and is from   2 i2   for some i1  i2  We insert p in SK Y      if p 4       j1  j2      j1   i1  j2   i2  4  where p 4  is the coordinate of p on dimension 4  An argument similar to the proof of Lemma 3 1 shows that p     SK Y      if and only if the above inequality holds  Note that checking the inequality happens in memory  and incurs no I O cost  Finally  as the points of SK Y      enter SK Y      in ascending order of their 3rd coordinates  they can be written to the disk in the same order  Running time  Let H K  be  the I O cost  of preMerge 4d when    has K points  We have H K      O K B  if K     M s    H K s    O K B  otherwise where s          M B   This recurrence gives H K    O  K B  logM B K B    Following the notations in Section 3 1  denote by G N  the cost of a skyline merge when the dataset P has size N  and by F N  the cost of our 4 d skyline algorithm  G N  equals H N  plus the overhead of sorting SK Y  P   Hence  G N    O  N B  logM B N B    With the above  we solve the recurrence in Equation 3 as F N    O  N B  log 2 M B N B    3 3 Higher dimensionalities We are now ready to extend our technique to dimensionality d     5  th</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pisemp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#index_structure_external_memory"/>
        <doc>Beyond Simple Aggregates  Indexing for Summary Queries ### Zhewei Wei Ke Yi Hong Kong University of Science and Technology Clear Water Bay  Hong Kong  China  wzxac  yike  cse ust hk ABSTRACT Database queries can be broadly classi   ed into two categories  reporting queries and aggregation queries  The former retrieves a collection of records from the database that match the query   s conditions  while the latter returns an aggregate  such as count  sum  average  or max  min   of a particular attribute of these records  Aggregation queries are especially useful in business intelligence and data analysis applications where users are interested not in the actual records  but some statistics of them  They can also be executed much more e   ciently than reporting queries  by embedding properly precomputed aggregates into an index  However  reporting and aggregation queries provide only two extremes for exploring the data  Data analysts often need more insight into the data distribution than what those simple aggregates provide  and yet certainly do not want the sheer volume of data returned by reporting queries  In this paper  we design indexing techniques that allow for extracting a statistical summary of all the records in the query  The summaries we support include frequent items  quantiles  various sketches  and wavelets  all of which are of central importance in massive data analysis  Our indexes require linear space and extract a summary with the optimal or near optimal query cost  Categories and Subject Descriptors E 1  Data   Data structures  F 2 2  Analysis of algorithms and problem complexity   Nonnumerical algorithms and problems General Terms Algorithms  theory Keywords Indexing  summary queries Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  PODS   11  June 13   15  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0660 7 11 06     10 00 ###  1  INTRODUCTION A database system   s primary function is to answer users    queries  These queries can be broadly classi   ed into two categories  reporting queries and aggregation queries  The former retrieves a collection of records from the database that match the query   s conditions  while the latter only produces an aggregate  such as count  sum  average or max  min   of a particular attribute of these records  With reporting queries  the database is simply used as a data storage retrieval tool  Many modern business intelligence applications  however  require ad hoc analytical queries with a rapid execution time  Users issuing these analytical queries are interested not in the actual records  but some statistics of them  This has therefore led to extensive research on how to perform aggregation queries e   ciently  By augmenting a database index  very often a B tree  with properly precomputed aggregates  aggregation queries can be answered e   ciently at query time without going through the actual data records  However  reporting and aggregation queries provide only two extremes for analyzing the data  by returning either all the records matching the query condition or one  or a few  single valued aggregates  These simple aggregates are not expressive enough  and data analysts often need more insight into the data distribution  Consider the following queries   Q1  In a company   s database  What is the distribution of salaries of all employees aged between 30 and 40   Q2  In a search engine   s query logs  What are the most frequently queried keywords between May 1 and July 1  2010  The analyst issuing the query is perhaps not interested in listing all the records in the query range one by one  while probably not happy with a simple aggregate such as average or max   either   What would be nice is some summary on the data  which is more complex than the simple aggregates  yet still much smaller than the raw query results  Some useful summaries include the frequent items  the     quantiles for  say       0 1  0 2          0 9  a sketch  e g   the Count Min sketch  8  or the AMS sketch  4    or some compressed data representations like wavelets  All these summaries are of central importance in massive data analysis  and have been extensively studied for o   ine and streaming data  Yet  to use the existing algorithms  one still has to    rst issue a reporting query to retrieve all query results  and then construct the desired summary afterward  This is clearly time consuming and wasteful  In this paper  we propose to add a native support for summary queries in a database index  such that a summary canbe returned in time proportional to the size of the summary itself  not the size of the raw query results  The problem we consider can be de   ned more precisely as follows  Let D be a database containing N records  Each record p     D is associated with a query attribute Aq p  and a summary attribute As p   drawing values possibly from di   erent domains  A summary query speci   es a range constraint  q1  q2  on Aq and the database returns a summary on the As attribute of all records whose Aq attribute is within the range  For example  in the query  Q1  above  Aq is    age    and As is    salary     Note that As and Aq could be the same attribute  but it is more useful when they are di   erent  as the analyst is exploring the relationship between two attributes  Our goal is to build an index on D so that a summary query can be answered e   ciently  As with any indexing problem  the primary measures are the query time and the space the index uses  The index should also work in external memory  where it is stored in blocks of size B  and the query cost is measured in terms of the number of blocks accessed  I Os   Finally  we also would like the index to support updates  i e   insertion and deletion of records  1 1 Related work on indexing for aggregation queries In one dimension  most aggregates can be supported easily using a binary tree  a B tree in external memory   At each internal node of the binary tree  we simply store the aggregate of all the records below the node  This way an aggregation query can be answered in O log N  time  O logB N  I Os in external memory   In higher dimensions  the problem becomes more di   cult and has been extensively studied in both the computational geometry and the database communities  Solutions are typically based on space partitioning hierarchies  like partition trees  quadtrees and R trees  where an internal node stores the aggregate for its subtree  There is a large body of work on spatial data structures  please refer to the survey by Agarwal and Erickson  2  and the book by Samet  26   When the data space forms an array  the data cube  13  is an e     cient structure for answering aggregation queries  However  all the past research  whether in computational geometry or databases  has only considered queries that return simple aggregates like count  sum  max  min   and very recently top k  1  and median  7  18   The problem of returning complex summaries has not been addressed  1 2 Related work on  non indexed  summaries There is also a vast literature on various summaries in both the database and algorithms communities  motivated by the fact that simple aggregates cannot well capture the data distribution  These summaries  depending on the context and community  are also called synopses  sketches  or compressed representations  However  all past research has focused on how to construct a summary  either o   ine or in a streaming fashion  on the entire data set  No one has considered the indexing problem where the focus is to intelligently compute and store auxiliary information in the index at pre computation time  so that a summary on a requested subset of the records in the database can be built quickly at query time  Since we cannot a   ord to look at all the requested records to build the summary at query time  this poses new challenges that past research cannot address  All existing construction algorithms need to at least read the data records once  The problem of how to maintain a summary as the underlying data changes  namely under insertions and deletions of records  has also been extensively studied  But this should not be confused with our dynamic index problem  The former maintains a single summary for the entire dynamic data set  while the latter aims at maintaining a dynamic structure from which a summary for any queried subset can be extracted  which is more general than the former  Of course for the former there often exist smallspace solutions  while for the indexing problem  we cannot hope for sublinear space  as a query range may be small enough so that the summary degenerates to the raw query results  Below we review some of the most fundamental and most studied summaries in the literature  Let D be a bag of items  and let fD x  be the frequency of x in D  Heavy hitters  A heavy hitters summary allows one to extract all frequent items approximately  i e   for a userspeci   ed 0        1  it returns all items x with fD x       D  and no items with fD x                D   while an item x with             D      fD x         D  may or may not be returned  A heavy hitters summary of size O 1     can be constructed in one pass over D  using the MG algorithm  23  or the SpaceSaving algorithm  22   Sketches  Various sketches have been developed as a useful tool for summarizing massive data  In this paper  we consider the two most widely used ones  the Count Min sketch  8  and the AMS sketch  4   They summarize important information about D and can be used for a variety of purposes  Most notably  they can be used to estimate the join size of two data sets  with self join size being a special case  Given the Count Min sketches  resp  AMS sketches  of two data sets D1 and D2  we can estimate  D1   D2  within an additive error of   F1 D1 F1 D2   resp     p F2 D1 F2 D2   with probability at least 1         3  8   where Fk is the kth frequency moment of D  Fk D    P x f k D x   Note that p F2 D      F1 D   so the error of the AMS sketch is no larger  However  its size is O  1    2   log 1       which is larger than the Count Min sketch   s size O  1     log 1       so they are not strictly comparable  Which one is better will depend on the skewness of the data sets  In particular  since F1 D     D   the error of the Count Min sketch does not depend on the skewness of the data  but F2 D  could range from  D  for uniform data to  D  2 for highly skewed data  Quantiles  The quantiles  a k a  the order statistics   which generalize the median  are important statistics about the data distribution  Recall that the    quantile  for 0        1  of a set D of items from a totally ordered universe is the one ranked at    D  in D  for convenience  for the quantile problem it is usually assumed that there are no duplicates in D   A quantile summary contains enough information so that for any 0        1  an    approximate    quantile can be extracted  i e   the summary returns a       quantile where                                 A quantile summary has size O 1      and can be easily computed by sorting D  and then taking the items ranked at    D   2   D   3   D            D   Wavel ets  Wavelet representations  or just wavelets for short  take a di   erent approach to approximating the data distribution by borrowing ideas from signal processing  Suppose the records in D are drawn from an ordered universe u     1          u  and let fD    fD 1           fD u   be the frequency vector of D  Brie   y speaking  in wavelet transformation we take u inner products si    fD  wi  where wi  i   1          u are the wavelet basis vectors  please refer to  12  20  for details on wavelet basis vectors   The si   s are called the wavelet coe   cients of fD  If we kept all u wavelet coe   cients  we would be able to reconstruct fD exactly  but this would not be a    summary     The observation is that  for most real world distributions  fD yields few wavelet coe   cients with large absolute values  Thus for a parameter k  even if we keep the k coe   cients with the largest absolute values  and assume all the other coe   cients are zero  we can still reconstruct fD reasonably well  In fact  it is well known that among all the choices  retaining the k largest  in absolute value  coe   cients minimizes the  2 error between the original fD and the reconstructed one  Matias et al   20  were the    rst to apply wavelet transformation to approximating data distributions  After that  wavelets have been extensively studied  9  11  12  15  21  29   and have been shown to be highly e   ective at summarizing many real world data distributions  All the aforementioned work studies how to construct or maintain the summary on the given D  In our case  D is the As attributes of all records whose Aq attributes are within the query range  Our goal is to design an index so that the desired summary on D can be constructed e   ciently without actually going through the elements of D  1 3 Other related work A few other lines of work also head to the general direction of addressing the gap between reporting all query results and returning some simple aggregates  Lin et al   19  and Tao et al   27  propose returning only a subset of the query results  called    representatives     But the    representatives    do not summarize the data as we do  They also only consider skyline queries  The line of work on online aggregation  16  17  aims at producing a random sample of the query results at early stages of long running queries  in particular  joins  A random sample indeed gives a rough approximation of the data distribution  but it is much less accurate than the summaries we consider  For heavy hitters and quantiles  a random sample of size    1    2   is needed  28  to achieve the same accuracy as the O 1     sized summaries we mentioned earlier  for estimating join sizes  a random sample of size         N  is required to achieve a constant approximation  which is much worse than using the sketches  3   Furthermore  the key di   erence is that they focus on query processing techniques for joins rather than indexing issues  Correlated aggregates  10  aim at exploring the relationship between two attributes  They are computed on one attribute subject to a certain condition on the other  However  this condition has to be speci   ed in advance and the goal is to compute the aggregate in the streaming setting  thus the problem is fundamentally di   erent from ours  1 4 Our results To take a uni   ed approach we classify all the summaries mentioned in Section 1 2 into F1 based ones and F2 based ones  The former includes heavy hitters  the Count Min sketch  and quantiles  all of which provide an error guarantee of the form   F1 D   note that an    approximate quantile is a value with a rank that is o    by   F1 D  from the correct rank   The latter includes the AMS sketch and wavelets  both of which provide an error guarantee related to F2 D   In Section 2 we    rst design a baseline solution that works for all decomposable summaries  A summary is decomposable if given the summaries for t data sets  bags of elements  D1          Dt with error parameter     we can combine them together into a summary on D1             Dt with error parameter O      where   denotes multiset addition  All the F1 and F2 based summaries have this property and thus can be plugged into this solution  Assuming that we can combine the summaries with cost linear to their total size  the resulting index has linear size and answers a summary query in O s   log N  time  where s   is the size of the summary returned  It also works in external memory  with the query cost being O  s   B log N  I Os if s       B and O  log N  log B s     I Os if s     B  Note that this decomposable property has been exploited in many other works on maintaining summaries in the streaming context  5  6  8   In Section 3 we improve upon this baseline solution by identifying another  stronger decomposable property of the F1 based summaries  which we call exponentially decomposable  The size of the index remains linear  while its query cost improves to O log N   s     In external memory  the query cost is O logB N   s   B  I Os  This resembles the classical B tree query cost  which includes an O logB N  search cost and an    output    cost of O s   B   whereas the output in our case is a summary of size s    This is clearly optimal  in the comparison model   For not too large summaries s     O B   the query cost becomes just O logB N   the same as that for a simple aggregation query or a lookup on a B tree  In Section 4  we demonstrate how various summaries have the desired decomposable or exponentially decomposable property and thus can be plugged into our indexes  Finally we show how to support updates in Section 5  2  A BASELINE SOLUTION In this and the next section  we will describe our structures without instantiating with any particular summary  Instead we just use       summary    as a placeholder for any summary with error parameter     Let S     D  denote an    summary on data set D  We use s   to denote the size of an    summary 1   Internal memory structure  Based on the decomposable property of a summary  a baseline solution can be designed using standard techniques  We    rst describe the internal memory structure  Sort all the N data records in the database on the Aq attribute and partition them into N s   groups  each of size s    Then we build a binary tree T on top of these groups  where each leaf  called a fat leaf  stores a group of s   records  For each internal node u of T   let Tu denote the subtree of T rooted at u  We attach to u an    summary on the As attribute of all records stored in the subtree below u  Since each    summary has size s   and the number of internal nodes is O N s     the total size of the structure is O N   To answer a query  q1  q2   we do a search on T   It is well known that any range  q1  q2  can be decomposed into O log N s     disjoint canonical subtrees Tu  plus at most two fat leaves that may partially overlap  We retrieve the    summaries attached to the roots of these 1 Strictly speaking we should write s   D  But as most     summaries have sizes independent of D  we drop the subscript D for brevity subtrees  For each of the fat leaves  we simply read all the s   records stored there  Then we combine all of them into an O     summary for the entire query using the decomposable property  We can adjust    by a constant factor in the construction to ensure that the output is an    summary  The total query time is thus the time required to combine the O log N s     summaries  For the Count Min sketch and AMS sketch  the combining time is linear in the total size of the summaries  so the query time is O s   log N   For the quantile summary and heavy hitters summary the query time becomes O s   log N log log N  2 as we need to merge O log N s     sorted lists  Details in Section 4   External memory index  The baseline solution easily extends to external memory  If s       B  then each internal node and fat leaf occupies    s   B  blocks  so we can simply store each of them separately  The space is still linear and we load O log N  nodes on each query  The query cost becomes O  s   B log N  I Os for the Count Min and AMS sketch and O  s   B log N logM B log N  I Os for the quantile and heavy hitters summary  For s     B  each node occupies a fraction of a block  and we can pack multiple nodes in one block  We use a standard B tree blocking of the tree T where each block contains    B s    nodes  except possibly the root block  Thus each block stores a subtree of height    log B s     of T   Then standard analysis shows that the nodes we need to access are stored in O log N  log B s     blocks  This implies a query cost of O logB s   N  I Os for the Count Min and AMS sketch and O logB s   N logM B logB s   N  I Os for the quantile and heavy hitters summary  3  OPTIMAL INDEXING FOR F1 BASED SUMMARIES The baseline solution of the previous section is not that impressive  Its    output    term has an extra O log N  factor  in external memory  we are missing the ideal O logB N  term which is the main bene   t of block accesses  The main bottleneck in the baseline solution is not the search cost  but the fact that we need to assemble O log N  summaries  each of size s    In the absence of additional properties of the summary  it is impossible to make further improvement  Fortunately  we observe that many of the F1 based summaries have what we call the exponentially decomposable property  which allows us to assemble summaries of exponentially decreasing sizes  This turns out to be the key to optimality for indexing these summaries  Definition 1  Exponentially decomposable   For 0        1  a summary S is    exponentially decomposable if there exists a constant c   1  such that for any t multisets D1          Dt with their sizes satisfying F1 Di         i   1 F1 D1  for i   1          t  given S     D1   S c    D2         S c t   1     Dt    1  we can construct an O     summary for D1                Dt   2  the total size of S     D1           S c t   1     Dt  is O s    and they can be combined in O s    time  and  3  for any multiset D  the total size of S     D           S c t   1     D  is O s     Intuitively  since an F1 based summary S     D  provides an error bound of    D   the total error from S     D1   S c    D2   2 In fact  an alternative solution achieves query time O s   log N  log log N  by issuing s   range quantile queries to the data structure in  7   but this solution does not work in external memory          S c t   1     Dt  is    D1    c   D2               c t   1    Dt         D1     c      D1                c    t   1    D1   If we choose c such that c     1  then the error is bounded by O    D1    satisfying  1   Meanwhile  the F1 based summaries usually have size s        1      so  2  and  3  can be satis   ed  too  In Section 4 we will formally prove the    exponentially decomposable property for all the F1 based summaries mentioned in Section 1 2  3 1 Optimal internal memory structure Let T be the binary tree built on the Aq attribute as in the previous section  Without loss of generality we assume T is a complete balanced binary tree  otherwise we can always add at most N dummy records to make N s   a power of 2 so that T is complete  We    rst de   ne some notation on T   We use S     u  to denote the    summary on the As attribute of all records stored in u   s subtree  Fix an internal node u and a descendant v of u  let P u  v  to be the set of nodes on the path from u to v  excluding u  De   ne the left sibling set of P u  v  to be L u  v     w   w is a left child and has a right sibling     P u  v   and similarly the right sibling set of P u  v  to be R u  v     w   w is a right child and has a left sibling     P u  v    To answer a query  q1  q2   we    rst locate the two fat leaves a and b in T that contain q1 and q2  respectively  Let u be the lowest common ancestor of a and b  We call P u  a  and P u  b  the left and respectively the right query path  We observe that the subtrees rooted at the nodes in R u  a      L u  b  make up the canonical set for the query range  q1  q2   Focusing on R u  a   let w1          wt be the nodes of R u  a  and let d1              dt denote their depths in T  the root of T is said to be at depth 0   Since T is a balanced binary tree  we have F1 wi       1 2  di   d1 F1 w1  for i   1          t  Here we use F1 w  to denote the    rst frequency moment  i e   size  of the point set rooted at node w  Thus  if the summary is  1 2  exponentially decomposable  and we have S c di   d1     wi  for i   1          t at our disposal  we can combine them and form an O     summary for all the data covered by w1          wt  We do the same for L u  b   Finally  the two fat leaves can always supply the exact data  it is a summary with no error  of size O s    in the query range  Plus the initial O log N  search cost for locating R u  a  and L u  b   the query time now improves to the optimal O log N   s     It only remains to show how to supply S c di   d1     wi  for each of the wi   s  In fact  we can a   ord to attach to each node u     T all the summaries  S     u   S c    u         S c q     u  where q is an integer such that scq      O 1   Nicely  these summaries still have total size O s    by the exponentially decomposable property  thus the space required by each node is still O s    as in the previous section  and the total space remains linear  A schematic illustration of the overall structure is shown in Figure 1  Theorem 1  For any  1 2  exponentially decomposable summary  a database D of N records can be stored in an internal memory structure of linear size so that a summary query can be answered in O log N   s    time             summary   3 2     summary    3 2   2     summary                                                                                                                               Query range Figure 1  A schematic illustration of our internal memory structure  The grayed nodes form the canonical decomposition of the query range  and the grayed summaries are those we combine into the      nal summary for the queried data  In this example we use c   3 2   3 2 Optimal external memory indexing In this section we show how to achieve the O logB N   s   B  I O query cost in external memory still with linear space  Here  the di   culty that we need to assemble O log N  summaries lingers  In internal memory  we managed to get around it by the exponentially decomposable property so that the total size of these summaries is O s     However  they still reside in O log N  separate nodes  If we still use a standard B tree blocking  for s       B we need to access    log N  blocks  for s     B  we need to access    log N  log B s     blocks  neither of which is optimal  Below we    rst show how to achieve the optimal query cost by increasing the space to super linear  then propose a packed structure to reduce the space back to linear  Consider an internal node u and one of its descendants v  Let the sibling sets R u  v  and L u  v  be as previously de   ned  In the following we only describe how to handle the R u  v    s  we will do the same for the L u  v    s  Suppose R u  v  contains nodes w1          wt at depths d1          dt  We de   ne the summary set for R u  v  with error parameter    to be RS u  v         S     w1   S c d2   d1     w2           S c dt   d1     wt    The following two facts easily follow from the exponentially decomposable property  Fact 1  The total size of the summaries in RS u  v      is O s     Fact 2  The total size of all the summary sets RS u  v       RS u  v  c            RS u  v  c t     is O s     The indexing structure  We    rst build the binary tree T as before with a fat leaf size of s    Before attaching any summaries  we block T in a standard B tree fashion so that each block stores a subtree of T of size    B   except possibly the root block which may contain 2 to B nodes of T   The resulting blocked tree is essentially a B tree where each leaf occupies O s   B  blocks and each internal node occupies 1 Leaf size  s      B  O log B  Figure 2  The standard B tree blocking of a binary tree  rB v2 RS rB  v2      RS rB  v2  c    u v1 RS u  v1      Figure 3  The summaries we store for an internal block B  block  Please see Figure 2 for an example of the standard B tree blocking  Consider an internal block B in the B tree  Below we describe the additional structures we attach to B  Let TB be the binary subtree of T stored in B and let rB be the root of TB  To achieve the optimal query cost  the summaries attached to the nodes of TB that we need to retrieve for answering any query must be stored consecutively  or in at most O 1  consecutive chunks  Therefore  the idea is to store all the summaries for a query path in TB together  which is the reason we introduced the summary set RS u  v       The detailed structures that we attach to B are as follows  1  For each internal node u     TB and each leaf v in u   s subtree in TB  we store all summaries in RS u  v      sequentially  2  For each leaf v  we store the summaries in RS rB  v  c j     sequentially  for all j   0          q  Recall that q is an integer such that scq      O 1   3  For the root rB  we store S c j     rB  for j   0          q  An illustration of the    rst and the second type of structures is shown in Figure 3  The size of the structure can be determined as follow 1  For each leaf v     TB  there are at most O log B  ancestors of v  so there are in total O B log B  such pairs  u  v   For each pair we use O s    space  so the space usage is O s  B log B   2  For each leaf v     TB we use O s    space  so the space usage is O s  B   3  For the root rB  the space usage is O s     Summing up the above cases  the space for storing the summaries of any internal block B is O s  B log B   Note that each internal block has fanout    B   and each leaf has size    s     so there are in total at most O N  Bs     internal blocks  and thus the total space usage is O N log B   Next we show that this structure can indeed be used to answer queries in the optimal O logB N   s   B  I Os  Query procedure  Given a query range  q1  q2   let a and b be the two leaves containing q1 and q2  respectively  We focus on how to retrieve the necessary summaries for the right sibling set R u  a   where u is the lowest common ancestor of a and b  the left sibling set L u  b  can be handled symmetrically  By the previous analysis  we need exactly the summaries in RS u  a       Recall that R u  a  are the right siblings of the left query path P u  a   Let B0          Bl be the blocks that P u  a  intersects from u to a  The path P u  a  is partitioned into l   1 segments by these l   1  blocks   Let P u  v0   P r1  v1           P rl  vl   a  be the l   1 segments  with ri being the root of the binary tree TBi in block Bi and vi being a leaf of TBi   i   0          l  Let w1          wt be the nodes in R u  a   at depths d1          dt of T   We claim that wi is either a node of TBk for some k      0          l   or a right sibling of rk for some k      0          l   which makes wi a root of some other block  This is because by the definition of R u  a   we know that wi is a right child whose left sibling is in some Bk  If wi is not in Bk  it must be the root of some other block  Recall that we need to retrieve S c di   d1     wi  for i   1          t  Below we show how this can be done e   ciently using our structure  For the wi   s in the    rst block B0  since we have stored all summaries in RS u  v0      sequentially for B0  case 1    they can be retrieved in O 1   s   B  I Os  For any wi being the root of some other block B   not on the path B0          Bl  since we have stored the summaries S c j     wi  for j   0          q for every block  case 3    the required summary S c di   d1     wi  can be retrieved in O 1   s c di   d1     B  I Os  Note that the number of such wi   s is bounded by O logB N   so the total cost for retrieving summaries for these nodes is at most O logB N   s   B  I Os  The rest of the wi   s are in B1          Bl  Consider each Bk  k   1          l  Recall that the segment of the path P u  a  in Bk is P rk  vk   and the wi   s in Bk are exactly R rk  vk   We have stored RS rk  vk  c j     for Bk for all j  case 2    so no matter at which relative depths di     d1 the nodes in R rk  vk  start and end  we can always    nd the required summary set  Retrieving the desired summary set takes O   1   s cd    d1     B    I Os  where d   is the depth of the highest node in R rk  vk   Summing over all blocks B1          Bl  the total cost is O logB N   s   B  I Os  Reducing the size to linear  The structure above has a super linear size O N log B   Next we show how to reduce its size back to O N  while not a   ecting the optimal query cost  u ul ur   w1 u   kh h w2 w3 S c    w S     w1  S c 2  2     w3  One summary for each node in u      s subtree Figure 4  A schematic illustration of our packed structure  Observe that the log B factor comes from case 1   where we store RS u  v      for each internal node u and each leaf v in u   s subtree in u   s block B  Focus on B and the binary tree TB stored in it  Abusing notation  we use Tu to denote the subtree rooted at u in TB  Assume Tu has height h  in TB   Our idea is to pack the RS u  v        s for some leaves v     Tu to reduce the space usage  Let ul and ur be the left and right child of u  respectively  The    rst observation is that we only need to store RS u  v      for each leaf v in ul   s subtree  This is because for any leaf v in ur   s subtree  the sibling set R u  v  is the same as R ur  v   so RS u  v        RS ur  v       which will be stored when considering ur in place of u  For any leaf v in ul   s subtree  observe that the highest node in R u  v  is ur  This means for a node w     R u  v  with height i in tree Tu  the summary for w in RS u  v      is S c h   i   1     w   Let u   be an internal node in ul   s subtree  and suppose u   has kh leaves below it  We will decide later the value of kh and  thus  the height log kh at which u   is chosen  the leaves are de   ned to be at height 0   We do the following for each u   at height log kh in ul   s subtree  Instead of storing the summary set RS u  v      for each leaf v in u      s subtree  we store RS u  u          which is the common pre   x of all the RS u  v        s  together with a summary for each of the nodes in u      s subtree  More precisely  for each node w in u      s subtree  if its height is i  we store a summary S c h   i   1     w   All these summaries below u   are stored sequentially  A schematic illustration of our packed structure is shown in Figure 4  Recall that all the summary sets we store in case 1  are used to cover the top portion of the query path P u  v0  in block B0  i e   RS u  v0       Clearly the packed structure still serves this purpose  We    rst    nd the u   which has v0 as one of its descendants  Then we load RS u  u          followed by the summaries S c h   i   1   w  required in RS u  v0       Loading RS u  u         still takes O 1 s   B  I Os  but loading the remaining individual summaries may incur many I Os since they may not be stored sequentially  Nevertheless  if we ensure that all the individual summaries below u   have total size O s     then loading any subset of them does not take more than O 1   s   B  I Os  Note that there are kh 2 i nodes at height i in u   s subtree  the total size of all sum maries below u   is logXkh i 0 kh 2 i sch   i   1       1  Thus it is su   cient to choose kh such that  1  is    s     Note that such a kh always exists 3   When kh   1    1  is sch   1      O s     when kh takes the maximum possible value kh   2 h   1   the last term  when i   h  in the summation of  1  is s    so  1  is at least    s     every time kh doubles   1  increases by at most O s     It only remains to show that by employing the packed structure  the space usage for a block is indeed O Bs     For a node u at height h in TB  the number of u      s at height log kh under u is 2 h  kh  For each such u     storing RS u  u          as well as all the individual summaries below u     takes O s    space  So the space required for node u is O 2 h s   kh   There are O B 2 h   nodes u at height h  Thus the total space required is O   logXB h 1 2 h s   kh    B 2 h     O   logXB h 1 Bs   kh     Note that the choice of kh implies that s   kh   O  logXkh i 0 1 2 i sch   i   1        O   hX   1 i 0 1 2 i sch   i   1        so the total size of the packed structures in B is bounded by logXB h 1 Bs   kh     B logXB h 0 hX   1 i 0 1 2 i sch   i   1      B logXB h 0 hX   1 i 0 1 2h   i   1 sc i        B logXB i 0 sc i    logXB h i 1 2h   i   1     2B logXB i 0 sc i      O Bs     Theorem 2  For any  1 2  exponentially decomposable summary  a database D of N records can be stored in an external memory index of linear size so that a summary query can be answered in O logB N   s   B  I Os  Remark  One technical subtlety is that the O s    combining time in internal memory does not guarantee that we can combine the O log N  summaries in O s   B  I Os in external memory  However if the merging algorithm only makes linear scans on the summaries  then this is not a problem  as we shall see in Section 4  4  SUMMARIES In this section we demonstrate the decomposable or exponentially decomposable properties for the summaries mentioned in Section 1 2  Thus  they can be used in our indexes in Section 2 and 3  3 We de   ne kh in this implicit way for its generality  When instantiating into speci   c summaries  there are often closed forms for kh  For example when s        1     and 1   c   2  kh      c h    4 1 Heavy hitters Given a multiset D  let fD x  be the frequency of x in D  The MG summary  23  with error parameter    consists of s     1    items and their associated counters  For any item x in the counter set  the MG summary maintains an estimated count    fD x  such that fD x        F1 D         fD x      fD x   for any item x not in the counter set  it is guaranteed that fD x        F1 D   Thus in either case  the MG summary provides an additive   F1 D  error  fD x        F1 D         fD x      fD x  for any x  The SpaceSaving summary is very similar to the MG summary except that the SpaceSaving summary provides an    fD x  overestimating fD x   fD x         fD x    fD x      F1 D   Thus they clearly solve the heavy hitters problem  The MG summary is clearly decomposable  Below we show that it is also    exponentially decomposable for any 0        1  The same proof also works for the SpaceSaving summary  Consider t multisets D1          Dt with F1 Di         i   1 F1 D1  for i   1          t  We set c   1           1  Given a series of MG summaries S     D1   S c    D2           S c t   1     Dt   we combine them by adding up the counters for the same item  Note that the total size of these summaries is bounded by Xt   1 j 0 scj      Xt   1 j 0 1 c j      O 1       O s     In order to analyze the error in the combined summary  let fj  x  denote the true frequency of item x in Dj and    fj  x  be the estimator of fj  x  in S c j   1     Dj    The combined summary uses Pt j 1    fj  x  to estimate the true frequency of x  which is Pt j 1 fj  x   Note that fj  x         fj  x      fj  x      c j   1   F1 Dj   for j   1          t  Summing up the    rst inequality over all j yields Pt j 1 fj  x      Pt j 1    fj  x   For the second inequality  we have Xt j 1    fj  x      Xt j 1 fj  x      Xt j 1 c j   1   F1 Dj       Xt j 1 fj  x      Xt j 1               j   1   F1 D1      Xt j 1 fj  x        F1 D1  Xt j 1           j   1   Xt j 1 fj  x      O   F1 D1    Therefore the error bound is O   F1 D1     O    F1 D1               Dt    To combine the summaries we require that each summary maintains its  item  counter  pairs in the increasing order of items  we impose an arbitrary ordering if the items are from an unordered domain   In this case each summary can be viewed as a sorted list and we can merge the t sorted lists into a single list  where the counters for the same item are added up  Note that if each summary is of size s    then we need to employ a t way merging algorithm and it takes O s  tlog t  time in internal memory and O  s  t B logM B t  I Os in external memory  However  when the sizes of the t summariesform a geometrically decreasing sequence  we can repeatedly perform two way merges in a bottom up fashion with linear total cost  The merging algorithm starts with an empty list  at step i  it merges the current list with the summary S   t 1   i  Dt 1   i   Note that in this process every counter of S   j   Dj   is merged j times  but since the size of S   j   Dj   is 1 cj   1      the total running time is bounded by Xt j 1 j c j   1      O     1         O s     In external memory we can perform the same trick and achieve the O s   B  I O bound if the smallest summary S c t   1     Dt  has size 1 c t   1      B  otherwise we can take the smallest k summaries  where k is the maximum number such that the smallest k summaries can    t in one block  and merge them in the main memory  In either case  we can merge the t summaries in s   B I Os  4 2 Quantiles Recall that in the    approximate quantile problem  we are given a set D of N items from a totally ordered universe  and the goal is to have a summary S     D  from which for any 0        1  a record with rank in             N          N  can be extracted  It is easy to obtain a quantile summary of size O 1      We simply sort D and take an item every   N consecutive items  Given any rank r     N  there is always an element within rank  r       N  r     N   Below we show that quantile summaries are    exponentially decomposable  Suppose we are given a series of such quantile summaries S   1  D1   S   2  D2           S   t  Dt   for data sets D1          Dt  We combine them by sorting all the items in these summaries  We claim this forms an approximate quantile summary for D   D1                    Dt with error at most Pt j 1   jF1 Dj    that is  given a rank r  we can    nd Pan item in the combined summary whose rank is in  r     t j 1   jF1 Dj    r   Pt j 1   jF1 Dj    in D  For an element x in the combined summary  let yj and zj be the two consecutive elements in S   j   Dj   such that yj     x     zj   We de   ne rmin j  x  to be the rank of yj in Dj and rmax j  x  to be rank of zj in Dj   In other words  rmin j  x   resp  rmax j  x   is the minimum  resp  maximum  possible rank of x in Dj   We state the following lemma that describes the properties of rmin j  x  and rmax j  x   Lemma 1   1  For an element x in the combined summary  Xt j 1 rmax j  x      Xt j 1 rmin j  x      Xt j 1   jF1 Dj     2  For two consecutive elements x1     x2 in the combined summary  Xt j 1 rmin j  x2      Xt j 1 rmin j  x1      Xt j 1   jF1 Dj    Proof  Since rmax j  x  and rmin j  x  are the local ranks of two consecutive elements in S   j   Dj    we have rmax j  x      rmin j  x        jF1 Dj    Taking summation over all j  part  1  of the lemma follows  We also note that if x1 and x2 are consecutive in the combined summary  rmin j  x1  and rmin j  x2  are local ranks of either the same element or two consecutive elements of S   j   Dj     In either case we have rmin j  x2      rmin j  x1        jF1 Dj    Summing over all j proves part  2  of the lemma  Now for each element x in the combined summary  we compute the global minimum rank rmin  x    Pt j 1 rmin j  x   Note that all these global ranks can be computed by scanning the combined summary in sorted order  Given a query rank r  we    nd the smallest element x with rmin P  x      r     t j 1   jF1 Dj    We claim that the actual rank of x in D is in the range  r     Pt j 1   jF1 Dj    r   Pt j 1   jF1 Dj     Indeed  we observe that the actual rank of x in set D is in the range   Pt j 1 rmin j  x   Pt j 1 rmax j  x   so we only need to prove that this range is contained by  r    Pt j 1   jF1 Dj    r  Pt j 1   jF1 Dj     The left side trivially follows from the choice of x  For the right side  let x   be the largest element in the new summary such that x       x  By the choice of x  we have Pt j 1 rmin j  x       r     Pt j 1   jF1 Dj    By Lemma 1 we have Pt j 1 rmin j  x      Pt j 1 rmin j  x         Pt j 1   jF1 Dj   and Pt j 1 rmax j  x      Pt j 1 rmin j  x      Pt j 1   jF1 Pj    Summing up these three inequalities yields Pt j 1 rmax j  x      r   Pt j 1   jF1 Dj    so the claim follows  For    exponentially decomposability  the t data sets have F1 Di         i   1 F1 D1  for i   1          t  We choose c   1           1  The summaries S   1  D1   S   2  D2           S   t  Dt  have   i   c i   1     Therefore we can combine them with error Xt j 1 c j   1   F1 Dj       Xt j 1                 j   1   F1 D1      F1 D1  Xt j 1           j   1   O   F1 D1     O   F1 D1                    Dt    To combine the t summaries  we notice that we are essentially merging k sorted lists with geometrically decreasing sizes  so we can adapt the algorithm in Section 4 1  The cost of merging the t summaries is therefore O s    in internal memory and O s   B  I Os in external memory  The size of combined summary is Xt j 1 1 c j   1      O     1         O s     4 3 The Count Min sketch Given a multiset D where the items are drawn from a universe  u     1          u   Let fD x  be the frequency of x in D  The Count Min sketch makes use of a 2 universal hash function h    u       1     and a collection of 1    counters C 1           C 1      Then it computes C j    P h x  j fD x  for j   1          1     A single collection of 1    counters achieve a constant success probability for a variety of estimation purposes  and the probability can be boosted to 1        by using O log 1      copies with independent hash functions  Here we only show the decomposability of a single copy  the same result also holds for O log 1      copies  Given multiple Count Min sketches with the same h  hence the same number of counters   they can be easily combined by adding up the corresponding counters  So the Count Minsketch is decomposable  However  for exponentially decomposability we are dealing with t Count Min sketches with exponentially increasing      s  hence di   erent hash functions  so they cannot be easily combined  Thus we simply put them together without combining any counters  Although the resulting summary is not a true Count Min sketch  we argue that it can be used to serve all the purposes a CountMin is supposed to serve  More precisely  for t data sets D1          Dt with F1 Di         i   1 F1 D1   we have t Count Min sketches S     D1           S c t   1     Dt   The i th sketch S c j   1     Dt  uses a hash function hi    u       1 c j   1      Again we set c   1          Note that the total size of all the sketches is O 1      1 c     1 c 2                  O 1       O s     so we only need to show that the error is the same as what a Count Min sketch S     D1          Dt  would provide  Below we consider the problem of estimating inner products  join sizes   which has other applications  such as point queries and self join sizes  as special cases  PLet fi denote the frequency vector of Di  and let f   t i 1 fi be the frequency vector of D   D1           Dt  The goal is to estimate inner product  f  g  where g is the frequency vector of some other data set  Note that when g is a standard basic vector  i e   containing only one    1       f  g  becomes a point query  when g   f   f  g  is the self join size of f  We distinguish between two cases   1  g is given explicitly  and  2  g is also represented by a summary returned by our index  i e   a collection of t Count Min sketches S     G1           S c t   1     Gt   where g   Pt i 1 gi and gi is the frequency vector of Gi  Recall that the Count Min sketch estimates  f  g  with an additive error of   F1 f F1 g   and we will show that we can do the same when f is represented by the collection of t Count Min sketches  Inner product with an explicit vector  For a g given explicitly  we can construct a Count Min sketch S c i   1     g  for g with hash function hi  for i   1          t  We observe that  f  g  can be expressed as Pt i 1  fi  g   and  fi  g  can be estimated using S c i   1     Di  and S c i   1     g  as described in  8  since they use the same hash function  The error is c i   1     fi  1  g  1      c    i   1     f1  1  g  1  For c   1          the total error is bounded by Xt i 1     i   1  2     f1  1  g  1   O     f1  1  g  1    O    F1 f F1 g    as desired  Inner product with a vector returned by a summary query  Next we consider the case where g is also represented by a series 4 of t Count Min sketches S     G1           S c t   1     Gt  with F1 Gi         i   1 F1 G1   We will show how to estimate  f  g  using the two series of sketches  This will allow the user to estimate the join size between the results of two queries  Note that this includes the special case of estimating the self join size of f  In this case we will inevitably face the problem of pairing two sketches of di   erent sizes  To do so we need more insight into the hash functions used  Suppose 1    is a power of 2  4 More precisely  g is represented by two such series  one from the left query path and one from the right query path  and so is f  But we can decompose  f  g  into 4 subproblems by considering the cross product of these series  where each subproblem involves only a single series of</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pssp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#pssp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#streaming_and_sampling"/>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing"/>
        <doc>Efficiently Evaluating Complex Boolean Expressions Marcus Fontoura ### Suhas Sadanandan Jayavel Shanmugasundaram Sergei Vassilvitski Erik Vee Srihari Venkatesan Jason Zien Yahoo  Research  701 First Ave   Sunnyvale  CA 94089  marcusf  suhas  jaishan  sergei  erikvee  venkates  jasonyz  yahoo inc com ABSTRACT The problem of e   ciently evaluating a large collection of complex Boolean expressions     beyond simple conjunctions and Disjunctive Conjunctive Normal Forms  DNF CNF      occurs in many emerging online advertising applications such as advertising exchanges and automatic targeting  The simple solution of normalizing complex Boolean expressions to DNF or CNF form  and then using existing methods for evaluating such expressions is not always e   ective because of the exponential blow up in the size of expressions due to normalization  We thus propose a novel method for evaluating complex expressions  which leverages existing techniques for evaluating leaf level conjunctions  and then uses a bottom up evaluation technique to only process the relevant parts of the complex expressions that contain the matching conjunctions  We develop two such bottom up evaluation techniques  one based on Dewey IDs and another based on mapping Boolean expressions to one dimensional intervals  Our experimental evaluation based on data obtained from an online advertising exchange shows that the proposed techniques are e   cient and scalable  both with respect to space usage as well as evaluation time  Categories and Subject Descriptors H 2 4  Systems   Query processing General Terms Algorithms Performance Keywords Boolean expressions  Pub sub  Dewey  Interval 1 ###  INTRODUCTION We consider the problem of e   ciently evaluating a large collection of arbitrarily complex Boolean expressions  given Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   10  June 6   11  2010  Indianapolis  Indiana  USA  Copyright 2010 ACM 978 1 4503 0032 2 10 06     5 00  an assignment of attributes to values  The problem of ef     ciently evaluating Boolean expressions has many applications  including  1  publish subscribe systems  8   where a subscription can be modeled as a Boolean expressions and an event can be modeled as a collection of attribute value pairs  and the goal is to rapidly return the subscriptions that match an event  and  2  online display advertising systems  10  13  18   where an advertiser campaign can be modeled as a Boolean expression targeting user visit features  and a user visit can be modeled as a collection of attributevalue pairs  and the goal is to rapidly return the set of advertiser campaigns that are eligible for the user visit  Current solutions for evaluating Boolean expressions  however  are primarily limited to simple Boolean expressions such as conjunctions  1  9  14  20  21  and conjunctive disjunctive normal form expressions  5  17   While these restrictions are reasonable for many of the above applications  some emerging applications require support for arbitrarily complex Boolean expressions  as discussed below  1 1 Motivating Applications Online Display Advertising Exchanges  One of the emerging trends in online display advertising is the notion of an advertising exchange  or simply  an ad exchange  An ad exchange is essentially an electronic hub that connects online publishers to advertisers  either directly or through intermediaries  An ad exchange can thus be represented as a directed graph  where the nodes are publishers  advertisers and intermediaries  and an edge exists between a node X and a node Y if X agrees to sell advertisement slots associated with user visits to Y  note that X may either be a publisher who generates advertisement slots through user visits  or an intermediary who obtains user visits from publishers or other intermediaries   In addition  each edge is typically annotated with a Boolean expression that restricts the set of user visits that can be sold through that edge  for various business reasons such as protecting certain user visits for sales through speci   c channels  edges   For instance  a node X may only want to sell to Y male user visits to Sports pages  and female user visits that are not to Finance pages  this could be represented as a Boolean expression on the X Y edge   Gender      Male      Category      Sports        Gender      F emale      Category 6     F inance   Given the ad exchange graph  an advertiser A can decide to book an advertising campaign with a publisher P  even though A may only be indirectly connected to P  For instance  if an advertiser A wants to book a campaign with P that targets users in age category 4 who are interested in NFL  and age category 5 who are interested in NBA  1this campaign can also be represented as a Boolean expression   Age      4      Interest      NF L        Age      5      Interest      NBA    Note  however  that any user visit from P that satis   es the above Boolean expression cannot simply be served the ad from A  the user visit should also satisfy all the Boolean expressions on the path from P to A 1   Consequently  the campaign booked with P is the conjunction of the advertiser campaign Boolean expression  and the Boolean expressions along the path from P to A  Since advertiser and targeting constraints can themselves be complicated DNF or other expressions  conjunctions of such expressions quickly leads to fairly complex Boolean expressions which are booked to publishers  When a user visits a publisher Web page  the user visit can be viewed as an attribute value assignment such as the one below   Gender   Male  Interest   NF L  Category   Sports  and the goal is to rapidly    nd all the ad campaigns booked to P that can be satis   ed by the user visit  so that the best ads can then be selected to be shown to the user  In other words  the exchange has to rapidly evaluate complex Boolean expressions to determine which one satisfy the given assignment of attributes to values  Automatic user targeting  A related application that generates complex Boolean expressions is automatic user targeting for display advertising  Unlike manual user targeting where advertisers specify the Boolean expression of interest  as in the examples above   in automatic user targeting  the system automatically generates targeting expressions that try to maximize advertiser objectives such as clicks or conversions  For instance  an advertiser who is interested in obtaining clicks on ads may specify a fairly broad targeting constraint  and allow the system to explore the high dimensional targeting space to generate boolean targeting expressions that optimize desired measurement objectives  Clearly these can get quite complex very quickly because they are automatically generated  Given many such advertiser campaigns  the advertising system again needs to rapidly evaluate these Boolean expressions given an attribute assignment  user visit   1 2 Contributions Given the above motivating applications  we now turn to the issue of e   ciently evaluating arbitrarily complex Boolean expressions  A simple evaluation method  of course  is to sequentially loop over all the Boolean expressions  and do a recursive top down evaluation of the expression tree given the attribute value assignment  This method has the obvious downside of having to evaluate every single expression  even though the assignment may only match a small fraction of them  Another simple method is convert the Boolean expressions to DNF or CNF form  and leverage state of the art techniques  e g    5  17   to e   ciently evaluate these expressions  Again  this method has the downside of an exponential blow up in the size the expressions due to normalization  this issue is exacerbated by the fact that most online ad systems are entirely memory resident  for latency reasons   which leads to excessive memory requirements  Given the limitations of the obvious approaches  the question that 1 There might be multiple paths from P to A and the best path is usually chosen based on revenue and other constraints that are not germane to the current discussion  arises is the following  is there a way to evaluate Boolean expressions that does not require evaluating every expression  and that does not result in an exponential space blow up  The main technical contribution of this paper is a novel evaluation method that addresses the above issues  The method consists of two key steps  In the    rst step  existing conjunction matching techniques  9  17  21  are used to    nd the leaf level conjunctions of the  un normalized  Boolean expressions that match the given assignment  In addition  each conjunction is annotated with a compact description of where it occurs in the Boolean expressions  In the second step  the matching conjunctions along with the information on where they occur is used to perform a bottom up evaluation of the Boolean expressions  The bottom up evaluation is performed in a single pass over the matching conjunctions  and only selectively evaluates the expressions     and parts of these expressions     that have at least one matching conjunction  The above two step approach thus leverages existing conjunction matching techniques without blowing up the size of the expressions  and also avoids explicitly evaluating all expressions by using selective bottom up evaluation of only those  parts of  expressions that can possibly be satis     ed  As mentioned above  the key idea that enables the bottomup evaluation of Boolean expressions is the annotation that identi   es the position of each conjunction within the Boolean expression  There are two annotation variants that we consider  both of which work on the Boolean tree representation of expressions  In the    rst variant  each conjunction is identi   ed based on a Dewey labeling of the Boolean expression tree  similar to the Dewey labeling of an XML tree  16    Given this labeling  the bottom up evaluator uses a stackbased algorithm to e   ciently    nd the ids of the contracts that evaluate to true  In the second variant  each Boolean expression tree is mapped to a one dimensional space  and the bottom up evaluator uses a simple interval matching technique to    nd the ids of the matching contracts  While both approaches are e   cient  one of the advantages of the one dimensional mapping is that the conjunction annotations are    xed length  as compared to variable length Dewey labels  We have implemented the proposed methods and evaluated them using data obtained from an online display advertising exchange  Our performance results show that the proposed methods signi   cantly outperform existing methods  both in terms of latency and memory requirements  1 3 Roadmap The rest of the paper is organized as follows  In Section 2  we describe the problem and the system architecture  In Section 3  we present the evaluation method based on Dewey labeling  and in Section 4  we present the evaluation method based on the one dimensional interval mapping  In Section 5  we present our experimental results and  in Section 6  we discuss related work  Finally  in Section 7  we present our conclusions  2  PROBLEM DESCRIPTION Our problem is to e   ciently    nd which Boolean expressions from a large set are satis   ed by an input assignment  An assignment is a set of attribute name and value pairs  A1   v1  A2   v2          For example  a woman in California may have the assignment  Gender   F  State   CA   2An assignment does not necessarily specify all the possible attributes  Allowing unspeci   ed attributes is important to support high dimensional data where the number of attributes may be in the order of hundreds  Consequently  our model does not restrict assignments to use a    xed set of possible attributes known in advance  A Boolean expression  BE  is a tree in which intermediate nodes are of two types  AND nodes and OR nodes  Leaf nodes in the tree are simple conjunctions of basic     and 6    predicates  The predicate State      CA  NY    for example  means that the state can either be California or New York while the predicate State 6     CA  NY   means the state cannot be either of the two states  Notice that the     and 6    primitives subsume simple   and   predicates  6 Without loss of generality  we restrict our BE trees to have alternating AND OR nodes in every path from the root to the leafs  Any arbitrarily complex BE can be represented by these alternating AND OR trees with conjunction leafs  including DNFs  i e   disjunctive normal form   CNFs  i e   conjunctive normal form   ANDs of DNFs  ORs of CNFs  and so on  2 1 System Architecture The overall system architecture is presented in Figure 1  In an o   ine process  before query evaluation starts  BEs are annotated and indexed  The Conjunction Annotator module is responsible for annotating each conjunction with a compact description of where it occurs in the BE  These annotations are stored in a Conjunction Annotations database  The conjunctions are then indexed by the Conjunction Index Builder  Our approach works with any existing scheme for indexing and evaluating conjunctions  e g   1  9  14  17   During runtime  given an assignment  the index is used to retrieve the matching conjunctions  Given these set of matching conjunctions  the Expression Evaluator uses the Conjunction Annotations database to retrieve the annotations for the conjunctions that need to be evaluated  The job of the Expression Evaluator is to e   ciently verify if the entire BE tree can be satis   ed from the conjunctions retrieved by the index  We highlighted components Expression Evaluator and Expression Annotator since these are the main contributions of the paper  Sections 3 and 4 describe two di   erent annotation schemes and evaluation strategies for these components  Scalability  latency and updates  The main focus points of this paper are the Expression Evaluator and Conjunction Annotator components and we can reuse any conjunction indexing scheme  Each of these di   erent schemes will handle scalability  latency and updates di   erently  Our driving applications are online advertising systems  which have to process billions of requests a day  However  the update volume is typically many orders of magnitude less than the read volume  Fortunately  there are several conjunction indexing schemes optimized for this scenario  e g    17   For instance  scalability can be solved by index replication and partitioning  latency can be solved by keeping the indexes in main memory  while updates can handled by keeping small    delta    indexes in addition to the main indexes  17   3  DEWEY ID MATCHING Our    rst algorithm uses the notion of Dewey IDs to perform boolean expression matching  We    rst describe how Dewey IDs are generated  and then how they are used in Conjunc on       Annotator    Conjunc on    Index   Builder    Conjunc on    Index    Boolean    Expression    O   ine    Assignment    Index    Evaluator    Conjunc on    Annota ons    Expression    Evaluator    Online    Matching    Conjunc ons    Matching   Boolean   Expression    Figure 1  Online and o   ine architectural view of the system  We focus on the highlighted components  Expression Evaluator and Conjunction Annotator  evaluating BEs  The main challenge comes from the fact that we do not store the Boolean Expression Tree for evaluation  Rather  we reconstruct the relevant parts of the tree only from the information encoded in the matching Dewey IDs and decide whether the overall tree evaluates to true  3 1 Conjunction Annotator We    rst describe the information stored with each conjunction  As we mentioned earlier  any BE can be expressed as an alternating AND OR Boolean tree  We label each leaf node of the tree with its corresponding Dewey ID as follows  1  Without loss of generality let the root of the tree be an AND node  We can always add an arti   cial AND at the top if needed  2  Let edges to the children of a node be sequentially numbered starting from 1  with the last child marked with a special symbol      The root to node numbering  based on those edge numbers  is referred to as the Dewey ID of a node  3  The length of a Dewey ID is the number of edges from the root to the node  Observe that a node with an odd length Dewey ID is beneath an AND  and a node with an even length Dewey ID is beneath an OR  For example  consider the BE tree in Figure 2  The label of D is 1    3 1     to reach D from the root  one takes the    rst branch  which happens to be the last branch as well  as denoted by       then the third branch  and then the    rst branch again  The Dewey IDs of other leaves are given in the Figure  3 2 Expression Evaluator We    rst give intuition on the functioning of the algorithm  We then describe it in more detail  The algorithm acts recursively on the Boolean expression tree  Note that this tree is not actually available during online processing  We only have the list of matching dewey IDs  However  these dewey IDs implicitly encode this tree  or more precisely  the portion of the tree where the dewey IDs lie   As we process the IDs  we create this    virtual    tree on the    y  3C D A B OR AND AND F E AND A 1  1 1 B C D E F 1  1 2  1  2 1  3 1 1  3 2 1  3 3  Figure 2  An example of a BE tree with Dewey ID labels  The special symbol     indicates the last child of an AND node  Algorithm 1 The Dewey Evaluation Algorithm Require  deweyList  a list of dewey IDs in sorted order  1  Initialize curr     deweyList getFirst    curr and deweyList are global variables  2  return EvaluateAND Empty DeweyId  Throughout the running of the algorithm  we alternate calls to EvaluateAND and EvaluateOR  Each call takes as input a dewey label  which we call dewLabel  We think of each call as corresponding to exploring a node in the Boolean expression tree  and this label corresponds precisely to the dewey ID of that node  A call to EvaluateAND is like exploring an AND node of the tree  while EvaluateOR corresponds to exploring an OR node  We iterate through the list of dewey IDs  in sorted order  The value of curr is the dewey ID we are currently considering  Note the curr corresponds to a leaf node in the Boolean expression tree  At this point  it is helpful to imagine a depth    rst traversal of the virtual nodes of the Boolean expression tree  If we are exploring a node in the virtual tree that is an ancestor of the leaf node corresponding to curr which is equivalent to the dewey label in the current call being a pre   x of curr   then we move down toward that leaf  When we reach a leaf  we evaluate it true  since the index only returns those conjunctions which evaluate to true  We then pop up a level  partially evaluating that node  The curr dewey ID is updated to the next dewey ID in the list  We continuing popping up levels  evaluating nodes as we go   until we reach an ancestor of the newly updated curr  Continuing in this manner allows us to evaluate the entire expression  We now walk through the algorithm  Pseudo code is shown in Algorithms 1 2  and  3  We use several helper functions  First  Child  which takes as input a dewey pre     x and a dewey ID   The input pre   x must be a pre   x of the input dewey ID   It returns the dewey ID of entry where they    rst di   er  For example  Child 0 1 2  0 1 2 3 4  returns 0 1 2 3  Note that Child returns the Dewey label for one of the children of the dewey pre   x  The other two functions  work on the dewey IDs  Last returns the value of the last Algorithm 2 The EvaluateOR Algorithm Require  dewLabel  current position in the tree  1  if dewLabel   curr then  We are at a leaf   2  return true 3  end if 4  Initialize result     false 5  while dewLabel is a pre   x for curr do  curr is a descendant of this node  6  Let child     Child dewLabel  curr  7  result     result     EvaluateAND child  8  curr     deweyList next   9  end while 10  curr     deweyList prev   11  return result Algorithm 3 The EvaluateAND Algorithm Require  dewLabel  current position in the tree  1  if dewLabel   curr then  We are at a leaf   2  return true 3  end if 4  Initialize result     true  lastExplored     0  and lastChild   false 5  while dewLabel is a pre   x for curr do  curr is a descendant of this node  6  Let child     Child dewLabel  curr  7  lastExplored     lastExplored   1 8  if Last child   6 lastExplored then 9  result     false 10  end if 11  lastChild   Marked child  12  result     result     EvaluateOR child  13  curr     deweyList next   14  end while 15  curr     deweyList prev   16  return result   lastChild id  For example Last 0 1 2  returns 2  Finally  Marked returns true if the last node of the Dewey id is marked with a     and false otherwise  The algorithm is initialized by setting curr to the    rst element in the sorted deweyID list  It then calls EvaluateAND  with input dewey label of    Empty DeweyID     We think of this    rst call as moving to the root node of the Boolean expression tree  Now  within the EvaluateAND call  our base case  Steps 1 to 3  corresponds to being at a virtual leaf node  in which case we return true  The while loop  Step 5  iterates through all ancestors in the dewey list of the node currently being explored  It does this by    rst exploring the child under which the curr dewey ID lies  Thus  we recursively call EvaluateOR with its label corresponding to the child of the currently explored node  After this evaluation takes place  we AND its result with our result so far  Note that curr may have been updated during the EvaluateOR call  We continue to iterate through each of the children  In the call to EvaluateAND  we need every child to evaluate to true  We ensure that every child is explored my maintaining lastExplored  and checking that we never jump over a child  Steps 7 to 10   We also check that we encounter a starred dewey ID along the way  43 3 Example We walk through an example of the Evaluation algorithm  Let the set of Dewy IDs presented to the expression evaluator be  1    1 1  1    3 1  1    3 2  1    3 3    Is the BE satis   ed  The Dewey IDs represent nodes A  D  E and F in Figure 2  so it is easy to see that the expression is satis   ed  However  remember that the evaluation algorithm does not know what the tree for the BE was  it only sees the matching Dewey IDs  The Dewey evaluation algorithm    rst looks at id 1    1 1  and recursively calls EvaluateAND and EvaluateOR until it reaches the leaf  A   It then pops up a level to the AND at position 1    1 and increments curr  Since the next id  1    3 1 does not have 1    1 as a pre   x  the evaluation stops  and the AND is evaluated to false since the lastChild was never set to true  The algorithm then proceeds to evaluate the AND at position 1    3  It successfully evaluates all of the leaves  at which point the result is set to true and so is lastChild  the latter being set to true during the evaluation of 1    3 3     since the id ends in the special symbol      Therefore the OR at position 1    is set to true as well  Finally  the original call to EvaluateAND returns with true  3 4 Correctness Theorem 1  The Dewey Evaluation algorithm is correct  Proof  The proof of correctness follows quickly from the recursive nature of the algorithm  We sketch the proof that EvaluateAND and EvaluateOR both evaluate correctly  and further  the value of curr after the call is set to the last dewey ID for which the dewey label of the call is a pre   x  Clearly  the algorithm produces the correct result when the Boolean expression tree is a single node  By induction  assume that the algorithm works on a tree of depth d     1  and consider a tree of depth d  There are two cases  whether the top level node is an AND or an OR  Suppose it is an AND  the OR case is even simpler   In the call to EvaluateAND  we call EvaluateOR iteratively for each child of the explored node  By induction  each of these calls returns the correct result  The method returns false if one of these subroutines returned false  since all of the results are ANDed together   one of the children is skipped  the check of lastExplored   or if the    nal child was not seen  the check of lastChild   Otherwise we can conclude that all of the children of the AND returned true  and thus this node evaluates to true as well  Finally  we note the running time  Theorem 2  Let   be the set of leaves returned  and for a leaf n       denote by len n  the length of the Dewey ID of n  Then the Dewey Evaluation algorithm runs in time O  P n     len n    Proof  The running time follows from the fact that we evaluate each of the returned Dewey IDs one level at a time  so the time to process an id n is proportional to len n   We note that while the pseudocode presented is not optimized  one can  for example exit the EvaluateAND loop as soon as the result is set to false   this does not change the worst case running time of the algorithm  4  INTERVAL IDS We describe an alternative algorithm for evaluating BE trees  At a high level the algorithm works by mapping each leaf node of the BE tree onto a 1 dimensional interval on the real line  A contract is satis   ed if there is a subset of intervals that cover the real line without overlap  At    rst glance it sounds like we have made our problem more di     cult  however  the matching algorithm is simpler and more intuitive than the Dewey ID evaluation algorithm  Moreover  for each conjunction we need to store only two    xed length values  namely the beginning and the end of the corresponding interval  Hence  the amount of the stored information is constant  4 1 Intuition Consider an arbitrary BE tree  we will map the leaf nodes of the tree to intervals on the real line  We denote an interval  s  t  as hs  ti   In what follows s and t will always be integers   Fix M to be the maximum number of leaves in a BE tree  We will represent each contract by a line segment h1  Mi  Each leaf of the tree will be mapped to a subinterval of h1  Mi  The key to the algorithm lies in the mapping of conjunctions to intervals  We aim to    nd a mapping so that a contract is satis   ed if and only if there exists a subset of satis   ed leaves covering the entire segment h1  Mi without overlap  To develop the intuition  we    rst describe two simple cases  Consider a hypothetical Boolean expression  A     B shown in Figure 3  The contract is satis   ed if either of the two leaves is satis   ed  Therefore  the interval mapping scheme assigns the same interval h1  Mi to both A and B  Invariant 1  Consider a BE tree  and a node n corresponding to an OR  Then every child of n has the same interval as n  OR A B 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 A B Figure 3  The children of an OR node inherit the same interval as the parent  The situation is the opposite for an AND node  Consider a hypothetical Boolean expression  A     B shown in Figure 4  The contract is satis   ed only if both of the leaves are satis   ed  The interval mapping scheme splits the interval h1  Mi of the parent node into two non overlapping intervals  h1  xi and hx  Mi for the children  We describe the exact choice for x later  More generally  this leads to a second invariant  Invariant 2  Consider a BE tree  and a node n corresponding to an AND  Then the interval corresponding to n is partitioned among its children  5AND A B 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 A B Figure 4  The children of an AND node partition the interval  It is straightforward to apply these two invariants recursively  A more complicated example  corresponding to  A     B     C     D     E    F  is shown in Figure 5  Notice that there are three ways the tree can be satis   ed  Either both A and B are satis   ed  C is satis   ed  or all three of D  E and F are satis   ed  This example also demonstrates why we must    nd a set of non overlapping intervals that fully cover h1  10i  If B and D are returned  then h1  10i     h1  4i     h2  10i  that is  together B and D cover the whole interval  However  since B and D overlap  this is not an eligible assignment  1 C D A B OR AND AND F E A B 2 3 4 5 6 7 8 9 10 C 1 2 3 4 5 6 7 8 9 10 D 1 2 3 4 E 5  F 6 7 8 9 10 Figure 5  A more complex example of intervals  Another potential pitfall in assigning intervals is shown in Figure 6  Here because B and E share the same starting point  an assignment of B and D would evaluate to true  together B and D cover the whole interval h1  10i without overlap  Thus we have the third invariant   1 C D A B OR AND AND F E A B 2 3 4 5 6 7 8 9 10 C 1 2 3 4 5 6 7 8 9 10 D E 2 3 4 5 F 6 7 8 9 10 1 Figure 6  An invalid labeling  If B and D are true  the full interval is covered without overlap  Invariant 3  Given a boolean expression tree  and let C be the set of children of AND nodes  Let F be the set of    rst  left most  children of AND nodes  Then no two nodes in C   F have the same starting point for their intervals  The last invariant precisely precludes the case in Figure 6  Since both B and E are second children of an AND node  their corresponding intervals must start at di   erent positions  We will refer to the assignment of intervals to the leaves of a tree as the labeling of the tree  Definition 1  We call a labeling valid if Invariants 1  2 and 3 are maintained  We describe an algorithm for generating a valid labeling in the next section  and in Section 4 3 show how we can quickly evaluate whether the full BE is true  based only on the intervals corresponding to the satis   ed conjunctions  4 2 Conjunction Annotation In this section we describe an algorithm for providing a valid labeling of the nodes of the tree  Let the size of a node be the total number of children in its subtree  with size of a leaf node set to 1   Further  for each node n  let n leftLeaves denote the total number of leaves appearing before n in a pre order traversal of the tree  The algorithm is recursive  it starts by labeling the root node with interval h1  Mi where M is the maximum number of leaves supported  and then calling the subroutine presented in Algorithm 4  Algorithm 4 The Label algorithm Require  Node n  1  if n is a leaf then 2  return 3  else if n is an OR node then 4  for all children c of n do 5  c begin     n begin 6  c end     n end 7  Label c  8  end for 9  else if n is an AND node then 10  for    rst child c do 11  c begin     n begin 12  c end     n leftLeaves   c size 13  Label c  14  curr     c end 1 15  end for 16  for all intermediate children c of n do 17  c begin     curr  18  c end     curr   c size  1  19  Label c  20  curr     c end 1 21  end for 22  for last child c do 23  c begin     curr  24  c end     n end  25  Label c  26  end for 27  end if For example  consider the Tree in Figure 5 with M   10  When labeling node A  we have n leftLeaves   0  since there are no prior leaf nodes in an in order traversal of the tree  Therefore the interval for A is h1  1i  B is relegated the rest 6Interval matched 0 1 2 3 4 5 6 7 8 9 10 Initial 1 0 0 0 0 0 0 0 0 0 0 A   h1  1i 1 1 0 0 0 0 0 0 0 0 0 D   h1  4i 1 1 0 0 1 0 0 0 0 0 0 E   h5  5i 1 1 0 0 1 1 0 0 0 0 0 F   h6  10i 1 1 0 0 1 1 0 0 0 0 1 Table 1  The matched array during the evaluation of the algorithm on A  D  E  F  For instance  row D shows the array matched after adding the interval for D  of the interval  h2  10i  The interval for C is easy  since it is a child of an OR node  it must be h1  10i  Now consider the label for D  For D   s parent node  n leftLeaves is set to three  therefore the endpoint of the interval for D is 1 3   4  The intervals for E and F follow  Note that the labeling of the tree can be constructed in a single in order traversal of the BE tree  4 3 Expression Evaluation The input to the evaluation algorithm is a set of intervals  one for each matching conjunction  The algorithm attempts to    nd a non overlapping set of intervals that cover the range h1  Mi  To do so  the algorithm will maintain a Boolean array matched  where matched i  is true if there is a nonoverlapping set of intervals that ends in i  We give the full matching algorithm below  Algorithm 5 Match Algorithm  Require  I  set of intervals hbegin  endi sorted by begin 1  matched     Boolean Array of length M   1 2  Initialize matched i  to false for all i 3  matched 0    true 4  for all intervals hbegin  endi in I do 5  if matched begin 1  then 6  matched end      true 7  end if 8  end for 9  if matched M  then 10  return true 11  else 12  return false 13  end if Consider again the example in Figure 5  and suppose that A  D  E  F were returned as matching conjunctions  The algorithm maintains the state of the matched array  with the individual steps presented in Table 1  Note that processing each interval requires only two probes into the boolean array  4 4 Correctness In this section we prove that both the labeling algorithm and the label evaluation algorithms are correct  Theorem 3  The Label Algorithm produces a valid labeling  Proof  It is easy to see that Invariants 1 and 2 are trivially satis   ed  that is every child of an OR node has the same interval as the parent  and the children of an AND node partition the interval among themselves  It remains to show invariant 3  Let C be the set of children of AND nodes  and F be the set of    rst  or leftmost children  We show that the interval corresponding to every node n     C  F starts at n leftLeaves   1  Recall that n leftLeaves is the number of leaves occurring before n in a pre order traversal of the tree  If this holds than no two nodes in C   F can have the same starting points for intervals  We prove the claim by induction on the depth of the node  For the base case  consider the root node n  and let cP1  c2          ck be its k children  Observe that ci leftLeaves   i   1 j 1 cj  size  and the base case follows  Suppose that the claim holds for nodes at depth d  Let n be a node at depth d  1 and c1  c2          ck be its k children  Since ci leftLeaves   n leftLeaves   Pi   1 j 1 cj  size  the theorem follows  We now show that the matching algorithm is correct  We begin with a theorem about valid labelings  Theorem 4  Consider a BE tree with a valid labeling  Let I be the set of intervals corresponding to leaf nodes in N that evaluate to true  Then the BE is satis   ed i    there exists a subset I 0     I such that    i   I 0 hbegini  endii   h1  Mi and any two intervals i  i 0     I 0 are disjoint  hbegini  endii     hbegini 0   endi 0 i        To prove the theorem  we    rst show that given a satisfying assignment to the BE we can    nd a set of intervals I 0 as described above  We then prove the converse  that is  given a set of intervals satisfying the condition above  we show that the BE must be satis   ed  Proof  To prove the forward direction  consider the minimal set of leaves of the tree that lead to a satis   ed assignment  a set is minimal if removing any element would lead to the BE evaluating as false   Then the set of corresponding intervals covers the whole segment h1  Mi and is non overlapping  The formal proof is by induction on the height of the tree  since an AND partitions the interval  to be satis   ed all of its children must be satis   ed  therefore its interval would be fully covered by its children  On the other hand  since all of the children of an OR inherit the same interval  only one of the children needs to be satis   ed  otherwise the initial set of trees is not minimal   Therefore the intervals corresponding to the minimal set of leaves satisfy the conditions of the theorem  To prove the reverse direction  the main obstacle is to show that an interval corresponding to an AND node can only be fully covered without overlap by the nodes corresponding to its children  This is guaranteed by Invariant 3  Since the starting points of the intervals of all intermediate AND children are disjoint  and AND node can only be satis   ed by its children because no subset of other children would result in a continuous and non overlapping interval  The formal proof again proceeds by induction on the height of the tree rotted at the AND node  Finally  we can prove the correctness of the Match algorithm  Let   be the number of leaves of the tree returned by the indexing system  Theorem 5  The Match algorithm is correct and runs in time O     Proof  To prove correctness  we show that the Matching algorithm    nds a non overlapping subset of intervals covering the whole interval if one exists  The invariant maintained by the algorithm is that matched i  is set to true only if there exists a non overlapping set of intervals that cover 7the interval h1  ii  When processing an individual interval i   hbegin  endi  we check to see if begin continues a previous interval  in which case matched end  is set to true  Since the intervals are sorted by the begin position when processing  all intervals that end before begin will have been processed before  since they must begin even earlier   To show the running time  observe that each evaluation of an interval results in at most two lookups into a boolean array  4 5 Discussion The Interval evaluation algorithm has a number of appealing properties that improve upon the Dewey ID algorithm      It can handle trees of very large depth with a    xed size Interval ID  In contrast  the Dewey ID grows linearly with the depth of the tree  If the total number of leaves is n  the space taken by storing the interval is O log n   whereas the space required for some Dewey ids may be as large is     n       Although we described the algorithm in the context of AND OR trees  it can naturally handle arbitrary BE trees without any change to the code      Finally  the Interval algorithm is faster  requiring only two memory look up calls for each interval  instead of being linear in the size of the Dewey IDs  5  EXPERIMENTS In this section  we evaluate our Dewey and Interval matching algorithms for BE evaluation on synthetic datasets from an online advertising application  We compare their performance to other e   cient algorithms for BE evaluation and study how the algorithms behave for di   erent scenarios  such as BE tree depth and selectivity  All algorithms were implemented in C    and our experiments were run on a 2 5GHz Intel R  Xeon R  processor with 16GB of RAM  5 1 Data set In order to test the e   ciency of the Dewey evaluation  Interval evaluation and other algorithms  we used a synthetic dataset generated from statistics of real advertising contracts  To gather the statistics we looked at individual conjunctions appearing in each contract  For example  a contract looking for Males from California would specify Gender      Male      State      CA   We    rst collected statistics over the size of these conjunctions  We denote by ci the probability of seeing a conjunction on i elements  We then recorded how often each attribute i  e g  gender  state  etc  is present in the contracts  which we denote by ai  For example  if half the contracts targeted on gender then agender   1 2  For each attribute i  we collected statistics on the targeting values for this attribute  We denote the distribution by pattribute target   For example  if  from the set of contracts targeting on gender  2 3 targeted males  and 1 3 targeted females  then we say pgender male    2 3 and pgender female    1 3  These statistics served as input to the BE generator  We    rst generated the logical BE tree  namely an alternating AND OR tree  Without loss of generality the root node was selected to be an AND  The tree was then generated recursively  For each node  we    rst decide how many children the node will have  If it has 0  then we stop and mark the node as a conjunction  otherwise we generate the children  mark them as OR nodes  or in the case of processing an OR node  we mark the children as AND nodes   and recurse  The input to the tree generator speci   ed the minimum and maximum depths of the desired tree  and the probability distribution on the number of children  If the node being generated is below the minimum depth  then the number of children is set to be non zero  If the node is at the maximum depth  then the number of children is set to 0  Otherwise  we select from the distribution  In our experiments  the number of children was 2 with probability 0 7  3 with probability 0 2  and 1 or 4 with probability 0 05  This resulted in a wide distribution on the trees  Given the tree structure  we then generated a conjunction for each leaf node of the tree  To generate a</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#saqpp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#advanced_query_processing"/>
        <doc>Answering Why not Questions on Top k Queries ### Zhian He Eric Lo Department of Computing  Hong Kong Polytechnic University fcszahe  ericlog comp polyu edu hk Abstract   After decades of effort working on database performance  the quality and the usability of database systems have received more attention in recent years  In particular  the feature of explaining missing tuples in a query result  or the so called    why not    questions  has recently become an active topic  In this paper  we study the problem of answering why not questions on top k queries  Our motivation is that we know many users love to use top k queries when they are making multi criteria decisions  However  they often feel frustrated when they are asked to quantify their feeling as a set of numeric weightings  and feel even more frustrated after they see the query results do not include their expected answers  In this paper  we use the queryre   nement method to approach the problem  Given as inputs the original top k query and a set of missing tuples  our algorithm returns to the user a re   ned top k query that includes the missing tuples  A case study and experimental results show that our approach returns high quality explanations to users ef   ciently  I ###  INTRODUCTION Although the performance of database systems has gained dramatic improvement in the past decades  they also become more and more dif   cult to use than ever  1   In recent years  there is a growing effort to improve the usability of database systems  For example  to help end users query the database more easily  the features of keyword search  2  or form based search  3  could be added to a database system to assist users to    nd their desired results  As another example  the features of query recommendation  4  or query autocompletion  5  could be added to a database system in order to help users to formulate SQL queries  Among all the studies that focus on improving databases    usability  the feature of explaining missing tuples in database queries  or the so called    whynot     6  questions  has recently received growing attentions  In practice  users often feel frustrated when they    nd their expected tuples are not in the query results without any explanation  and intuitively  they will ask such a question  why are my expected tuples not in the results  If a database system can give a good explanation for it  it would be very useful for users to understand and re   ne the query  There are different ways to answer    why not    questions  For example   6  answers a user   s why not question on SelectProject Join  SPJ  queries by telling her which query operator s  eliminated her desired tuples  In  7  and  8   the missing answers of SPJ  7  and SPJUA  SPJ   Union   Aggregation  queries  8  are explained by telling the user how the data should be modi   ed  e g   adding a tuple  if she wants the missing answer back to the result  Another work that goes one step further is  9   which tells the user how to revise her original SPJA queries so that the missing answers can return to the result  These works  however  have been concentrated on only traditional SPJUA queries  and none of these can answer why not questions on preference queries such as topk queries yet  In fact  answering why not questions on top k queries is very useful because while users love to use top k queries when making multi criteria decisions  they often feel frustrated when they are forced quantify their preferences as a set of numeric weightings  Moreover  they would feel even more frustrated when their expected answers are missing in the query result  For instance  a customer Mary is going to Paris for a holiday  and this is her    rst time to be there  so she needs to choose a hotel carefully from a list of hotels in Paris  Yet  the list is so long to read  therefore  Mary decides to look at the top 3 hotels based on a set of weightings she sets on the attributes    Price        Distance to Downtown     and    Ratingby other customers     To her surprise  hotel Hilton  which is very famous around the world  and Mary   s favorite  is not in the result  Now  Mary may feel frustrated     Why is my favorite hotel not in the top 3 result  Is that because there is no Hilton Hotel in Paris  Should I revise my weightings  Or my Hilton Hotel can be back to result if I simply revise my query to look for top 5 hotels instead of top 3 hotels     In this paper  we present methods to answer why not questions on top k queries  We follow the latest query re   nement  9  approach that suggests users how to re   ne their top k query in order to get their expected answers back  Note that the result of a top k query depends on two sets of parameters   1  the number of objects to be included in the result  i e   the k value  and  2  the set of weightings  w speci   ed by the user  In some cases  the missing object can easily be back in the result if the k value is slightly increased  e g   re   ning a top 3 query to be a top 5 query   However  there can be cases where the missing object will not be included in the result unless the k value is dramatically increased  e g   re   ning a top 3 query to be a top 10000 query   and for those cases  modifying the set of weightings  w instead of k may make more sense from the users perspective  Furthermore  there could be cases in which the best option is to slightly modify both the k value and the set of weightings  w together  instead of signi   cantly modify one of them alone  To address this problem  we    rst de   ne a penalty model to capture a user   s tolerance to the changes of weightings  w and k on her original query  Next  we show that    nding the re   ned top k query with the least penalty is actually computational expensive  Afterwards  we present an ef   cient algorithm that uses sampling to obtain the best approximate answer in reasonable time  We evaluate the quality and the performance of our method using real data and synthetic data As an interesting preview  when we attempted to    nd the top 3 players in the NBA history  we found that Michael Jordan was amazingly missing in the result  So  we immediately posed a why not question    why is Michael Jordon not in top 3      In 0 15 seconds  our method returned a suggestion that we could re   ne the query to be a top 5 query without changing our weightings  The result told us that our initial set of weightings was actually reasonable  but we should have asked for more players in the top k query initially  The rest of the paper is organized as follows  Up next in Section II  we formulate the problem and illustrate that it is computationally expensive to obtain the exact answer  In Section III  we present our method to solve the problem ef   ciently with provable quality guarantee  In Section IV  we report experimental results  In Section V  we discuss related work and we conclude the paper in Section VI  II  PRELIMINARY In this section  we give a formal de   nition to the problem of answering why not questions on a top k query  Afterwards  we discuss the dif   culty of solving the problem exactly  A  Problem Statement Given a database of n objects  each object  p with d attribute values can be represented as a point  p   jp 1  p 2        p d j in a d dimensional data space  For simplicity  we assume that all attribute values are numeric and a smaller value means a better score  A top k query is composed of a scoring function  a result set size k  and a weighting vector  w   jw 1  w 2        w d j  In this paper  we accept the scoring function score as a linear function  where score  p   w     p    w  k as any positive integer  Pand we assume the weighting space subject to the constraints w i    1 and 0   w i    1  The query result would then be a set of k objects whose scores are the smallest  in case objects with the same scores are tie at rank k th  only one of them is returned   Initially  a user speci   es a top k query qo ko   wo   After she gets the result  she may pose a why not question on qo with a set of missing objects M   f m1           mjg  and hope the system to return her a re   ned top k query q 0  k 0    w 0   such that all objects in M appear in the result of q 0 under the same scoring function   A special case is that a missing object  mi is indeed not in the database  we describe more on this later   We use  k and  w to measure the quality of the re   ned query  where  k   max 0  k 0  ko  and  w   jj  w 0   wojj2  We de   ne  k this way is to deal with the possibilities that a re   ned query may obtain a k 0 value smaller than the original ko value  For instance  assume a user has issued a top 10 query and the system returns a re   ned top 3 query with a different set of weightings  We regard  k as 0 in this case because the user essentially does not need to change her original k  In order to capture a user   s tolerance to the changes of k and  w on her original query qo  a basic penalty model that sets the penalties  k and  w to  k and  w  respectively  where  k    w   1  is as follows  P enalty k 0    w 0      k k    w w  1  Note that the basic penalty model favors changing weightings more than changing k because  k could be a large integer whereas  w is generally small  One possible way to mitigate this discrimination is to normalize them respectively  To do so  we normalize  k using  k m o  ko   where k m o is the rank of the missing object  m under the original weighting vector  wo  To explain this  we have to consider the answer space of why not queries  which consists of two dimensions   k and  w  Obviously  a re   ned query q 0 1 is better than  or dominates  10   another re   ned query q 0 2   if both its re   nements on k  i e    k  and w  i e    w  are smaller than that of q 0 2   For a re   ned query q 0 with  w   0  its corresponding  k must be k m o  ko  Any other possible re   ned queries with  w   0 and  k    k m o  ko  must be dominated by q 0 in the answer space  In other words  a re   ned query with  w   0 must have its  k values smaller than k m o  ko or else it is dominated by q 0 and could not be the best re   ned query  Therefore  k m o  ko is the largest possible value for  k and we use that value to normalize  k  Similarly  let the original weighting qvector  wo   jwo 1  wo 2        wo d j  we normalize  w using 1   P wo i  2   because  Lemma 1  In our concerned weighting space  given  wo and an arbitrary weighting vector  w   jw 1  w 2        w d j   w   q 1   P wo i  2   pPProof  First  we have  w   k  w   wok2    w i   wo i   2   Since w i  and wo i  are both nonnegative  then we can have pP  w i   wo i   pP 2    w i  2   wo i  2     pP w i  2   P wo i  2   It is easy to know that P w i  2     P w i   2   As P w i    1  we know that P w i  2   1  Therefore  we have  w   pP w i  2   P wo i  2   p 1   P wo i  2   Now  we have a normalized penalty function as follows  P enalty k 0    w 0      k  k  km o  ko     w  w p 1   P wo i  2  2  The problem de   nition is as follows  Given a why not question fM  qog  where M is a non empty set of missing objects and qo is the user   s initial query  our goal is to    nd a re   ned top k query q 0  k 0    w 0   that includes M in the result and with the smallest penalty  In this paper  we use Equation 2 as the penalty function  Nevertheless  our solution indeed works for all kinds of monotonic  with respect to both  k and  w  penalty functions  For better usability  we do not explicitly ask users to specify the values for  k and  w  Instead  users are prompted to answer a simple multiple choice question 1 illustrated in Figure 1  Let us give an example  Figure 2 shows a 2 D data set with    ve data objects  p1   p2   p3   p4  and  m  Assume a user 1 The number of choices and the pre de   ned values for  k and  w  of course  could be adjusted  Furthermore  one might prefer to minimize the difference between the new result set and the original one  instead of minimizing the difference between the re   ned query and the original query  In that case  we suggest the user to choose the option where  w is a big value  i e   not prefer modify the weightings  because  11  has pointed out that similar weightings generally lead to similar top k results              Choice           Question Prefer modifying k or your weightings  Prefer modify k  k   0 1   w   0 9 Prefer modify weightings  k   0 9   w   0 1 Never mind  Default   k   0 5   w   0 5 Fig  1  A multiple choice question for freeing users to specify  k and  w d 2  0 d 1  p   3    2 5    p   4    6 6    p   1    2 2    p   2    5 2    m       4 4    ranking ID score 1  p1 2 2  p2 3 5 2  p3 3 5 Top 3 query qo ko   3   wo   j0 5 0 5j  Fig  2  A 2 D example has issued a top 3 query qo ko   3   wo   j0 5 0 5j  and wonders why the point  m is missing in the query result  So  she would like to    nd the reason by declaring a why not question ff mg  qog  using the default penalty preference    Never mind      k    w   0 5   In the example   m ranks 4 th under  wo  so we get k m o   4  k m o  ko   4  3   1  and p 1   P w2 oi   1 2  Table I lists some example re   ned queries that include  m in their results  Among those  re   ned query q 0 1 dominates q 0 2 because both its  k and  w are smaller than that of q 0 2   The best re   ned query in the example is q 0 3  penalty 0 12   At this point  readers may notice that the best re   ned query is located on the skyline of the answer space  Later  we will show how to exploit properties like this to abtain better ef   ciency in our algorithm  TABLE I EXAMPLE OF CANDIDATE REFINED QUERIES Re   ned Query  ki  wi Penalty q 0 1  4  j0 5 0 5j  1 0 0 5 q 0 2  5  j0 6 0 4j  2 0 14 1 06 q 0 3  3  j0 7 0 3j  0 0 28 0 12 q 0 4  3  j0 2 0 8j  0 0 42 0 175 q 0 5  3  j0 21 0 79j  0 0 41 0 17 q 0 6  3  j0 22 0 78j  0 0 4 0 167 B  Problem Analysis First  let us consider there is only one missing object  m in the why not question  In the data space  we say an object  a dominates object   b  if a i    b i  for i    1          d  and there exists at least one a i    b i   In a data set  if there are kd objects dominating  m and n objects incomparable with  m  then the ranking of  m could be  kd   1           kd   n   1   For these n 1 possible rankings r1  r2          rn 1 of  m  each ranking ri has a corresponding set Wri of weighting vectors  where each weighting vector  wri 2 Wri makes  m ranks rith  As such  any re   ned queries q 0  ri    wri   are also candidate answers  because when k   ri   the missing tuple  m is in the result  with rank ri   Recall that our objective is to    nd a combination of k and weighting  wri that minimizes Equation 2  However  the weighting vector set Wri is actually either empty or a set of convex polytopes  Lemma 2   That means when Wri is not empty  then there are an in   nite number of points  wri 2 Wri   which also makes the number of candidate answers in   nite  Lemma 2  Wri is either empty or a set of convex polytopes  Proof  Given a data space and a missing object  m  assume there are kd objects dominate  m  and the number of incomparable objects with  m is n  To    nd the set of weighting vectors Wri   where ri   kd   j and j 2  1  n   1   we have to solve a set of linear inequality systems  We use I to stand for the set of incomparable objects with respect to  m  Now  we arbitrarily put j  1 objects from I into a new set E  and put the rest into another set F  Let any object  e 2 E satisfy the following inequality   e    wri    m    wri  3  which means E is the set of objects that have scores better than  m  Similarly  any object   f 2 F is an object that has score not better than  m    f    wri    m    wri  4  Now we have a set of linear inequalities for all objects in E and F  Together with the constraints P wri  i    1 and wri  i  2  0  1   we can get a linear inequality system  The solution of this inequality system is a set of weighting vectors that make  m rank  kd   j  th  In fact  there are C n j1 such linear inequality systems and Wri is essentially the union of the solutions of them  The boundary theory of linear programming  12  shows that a linear inequality system is a polyhedron  which is the intersection of a    nite set of half spaces  A polyhedron is either empty  unbounded  or a convex polytope  In our case  the polyhedrons are either empty or convex polytopes  because they are additionally bounded by the constraints P w i    1 and w i  2  0  1   Therefore  the weighting vectors Wri is the union of a set of convex polytopes or an empty set if there are no solutions for all the inequality systems  As Wri is a set of convex polytopes with in   nite points if it is not empty  the number of candidate answers in the answer space is also in   nite  Therefore  we conclude that searching the optimal re   ned query for one missing object in an in   nite answer space is unrealistic  2 Moreover  the problem would not become easier when M has multiple missing objects  III  ANSWERING WHY NOT In this section  we present our method to answer why not questions on top k queries  According to the problem analysis 2One exact solution that uses a quadratic programming  QP  solver  13  is as follows  For each ranking value ri   kd   j  j 2  1  n   1   we can compute  k   max ri  ko  0   In order to make Equation 2 as small as possible under this ri  we have to    nd a  wri 2 Wri such that jj  wri   wojj2 is minimized  Since general QP solver requires the solution space be convex  we have to    rst divide Wri into Cn j1 convex polytopes  Each convex polytope corresponds to a quadratic programming problem  After solving all these quadratic programming problems  the best  wri could then be identi   ed  For all ranking ri to be considered  there are Pn 1 j 1 Cn j1   2 n  n is the number of incomparable objects with  m  quadratic programming problems at the worst case  so this exact solution is impractical         presented above     nding the best re   ned query is computationally dif   cult  Therefore  we trade the quality of the answer with the running time  Speci   cally  instead of considering the whole in   nite answer space  we propose a special samplingbased algorithm that    nds the best approximate answer  A  Basic Idea Let us start the discussion with an assumption that there is only one missing object  m  First  suppose we have a list of weighting vectors S      wo   w1   w2           ws   where  wo is the weighting vector in the user   s original query qo  For each weighting vector  wi 2 S  we formulate a progressive top k query q 0 i using  wi as the weighting  Each query q 0 i is executed by a progressive top k algorithm  e g    14    15    16    which progressively reports each top ranking object one by one  until the missing object  m comes forth to the result set with a ranking ri   If  m does not appear in the result of the    rst executed query  we report to the user that  m does not exist in the database and the process terminates  Assuming  m exists in the database  then after s 1 progressive top k executions  we have s   1 re   ned queries q 0 i  ri    wi   where i   o  1  2          s  with missing object  m known to be rank ri th exactly  Finally  the re   ned query q 0 i  ri    wi  with the least penalty is returned to the user as the answer  In the following  we discuss where to get the list S of weighting vectors  Section III B   Then  we discuss how large the list S should be  Section III C   Afterwards  we present the algorithm  Section III D   Finally  we present how to deal with multiple missing objects  Section III E   B  Where to get weighting vectors  In the basic idea of the algorithm  the size of S plays a crucial role in the algorithm ef   ciency and the solution quality  Having one more sample weighting in S  on the one hand  may increase the chance of having a better quality solution  on the other hand  that would de   nitely increase the number of progressive top k operations by one and thus increase the running time  So  one of our objectives is to keep S as small as possible and at the same time put only high quality weightings  e g   only those that may yield the optimal solution  into S  Recall that if there are kd objects dominate the missing object  m and there are n objects incomparable with  m  the best and the worst ranking of  m are kd   1 and n   kd   1  respectively  For these n 1 possible rankings r1  r2          rn 1 of  m  each ranking ri is associated with a weighting vector set Wri such that for each weighting vector  wri 2 Wri    m ranks ri th in the corresponding re   ned query q 0  ri    wri    So altogether there is a set W that contains n 1 weighting vector sets  W   fWr1           Wri           Wrn 1 g  In the following  we are going to show that if the re   ned query q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt that minimizes Equation 2 in terms of  k and  w has a weighting vector  wopt on the boundaries of the weighting vector sets W  Theorem 1   Furthermore  re   ned queries with weightings on the boundaries of the weighting vector sets W would make missing object  m have a ranking no worse than other re   ned queries whose weightings not on the boundaries  Lemma 3   Therefore  in addition to the original weighting vector  wo  the rest of the weighting vectors  w1   w2           ws in S should be sampled from the space formed by the boundaries of those n   1 weighting vector sets in W  Theorem 1  If q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt   which minimizes Equation 2  has a weighting  wopt on the boundaries of the n 1 weighting vector sets in W  Proof  According to Lemma 2  if a weighting vector set Wri 2 W is not empty  then it is the union of a set of convex polytopes CPri   So  the boundaries of Wri are essentially the union of the boundaries of each convex polytope in CPri   Let CP   S CPri   Theorem 1 can be re stated as follow  if q 0 o  k m o    wo  is not the optimal answer  then the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of CP  Let CPn  wo 2 CP be the set of convex polytopes that do not contain  wo  Further  let CP  wo   CP  CPn  wo be the set of convex polytopes that contain  wo  Since all the convex polytopes are disjoint  as no weighting can satisfy two different linear inequality systems described in Lemma 2 at the same time    wo is in only one convex polytope  As such  CP  wo essentially contains only one convex polytope  Now  to prove the theorem  we need to prove the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of   1   CPn  wo or   2   CP  wo   We now start with proving   1    To do so  we    rst assume the optimal re   ned query q 0 opt has a weighting  wopt NOT in CP  wo or on its boundaries  and show that  Lemma 3  For all convex polytopes cp 2 CP  any re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp  has  m ranks r b  th under  wr b   which is not worse than rank r 0  th  i e   r b   r 0    where r 0 is the ranking of  m under another re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 not on the boundaries of cp  Lemma 3   Lemma 4  For all convex polytopes cpn  wo 2 CPn  wo   there exists a re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cpn  wo   has its  w   jj  wr 0   wojj2  where  wr 0 is the weighting vector of any other re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 not on the boundaries of cpn  wo  Lemma 4   If both Lemma 3 and Lemma 4 hold  then   1   is true  Proof of Lemma 3  We prove it by induction  First  recall that kd is the number of objects that dominate the missing object  m  In the base case  we want to show that  when there is only one object  p1 incomparable with  m in the data space  the Lemma is true  When there is only one incomparable point  p1  the whole weighting space is divided by the hyperplane H1   p1   m    w   0 into two convex polytopes cp   and cp     cp   is the convex polytope at the side   p1   m     w   0  cp   is the convex polytope at the side   p1   m     w   0  and hyperplane H1 is the boundary of cp   and cp     We use CP1 to denote the whole set of convex polytopes at this moment  Now  consider a re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp      m   s ranking r 0   kd   2 because  m is dominated by  p1 under  wr 0   Consider another     re   ned query q 0 b  r b    wr b    whose  wr b on the boundary H1   m   s ranking r b   kd   1 because  m and  p1 have the same score  Finally  if the re   ned query q 0  r 0    wr 0   has its weighting vector  wr 0 in cp   but not on the boundary of cp      m dominates  p1 under  wr 0 and thus  m   s ranking remains as r 0   kd   1  In the above  we can see that r b   r 0   thus the base case holds  Assume Lemma 3 is still true when there are i objects incomparable with  m in the data space and we use CPi to denote the set of convex polytopes constructed by the corresponding i hyperplanes  Now  we want to show that the lemma is true when there are i   1 incomparable objects in the data space  When the  i 1  th incomparable object  pi 1 is added  CPi is divided into three sets of convex polytopes  CP     CP     and CP G   CP   consists of any convex polytope cp   that is completely at the side of   pi 1   m     w   0  CP   consists of any convex polytope cp   that is completely at the side of   pi 1   m    w   0  and CP G consists of any convex polytope cp G that intersects the hyperplane Hi 1    pi 1   m     w   0  We want to show that the lemma is true for all the three sets of convex polytopes above  For any cp   2 CP     the re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp     the addition of  pi 1 makes  m   s ranking r 0 increments by one  i e   r 0   r 0 1  because  pi 1 has a better score when the weighting vectors are at the side of   pi 1   m     w   0  For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp     the addition of  pi 1 makes  m   s ranking r b increments by one  i e   r b   r b   1  In this case  r b   r 0   the lemma still holds after  pi 1 is added  For any cp   2 CP     the re   ned query q 0  r 0    wr 0    whose  wr 0 in cp   but not on the boundary of cp     the addition of  pi 1 does not change  m   s ranking r 0   because  pi 1 has score worse than  m when the weighting vectors at the side of   pi 1  m    w   0  For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the boundaries of cp     the addition of  pi 1 also does not change  m   s ranking r b   So  in this case  r b   r 0   the lemma still holds after the pi 1 is added  For any cp G 2 CP G   since it intersects hyperplane Hi 1  cp G is divided into two new convex polytopes cp G   and cp G    with Hi 1 as their boundaries  The proof for the case of cp G   is similar to the case of any cp   2 CP     with both r b and r 0 get increased by one  so r b   r 0 is still true  For the case of cp G    we also have r b   r 0 like the case of any cp   2 CP     For the re   ned query q 0 b  r b    wr b    whose weighting vector  wr b on the hyperplane Hi 1  as the added object  pi 1 has the same score as  m at this moment   m   s ranking remains the same  And for those q 0  r 0    wr 0    whose wr 0 not on Hi 1  may either keep  m   s ranking or increase its ranking by one just like the discussion above  Therefore  we can assert that r b   r 0   is true in all cases  Proof of Lemma 4  Let  wr  be the weighting vector that is closest to  wo  i e   the one with the optimal  w   we prove the lemma by showing that  among all the weighting vectors in cpn  wo    wr  must be on the boundaries of cpn  wo   Assume  wr   is in cpn  wo but not on its boundaries  So   wr   is an interior point of cpn  wo and there exists an open ball B   cpn  wo centered at  wr    13   Since B is convex  we can    nd two points  wa   wb in B such that  wr      wa  wb 2   As  wr   is closest to  wo  we know that    wa  wr        wo  wr       0  see Lemma 5   Because  wa   2  wr     wb  we can substitute  wa into the inequality above and get    wr     wb     wo   wr       0  Now we have    wb   wr        wo   wr       0  which contradicts Lemma 5  below   Therefore   wr   must be on the boundaries of cpn  wo   Lemma 5  Let C   R n be a non empty closed convex set  Then  for every point  x 62 C   z   2 C is a closest point to  x iif   z  z         x  z       0 for all  z 2 C   13  Now  let us prove   2    i e   the optimal re   ned query q 0 opt has a weighting  wopt on the boundaries of CP wo   To do so  we assume the optimal re   ned query q 0 opt has a weighting  wopt NOT in CPn  wo or its boundaries  We    rst show that  for any re   ned query q 0  r 0    wr 0    whose weighting vector  wr 0 in CP  wo but not on its boundaries  is dominated by q 0 o  k m o    wo   Given that  wr 0 is in CP  wo but not on its boundaries  and also given that  wo is in CP  wo we consider two cases  in which  wo is  i  not on and  ii  on CP  wo    s boundaries  respectively  In case  i   since both  wr 0 and  wo are in CP  wo but not on its boundaries  they satisfy the same inequality system and thus we have r 0   k m o   In case  ii   in which  wo is on CP  wo    s boundaries  we can apply Lemma 3 and thus we have k m o   r 0   Combining two cases together we have k m o   r 0 always holds  Note that when  wr 0  6  wo   w 0 r    wo  Hence  among all re   ned queries q 0  r 0    wr 0    whose  wr 0 in CP  wo   only those with  wr 0 on the boundaries of CP  wo have chances not to be dominated by q 0 o  k m o    wo   Finally  as q 0 o  k m o    wo  is not the optimal answer  the given condition in Theorem 1   so we know that if the optimal weighting  wopt is not on the boundaries of CPn  wo  from   1     then it is on the boundaries of CP  wo  from   2     Thus Theorem 1 is proven  Consider the dataset in Figure 2 as an example  Point  m  who is missing in the top 3 results of the original query qo  has two incomparable points  p2 and  p3  To    nd out the set of weighting vectors Wri that makes m ranks third  i e   ri   3  we can solve the following inequality systems separately     p2    wri    m    wri  p3    wri    m    wri  5     p2    wri    m    wri  p3    wri    m    wri  6  The union of the results above is Wri 3  In the systems above  we have two hyperplanes H1    p3   m     wri   0 and H2     p2   m     wri   0 that divide the weighting space like Figure 3  The two hyperplanes intersect the weighting constraint planes P wri  i    1 and wri  i  2  0  1  and results in three convex polytopes cp1  cp2  and cp3  in 2 d case the polytopes are line segments   The union of cp1 and cp3 is the corresponding weighting vector set Wri 3  and cp2 is the weighting vector set Wri 4  In this 2 D example  the                intersections  w1   j1 3 2 3j and  w2   j2 3 1 3j between the two hyperplanes and the weighting constraint planes are the boundaries of the polytopes  Note that by some back ofenvelop calculation  we can derive that m ranks third under weightings  w1 and  w2  This aligns with Lemma 3  which states that the ranking of m on the boundaries  rank third  is not worse than its ranking not on the boundaries  rank fourth in cp2 and third in cp1 and cp3   w 2  w 1  1 0 1   2 3 1 3  cp1 H1 H2 cp3 cp2 w   1  1 3 2 3  w   o w   2 r 3 r 4 r 3 Fig  3  Convex polytopes for m shown under the weighting space C  How large the list of weighting vectors should be  Having known that the list of weighting vectors S should be obtained from the boundaries of the weighting sets in W  the next question is  given that there are still an in   nite number of points  weightings  on the boundaries of the weighting sets in W  how many sample weightings from the boundaries should we put into S in order to obtain a good approximation answer  Recall that more sample weightings in S will increase the number of progressive top k executions and thus the running time  Therefore  we hope S to be as small as possible while maintaining good approximation  We say a re   ned query is the best T  re   ned query if its penalty is smaller than  1  T   re   ned queries in the whole  in   nite  answer space  and we hope the probability of getting at least one such re   ned query is larger than a certain threshold P r  1   1  T   s   P r  7  Equation 7 is general  In our algorithm  we use it based on a smaller sample space that contains high quality weightings  The sample size s is independent of the data size but controlled by two parameters  T  and P r  Following our usual practice of not improving usability  i e   why not queries  by increasing users    burden  e g   specifying parameter values for  k  T   and P r   we make T   and P r as system parameters and let users to override their values only when necessary  D  Algorithm To begin  let us    rst outline the three core phases of the algorithm  which is slightly different from the basic idea mentioned in Section III A for better ef   ciency   PHASE 1  It    rst samples s weightings  w1   w2           ws from the boundaries of the weighting vector sets W and add them into S  which initially contains  wo   PHASE 2  Next  for some weighting vectors  wi 2 S  it executes a progressive top k query using  wi as the weighting until a stopping condition is met  Let us denote that operation as ri   PROGRESS q 0 i    wi   STOPPING CONDITION   In the basic idea mentioned in Section III A  we have to execute s   1 progressive top k queries for all s   1 weightings in S  In this section  we present a technique to skip many of those progressive top k operations so as to improve the ef   ciency  Section III D Technique  ii    In addition  the stopping condition in the basic idea is to proceed until the missing object  m comes forth to the result  However  if  m ranks very poorly under some weighting  wi   the corresponding progressive top k operation may be quite slow because it has to access many tuples in the database  In this section  we present a much more aggressive and effective stopping condition that makes most of those operations stop early even before  m is seen  Section III D Technique  i    These two techniques together can signi   cantly reduce the overall running time of the algorithm   PHASE 3  Using ri as the re   ned k 0    wi as the re   ned weighting  w 0   the  k 0    wi  combination with the least penalty is formulated as a re   ned query q 0  k 0    wi  and returned to the user as the why not answer  We    rst provide the details of PHASE 1  First   wo  the weighting vector in the user   s original query qo  is added to S  Next  we use the method in  17  to locate the set I of objects incomparable with  m  After that  we randomly pick a point pi from I and use Gaussian Jordan method  18  to ef   ciently    nd the intersection between the hyperplane   pi   m     w   0 and the constraint plane P w i    1  Then  we randomly pick a point  weighting  from the intersection  If all components of this weighting are non negative  w i    0   we add it to S  The process repeats until s weightings have been collected  As we show in the experiments  this phase can be done very ef   ciently because    nding I for one  or a few  missing object s  and solving plane intersections using Gaussian Jordon method incur almost negligible costs  Technique  i      Stopping a progressive top k operation earlier In PHASE 2 of our algorithm  the basic idea is to execute the progressive top k query until  m appears in the result  with rank ri   Denoting that operation as r1   PROGRESS q 0 1    w1   UNTIL SEE   m   In the following  we show that it is actually possible for a progressive top k execution to stop early even before  m shows up in the result  Consider an example that a user speci   es a top 2 query qo ko   2   wo  and a why not question about missing object  m is posed  Assume that the list of weightings S is    wo   w1   w2   w3   Furthermore  assume that PROGRESS q 0 o    wo   UNTIL SEE   m  is    rstly executed and  m   s actual ranking under  wo is 6  Now  we have our    rst candidate re   ned query q 0 o  k m o   6   wo   with  ko   6  2   4 and  wo   0  The corresponding penalty  denoted as  Pq 0 o   could be calculated using Equation 2  Remember that we want to    nd the re   ned query with the least penalty Pmin  So  at this     moment  we set a penalty variable Pmin   Pq 0 o   According to our basic idea  we should execute another progressive top k using weighting vector  say   w1  until  m shows up in the result set with a ranking r1  However  we notice that the skyline property in the answer space can help to stop that operation earlier  even before  m is seen  Given the    rst candidate re   ned query q 0 o  k m o   6   wo  with  wo   0 and  ko   4  any other candidate re   ned queries q 0 i with  ki   4 must be dominated by q 0 o   In our example  since the    rst executed progressive top k execution  all the subsequent progressive top k executions can stop once  m does not show up in the top 6 tuples     k    w q  o q  2 q  1    w1 q  3 0 3 4    w2    w    w3  2  1    k   4    k T    k q   2 L f Slope Pmin Fig  4  Example of answer space Figure 4 illustrates the answer space of the example  The idea above essentially means that all other progressive top k executions with m does not show up in top 6  i e    ki   4  see the dotted region   e g   q 0 1   can stop early at top 6  because after that  they have no chance to dominate q 0 o anymore  While useful  we can actually be even more aggressive in many cases  Consider another candidate re     ned query  say  q 0 2   in Figure 4  Assume that r2   PROGRESS q 0 2    w2   UNTIL SEE   m    5  i e    k2   5  2   3   which is not covered by the above technique  since  k2  6 4   However  q 0 2 can also stop early  as follows  In Figure 4  we show the normalized penalty Equation 2 as a slope Pmin that passes through the best re   ned query so far  currently q 0 o    All re   ned queries lie on the slope have the same penalty value as Pmin  In addition  all re   ned queries that lie above the slope actually have penalty larger than Pmin  and thus dominated by q 0 o in this case  Therefore  similar to the skyline discussion above  we can determine an even tighter threshold ranking r T   for stopping the subsequent progressive top k operations even earlier  r T    k T   ko  where  k T   b Pmin   w  w p 1   P w2 oi   k m o  ko  k c  8  Equation 8 is a rearrangement of Equation 2  with P enalty k 0    w 0     Pmin  and with the original top k value ko added  Back to our example in Figure 4  given that the weighting of candidate re   ned query q 0 2 is  w2  we can    rst compute its  w2 value  Then  we can project  w2 onto the slope Pmin  currently Pmin   Pq 0 o   to obtain the corresponding  k T value  which is 2 in Figure 4  That means  if we carry out a progressive top k operation using  w2 as the weighting  and if  m still does not appear in result after the top 4 tuples  r T    k T   ko   2   2   4  are seen  then we can stop it early because the penalty of q 0 2 is worse than the penalty Pmin of the best re   ned query  q 0 o   seen so far  Following the discussion above  we now have two early stopping conditions for the progressive topk algorithms  UNTIL SEE   m and UNTIL RANK r T   Except the    rst progressive top k operation in which PROGRESS q 0 o    wo   UNTIL SEE   m  must be used  the subsequent progressive top k operations can use    UNTIL SEE   m OR UNTIL RANK r T     as the stopping condition  We remark that the conditions UNTIL RANK r T and UNTIL SEE   m are both useful  For example  assume that the actual ranking of  m under  w2 is 3  which gives it a  k2   1  see q 00 2 in Figure 4   Recall that by projecting  w2 on to the slope of Pmin  we can stop the progressive top k operation after r T   2   2   4 tuples have been seen  However  using the condition UNTIL SEE   m  we can stop the progressive top k operation when  m shows up at rank three  This drives us to use    UNTIL SEE   m OR UNTIL RANK r T     as the stopping condition  Finally  we remark that the pruning power of this technique increases when the algorithm proceeds  For example  after q 00 2 has been executed  the best re   ned query seen so far should be updated as q 00 2  because its penalty is better than q 0 o    Therefore  Pmin now is updated as Pq 00 2 and the slope Pmin should be updated to pass through q 00 2 now  the dotted slope in Figure 4   Because Pmin is continuously decreasing   k T and thus the threshold ranking r T would get smaller and smaller and the subsequent progressive top k operations can terminate even earlier and earlier when the algorithm proceeds  With the same token  we also envision that the pruning power of this technique is stronger when we have a large  w  or small  k  because they make  k T decreases at a faster rate  see Equation 8   Technique  ii   Skipping progressive top k operations In PHASE 2 our algorithm  the basic idea is to execute progressive top k queries for all weightings in S  We now illustrate how some of those executions could be entirely skipped  so that the overall running time can be further reduced  The    rst part of the technique is based on the observation that similar weighting vectors may lead to similar top k results  11   Therefore  if a weighting  wj is similar to a weighting  wi and if operation PROGRESS q 0 i    wi   for  wi has already been executed  then the query result Ri of PROGRESS q 0 i    wi   could be exploited to deduce the highest ranking of the missing object  m under  wj   If the deduced highest ranking of  m is worse than the threshold ranking rT   then we can skip the   entire PROGRESS q 0 j    wj    operation  We illustrate the above by reusing our running example  Assume that we have cached the result sets of executed progressive top k queries  Let Ro be the result set of the    rst executed query PROGRESS q 0 o    wo   UNTIL SEE   m  and Ro     p1   p2   p3   p4   p5   m   Then  when we are considering the next weighting vector  say   w1  in S  we    rst follow Technique  i  to calculate the threshold ranking r T   In Figure 4  projecting  w1 onto slope Pq 0 o we get r T   3   2   5  Next we calculate the scores of all objects in Ro using  w1 as the weighting  Assume that the scores of  p1   p2   p3   p4  and  p5 are also smaller  better  than  m under  w1  in this case  we know the rankings of  p1   p2   p3   p4  and  p5 are all smaller  better  than the ranking of m  i e   the ranking of  m is at least 5   1   6  which is worse than r T   5  So  we can skip the entire PROGRESS q 0 1    w1   operation even without starting it  The above caching technique is shown to be the most effective between similar weighting vectors  11   Therefore  we design the algorithm in a way that the list of weightings S is sorted according to their corresponding  wi values  of course   wo is the in the head of the list since  wo   0   The second part of the technique is to exploit the best possible ranking of m  under all possible weightings  to set up an early termination condition for the whole algorithm  so that after a certain number of progressive top k operations have been executed  the algorithm can terminate early without executing the subsequent progressive top k operations  Recall that the best possible ranking of  m is kd   1  where kd is the number of objects that dominate  m  Therefore  the lower bound of  k  denoted as  kL  equals to max kd   1  ko  0   By de   nition in Section II   k   0   So  this time  we project  kL onto slope Pmin in order to determine the corresponding maximum feasible  w value  Naming that value as  w f   For any  w    w f   it means     m has  k    kL     which is impossible  As our algorithm is designed to examine the weightings in their increasing order of  w values  when a weighting wi 2 S has jjwi  wojj    w f   PROGRESS q 0 i    wi   and all subsequent progressive top k operations PROGRESS q 0 i 1    wi 1            PROGRESS q 0 s    ws   could be skipped  Reusing Figure 4 as an example and assume that the number of objects that dominated  m is 2  By projecting  kL   max kd   1  ko  0    1 onto slope the Pq 0 o   we could determine the corresponding  w f value  So  when the algorithm    nishes executing progressive top k operation for weighting  w2  PHASE 2 of the algorithm can terminate at that point because all the subsequent  wi are larger than  w f   As a    nal remark  we would like to point out that the pruning power of this technique also increases when the algorithm proceeds  For instance  in Figure 4  if q 00 2 has been executed  slope Pmin is changed from slope Pq 0 o to slope Pq 00 2   Projecting  kL onto the new Pmin slope would result in a smaller  w f   which in turns increases the chance of terminating PHASE 2 earlier  The pseudo code of the complete idea is presented in Algorithm 1  It is self explanatory and mainly summarizes what we have discussed above  so we do not give it a walkthrough here  Algorithm 1 Answering Why not Question on a Top K Query Input  The dataset D  original top k query qo ko   wo   missing object  m  penalty settings  k   w  T  and P r Output  A re   ned query q 0  k 0    w 0   1  Set list of weighting S      wo   2  Result of a top k query Ro  3  Rank of missing object k m o   4   Ro  k m o     PROGRESS q 0 o   wo   UNTIL SEE   m  5  if k m o     then 6  return     m is not in the D    7  end if Phase 1  8  Use  17  to determine the number of points kd that dominate  m and the set of points I incomparable with  m  9   kL   max kd   1  ko  0   10  Determine s from T  and P r using Equation 7  11  Sample s weightings from the hyperplanes boundaries constructed by I and  m and add them to S  12  Sort S according to their  wi values  Phase 2  13  R    Ro   wo     Cache the results 14  Pmin   P enalty k m o    wo   15   kL   max kd   1  ko  0     Calculate the lower bound ranking of m 16   w f    Pmin k  kL km o ko   p 1  P w2 oi  w     Project  kL to determine early termination point  wf 17  for all  wi 2 S do 18  if  wi    wf then 19  break    Technique  ii      early algorithm termination 20  end if 21   k T  b Pmin   w p  wi 1  P w2 oi   km o ko  k c  22  r T   k T   ko  23  if there exist r T objects in some Ri 2 R having scores better  m under  wi then 24  continue    Technique  ii      use cached result to skip a progressive top k 25  end if 26   Ri  ri    PROGRESS q 0 o   wi   UNTIL SEE   m OR UNTIL RANK r T      Technique  i      stopping a progressive top k early 27  Pi   P enalty ri   wi   28  R   R    Ri   wi   29  if Pi   Pmin then 30  Pmin   Pi  31   w f    Pmin   k  kL km o ko   p 1  P w2 oi  w   32  end if 33  end for Phase 3  34  return the best re   ned query q 0  k 0    w 0   whose penalty Pmin  E  Multiple Missing Objects To deal with multiple missing objects in a why not question  we have to modify our algorithm a little bit  First  we do a simple    ltering on the set of missing objects M  Speci   cally  among all the missing objects in M  if there is a missing object  mi dominates another one  mj in the data space  then we can           remove the dominating object  mi from M for the reason that every time  mj appears into the top k result   mi is certainly in the result as well  So  we only need to consider  mj   Let M0 be the set of missing objects after the    ltering step  The next modi   cation to the algorithm is related to PHASE 1        nding good weightings and put them into S  First  the set I should now consist of incomparable points of all objects in M0   Second  we should randomly select a hyperplane   pi   mi   w   0  where  pi is a point in I  After that  as usual  we sample a point on the intersection of the hyperplanes plus the constraint plane P w i    1  0   w i    1   That way  the whole method still obeys Theorem 1  The modi   cation related to Technique  i  is as follows  For the condition UNTIL SEE   m  it should now be UNTIL SEE ALL OBJECTS IN M0   For example  k m o in Algorithm 1 Line 4 should now refer to the ranking of the missing object with the highest score  The threshold ranking r T for the condition UNTIL RANK r T should also be computed based on the above k m o instead  The modi   cations related to Technique  ii  is as follows  We now have to identify the lower bound of  kL for a set of missing objects M0 instead of a single missing object  With a set of missing objects M0   f m1         mng  we use DOMi to represent the set of objects that dominate  mi   So   kL for M0 is max jDOM1   DOM2           DOMn   M0 j  ko  0   IV  EXPERIMENTAL STUDY We evaluate our proposed solution using both synthetic and real data  By default  we set the system parameters T  and P r as 0 2  and 0 8  respectively  resulting in a sample size of 800 weightings   The algorithms are implemented in C   and the experiments are run on a Ubuntu PC with Intel 2 67GHz i5 Dual Core processor and 4GB RAM  We adopt  16  as our progressive top k algorithm  A  Case Study We use the NBA data set in the case study  The NBA data set contains 21961 game statistics of all NBA players from 1973 2009  Each record represents the career performance of a player  player name  Player   points per game  PTS   rebounds per game  REB   assists per game  AST   steals per game  STL   blocks per game  BLK      eld goal percentage  FG   free throw percentage  FT   and three point percentage  3PT   For comparison  we also implemented a version of our algorithm in which weightings are randomly sampled from the whole weighting space  We refer to that version as WWS  In the following  we present several interesting cases  Case 1  Finding the top 3 centers in NBA history   The    rst case was to    nd the top 3 centers in the NBA history  Therefore  we issued a top 3 query q1 with equal weighting  0 2  on    ve attributes PTS  REB  BLK  FG  and FT  The initial result was  fW  Chamberlain  Abdul Jabbar  Shaquille O   nealg  which means Chamberlain ranked    rst  followed by Jabber  and O   neal   Because we were curious why Yao Ming was not in the result  we issued a why not question ffYao Mingg  q1g using the    Prefer modify weighting    option  since we wanted to see Yao in top 3  In 156ms  our algorithm returned a re   ned query q 0 1 with k 0 1   3 and  w 0 1   j0 0243 0 0024 0 0283 0 0675 0 8775j  The re   ned query essentially indicated that we should have put more weights on a center   s free throw  FT  ability if we wish to see Yao in the top 3 result  The corresponding result of q 0 1 was  fAbdul Jabbar  Hakeem Olajuwon  Yao Mingg  where Yao Ming ranked third  The penalty value of q 0 1 was 0 069  As a comparison  WWS returned another re   ned query q 0WWS 1   using 154ms  However  q 0WWS 1 was a top 7 query that uses another set of weighting  Yao ranked 7 th   The penalty of q 0WWS 1 was 0 28  which was four times worse than q 0 1   Case 2  Finding the top 3 guards in NBA history   The second case was to    nd the top 3 guards in the NBA history  Therefore  we issued a top 3 query q2 with equal weighting   1 6   on six attributes PTS  AST  STL  FG  FT  and 3PT  The initial result was  fMichael Jordon  LeBron James  Oscar Robertsong  We were surprised why Kobe Bryant was not in the result  So  we posed a why not question ffKobe Bryantg  q2g using the    Prefer modify weighting    option  since we wanted to see Kobe Bryant in top 3  In 163ms  our algorithm returned a re   ned query q 0 2 with k 0 2   3 and  w 0 2   j0 0129 0 0005 0 0416 0 2316 0 3769 0 3364j  The corresponding result of q 0 2 was  fMichael Jordon  Pete Maverich  Kobe Byrantg  The penalty of q 0 2 was 0 035  As a comparison  WWS returned a re   ned query q 0WWS 2   in 161ms  However  q 0WWS 2 was a top 4 query  Kobe Bryant ranked 4 th   which con   icted with our    Prefer modify weighting    option  Thus  q 0WWS 2    s penalty was 0 2  which was more than    ve times worse than q 0 2   Case 3  Finding the top 3 players in NBA history   The third case was to    nd the top 3 players in the NBA history  Therefore  we issued a top 3 query q3 with equal weighting   1 8   on all eight numeric attributes  The initial result was  fW  Chamberlian  LeBron James  Elgin Baylorg  Amazingly  Michael Jordan was missing in the result  To understand why  we issued a why not question ffMichael Jordang  q3g  using the    Prefer modify k    option  because we insisted that Michael Jordan should have a top ranking without twisting the weightings much  Using 150ms  our algorithm returned a re   ned query q 0 3 with k 0 3   5 and  w 0 3    wo  with the following result  fW  Chamberlain  LeBron James  Elgin Baylor  Bob Pettit  Michael Jordang  The re   ned query q 0 3 essentially means that our initial weightings were reasonable but we should have looked for the top 5 players instead  In this case  both versions of our algorithms came up with the same re   ned query in 150ms  As a follow up  we were also interested in understanding why both Michael Jordan and Shaquille O   neal were not the top 3 players in the NBA history  Therefore  we issued another why not query ffMichael Jordan  Shaquille O   nealg  q3g  using the    Never mind    option  In 166ms  our algorithm returned a re   ned query q 00 3 with k 0   5 and  w 0 3   j0 138 0 0847 0 0639 0 1066 0 2481 0 1143 0 1231 0 1212j  The corresponding result of q 00 3 was  fW  Chamberlian  Michael Jordon  LeBorn James  Abudl   Jabber  Shaquille  O   nealg  The penalty of q 00 3 was 0 2  WWS returned another re   ned query q 00WWS 3   in 164ms  However  q 00WWS 3 was a top 5 query with penalty 0 27  which was higher than q 00 3   B  Performance We next turn our focus to the performance of our algorithm  We present experimental results based on three types of synthetic data  uniform  UN   correlated  CO  and anti correlated  AC   Since  the experiment results between UN and CO are very similar  we only present the results of UN and AC here  Table II shows the parameters we varied in the experiments  The default values are in bold faces  The default top k query qo has a setting of  k   ko   wo   j 1 d       1 d j  where d is the number of dimensions  attributes involved   By default  the why not question asks for a missing object that is ranked  10   ko   1  th under  wo  TABLE II PARAMETERS SETTING Parameter Ranges Data size 100K  500K  1M  1 5M  2M Dimension 2  3  4  5 ko 5  10  50  100 Actual ranking of  m under qo 11  101  501  1001 T  0 3   0 25   0 2   0 15   0 1  P r 0 5  0 6  0 7  0 8  0 9 jMj 1  2  3  4  5 Varying Data Size  Figure 5 shows the running time of our algorithm under different data sizes  using different penalty options  PMK stands for    Prefer modifying k     PMW stands for    Prefer modifying weighting     NM stands for    Never mind     We can see our algorithm for answering why not questions scales linearly with the data size  The running time scales linearly  but at a faster rate  on AC data because of the general fact that progressive top k operations on anticorrelated data takes a longer time to    nish  16    0  0 2  0 4  0 6  0 8  1 1 5 10 15 20 Total running time  sec  Data size 100K  PMW NM PMK  a  Uniform Data  0  0 5  1  1 5  2  2 5  3 1 5 10 15 20 Total running time  sec  Data size 100K  PMW NM PMK  b  Anti correlated Data Fig  5  Varying data size Varying Query Dimension  Figure 6 shows the running time of our algorithm using top k queries in different number of query dimensions  In general  answering why not questions for top k queries in a higher query dimension needs more time because the execution time of a progressive top k operation increases if a top k query involves more attributes  From the    gure  we see that our algorithm scales well with the number of dimensions  Varying ko  Figure 7 shows the running time of our algorithm using top k queries with different ko values  In this  0  0 2  0 4  0 6  0 8  1 2 3 4 5 Total running time  sec  Dimensionality PMW NM PMK  a  Uniform Data  0  0 5  1  1 5  2  2 5  3  3 5  4 2 3 4 5 Total running time  sec  Dimensionality PMW NM PMK  b  Anti correlated Data Fig  6  Varying query dimension experiment  when a top 5 query  ko   5  is used  the corresponding why not question is to ask why the object in rank 51 th is missing  Similarly  when a top 50 query  ko   50  is used  the corresponding why not question is to ask why the object in rank 501 th is missing  Naturally  when ko increases  the time to answer a why not question should also increase because the execution time of a progressive top k operation also increases with k  Figure 7 shows that our algorithm scales well with ko  The running time of our algorithm increases very little under the PMK option  Recall that in Section III D we mentioned that the effectiveness of our pruning techniques is more pronounced when the PMK option is used  In this experiment  when we scaled up ko to a large value  the algorithm running time became higher  As such  the stronger pruning effectiveness of the PMK option became more obvious in the experimental result   0  0 5  1  1 5  2  2 5  3  3 5  4 10 50 100 Total running time  sec  k o PMW NM PMK  a  Uniform Data </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>SkimpyStash  RAM Space Skimpy Key Value Store on Flash based Storage ### Biplob Debnath     1 Sudipta Sengupta     Jin Li         Microsoft Research  Redmond  WA  USA    EMC Corporation  Santa Clara  CA  USA ABSTRACT We present SkimpyStash  a RAM space skimpy key value store on    ash based storage  designed for high throughput  low latency server applications  The distinguishing feature of SkimpyStash is the design goal of extremely low RAM footprint at about 1     0 5  byte per key value pair  which is more aggressive than earlier designs  SkimpyStash uses a hash table directory in RAM to index key value pairs stored in a log structured manner on    ash  To break the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key  it    moves  most of the pointers that locate each key value pair from RAM to    ash itself  This is realized by  i  resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and  ii  storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  hence incurring multiple    ash reads per lookup  Two further techniques are used to improve performance   iii  two choice based load balancing to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and a bloom    lter in each hash table directory slot in RAM to disambiguate the choice during lookup  and  iv  compaction procedure to pack bucket chain records contiguously onto    ash pages so as to reduce    ash reads during lookup  The average bucket size is the critical design parameter that serves as a powerful knob for making a continuum of tradeoffs between low RAM usage and low lookup latencies  Our evaluations on commodity server platforms with real world data center applications show that SkimpyStash provides throughputs from few 10 000s to upwards of 100 000 get set operations sec  Categories and Subject Descriptors H 3 Information Storage and Retrieval  H 3 1 Content Analysis and Indexing   Indexing Methods General Terms Algorithms  Design  Experimentation  Performance  1Work done while Biplob Debnath was at Microsoft Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  Keywords Key value store  Flash memory  Indexing  RAM space ef   cient index  Log structured index ###  1  INTRODUCTION A broad range of server side applications need an underlying  often persistent  key value store to function  Examples include state maintenance in Internet applications like online multi player gaming and inline storage deduplication  as described in Section 3   A high throughput persistent key value store can help to improve the performance of such applications  Flash memory is a natural choice for such a store  providing persistency and 100 1000 times lower access times than hard disk  Compared to DRAM     ash access times are about 100 times higher  Flash stands in the middle between DRAM and disk also in terms of cost     it is 10x cheaper than DRAM  while 20x more expensive than disk     thus  making it an ideal gap    ller between DRAM and disk  It is only recently that    ash memory  in the form of Solid State Drives  SSDs   is seeing widespread adoption in desktop and server applications  For example  MySpace com recently switched from using hard disk drives in its servers to using PCI Express  PCIe  cards loaded with solid state    ash chips as primary storage for its data center operations  5   Also recently  Facebook released Flashcache  a simple write back persistent block cache designed to accelerate reads and writes from slower rotational media  hard disks  by caching data in SSDs  6   These applications have different storage access patterns than typical consumer devices and pose new challenges to    ash media to deliver sustained and high throughput  and low latency   These challenges arising from new applications of    ash are being addressed at different layers of the storage stack by    ash device vendors and system builders  with the former focusing on techniques at the device driver software level and inside the device  and the latter driving innovation at the operating system and application layers  The work in this paper falls in the latter category  To get the maximum performance per dollar out of SSDs  it is necessary to use    ash aware data structures and algorithms that work around constraints of    ash media  e g   avoid or reduce small random writes that not only have a higher latency but also reduce    ash device lifetimes through increased page wearing   In the rest of the paper  we use NAND    ash based SSDs as the architectural choice and simply refer to it as    ash memory  We describe the internal architecture of SSDs in Section 2  Recently  there are several interesting proposals to design keyvalue stores using    ash memory  10  11  15  16   These designs use a combination RAM and    ash memory     they store the full key value pairs on    ash memory and use a small amount of metadata per key value pair in RAM to support faster insert andlookup operations  For example  FAWN  11   FlashStore  16   and ChunkStash  15  each require about six bytes of RAM space per key value pair stored on    ash memory  Thus  the amount of available RAM space limits the total number of key value pairs that could be indexed on    ash memory  As    ash capacities are about an order of magnitude bigger than RAM and getting bigger  RAM size could well become the bottleneck for supporting large    ashbased key value stores  By reducing the amount of RAM bytes needed per key value pair stored on    ash down to the extreme lows of about a byte  SkimpyStash can help to scale key value stores on    ash on a lean RAM size budget when existing current designs run out  Our Contributions In this paper  we present the design and evaluation of SkimpyStash  a RAM space skimpy key value store on    ash based Storage  designed for high throughput server applications  The distinguishing feature of SkimpyStash is the design goal of extremely low RAM footprint at about 1 byte per key value pair  which is more aggressive than earlier designs like FAWN  11   BufferHash  10   ChunkStash  15   and FlashStore  16   Our base design uses less than 1 byte in RAM per key value pair and our enhanced design takes slightly more than 1 byte per key value pair  By being RAM space frugal  SkimpyStash can accommodate larger    ash drive capacities for storing and indexing key value pairs  Design Innovations  SkimpyStash uses a hash table directory in RAM to index key value pairs stored in a log structure on    ash  To break the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key  it    moves  most of the pointers that locate each key value pair from RAM to    ash itself  This is realized by  i  Resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and  ii  Storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  hence incurring multiple    ash reads per lookup  Two further techniques are used to improve performance   iii  Two choice based load balancing  12  to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and a bloom    lter  13  in each hash table directory slot in RAM for summarizing the records in that bucket so that at most one bucket chain on    ash needs to be searched during a lookup  and  iv  Compaction procedure to pack bucket chain records contiguously onto    ash pages so as to reduce    ash reads during lookup  The average bucket size is the critical design parameter that serves as a powerful knob for making a continuum of tradeoffs between low RAM usage and low lookup latencies  Evaluation on data center server applications  SkimpyStash can be used as a high throughput persistent key value storage layer for a broad range of server class applications  We use real world data traces from two data center applications  namely  Xbox LIVE Primetime online multi player game and inline storage deduplication  to drive and evaluate the design of SkimpyStash on commodity server platforms  SkimpyStash provides throughputs from few 10 000s to upwards of 100 000 get set operations sec on the evaluated applications  Figure 1  Internal architecture of a Solid State Drive  SSD  The rest of the paper is organized as follows  We provide an overview of    ash memory in Section 2  In Section 3  we describe two motivating real world data center applications that can bene   t from a high throughput key value store and are used to evaluate SkimpyStash  We develop the design of SkimpyStash in Section 4  We evaluate SkimpyStash in Section 5  We review related work in Section 6  Finally  we conclude in Section 7  2  FLASH MEMORY OVERVIEW Figure 1 gives a block diagram of an NAND    ash based SSD  In    ash memory  data is stored in an array of    ash blocks  Each block spans 32 64 pages  where a page is the smallest unit of read and write operations  A distinguishing feature of    ash memory is that read operations are very fast compared to magnetic disk drive  Moreover  unlike disks  random read operations are as fast as sequential read operations as there is no mechanical head movement  A major drawback of the    ash memory is that it does not allow in place updates  i e   overwrite   Page write operations in a    ash memory must be preceded by an erase operation and within a block  pages need be to written sequentially  The in place update problem becomes complicated as write operations are performed in the page granularity  while erase operations are performed in the block granularity  The typical access latencies for read  write  and erase operations are 25 microseconds  200 microseconds  and 1500 microseconds  respectively  9   The Flash Translation layer  FTL  is an intermediate software layer inside SSD  which makes linear    ash memory device act like a virtual disk  The FTL receives logical read and write commands from the applications and converts them to the internal    ash memory commands  To emulate disk like in place update operation for a logical page  Lp   the FTL writes data into a new physical page  Pp   maintains a mapping between logical pages and physical pages  and marks the previous physical location of Lp as invalid for future garbage collection  Although FTL allows current disk based application to use SSD without any modi   cations  it needs to internally deal with    ash physical constraint of erasing a block before overwriting a page in that block  Besides the in place update problem     ash memory exhibits another limitation     a    ash block can only be erased for limited number of times  e g   10K 100K   9   FTL uses various wear leveling techniques to even out the erase counts of different blocks in the    ash memory to increase its overall longevity  17   Recent studies show that current FTL schemes are very effective for the workloads with sequential access write patterns  However  for the workloads with random access patterns  these schemes show very poor performance  18  20  22   One of the design goals of SkimpyStash is to use    ash memory in FTLfriendly manner 3  KEY VALUE STORE APPLICATIONS We describe two real world applications that can use SkimpyStash as an underlying persistent key value store  Data traces obtained from real world instances of these applications are used to drive and evaluate the design of SkimpyStash  3 1 Online Multi player Gaming Online multi player gaming technology allows people from geographically diverse regions around the globe to participate in the same game  The number of concurrent players in such a game could range from tens to hundreds of thousands and the number of concurrent game instances offered by a single online service could range from tens to hundreds  An important challenge in online multi player gaming is the requirement to scale the number of users per game and the number of simultaneous game instances  At the core of this is the need to maintain server side state so as to track player actions on each client machine and update global game states to make them visible to other players as quickly as possible  These functionalities map to set and get key operations performed by clients on server side state  The real time responsiveness of the game is  thus  critically dependent on the response time and throughput of these operations  There is also the requirement to store server side game state in a persistent manner for  at least  the following reasons   i  resume game from interrupted state if and when crashes occur   ii  of   ine analysis of game popularity  progression  and dynamics with the objective of improving the game  and  iii  veri   cation of player actions for fairness when outcomes are associated with monetary rewards  We designed SkimpyStash to meet the high throughput and low latency requirement of such get set key operations in online multi player gaming  3 2 Storage Deduplication Deduplication is a recent trend in storage backup systems that eliminates redundancy of data across full and incremental backup data sets  28   It works by splitting    les into multiple chunks using a content aware chunking algorithm like Rabin    ngerprinting and using SHA 1 hash  24  signatures for each chunk to determine whether two chunks contain identical data  28   In inline storage deduplication systems  the chunks  or their hashes  arrive one ata time at the deduplication server from client systems  The server needs to lookup each chunk hash in an index it maintains for all chunk hashes seen so far for that backup location instance     if there is a match  the incoming chunk contains redundant data and can be deduplicated  if not  the  new  chunk hash needs to be inserted into the index  Because storage systems currently need to scale to tens of terabytes to petabytes of data volume  the chunk hash index is too big to    t in RAM  hence it is stored on hard disk  Index operations are thus throughput limited by expensive disk seek operations  Since backups need to be completed over windows of half a day or so  e g   nights and weekends   it is desirable to obtain high throughput in inline storage deduplication systems  RAM prefetching and bloom    lter based techniques used by Zhu et al   28  can avoid disk I Os on close to 99  of the index lookups  Even at this reduced rate  an index lookup going to disk contributes about 0 1msec to the average lookup time     this is about 10 3 times slower than a lookup hitting in RAM  SkimpyStash can be used as the chunk hash index for inline deduplication systems  By reducing the penalty of index lookup misses in RAM by orders of magnitude by serving such lookups from    ash memory  SkimpyStash can help to increase deduplication throughput  99019 94500 16064 5948 0 20000 40000 60000 80000 100000 seq reads rand reads seq writes rand writes IOPS Figure 2  IOPS for sequential random reads and writes using 4KB I O request size on a 160GB fusionIO drive  4  SkimpyStash DESIGN We present the system architecture of SkimpyStash and the rationale behind some design choices in this section  4 1 Coping with Flash Constraints The design is driven by the need to work around two types of operations that are not ef   cient on    ash media  namely  1  Random Writes  Small random writes effectively need to update data portions within pages  Since a  physical     ash page cannot be updated in place  a new  physical  page will need to be allocated and the unmodi   ed portion of the data on the page needs to be relocated to the new page  2  Writes less than    ash page size  Since a page is the smallest unit of write on    ash  writing an amount less than a page renders the rest of the  physical  page wasted     any subsequent append to that partially written  logical  page will need copying of existing data and writing to a new  physical  page  To validate the performance gap between sequential and random writes on    ash  we used Iometer  3   a widely used performance evaluation tool in the storage community  on a 160GB fusionIO SSD  2  attached over PCIe bus to an Intel Core 2 Duo E6850 3GHz CPU  The number of worker threads was    xed at 8 and the number of outstanding I Os for the drive at 64  The results for IOPS  I O operations per sec  on 4KB I O request sizes are summarized in Figure 2  Each test was run for 1 hour  The IOPS performance of sequential writes is about 3x that of random writes and worsens when the tests are run for longer durations  due to accumulating device garbage collection overheads   We also observe that the IOPS performance of  random sequential  reads is about 6x that sequential writes   The slight gap between IOPS performance of sequential and random reads is possibly due to prefetching inside the device   Given the above  the most ef   cient way to write    ash is to simply use it as an append log  where an append operation involves a    ash page worth of data  typically 2KB or 4KB  This is the main constraint that drives the rest of our key value store design  Flash has been used in a log structured manner and its bene   ts reported in earlier works  11  14  15  16  19  23  27   4 2 Design Goals The design of SkimpyStash is driven by the following guiding principles      Support low latency  high throughput operations  This requirement is extracted from the needs of many server classapplications that need an underlying key value store to function  Two motivating applications that are used for evaluating SkimpyStash are described in Section 3      Use    ash aware data structures and algorithms  This principle accommodates the constraints of the    ash device so as to extract maximum performance out of it  Random writes and in place updates are expensive on    ash memory  hence must be reduced or avoided  Sequential writes should be used to the extent possible and the fast nature of random sequential reads should be exploited      Low RAM footprint per key independent of key value size  The goal here is to index all key value pairs on    ash in a RAM space ef   cient manner and make them accessible using a small number of    ash reads per lookup  By being RAM space frugal  one can accommodate larger    ash drive capacities and correspondingly larger number of keyvalue pairs stored in it  Key value pairs can be arbitrarily large but the RAM footprint per key should be independent of it and small  We target a skimpy RAM usage of about 1 byte per key value pair  a design point that is more aggressive than earlier designs like FAWN  11   BufferHash  10   ChunkStash  15   and FlashStore  16   4 3 Architectural Components SkimpyStash has the following main components  A base version of the design is shown in Figure 3 and an enhanced version in Figure 5  We will get to the details shortly  RAM Write Buffer  This is a    xed size data structure maintained in RAM that buffers key value writes so that a write to    ash happens only after there is enough data to    ll a    ash page  which is typically 2KB or 4KB in size   To provide strict durability guarantees  writes can also happen to    ash when a con   gurable timeout interval  e g   1 msec  has expired  during which period multiple key value pairs are collected in the buffer   The client call returns only after the write buffer is    ushed to    ash  The RAM write buffer is sized to 2 3 times the    ash page size so that key value writes can still go through when part of the buffer is being written to    ash  RAM Hash Table  HT  Directory  The directory structure  for key value pairs stored on    ash  is maintained in RAM and is organized as a hash table with each slot containing a pointer to a chain of records on    ash  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  The chain of records on    ash pointed to by each slot comprises the bucket of records corresponding to this slot in the HT directory  This is illustrated in Figure 3  The average number of records in a bucket  k  is a con     gurable parameter  In summary  we resolve hash table directory collisions by linear chaining and store the chains in    ash  In an enhancement of the design  we use two choice based load balancing to reduce wide variation in bucket sizes  hence  chain lengths and associated lookup times   and introduce a bloom    lter in each hash table directory slot in RAM for summarizing the records in that bucket so that at most one bucket chain on    ash needs to be searched during a lookup  These enhancements form the the core of our design and are discussed in detail in Section 4 5  Flash store  The    ash store provides persistent storage for the keyvalue pairs and is organized as a circular append log  Key value pairs are written to    ash in units of a page size to the tail of the log  When the log accumulates garbage  consisting of deleted or key value RAM Flash Memory key value key value key value             key value key value Hash table  directory Sequential log null null null Keys  ordered  by write  time in  log ptr Figure 3  SkimpyStash architecture showing the sequential log organization of key value pair records on    ash and base design for the hash table directory in RAM   RAM write buffer is not shown   older values of updated records  beyond a con   gurable threshold  the pages on    ash from the head of the log are recycled     valid entries from the head of the log are written back to the end of the log  This also helps to place the records in a given bucket contiguously on    ash and improve read performance  as we elaborate shortly  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its HT bucket chain  on    ash  4 4 Overview of Key Lookup and Insert Operations To understand the relationship of the different storage areas in our design  it is helpful to follow the sequence of accesses in key insert and lookup operations performed by the client application  A key lookup operation  get     rst looks up the RAM write buffer  Upon a miss there  it lookups up the HT directory in RAM and searches the chained key value pair records on    ash in the respective bucket  A key insert  or  update  operation  set  writes the key value pair into the RAM write buffer  When there are enough key value pairs in RAM write buffer to    ll a    ash page  or  a con   gurable timeout interval since the client call has expired  say 1 msec   these entries are written to    ash and inserted into the RAM HT directory and    ash  A delete operation on a key is supported through insertion of a null value for that key  Eventually the null entry and earlier inserted values of the key on    ash will be garbage collected  When    ash usage and fraction of garbage records in the    ash log exceed a certain threshold  a garbage collection  and compaction  operation is initiated to reclaim storage on    ash in a manner similar to that in log structured    le systems  26   This garbage collection operation starts scanning key value pairs from the  current  head of the log     it discards garbage  invalid or orphaned  as de   ned later  key value pair records and moves valid key value pair records from the head to the tail of the log  It stops when    oor thresholds are reached for    ash usage or fraction of garbage records remaining in the    ash log  The functionalities of  i  client key lookup insert operations   ii 0 1 2 3 4 5 6 7 0 4 8 12 16 20 24 28 32 RAM bytes per key value pair Avg  keys per bucket  k  Base design Enhanced Design Figure 4  RAM space usage per key value pair in SkimpyStash for the base and enhanced designs as the average number of keys per bucket  k  is varied  writing key value pairs to    ash store and updating RAM HT directory  and  iii  reclaiming space on    ash pages are handled by separate threads in a multi threaded architecture  Concurrency issues with shared data structures arise in our multi threaded design  which we address but do not describe here due to lack of space  4 5 Hash Table Directory Design At a high level  we use a hash table based index in RAM to index the key value pairs on    ash  Earlier designs like FAWN  11  and ChunkStash  15  dedicate one entry of the hash table to point to a single key value pair on    ash together with a checksum that helps to avoid  with high probability  following the    ash pointer to compare keys for every entry searched in the hash table during a lookup  The RAM overhead in FAWN and ChunkStash is 6 bytes per key value pair stored on    ash  With such a design  we cannot get below the barrier of a    ash pointer  say  4 bytes  worth of RAM overhead per key value pair  even if we ignore the other    elds  like checksums  in the hash table entry   Our approach  at an intuitive level  is to move most of the pointers that locate each key value pair from RAM to    ash itself  We realize this by     Resolving hash table collisions using linear chaining  where multiple keys that resolve  collide  to the same hash table bucket are chained in a linked list  and     Storing the linked lists on    ash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on    ash  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  Because we store the chain of key value pairs in each bucket on    ash  we incur multiple    ash reads upon lookup of a key in the store  This is the tradeoff that we need to make with lookup times in order to be able to skimp on RAM space overhead per keyvalue pair  We will see that the average number of keys in a bucket  k  is the critical parameter that allows us to make a continuum of tradeoffs between these two parameters     it serves as a powerful knob for reducing RAM space usage at the expense of increase in lookup times  We    rst begin with the base design of our hash table based index in RAM  Thereafter  we motivate and introduce some enhancements to the design to improve performance  Base Design The directory structure  for key value pairs stored on    ash  is maintained in RAM and is organized as a hash table with each slot containing a pointer to a chain of records on    ash  as shown in Figure 3  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  The chain of records on    ash pointed to by each slot comprises the bucket of records corresponding to this slot in the HT directory  A hash function h is used to map keys to slots in the HT directory  The average number of records in a bucket  k  is a con   gurable parameter  Then  to accommodate up to some given number n key value pairs  the number of slots required in the HT directory is about n k  In summary  we resolve hash table directory collisions by linear chaining and store the chains in    ash  We next describe the lookup  insert update  and delete operations on this data structure  A lookup operation on a key uses the hash function h to obtain the HT directory bucket that this key belongs to  It uses the pointer stored in that slot to follow the chain of records on    ash to search the key  upon    nding the    rst record in the chain whose key matches the search key  it returns the value  The number of    ash reads for such a lookup is k 2 on the average  and at most the size of the bucket chain in the worst case  An insert  or  update  operation uses the hash function h to obtain the HT directory bucket that this key belongs to  Let a1 be the address on    ash of the    rst record in this chain  i e   what the pointer in this slot points to   Then a record is created corresponding to the inserted  or  updated  key value pair with its next pointer    eld equal to a1  This record is appended to the log on    ash and its address on    ash now becomes the value of the pointer in the respective slot in RAM  Effectively  this new record is inserted at the beginning of the chain corresponding to this bucket  Thus  if this insert operation corresponds to an update operation on an earlier inserted key  the most recent value of the key will be  correctly  read during a lookup operation  the old value being further down the chain and accumulating as garbage in the log   A delete operation is same as the insert  or  update  with null value for that key  Eventually the null entry on    ash and old values of the key will be garbage collected in the log  RAM Space Overhead for Base Design  Let us say that the pointer to    ash in each HT directory slot is 4 bytes   This accommodates up to 4GB of byte addressable log  If records are of a    xed size  say 64 bytes  then this can accommodate up to 256GB of 64 byte granularity addressable log  Larger pointer sizes  up to 8 bytes  can be used according to application requirements   Then  with a value of k   10 average bucket size  the RAM space overhead is a mere 4 k   0 4 bytes   3 2 bits per entry  independent of key value size  At this sub byte range  this design tests the extremes of low RAM space overhead per entry  The average number of    ash reads per lookup is k 2   5  with current SSDs achieving    ash read times in the range of 10  sec  this corresponds to a lookup latency of about 50   sec  The parameter k provides a powerful knob for achieving tradeoffs between low RAM space usage and low lookup latencies  The RAM space usage per key value pair for the base design as a function of k is shown in Figure 4  Design Enhancements We identify some performance inef   ciencies in the base design and develop techniques to address them with only a slight increase in the RAM space overhead per key value pair  The enhanced design is shown in Figure 5  Load Balancing across Buckets The hashing of keys to HT directory buckets may lead to skewed distributions in the number of keys in each bucket chain  thus creating variations in average lookup times across buckets  Thus  itkey value RAM Flash Memory key value key value key value             key value key value Hash table  directory Sequential log null null null Keys  ordered  by write  time in  log BF ptr Two choice load  balancing for key x Figure 5  SkimpyStash architecture showing the sequential log organization of key value pair records on    ash and enhanced design for the hash table directory in RAM   RAM write buffer is not shown   might be necessary to enforce fairly equal load balancing of keys across HT directory buckets in order to keep each bucket chain of about the same size  One simple way to achieve this is to use the power of two choice idea from  12  that has been applied to balance a distribution of balls thrown into bins  With a load balanced design for the HT directory  each key would be hashed to two candidate HT directory buckets  using two hash functions h1 and h2  and actually inserted into the one that has currently fewer elements  We investigate the impact of this design decision on balancing bucket sizes on our evaluation workloads in Section 5  To implement this load balancing idea  we add one byte of storage to each HT directory slot in RAM that holds the current number of keys in that bucket     this space allocation accommodates up to a maximum of 2 8     1   255 keys per bucket  Bloom Filter per Bucket This design modi   cation  in its current form  will lead to an increase in the number of    ash reads during lookup  Since each key will need to be looked up in both of its candidate buckets  the worst case number of    ash reads  hence lookup times  would double  To remove this latency impact on the lookup pathway  we add a bloom    lter  13  per HT directory slot that summarizes the keys that have been inserted in the respective bucket  Note that this bloom    lter in each HT directory slot can be sized to contain about k keys  since load balancing ensures that when the hash table reaches its budgeted full capacity  each bucket will contain not many more than k keys with very high probability  A standard rule of thumb for dimensioning a bloom    lter to use one byte per key  which gives a false positive probability of 2    hence the bloom    lter in each HT directory slot can be of size k bytes  The introduction of bloom    lters in each HT directory slot has another desirable side effect     lookups on non existent keys will almost always not involve any    ash reads since the bloom    lters in both candidate slots of the key will indicate that the key is not present  module false positive probabilities    Note that in the base design  lookups on non existent keys also lead to    ash reads and involve traversing the entire chain in the respective bucket   In an interesting reciprocity of bene   ts  the bloom    lters in each Bloom Filter Bucket  size pointer to key value  pair chain on flash                                 k bytes 1 byte 4 bytes Figure 6  RAM hash table directory slot and sizes of component    elds in the enhanced design of SkimpyStash  The parameter k is the average number of keys in a bucket  bucket not only help in reducing lookup times when two choice load balancing is used but also bene   t from load balancing  Load balancing aims to keep the number of keys in each bucket upper bounded  roughly  by the parameter k  This helps to keep bloom    lter false positive probabilities in that bucket bounded as per the dimensioned capacity of k keys  Without load balancing  many more than k keys could be inserted into a given bucket and this will increase the false positive rates of the respective bloom    lter well beyond what it was dimensioned for  The additional    elds added to each HT directory slot in RAM in the enhanced design are shown in Figure 6  During a lookup operation  the key is hashed to its two candidate HT directory buckets and the chain on    ash is searched only if the respective bloom    lter indicates that the key may be there in that bucket  Thus  accounting for bloom    lter false positives  the chain on    ash will be searched with no success in less than 2  of the lookups  When an insert operation corresponds to an update of an earlier inserted key  the record is always inserted in the same bucket as the earlier one  even if the choice determined by load balancing  out of two candidate buckets  is the other bucket  If we followed the choice given by load balancing  the key may be inserted in the bloom    lters of both candidate slots     this would not preserve the design goal of traversing at most one bucket chain  with high probability  on    ash during lookups  Moreover  the same problem would arise with version resolution during lookups if different versions of a key are allowed to be inserted in both candidate buckets  This rule also leads to ef   ciencies during garbage collection operations since all the obsolete values of a key appear in the same bucket chain on    ash  Note that this complication involving overriding of the load balancing based choice of insertion bucket can be avoided when the application does not perform updates to earlier inserted keys     one example of such an application is storage deduplication as described in Section 3  In summary  in this enhancement of the base design  two choice based load balancing strategy is used to reduce variations in the the number of keys assigned to each bucket  hence  chain lengths and associated lookup times   Each HT directory slot in RAM also contains a bloom    lter summarizing the keys in the bucket and a size  count     eld storing the current number of keys in that bucket  RAM Space Overhead for Enhanced Design  With this design modi   cation  the RAM space overhead per bucket now has three components  namely      Pointer to chain on    ash  4 bytes       Bucket size  1 byte   and     Bloom    lter  k bytes   This space overhead per HT directory slot is amortized over an average of k keys  in that bucket   hence the RAM space overhead per entry can be computed as  k   1   4  k   1   5 k whichis about 1 5 bytes for k   10  The average number of    ash reads per lookup is still k 2   5  with high probability   with current SSDs achieving    ash read times in the range of 10  sec  this corresponds to a lookup latency of about 50   sec  Moreover  the variation across lookup latencies for different keys is better controlled in this design  compared to the base design  as bucket chains are about the same size due to two choice based load balancing of keys across buckets  The RAM space usage per key value pair for the enhanced design as a function of k is shown in Figure 4  Storing key value pairs to    ash  Key value pairs are organized on    ash in a log structure in the order of the respective write operations coming into the system  Each slot in the HT directory contains a pointer to the beginning of the chain on    ash that represents the keys in that bucket  Each key value pair record on    ash contains  in addition to the key and value    elds  a pointer to the next record  in the order in its respective chain  on    ash  We use a 4 byte pointer  which is a combination of a page pointer and a page offset  The all zero pointer is reserved for the null pointer     in the HT directory slot  this represents an empty bucket  while on    ash this indicates that the respective record has no successor in the chain  RAM and Flash Capacity Considerations  We designed our RAM indexing scheme to use 1 byte in RAM per key value pair so as to maximize the amount of indexable storage on    ash for a given RAM usage size  Whether RAM or    ash capacity becomes the bottleneck for storing key value pairs on    ash depends further on the key value pair size  With 64 byte key value pair records  1GB of RAM can index about 1 billion key value pairs on    ash which occupy 64GB on    ash  This    ash memory capacity is well within the capacity range of SSDs shipping in the market today  from 64GB to 640GB   On the other hand  with 1024 byte key value pair records  the same 1GB of RAM can index 1 billion key value pairs which now occupy 1TB on    ash     at currently available SSD capacities  this will require multiple    ash drives to store the dataset  4 6 Flash Storage Management Key value pairs are organized on    ash in a log structure in the order of the respective write operations coming into the system  When there are enough key value pairs in the RAM write buffer to    ll a    ash page  or  when a pre speci   ed coalesce time interval is reached   they are written to    ash  The pages on    ash are maintained implicitly as a circular log  Since the Flash Translation Layer  FTL  translates logical page numbers to physical ones  this circular log can be easily implemented as a contiguous block of logical page addresses with wraparound  realized by two page number variables  one for the    rst valid page  oldest written  and the other for the last valid page  most recently written   We next describe two maintenance operations on    ash in SkimpyStash  namely  compaction and garbage collection  Compaction is helpful in improving lookup latencies by reducing number of    ash reads when searching bucket  Garbage collection is necessary to reclaim storage on    ash and is a consequence of    ash being used in a log structured manner  Compaction to Reduce Flash Reads during Lookups In SkimpyStash   a lookup in a HT directory bucket involves following the chain of key value records on    ash  For a chain length of c records in a bucket  this involves an average of c 2    ash reads  Over time  as keys are inserted into a bucket and earlier inserted keys are updated  the chain length on    ash for this bucket keeps Figure 7  Diagram illustrating the effect of compaction procedure on the organization of a bucket chain on    ash in SkimpyStash  increasing and degrading lookup times  We address this by periodically compacting the chain on    ash in a bucket by placing the valid keys in that chain contiguously on one or more    ash pages that are appended to the tail of the log  Thus  if m key value pairs can be packed onto a single    ash page  on the average   the number of    ash reads required to search for a key in a bucket of k records is k  2m  on the average and at most    k m    in the worst case  The compaction operations proceed over time in a bucket as follows  Initially  as key value pairs are added at different times to a bucket  they appear on different    ash pages and are chained together individually on    ash  When enough valid records accumulate in a bucket to    ll a    ash page  say  m of them   they are compacted and appended on a new    ash page at the tail of the log     the chain is now of a single    ash page size and requires one    ash read  instead of m  to search fully  Thereafter  as further keys are appended to a bucket  they will be chained together individually and appear before the compacted group of keys in the chain  Over time  enough new records may accumulate in the bucket to allow them to be compacted to a second    ash page  and so the process repeats  At any given time  the chain on    ash for each bucket now begins with a chained sequence of individual records followed by groups of compacted records  each group appearing on the same    ash page   This organization of a bucket chain as a result of compaction is illustrated in Figure 7  When a key value pair size is relatively small  say 64 bytes as in the storage deduplication application  there may not be enough records in a bucket to    ll a    ash page  since this number is  roughly  upper bounded by the parameter k  In this case  we may reap the same bene   ts of compaction by applying the procedure to groups of chains in multiple buckets at a time  The compaction operation  as described above  will lead to orphaned   or  garbage  records on the    ash log  Moreover  garbage records also accumulate in the log as a result of key update and delete operations  These need to be garbage collected on    ash as we describe next  Garbage Collection Garbage records  holes  accumulate in the log as a result of compaction and key update delete operations  These operations creategarbage records corresponding to all previous versions of respective keys  When a certain con   gurable fraction of garbage accumulates in the log  in terms of space occupied   a cleaning operation is performed to clean and compact the log  The cleaning operation considers currently used    ash pages in oldest    rst order and deallocates them in a way similar to garbage collection in log structured    le systems  26   One each page  the sequence of key value pairs are scanned to determine whether they are valid or not  The classi   cation of a key value pair record on    ash follows from doing a lookup on the respective key starting from the HT directory     if this record is the same as that returned by the lookup  then it is valid  if it appears later in the chain than a valid record for that key  then this record is invalid and corresponds to an obsolete version of they key  otherwise  the record is orphaned and cannot be reached by following pointers from the HT directory  this may happen because of the compaction procedure  for example   When an orphaned record is encountered at the head of the log  it is skipped and the head position of the log is advanced to the next record  From the description of the key insertion procedure in Section 4 5  it follows that the    rst record in each bucket chain  the one pointed to from the HT directory slot  is the most recently inserted record  while the last record in the chain is the earliest inserted record in that bucket  Hence  the last record in a bucket chain will be encountered    rst during the garbage collection process and it may be a valid or invalid  obsolete version of the respective key  record  A valid record needs to be reinserted at the tail of the log while an invalid record can be skipped  In either case  the next pointer in its predecessor record in the chain would need to be updated  Since we want to avoid in place updates  random writes  on    ash  this requires relocating the predecessor record and so forth all the way to the    rst record in the chain  This effectively leads to the design decision of garbage collecting entire bucket chains on    ash at a time  In summary  when the last record in a bucket chain is encountered in the log during garbage collection  all valid records in that chain are compacted and relocated to the tail of the log  This garbage collection strategy has two bene   ts      First  the writing of an entire chain of records in a bucket to the tail of the log also allows them to be compacted and placed contiguously on one or more    ash pages and helps to speed up the lookup operations on those keys  as explained above in the context of compaction operations  and     Second  since garbage  orphaned  records are created further down the log between the  current  head and tail  corresponding to the locations of all records in the chain before relocation   this helps to speedup the garbage collection process for the respective pages when they are encountered later  since orphaned records can be simply discarded   We investigate the impact of compaction and garbage collection on system throughput in Section 5  4 7 Crash Recovery SkimpyStash    s persistency guarantee enables it to recover from system crashes due to power failure or other reasons  Because the system logs all key value write operations to    ash  it is straightforward to rebuild the HT directory in RAM by scanning all valid    ash pages on    ash  Recovery using this method can take some time  however  depending on the total size of valid    ash pages that need to be scanned and the read throughput of    ash memory  If crash recovery needs to be executed faster so as to support    near  real time recovery  then it is necessary to checkpoint the Trace Total get  get set Avg  size  bytes  set ops ratio Key Value Xbox 5 5 millions 7 5 1 92 1200 Dedup 40 millions 2 2 1 20 44 Table 1  Properties of the two traces used in the performance evaluation of SkimpyStash  RAM HT directory periodically into    ash  in a separate area from the key value pair logs   Then  recovery involves reading the last written HT directory checkpoint from    ash and scanning key value pair logged    ash pages with timestamps after that and inserting them into the restored HT directory  During the operation of checkpointing the HT directory  all insert operations into it will need to be suspended  but the read operations can continue   We use a temporary  small in RAM hash table to provide index for the interim items and log them to    ash  After the checkpointing operation completes  key value pairs from the    ash pages written in the interim are inserted into the HT directory  Key lookup operations  upon missing in the HT directory  will need to check in these    ash pages  via the small additional hash table  until the later insertions into HT directory are complete  The    ash garbage collection thread is suspended during the HT directory checkpointing operation  since the HT directory entries cannot be modi   ed during this time  5  EVALUATION We evaluate SkimpyStash on real world traces obtained from the two applications described in Section 3  5 1 C  Implementation We have prototyped SkimpyStash in approximately 3000 lines of C  code  MurmurHash  4  is used to realize the hash functions used in our implementation to compute hash table directory indices and bloom    lter lookup positions  different seeds are used to generate different hash functions in this family  The metadata store on    ash is maintained as a    le in the    le system and is created opened in non buffered mode so that there are no buffering caching prefetching effects in RAM from within the operating system  The ReaderWriterLockSlim and Monitor classes in  NET 3 0 framework  1  are used to implement the concurrency control solution for multi threading  5 2 Evaluation Platform and Datasets We use a standard server con   guration to evaluate the performance of SkimpyStash  The server runs Windows Server 2008 R2 and uses an Intel Core2 Duo E6850 3GHz CPU  4GB RAM  and fusionIO 160GB    ash drive  2  attached over PCIe interface  We compare the throughput  get set operations per sec  on the two traces described in Table 1  We described two real world applications in Section 3 that can use SkimpyStash as an underlying persistent key value store  Data traces obtained from real world instances of these applications are used to drive and evaluate the design of SkimpyStash  Xbox LIVE Primetime trace We evaluate the performance of SkimpyStash on a large trace of get set key operations obtained from real world instances of the Microsoft Xbox LIVE Primetime online multiplayer game  7   In this application  the key is a dot separated sequence of strings with total length averaging 94 characters and the value averages around 1200 bytes  The ratio of get operations to set set operations is about 7 5 1 0 0 2 0 4 0 6 0 8 1 1 2 4 8 16 32 Stddev mean of bucket sizes Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 8  Xbox trace  Relative variation in bucket sizes  standarddeviation mean  for different values of average bucket size  k  for the base and enhanced designs   Behavior on dedup trace is similar   0 500 1000 1500 2000 2500 1 2 4 8 16 32 Avg  lookup time  usec  Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 9  Xbox trace  Average lookup  get  time for different values of average bucket size  k  for the base and enhanced designs  The enhanced design reduces lookup times by factors of 6x 24x as k increases  Storage Deduplication trace We have built a storage deduplication analysis tool that can crawl a root directory  generate the sequence of chunk hashes for a given average chunk hash size  and compute the number of deduplicated chunks and storage bytes  The enterprise data backup trace we use for evaluations in this paper was obtained by our storage deduplication analysis tool using 4KB chunk sizes  The trace contains 27 748 824 total chunks and 12 082 492 unique chunks  Using this  we obtain the ratio of get operations to set operations in the trace as 2 2 1  In this application  the key is a 20 byte SHA 1 hash of the corresponding chunk and the value is the meta data for the chunk  with key value pair total size upper bounded by 64 bytes  The properties of the two traces are summarized in Table 1  Both traces include get operations on keys that have not been set earlier in the trace   Such get operations will return null   This is an inherent property of the nature of the application  hence we play the traces    as is  to evaluate throughput in operations per second  5 3 Performance Evaluation We evaluate the performance impact of our design decisions and obtain ballpark ranges on lookup times and throughput  get set operations sec  for SkimpyStash on the Xbox LIVE Primetime online multi player game and storage deduplication application traces  from Table 1   We disable the log compaction procedure for all but the last set of experiments  In the last set of experiments  we investigate the impact of garbage collection  which also does compaction  on system throughput  0 20 40 60 80 100 1 2 4 8 16 32 Avg  lookup time  usec  Avg  keys per bucket  k   log scale  Dedup trace Base design Enhanced Design Figure 10  Dedup trace  Average lookup  get  time for different values of average bucket size  k  for the base and enhanced designs  The enhanced design reduces lookup times by factors of 1 5x 2 5x as k increases  Impact of load balancing on bucket sizes  In the base design  hashing keys to single buckets can lead to wide variations in the chain length on    ash in each bucket  and this translates to wide variations in lookup times  We investigate how two choice based load balancing of keys to buckets can help reduce variance among bucket sizes  In Figure 8  we plot the relative variation in the bucket sizes  standard deviation mean of bucket sizes  for different values of k   1  2  4  8  16  32 on the Xbox trace  the behavior on dedup trace is similar   This metric is about 1 4x 6x times better for the enhanced design than for the based design on both traces  The gap starts at about 1 4x for the case k   1 and increases to about 6x for the case k   32  This provides conclusive evidence that two choice based load balancing is an effective strategy for reducing variations in bucket sizes  Key lookup latencies  The two ideas of load balancing across buckets and using a bloom    lter per bucket help to signi   cantly reduce average key lookup times in the enhanced design  with the gains increasing as the average bucket size parameter k increases  At a value of k   8  the average lookup time in the enhanced design is 20   sec for the Xbox trace and 12   sec for the dedup trace  In Figures 9 and 10  we plot the average lookup time for different values of k   1  2  4  8  16  32 on the two traces respectively  As the value of k increases  the gains in the enhanced design increase from 6x to 24x for the Xbox trace and from 1 5x to 2 5x for the dedup trace  The gains are more for the Xbox trace since that trace has many updates to earlier inserted keys  while the dedup trace has none   hence chains accumulate garbage records and get longer over time and bloom    lters help even more to speedup lookups on non existing keys  by avoiding searching the entire chain on    ash   Throughput  get set operations sec   The enhanced design achieves throughputs in the range of 10 000 69 000 ops sec on the Xbox trace and 34 000 165 000 ops sec on the dedup trace  with throughputs decreasing  as expected  with increasing values of k  This is shown in Figures 11 and 12  The throughput gains for the enhanced design over the base design are in the range of 3 5x 20x on the Xbox trace and 1 3x 2 5x on the dedup trace  with the gains increasing as the parameter k increases in value  These trends are in line with that of average lookup times  The higher throughput of SkimpyStash on the dedup trace can be explained as follows  The write mix per unit operation  get or set  in the dedup trace is about 2 65 times that of the Xbox trace  However  since the key value pair size is about 20 times smaller for the dedup trace  the number of syncs to stable storage per write0 10000 20000 30000 40000 50000 60000 70000 1 2 4 8 16 32 Throughput  get set ops sec  Avg  keys per bucket  k   log scale  Xbox trace Base design Enhanced Design Figure 11  Xbox trace  Throughput  get set ops sec  for different values of average bucket size  k  for the base and enhanced designs  The enhanced design improves throughput by factors of 3 5x 20x as k increases  0 20000 40000 60000 80000 100000 120000 140000 160000 1 2 4 8 16 32 Throughput  get set ops sec  Avg  keys per bucket  k   log scale  Dedup trace Base design Enhanced Design Figure 12  Dedup trace  Throughput  get set ops sec  for different values of average bucket size  k  for the base and enhanced designs  The enhanced design improves throughput by factors of 1 3x 2 5x as k increases  operation is about 20 times less  Overall  the number of syncs to stable storage per unit operation is about 7 6 times less for the dedup trace  Moreover  bucket chains get longer in the Xbox trace due to invalid  garbage  records accumulating from key updates  For these reasons  SkimpyStash obtains higher throughputs on the dedup trace  In addition  since the dedup trace has no key update operations  the lookup operation can be avoided during key inserts for the dedup trace  this also contributes to the higher throughput of dedup trace over the Xbox trace  Garbage collection can help to improve performance on the Xbox trace  as we discuss next  Impact of garbage collection activity  We study the impact of garbage collection  GC  activity on system throughput  ops sec  and lookup times in SkimpyStash  The storage deduplication application does not involve updates to chunk metadata  hence we evaluate the impact of garbage collection on the Xbox trace  We    x an average bucket size of k   8 for this set of experiments  The aggressiveness of the garbage collection activity is controlled by a parameter g which is</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdhp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#databases_on_new_hardware"/>
        <doc>Design and Evaluation of Main Memory Hash Join Algorithms for Multi core CPUs ### Spyros Blanas Yinan Li Jignesh M  Patel University of Wisconsin   Madison  sblanas  yinan  jignesh  cs wisc edu ABSTRACT The focus of this paper is on investigating e   cient hash join algorithms for modern multi core processors in main memory environments  This paper dissects each internal phase of a typical hash join algorithm and considers di   erent alternatives for implementing each phase  producing a family of hash join algorithms  Then  we implement these main memory algorithms on two radically di   erent modern multiprocessor systems  and carefully examine the factors that impact the performance of each method  Our analysis reveals some interesting results     a very simple hash join algorithm is very competitive to the other more complex methods  This simple join algorithm builds a shared hash table and does not partition the input relations  Its simplicity implies that it requires fewer parameter settings  thereby making it far easier for query optimizers and execution engines to use it in practice  Furthermore  the performance of this simple algorithm improves dramatically as the skew in the input data increases  and it quickly starts to outperform all other algorithms  Based on our results  we propose that database implementers consider adding this simple join algorithm to their repertoire of main memory join algorithms  or adapt their methods to mimic the strategy employed by this algorithm  especially when joining inputs with skewed data distributions  Categories and Subject Descriptors H 2 4   Database Management   Systems   Query processing  Relational databases General Terms Algorithms  Design  Performance Keywords hash join  multi core  main memory Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  ### 1  INTRODUCTION Large scale multi core processors are imminent  Modern processors today already have four or more cores  and for the past few years Intel has been introducing two more cores per processor roughly every 15 months  At this rate  it is not hard to imagine running database management systems  DBMSs  on processors with hundreds of cores in the near future  In addition  memory prices are continuing to drop  Today 1TB of memory costs as little as  25 000  Consequently  many databases now either    t entirely in main memory  or their working set is main memory resident  As a result  many DBMSs are becoming CPU bound  In this evolving architectural landscape  DBMSs have the unique opportunity to leverage the inherent parallelism that is provided by the relational data model  Data is exposed by declarative query languages to user applications and the DBMS is free to choose its execution strategy  Coupled with the trend towards impending very large multi cores  this implies that DBMSs must carefully rethink how they can exploit the parallelism that is provided by the modern multi core processors  or DBMS performance will stall  A natural question to ask then is whether there is anything new here  Beginning about three decades ago  at the inception of the    eld of parallel DBMSs  the database community thoroughly examined how a DBMS can use various forms of parallelism  These forms of parallelism include pure sharednothing  shared memory  and shared disk architectures  17   If the modern multi core architectures resemble any of these architectural templates  then we can simply adopt the methods that have already been designed  In fact  to a large extent this is the approach that DBMSs have haven taken towards dealing with multi core machines  Many commercial DBMSs simply treat a multi core processor as a symmetric multi processor  SMP  machine  leveraging previous work that was done by the DBMS vendors in reaction to the increasing popularity of SMP machines decades ago  These methods break up the task of a single operation  such as an equijoin  into disjoint parts and allow each processor  in an SMP box  to work on each part independently  At a high level  these methods resemble variations of query processing techniques that were developed for parallel shared nothing architectures  6   but adapted for SMP machines  In most commercial DBMSs  this approach is re   ected across the entire design process  ranging from system internals  join processing  for example  to their pricing model  which is frequently done by scaling the SMP pricing model  On the other hand  open source DBMSs havelargely ignored multi core processing and generally dedicate a single thread process to each query  The design space for modern high performance main memory join algorithms has two extremes  One extreme of this design space focuses on minimizing the number of processor cache misses  The radix based hash join algorithm  2  is an example of a method in this design class  The other extreme is to focus on minimizing processor synchronization costs  In this paper we propose a    no partitioning    hash join algorithm that does not partition the input relations to embody an example of a method in this later design space  A crucial question that we ask and answer in this paper is what is the impact of these two extreme design points in modern multi core processors for main memory hash join algorithms  A perhaps surprising answer is that for modern multi core architectures  in many cases the right approach is to focus on reducing the computation and synchronization costs  as modern processors are very e   ective in hiding cache miss latencies via simultaneous multi threading  For example  in our experiments  the    no partitioning    hash join algorithm far outperforms the radix join algorithm when there is skew in the data  which is often the case in practice   even while it incurs many more processor cache and TLB misses  Even with uniform data  the radix join algorithm only outperforms the    no partitioning    algorithm on a modern Intel Xeon when the parameters for the radix join algorithm are set at or near their optimal setting  In contrast  the nonpartitioned algorithm is    parameter free     which is another important practical advantage  Re   ecting on the previous work in this area  one can observe that the database community has focused on optimizing query processing methods to reduce the number of processor cache and TLB misses  We hope that this paper opens up a new discussion on the entire design space for multi core query processing techniques  and incites a similar examination of other aspects of query processing beyond the single hash join operation that we discuss in this paper  This paper makes three main contributions  First  we systematically examine the design choices available for each internal phase of a canonical main memory hash join algorithm     namely  the partition  build  and probe phases     and enumerate a number of possible multi core hash join algorithms based on di   erent choices made in each of these phases  We then evaluate these join algorithms on two radically di   erent architectures and show how the architectural di   erences can a   ect performance  Unlike previous work that has often focused on just one architecture  our use of two radically different architectures lets us gain deeper insights about hash join processing on multi core processors  To the best of our knowledge  this is the    rst systematic exploration of multiple hash join techniques that spans multi core architectures  Second  we show that an algorithm that does not do any partitioning  but simply constructs a single shared hash table on the build relation often outperforms more complex algorithms  This simple    no partitioning    hash join algorithm is robust to sub optimal parameter choices by the optimizer  and does not require any knowledge of the characteristics of the input to work well  To the best of our knowledge  this simple hash join technique di   ers from what is currently implemented in existing DBMSs for multi core hash join processing  and o   ers a tantalizingly simple  e   cient  and robust technique for implementing the hash join operation  Finally  we show that the simple    no partitioning    hash join algorithm takes advantage of intrinsic hardware optimizations to handle skew  As a result  this simple hash join technique often bene   ts from skew and its relative performance increases as the skew increases  This property is a big advancement over the state of the art methods  as it is important to have methods that can gracefully handle skew in practice  8   The remainder of this paper is organized as follows  The next section covers background information  The hash join variants are presented in Section 3  Experimental results are described in Section 4  and related work is discussed in Section 5  Finally  Section 6 contains our concluding remarks  2  THE MULTI CORE LANDSCAPE In the last few years alone  more than a dozen di   erent multi core CPU families have been introduced by CPU vendors  These new CPUs have ranged from powerful dual CPU systems on the same die to prototype systems of hundreds of simple RISC cores  This new level of integration has lead to architectural changes with deep impact on algorithm design  Although the    rst multi core CPUs had dedicated caches for each core  we now see a shift towards more sharing at the lower levels of the cache hierarchy and consequently the need for access arbitration to shared caches within the chip  A shared cache means better single threaded performance  as one core can utilize the whole cache  and more opportunities for sharing among cores  However  shared caches also increase con   ict cache misses due to false sharing  and may increase capacity cache misses  if the cache sizes don   t increase proportionally to the number of cores  One idea that is employed to combat the diminishing returns of instruction level parallelism is simultaneous multithreading  SMT   Multi threading attempts to    nd independent instructions across di   erent threads of execution  instead of detecting independent instructions in the same thread  This way  the CPU will schedule instructions from each thread and achieve better overall utilization  increasing throughput at the expense of per thread latency  We brie   y consider two modern architectures that we subsequently use for evaluation  At one end of the spectrum  the Intel Nehalem family is an instance of Intel   s latest microarchitecture that o   ers high single threaded performance because of its out of order execution and on demand frequency scaling  TurboBoost   Multi threaded performance is increased by using simultaneous multi threading  HyperThreading   At the other end of the spectrum  the Sun UltraSPARC T2 has 8 simple cores that all share a single cache  This CPU can execute instructions from up to 8 threads per core  or a total of 64 threads for the entire chip  and extensively relies on simultaneous multi threading to achieve maximum throughput  3  HASH JOIN IMPLEMENTATION In this section  we consider the anatomy of a canonical hash join algorithm  and carefully consider the design choices that are available in each internal phase of a hash join algorithm  Then using these design choices  we categorize various previous proposals for multi core hash join processing  In the following discussion we also present information about some of the implementation details  as they often have a signi   cant impact on the performance of the technique that is described A hash join operator works on two input relations  R and S  We assume that  R     S   A typical hash join algorithm has three phases  partition  build  and probe  The partition phase is optional and divides tuples into distinct sets using a hash function on the join key attribute  The build phase scans the relation R and creates an in memory hash table on the join key attribute  The probe phase scans the relation S  looks up the join key of each tuple in the hash table  and in the case of a match creates the output tuple s   Before we discuss the alternative techniques that are available in each phase of the join algorithm  we brie   y digress to discuss the impact of the latch implementation on the join techniques  As a general comment  we have found that the latch implementation has a crucial impact on the overall join performance  In particular  when using the pthreads mutex implementation  several instructions are required to acquire and release an uncontended latch  If there are millions of buckets in a hash table  then the hash collision rate is small  and one can optimize for the expected case  latches being free  Furthermore  pthread mutexes have signi   cant memory footprint as each requires approximately 40 bytes  If each bucket stores a few  key  record id  pairs  then the size of the latch array may be greater than the size of the hash table itself  These characteristics make mutexes a prohibitively expensive synchronization primitive for buckets in a hash table  Hence  we implemented our own 1 byte latch for both the Intel and the Sun architectures  using the atomic primitives xchgb and ldstub  respectively  Protecting multiple hash buckets with a single latch to avoid cache thrashing did not result in signi   cant performance improvements even when the number of partitions was high  3 1 Partition phase The partition phase is an optional step of a hash join algorithm  if the hash table for the relation R    ts in main memory  If one partitions both the R and S relations such that each partition    ts in the CPU cache  then the cache misses that are otherwise incurred during the subsequent build and probe phases are almost eliminated  The cost for partitioning both input relations is incurring additional memory writes for each tuple  Work by Shatdal et al   16  has shown that the runtime cost of the additional memory writes during partitioning phase is less than the cost of missing in the cache     as a consequence partitioning improves overall performance  Recent work by Cieslewicz and Ross  4  has explored partitioning performance in detail  They introduce two algorithms that process the input once in a serial fashion and do not require any kind of global knowledge about the characteristics of the input  Another recent paper  11  describes a parallel implementation of radix partitioning  2  which gives impressive performance improvements on a modern multi core system  This implementation requires that the entire input is available upfront and will not produce any output until the last input tuple has been seen  We experiment with all of these three partitioning algorithms  and we brie   y summarize each implementation in Sections 3 1 1 and 3 1 2  In our implementation  a partition is a linked list of output bu   ers  An output bu   er is fully described by four elements  an integer specifying the size of the data block  a pointer to the start of the data block  a pointer to the free space inside the data block and a pointer to the next output bu   er that is initially set to zero  If a bu   er over   ows  then we add an empty output bu   er at the start of the list  and we make its next pointer point to the bu   er that over   owed  Locating free space is a matter of checking the    rst bu   er in the list  Let p denote the desired number of partitions and n denote the number of threads that are processing the hash join operation  During the partitioning phase  all threads start reading tuples from the relation R  via a cursor  Each thread works on a large batch of tuples at a time  so as to minimize synchronization overheads on the input scan cursor  Each thread examines a tuple  then extracts the key k  and      nally computes the partitioning hash function hp k   Next  it then writes the tuple to partition Rhp k  using one of the algorithms we describe below  When the R cursor runs out of tuples  the partitioning operation proceeds to process the tuples from the S relation  Again  each tuple is examined  the join key k is extracted and the tuple is written to the partition Shp k    The partitioning phase ends when all the S tuples have been partitioned  Note that we classify the partitioning algorithms as    nonblocking    if they produce results on the    y and scan the input once  in contrast to a    blocking    algorithm that produces results after bu   ering the entire input and scanning it more than once  We acknowledge that the join operator overall is never truly non blocking  as it will block during the build phase  The distinction is that the non blocking algorithms only block for the time that is needed to scan and process the smaller input  and  as we will see in Section 4 3  this a very small fraction of the overall join time  3 1 1 Non blocking algorithms The    rst partitioning algorithm creates p shared partitions among all the threads  The threads need to synchronize via a latch to make sure that the writes to a shared partition are isolated from each other  The second partitioning algorithm creates p     n partitions in total and each thread is assigned a private set of p partitions  Each thread then writes to its local partitions without any synchronization overhead  When the input relation is depleted  all threads synchronize at a barrier to consolidate the p     n partitions into p partitions  The bene   t of creating private partitions is that there is no synchronization overhead on each access  The drawbacks  however  are  a  many partitions are created  possibly so many that the working set of the algorithm no longer    ts in the data cache and the TLB   b  at the end of the partition phase some thread has to chain n private partitions together to form a single partition  but this operation is quick and can be parallelized  3 1 2 Blocking algorithm Another partitioning technique is the parallel multi pass radix partitioning algorithm described by Kim et al   11   The algorithm begins by having the entire input available in a contiguous block of memory  Each thread is responsible for a speci   c memory region in that contiguous block  A histogram with p     n bins is allocated and the input is then scanned twice  During the    rst scan  each thread scans all the tuples in the memory region assigned to it  extracts the key k and then computes the exact histogram of the hash values hp k  for this region  Thread i      0  n     1  stores the number of tuples it encountered that will hash to partition j      0  p   1  in histogram bin j   n i  At the end of the scan  all the n threads compute the pre   x sum on the histogramin parallel  The pre   x sum can now be used to point to the beginning of each output partition for each thread in the single shared output bu   er  Finally  each thread performs a second scan of its input region  and uses hp to determine the output partition  This algorithm is recursively applied to each output partition for as many passes as requested  The bene   t of radix partitioning is that it makes few cache and TLB misses  as it bounds the number of output destinations in each pass  This particular implementation has the bene   t that  by scanning the input twice for each pass  it computes exactly how much output space will be required for each partition  and hence avoids the synchronization overhead that is associated with sharing an output bu   er  Apart from the drawbacks that are associated with any blocking algorithm when compared to a non blocking counterpart  this implementation also places a burden on the previous operator in a query tree to produce the compact and contiguous output format that the radix partitioning requires as input  E   ciently producing a single shared output bu   er is a problem that has been studied before  5   3 2 Build phase The build phase proceeds as follows  If the partition phase was omitted  then all the threads are assigned to work on the relation R  If partitioning was done  then each thread i is assigned to work on partitions Ri 0   n  Ri 1   n  Ri 2   n  etc  For example  a machine with four cores has n   4  and thread 0 would work on partitions R0  R4  R8       thread 1 on R1  R5  R9       etc  Next  an empty hash table is constructed for each partition of the input relation R  To reduce the number of cache misses that are incurred during the next  probe  phase  each bucket of this hash table is sized so that it    ts on a few cache lines  Each thread scans every tuple t in its partition  extracts the join key k  and then hashes this key using a hash function h      Then  the tuple t is appended to the end of the hash bucket h k   creating a new hash bucket if necessary  If the partition phase was omitted  then all the threads share the hash table  and writes to each hash bucket have to be protected by a latch  The build phase is over when all the n threads have processed all the assigned partitions  3 3 Probe phase The probe phase schedules work to the n threads in a manner similar to the scheduling during the build phase  described above  Namely  if no partitioning has been done  then all the threads are assigned to S  and they synchronize before accessing the read cursor for S  Otherwise  the thread i is assigned to partitions Si 0   n  Si 1   n  Si 2   n  etc  During the probe phase  each thread reads every tuple s from its assigned partition and extracts the key k  It then checks if the key of each tuple r stored in hash bucket h k  matches k  This check is necessary to    lter out possible hash collisions  If the keys match  then the tuples r and s are joined to form the output tuple  If the output is materialized  it is written to an output bu   er that is private to the thread  Notice that there is parallelism even inside the probe phase  looking up the key for each tuple r in a hash bucket and comparing it to k can be parallelized with the construction of the output tuple  which primarily involves shu   ing bytes from tuples r and s   See Section 4 10 for an experiment that explores this further   3 4 Hash Join Variants The algorithms presented above outline an interesting design space for hash join algorithms  In this paper  we focus on the following four hash join variations  1  No partitioning join  An implementation where partitioning is omitted  This implementation creates a shared hash table in the build phase  2  Shared partitioning join  The    rst non blocking partitioning algorithm of Section 3 1 1  where all the threads partition both input sources into shared partitions  Synchronization through a latch is necessary before writing to the shared partitions  3  Independent partitioning join  The second nonblocking partitioning algorithm of Section 3 1 1  where all the threads partition both sources and create private partitions  4  Radix partitioning join  An implementation where each input relation is stored in a single  contiguous memory region  Then  each thread participates in the radix partitioning  as described in Section 3 1 2  4  EXPERIMENTAL EVALUATION We have implemented the hash join algorithms described in Section 3 4 in a stand alone C   program  The program    rst loads data from the disk into main memory  Data is organized in memory using traditional slotted pages  The join algorithms are run after the data is loaded in memory  Since the focus of this work in on memory resident datasets  we do not consider the time to load the data into main memory and only report join completion times  For our workload  we wanted to simulate common and expensive join operations in decision support environments  The execution of a decision support query in a data warehouse typically involves multiple phases  First  one or more dimension relations are reduced based on the selection constraints  Then  these dimension relations are combined into an intermediate one  which is then joined with a much larger fact relation  Finally  aggregate statistics on the join output are computed and returned to the user  For example  in the TPC H decision support benchmark  this execution pattern is encountered in at least 15 of the 22 queries  We try to capture the essence of this operation by focusing on the most expensive component  namely the join operation between the intermediate relation R  the outcome of various operations on the dimension relations  with a much larger fact relation S  To allow us to focus on the core join performance  we initially do not consider the cost of materializing Intel Nehalem CPU Xeon X5650   2 67GHz Cores 6 Contexts per core 2 Cache size  sharing 12MB L3  shared Memory 3x 4GB DDR3 Sun UltraSPARC T2 CPU UltraSPARC T2   1 2GHz Cores 8 Contexts per core 8 Cache size  sharing 4MB L2  shared Memory 8x 2GB DDR2 Table 1  Platform characteristics 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 20 40 60 80 100 120 140 160 180 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 1  Cycles per output tuple for the uniform dataset  the output in memory  adopting a similar method as previous work  7  11   In later experiments  see Section 4 8   we consider the e   ect of materializing the join result     in these cases  the join result is created in main memory and not    ushed to disk  We describe the synthetic datasets that we used in the next section  Section 4 1   In Section 4 2 we give details about the hardware that we used for our experiments  We continue with a presentation of the results in Sections 4 3 and 4 4  We analyze the results further in Sections 4 5 through 4 7  We present results investigating the e   ect of output materialization  and the sensitivity to input sizes and selectivities in Sections 4 8 through 4 10  4 1 Dataset We experimented with three di   erent datasets  which we denote as uniform  low skew and high skew  respectively  We assume that the relation R contains the primary key and the relation S contains a foreign key referencing tuples in R  In all the datasets we    x the cardinalities of R to 16M tuples and S to 256M tuples 1   We picked the ratio of R to S to be 1 16 to mimic the common decision support settings  We experiment with di   erent ratios in Section 4 9  In our experiments both keys and payloads are eight bytes each  Each tuple is simply a  key  payload  pair  so tuples are 16 bytes long  Keys can either be the values themselves  if the key is numeric  or an 8 byte hash of the value in the case of strings  We chose to represent payloads as 8 bytes for two reasons   a  Given that columnar storage is commonly used in data warehouses  we want to simulate storing  key  value  or  key  record id  pairs in the hash table  and  b  make comparisons with existing work  i e   11  4   easier  Exploring alternative ways of constructing hash table entries is not a focus of this work  but has been explored before  15   For the uniform dataset  we create tuples in the relation S such that each tuple matches every key in the relation R with equal probability  For the skewed datasets  we added skew to the distribution of the foreign keys in the relation S   Adding skew to the relation R would violate the primary key constraint   We created two skewed datasets  for two di   erent s values of the Zipf distribution  low skew with s   1 05 and high skew with s   1 25  Intuitively  the most 1 Throughout the paper  M 2 20 and K 2 10   popular key appears in the low skew dataset 8  of the time  and the ten most popular keys account for 24  of the keys  In comparison  in the high skew dataset  the most popular key appears 22  of the time  and the ten most popular keys appear 52  of the time  In all the experiments  the hash buckets that are created during the build phase have a    xed size  they always have 32 bytes of space for the payload  and 8 bytes are reserved for the pointer that points to the next hash bucket in case of over   ow  These numbers were picked so that each bucket    ts in a single  last level cache line for both the architectures  We size the hash table appropriately so that no over   ow occurs  4 2 Platforms We evaluated our methods on two di   erent architectures  the Intel Nehalem and the Sun UltraSPARC T2  We describe the characteristics of each architecture in detail below  and we summarize key parameters in Table 1  The Intel Nehalem microarchitecture is the successor of the Intel Core microarchitecture  All Nehalem based CPUs are superscalar processors and exploit instruction level parallelism by using out of order execution  The Nehalem family supports multi threading  and allows two contexts to execute per core  For our experiments  we use the six core Intel Xeon X5650 that was released in Q1 of 2010  This CPU has a uni   ed 12MB  16 way associative L3 cache with a line size of 64 bytes  This L3 cache is shared by all twelve contexts executing on the six cores  Each core has a private 256KB  8 way associative L2 cache  with a line size of 64 bytes  Finally  private 32KB instruction and data L1 caches connect to each core   s load store units  The Sun UltraSPARC T2 was introduced in 2007 and relies heavily on multi threading to achieve maximum throughput  An UltraSPARC T2 chip has eight cores and each core has hardware support for eight contexts  UltraSPARC T2 does not feature out of order execution  Each core has a single instruction fetch unit  a single    oating point unit  a single memory unit and two arithmetic units  At every cycle  each core executes at most two instructions  each taken from two di   erent contexts  Each context is scheduled in a round robin fashion every cycle  unless the context has ini 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 50 100 150 200 250 300 350 400 450 500 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 2  Cycles per output tuple for the low skew dataset  tiated a long latency operation  such as a memory load that caused a cache miss  and has to wait for the outcome  At the bottom of the cache hierarchy of the UltraSPARC T2 chip lies a shared 4MB  16 way associative write back L2 cache  with a line size of 64 bytes  To maximize throughput  the shared cache is physically split into eight banks  Therefore  up to eight cache requests can be handled concurrently  provided that each request hits a di   erent bank  Each core connects to this shared cache through a non blocking  pipelined crossbar  Finally  each core has a 8KB  4 way associative write through L1 data cache with 16 bytes per cache line that is shared by all the eight hardware contexts  Overall  in the absence of arbitration delays  the L2 cache hit latency is 20 cycles  4 3 Results We start with the uniform dataset  In Figure 1  we plot the average number of CPU cycles that it takes to produce one output tuple  without actually writing the output  for a varying number of partitions   Note that to convert the CPU cycles to wall clock time  we simply divide the CPU cycles by the corresponding clock rate shown in Table 1   The horizontal axis shows the di   erent join algorithms  bars    No        Shared        Independent      corresponding to the    rst three hash join variants described in Section 3 4  For the radix join algorithm  we show the best result across any number of passes  bars marked    Radix best      Notice that we assume that the optimizer will always be correct and pick the optimal number of passes  Overall  the build phase takes a very small fraction of the overall time  regardless of the partitioning strategy that is being used  across all architectures  see Figure 1   The reason for this behavior is two fold  First and foremost  the smaller cardinality of the R relation translates into less work during the build phase   We experiment with di   erent cardinality ratios in Section 4 9   Second  building a hash table is a really simple operation  it merely involves copying the input data into the appropriate hash bucket  which incurs a lot less computation than the other steps  such as the output tuple reconstruction that must take place in the probe phase  The performance of the join operation is therefore mostly determined by the time spent partitioning the input relations and probing the hash table  As can be observed in Figure 1 a  for the Intel Nehalem architecture  the performance of the non partitioned join algorithm is comparable to the optimal performance achieved by the partition based algorithms  The shared partitioning algorithm performs best when sizing partitions so that they    t in the last level cache  This    gure reveals a problem with the independent partitioning algorithm  For a high number of partitions  say 128K  each thread will create its own private bu   er  for a total of 128K     12     1 5 million output bu   ers  This high number of temporary bu   ers introduces two problems  First  it results in poor space utilization  as most of these bu   ers are    lled with very few tuples  Second  the working set of the algorithm grows tremendously  and keeping track of 1 5 million cache lines requires a cache whose capacity is orders of magnitude larger than the 12MB L3 cache  The radix partitioning algorithm is not a   ected by this problem  because it operates in multiple passes and limits the number of partition output bu   ers in each pass  Next  we experimented with the Sun UltraSPARC T2 architecture  In Figure 1 b  we see that doing no partitioning is at least 1 5X faster compared to all the other algorithms  The limited memory on this machine prevented us from running experiments with a high number of partitions for the independent partitioning algorithm because of the signi     cant memory overhead discussed in the previous paragraph  As this machine supports nearly    ve times more hardware contexts than the Intel machine  the memory that is required for bookkeeping is    ve times higher as well  To summarize our results with the uniform dataset  we see that on the Intel architecture the performance of the no partitioning join algorithm is comparable to the performance of all the other algorithms  For the Sun UltraSPARC T2  we see that the no partitioning join algorithm outperforms the other algorithms by at least 1 5X  Additionally  the no partitioning algorithm is more robust  as the performance of the other algorithms degrades if the query optimizer does not pick the optimal value for the number of partitions  4 4 Effect of skew We now consider the case when the distribution of foreign keys in the relation S is skewed  We again plot the average time to produce each tuple of the join  in machine cycles  in Figure 2 for the low skew dataset  and in Figure 3 for the high skew dataset 0 100 200 300 400 500 600 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  a  Intel Nehalem 0 100 200 300 400 500 600 700 1 64 256 512 1K 2K 4K 8K 32K 128K 64 256 512 1K 2K 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe  b  Sun UltraSPARC T2 Figure 3  Cycles per output tuple for the high skew dataset  Intel Sun Nehalem UltraSPARC T2 NO No   1 No   1 SN Indep    16 Indep    64 L2 S Shared   2048 Shared   2048 L2 R Radix   2048 Radix   2048 Table 2  Shorthand notation and corresponding partitioning strategy   number of partitions  By comparing Figure 1 with Figure 2  we notice that  when using the shared hash table  bar    No    in all graphs   performance actually improves in the presence of skew  On the other hand  the performance of the shared partitioning algorithm degrades rapidly with increasing skew  while the performance of the independent partitioning and the radix partitioning algorithms shows little change on the Intel Nehalem and degrades on the Sun UltraSPARC T2  Moving to Figure 3  we see that the relative performance of the non partitioned join algorithm increases rapidly under higher skew  compared to the other algorithms  The nonpartitioned algorithm is generally 2X faster than the other algorithms on the Intel Nehalem  and more than 4X faster than the other algorithms on the Sun UltraSPARC T2  To summarize these results  skew in the underlying join key values  data skew  manifests itself as partition size skew when using partitioning  For the shared partitioning algorithm  during the partition phase  skew causes latch contention on the partition with the most popular key s   For all partitioning based algorithms  during the probe phase  skew translates into a skewed work distribution per thread  Therefore  the overall join completion time is determined by the completion time of the partition with the most popular key   We explore this behavior further in Section 4 7 1   On the other hand  skew improves performance when sharing the hash table and not doing partitioning for two reasons  First  the no partitioning approach ensures an even work distribution per thread as all the threads are working concurrently on the single partition  This greedy scheduling strategy proves to be e   ective in hiding data skew  Second  performance increases because the hardware handles skew a lot more e   ciently  as skewed memory access patterns cause signi   cantly fewer cache misses  TLB TLB Cycles L3 Instruc  load store miss  tions miss miss partition 0 0 0 0 0 NO build 322 2 2 215 1 0 probe 15 829 862 54 762 557 0 partition 3 578 18 29 096 6 2 SN build 328 8 2 064 0 0 probe 21 717 866 54 761 505 0 partition 11 778 103 31 117 167 257 L2 S build 211 1 2 064 0 0 probe 6 144 35 54 762 1 0 partition 6 343 221 34 241 7 237 L2 R build 210 1 2 064 0 0 probe 6 152 36 54 761 1 0 Table 3  Performance counter averages for the uniform dataset  millions   4 5 Performance counters Due to space constraints  we focus on speci   c partitioning con   gurations from this section onward  We use    NO    to denote the no partitioning strategy where the hash table is shared by all threads  and we use    SN    to denote the case when we create as many partitions as hardware contexts  join threads   except we round the number of partitions up to the next power of two as is required for the radix partitioning algorithm  We use    L2    to denote the case when we create partitions to    t in the last level cache  appending     S    when partitioning with shared output bu   ers  and     R    for radix partitioning  We summarize this notation in Table 2  Notice that the L2 numbers correspond to the best performing con   guration settings in the experiment with the uniform dataset  see Figure 1   We now use the hardware performance counters to understand the characteristics of these join algorithms  In the interest of space  we only present our    ndings from a single architecture  the Intel Nehalem  We    rst show the results from the uniform dataset in Table 3  Each row indicates one particular partitioning algorithm and join phase  and each column shows a di   erent architectural event  First  notice the code path length  It takes  on average  about 55 billion instructions to complete the probe phase and an additional 50  to 65  of that for partitioning  depending on the algorithm of choice  The NO algorithm pays a high cost inTLB TLB Cycles L3 Instruc  load store miss  tions miss miss partition 0 0 0 0 0 NO build 323 3 2 215 1 0 probe 6 433 98 54 762 201 0 partition 3 577 17 29 096 6 1 SN build 329 8 2 064 0 0 probe 13 241 61 54 761 80 0 partition 36 631 79 34 941 67 106 L2 S build 210 5 2 064 0 0 probe 8 024 13 54 762 1 0 partition 5 344 178 34 241 5 72 L2 R build 209 4 2 064 0 0 probe 8 052 13 54 761 1 0 Table 4  Performance counter averages for the high skew dataset  millions   terms of the L3 cache misses during the probe phase  The partitioning phase of the SN algorithm is fast but fails to contain the memory reference patterns that arise during the probe phase in the cache  The L2 S algorithm manages to minimize these memory references  but incurs a high L3 and TLB miss ratio during the partition phase compared to the NO and SN algorithms  The L2 R algorithm uses multiple passes to partition the input and carefully controls the L3 and TLB misses during these phases  Once the cache sized partitions have been created  we see that both the L2 S and L2 R algorithms avoid incurring many L3 and TLB misses during the probe phase  In general  we see fewer cache and TLB misses across all algorithms when adding skew  in Table 4   Unfortunately  interpreting performance counters is much more challenging with modern multi core processors and will likely get worse  Processors have become a lot more complex over the last ten years  yet the events that counters capture have hardly changed  This trend causes a growing gap between the high level algorithmic insights the user expects and the speci   c causes that trigger some processor state that the performance counters can capture  In a uniprocessor  for example  a cache miss is an indication that the working set exceeds the cache   s capacity  The penalty is bringing the data from memory  an operation the costs hundreds of cycles  However  in a multi core processor  a memory load might miss in the cache because the operation touches memory that some other core has just modi   ed  The penalty in this case is looking in some other cache for the data  Although a neighboring cache lookup can be ten or a hundred times faster than bringing the data from memory  both scenarios will simply increment the cache miss counter and not record the cause of this event  To illustrate this point  let   s turn our attention to a case in Table 3 where the performance counter results can be misleading  The probe phase of the SN algorithm has slightly fewer L3 and TLB misses than the probe phase of the NO algorithm and equal path length  so the probe phase of the SN algorithm should be comparable or faster than probe phase of the NO algorithm  However  the probe phase of the NO algorithm is almost 25  faster  Another issue is latch contention  which causes neither L3 cache misses nor TLB misses  and therefore is not reported in the performance counters  For example  when comparing the uniform and high skew numbers for the L2 S algorithm  the number of the L3 cache misses during the high skew experiment is 0 2 4 6 8 10 12 0 2 4 6 8 10 12 Speedup Number of threads NO SN L2 S L2 R Figure 4  Speedup over single threaded execution  uniform dataset  30  lower than the number of the cache misses observed during the uniform experiment  However  partitioning performance worsens by more than 3X when creating shared partitions under high skew  The performance counters don   t provide clean insights into why the non partitioned algorithm exhibits similar or better performance than the other cache e   cient algorithms across all datasets  Although a cycle breakdown is still feasible at a macroscopic level where the assumption of no contention holds  for example as in Ailamaki et al   1    this experiment reveals that blindly assigning    xed cycle penalties to architectural events can lead to misleading conclusions  4 6 Speedup from SMT Modern processors improve the overall e   ciency with hardware multithreading  Simultaneous multi threading  SMT  permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures  We now evaluate the impact of SMT on the hash join algorithms  We    rst show a speedup experiment for the Intel Nehalem on the uniform dataset in Figure 4  We start by dedicating each thread to a core  and once we exceed the number of available physical cores  six for our Intel Nehalem   we then start assigning threads in a round robin fashion to the available hardware contexts  We observe that the algorithms behave very di   erently when some cores are idle  fewer than six threads  versus in the SMT region  more than six threads   With fewer than six threads all the algorithms scale linearly  and the NO algorithm has optimal speedup  With more than six threads  the NO algorithm continues to scale  becoming almost 11X faster than the single threaded version when using all available contexts  The partitioning based algorithms SN  L2 S and L2 R  however  do not exhibit this behavior  The speedup curve for these three algorithms in the SMT region either    attens completely  SN algorithm   or increases at a reduced rate  L2 R algorithm  than the nonSMT region  In fact  performance drops for all partitioning algorithms for seven threads because of load imbalance  a single core has to do the work for two threads   This imbalance can be ameliorated through load balancing  a technique that we explore in Section 4 7 1  Uniform 6 threads 12 threads Improvement NO 28 23 16 15 1 75X SN 34 04 25 62 1 33X L2 S 19 27 18 13 1 06X L2 R 14 46 12 71 1 14X High skew 6 threads 12 threads Improvement NO 9 34 6 76 1 38X SN 19 50 17 15 1 14X L2 S 38 37 44 87 0 86X L2 R 15 04 13 61 1 11X Table 5  Simultaneous multi threading experiment on the Intel Nehalem  showing billions of cycles to join completion and relative improvement  Uniform 8 threads 64 threads Improvement NO 37 30 12 64 2 95X SN 55 70 22 25 2 50X L2 S 51 62 23 86 2 16X L2 R 46 62 18 88 2 47X High skew 8 threads 64 threads Improvement NO 23 92 11 67 2 05X SN 70 52 49 54 1 42X L2 S 73 91 221 01 0 33X L2 R 66 01 43 16 1 53X Table 6  Simultaneous multi threading experiment on the Sun UltraSPARC T2  showing billions of cycles to join completion and relative improvement  We summarize the bene   t of SMT in Table 5 for the Intel architecture  and in Table 6 for the Sun architecture  For the Intel Nehalem and the uniform dataset  the NO algorithm bene   ts signi   cantly from SMT  becoming 1 75X faster  This algorithm is not optimized for cache performance  and as seen in Section 4 5  causes many cache misses  As a result  it provides more opportunities for SMT to ef     ciently overlap the memory accesses  On the other hand  the other three algorithms are optimized for cache performance to di   erent degrees  Their computation is a large fraction of the total execution time  therefore they do not bene   t signi   cantly from using SMT  In addition  we notice that the NO algorithm is around 2X slower than the L2 R algorithm without SMT  but its performance increases to almost match the L2 R algorithm performance with SMT  For the Sun UltraSPARC T2  the NO algorithm also bene   ts the most from SMT  In this architecture the code path length  i e  instructions executed  has a direct impact on the join completion time  and therefore the NO algorithm performs best both with and without SMT  As the Sun machine cannot exploit instruction parallelism at all  we see increased bene   ts from SMT compared to the Intel architecture  When comparing the high skew dataset with the uniform dataset across both architectures  we see that the improvement of SMT is reduced  The skewed key distribution incurs fewer cache misses  therefore SMT loses opportunities to hide processor pipeline stalls  4 7 Synchronization Synchronization is used in multithreaded programs to guarantee the consistency of shared data structures  In our join implementations  we use barrier synchronization when all the threads wait for tasks to be completed before they can proceed to the next task   For example  at the end of each pass of the radix partition phase  each thread has to wait until all other threads complete before proceeding   In this section  we study the e   ect of barrier synchronization on the performance of the hash join algorithm  In the interest of space  we only present results for the Intel Nehalem machine  Since the radix partitioning algorithm wins over the other partitioning algorithms across all datasets  our discussion only focuses on results for the non partitioned algorithm  NO  and the radix partitioning algorithm  L2 R   Synchronization has little impact on the non partitioned  NO  algorithm for both the uniform and the high skew datasets  regardless of the number of threads that are running  The reason for this behavior is the simplicity of the NO algorithm  First  there is no partition phase at all  and each thread can proceed independently in the probe phase  Therefore synchronization is only necessary during the build phase  a phase that takes less than 2  of the total time  see Figure 1   Second  by dispensing with partitioning  this algorithm ensures an even distribution of work across the threads  as all the threads are working concurrently on the single shared hash table  We now turn our attention to the radix partitioning algorithm  and break down the time spent by each thread  Unlike the non partitioned algorithm  the radix partitioning algorithm is signi   cantly impacted by synchronization on both the uniform and the high skew datasets  Figure 5 a  shows the time breakdown for the L2 R algorithm when running 12 threads on the Intel Nehalem machine with the high skew dataset  Each histogram in this    gure represents the execution    ow of a thread  The vertical axis can be viewed as a time axis  in machine cycles   White rectangles in these histograms represent tasks  the position of each rectangle indicates the beginning time of the task  and the height represents the completion time of this task for each thread  The gray rectangles represent the waiting time that is incurred by a thread that completes its task but needs to synchronize with the other threads before continuing  In the radix join algorithm  we can see    ve expensive operations that are synchronized through barriers   1  computing the threadprivate histogram   2  computing the global histogram   3  doing radix partitioning   4  building a hash table for each partition of the relation R  and  5  probing each hash table with a partition from the relation S  The synchronization cost of the radix partitioning algorithm accounts for nearly half of the total join completion time for some threads  The synchronization cost is so high under skew primarily because it is hard to statically divide work items into equally sized subtasks  As a result  faster threads have to wait for slower threads  For example  if threads are statically assigned to work on partitions in the probe phase  the distribution of the work assigned to the threads will invariably also be skewed  Thus  the thread processing the partition with the most popular key becomes a bottleneck and the overall completion time is determined by the completion time of the partition with the most popular keys  In Figure 5 a   this is thread 3 0 2 4 6 8 10 12 14 1 2 3 4 5 6 7 8 9 10 11 12 Cycles  billions  Thread ID work wait  a  High skew dataset 0 2 4 6 8 10 12 14 1 2 3 4 5 6 7 8 9 10 11 12 Cycles  billions  Thread ID work wait  b  High skew dataset with work stealing Figure 5  Time breakdown of the radix join  4 7 1 Load balancing If static work allocation is the problem  then how would the radix join algorithm perform under a dynamic work allocation policy and highly skewed input  To answer this question  we tweaked the join algorithm to allow the faster threads that have completed their probe phase to steal work from other slower threads  In our implementation  the unit of work is a single partition  In doing so  we slightly increase the synchronization cost because work queues need to now be protected with latches  but we balance the load better  In Figure 5 b  we plot the breakdown of the radix partitioning algorithm  L2 R  using this work stealing policy when running on the Intel Nehalem machine with the high skew dataset  Although the work is now balanced almost perfectly for the smaller partitions  the partitions with the most popular keys are still a bottleneck  In the high skew dataset  the most popular key appears 22  of the time  and thread 3 in this case has been assigned only a single partition which happened to correspond to the most popular key  In comparison  for this particular experiment  the NO algorithm can complete the join in under 7 billion cycles  Table 4   and hence is 1 9X faster  An interesting area for future work is load balancing techniques that permit work stealing at a    ner granularity than an entire partition with a reasonable synchronization cost  To summarize  under skew  a load balancing technique improves the performance of the probe phase but does not address the inherent ine   ciency of all the partitioning based algorithms  In essence  there is a coordination cost to be paid for load balancing  as thread synchronization is necessary  Skew in this case causes contention  stressing the cache coherence protocol and increasing memory tra   c  On the other hand  the no partitioning algorithm does skewed memory loads of read only data  which is handled very e   ciently by modern CPUs through caching  4 8 Effect of output materialization Early work in main memory join processing  7  did not take into account the cost of materialization  This decision was justi   ed by pointing out that materialization comes at a    xed price for all algorithms and  therefore  a join algorithm will be faster regardless of the output being materialized or Machine NO SN L2 S L2 R Intel Nehalem 23  4  7  10  Sun UltraSPARC T2 29  21  20  23  Table 7  Additional overhead of materialization with respect to total cycles without materialization on the uniform dataset  Scale 0 5 Scale 1 Scale 2 NO 7 65  0 47X  16 15  1 00X  62 27  3 86X  SN 11 76  0 46X  25 62  1 00X  98 82  3 86X  L2 S 8 47  0 47X  18 13  1 00X  68 48  3 78X  L2 R 5 82  0 46X  12 71  1 00X  DNF Table 8  Join sensitivity with varying input cardinalities for the uniform dataset on Intel Nehalem  The table shows the cycles for computing the join  in billions  and the relative di   erence to scale 1  discarded  Recent work by Cieslewicz et al   3  highlighted the trade o   s involved when materializing the output  In Table 7 we report the increase in the total join completion time when we materialize the output in memory for the uniform dataset and the partitioning strategies described in Table 2  If the join operator is part of a complex query plan  it is unlikely that the entire join output will ever need to be written in one big memory block  but  even in this extreme case  we see that no algorithm is being signi   cantly impacted by materialization  4 9 Cardinality experiments We now explore how sensitive our    ndings are to variations in the cardinalities of the two input relations  Table 8 shows the results when running the join algorithms on the Intel Nehalem machine  The numbers obtained from the uniform dataset  described in detail in Section 4 1  are shown in the middle column  We    rst created one uniform dataset where both relations are half the size  scale 0 5   This means the relation R has 8M tuples and the relation S has 128M tuples  We also created a uniform dataset where both relations are twice the size  scale 2   i e  the relation R has 32M tuples and the relation S has 512M tuples  The scale 2 dataset occupies 9GB out of the 12GB of memory our system has  Table 1  and leaves little working memory  but the serial0 100 200 300 400 500 600 700 800 1 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K 16 64 256 512 1K 2K 4K 8K 32K 128K Cycles per output tuple Number of partitions No Shared Independent Radix best partition build probe Figure 6  Experiment on Intel Nehalem with uniform dataset and  R   S   access pattern allows performance to degrade gracefully for all algorithms but the L2 R algorithm  The main memory optimizations of the L2 R algorithm cause many random accesses which hurt performance  We therefore mark the L2 R algorithm as not    nished  DNF   We now examine the impact of the relative size of the relations R and S  We    xed the cardinality of the relation S to be 16M tuples  making  R     S   and we plot the cycles per output tuple for the uniform dataset when running on the Intel Nehalem in Figure 6  First  the partitioning time increases proportionally to  R     S   Second  the build phase becomes signi   cant  taking at least 25  of the total join completion time  The probe phase  however  is at most 30  slower  and less a   ected by the</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Database Engines on Multicores  Why Parallelize When You Can Distribute   ###   Tudor Ioan Salomie Ionut Emanuel Subasu Jana Giceva Gustavo Alonso Systems Group  Computer Science Department ETH Zurich  Switzerland  tsalomie  subasu  gicevaj  alonso  inf ethz ch Abstract Multicore computers pve to having to redesign the database eose a substantial challenge to infrastructure software such as operating systems or databases  Such software typically evolves slower than the underlying hardware  and with multicore it faces structural limitations that can be solved only with radical architectural changes  In this paper we argue that  as has been suggested for operating systems  databases could treat multicore architectures as a distributed system rather than trying to hide the parallel nature of the hardware  We    rst analyze the limitations of database engines when running on multicores using MySQL and PostgreSQL as examples  We then show how to deploy several replicated engines within a single multicore machine to achieve better scalability and stability than a single database engine operating on all cores  The resulting system offers a low overhead alternatingine while providing signi   cant performance gains for an important class of workloads  Categories and Subject Descriptors H 2 4  Information Systems   DATABASE MANAGEMENT   Systems General Terms Design  Measurement  Performance Keywords Multicores  Replication  Snapshot Isolation ### 1  Introduction Multicore architectures pose a signi   cant challenge to existing infrastructure software such as operating systems  Baumann 2009  Bryant 2003  Wickizer 2008   web servers  Veal     This work is partly funded by the Swiss National Science Foundation under the programs ProDoc Enterprise Computing and NCCR MICS Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  EuroSys   11  April 10   13  2011  Salzburg  Austria  Copyright  c 2011 ACM 978 1 4503 0634 8 11 04       10 00 2007   or database engines  Hardavellas 2007  Papadopoulos 2008   In the case of relational database engines  and in spite of the intense research in the area  there are still few practical solutions that allow a more    exible deployment of databases over multicore machines  We argue that this is the result of the radical architectural changes that many current proposals imply  As an alternative  in this paper we describe a solution that works well in a wide range of use cases and requires no changes to the database engine  Our approach is intended neither as a universal solution to all use cases nor as a replacement to a much needed complete redesign of the database engine  Rather  it presents a new architecture for database systems in the context of multicores relying on well known distributed system techniques and proving to be widely applicable for many workloads  1 1 Background and Trends Most commercial relational database engines are based on a decades old design optimized for disk I O bottlenecks and meant to run on single CPU computers  Concurrency is achieved through threads and or processes with few of them actually running simultaneously  Queries are optimized and executed independently of each other with synchronization enforced through locking of shared data structures  All these features make the transition to modern hardware platforms dif   cult  It is now widely accepted that modern hardware  be it multicore  or many other developments such as    ash storage or the memory CPU gap  create problems for current database engine designs  For instance  locking has been shown to be a major deterrent for scalability with the number of cores  Johnson 2009a  and the interaction between concurrent queries when updates or whole table scans are involved can have a severe impact on overall performance  Unterbrunner 2009   As a result  a great deal of work proposed either ways to modify the engine or to completely redesign the architecture  Just to mention a few examples  there are proposals to replace existing engines with pure main memory scans  Unterbrunner 2009   to use dynamicprogramming optimizations to increase the degree of parallelism for query processing  Han 2009   to use helper cores to ef   ciently pre fetch data needed by working threads  Papadopoulos 2008   to modularize the engine into a sequence of stages  obtaining a set of self contained modules  which improve data locality and reduce cache problems  Harizopoulos 2005   or to remove locking contention from the storage engine  Johnson 2009a b   Commercially  the    rst engines that represent a radical departure from the established architecture are starting to appear in niche markets  This trend can be best seen in the several database appliances that have become available  e g   TwinFin of IBM Netezza  and SAP Business Datawarehouse Accelerator  see  Alonso 2011  for a short overview   1 2 Results Inspired by recent work in multikernel operating systems  Baumann 2009  Liu 2009  Nightingale 2009  Wentzlaff 2009   our approach deploys a database on a multicore machine as a collection of distributed replicas coordinated through a middleware layer that manages consistency  load balancing  and query routing  In other words  rather than redesigning the engine  we partition the multicore machine and allocate an unmodi   ed database engine to each partition  treating the whole as a distributed database  The resulting system  Multimed  is based on techniques used in LANs as part of computer clusters in the Ganymed system  Plattner 2004   adapted to run on multicore machines  Multimed uses a primary copy approach  the master database  running on a subset of the cores  The master database receives all the update load and asynchronously propagates the changes to satellite databases  The satellites store copies of the database and run on non overlapping subsets of the cores  These satellites are kept in sync with the master copy  with some latency  and are used to execute the read only load  queries   The system guarantees global consistency in the form of snapshot isolation  although alternative consistency guarantees are possible  Our experiments show that a minimally optimized version of Multimed exhibits both higher throughput with lower response time and more stable behavior as the number of cores increase than standalone versions of PostgreSQL and MySQL for standard benchmark loads  TPC W   1 3 Contribution The main contribution of Multimed is to show that a sharenothing design similar to that used in clusters works well in multicore machines  The big advantage of such an approach is that the database engine does not need to be modi   ed to run in a multicore machine  The parallelism offered by multicore is exploited through the combined performance of a collection of unmodi   ed engines rather than through the optimization of a single engine modi   ed to run on multiple cores  An interesting aspect of Multimed is that each engine is restricted in the number of cores and the amount of memory it can use  Yet  the combined performance of several engines is higher than that of a single engine using all the cores and all the available memory  Like any database  Multimed is not suitable for all possible use cases but it does support a wide range of useful scenarios  For TPC W  Multimed can support all update rates  from the browsing and shopping mix to the ordering mix  with only a slight loss of performance for the ordering mix  For business intelligence and data warehouse loads  Multimed can offer linear scalability by assigning more satellites to the analytical queries  Finally  in this paper we show a few simple optimizations to improve the performance of Multimed  For instance  partial replication is used to reduce the memory footprint of the whole system  Many additional optimizations are possible over the basic design  including optimizations that do not require modi   cation of the engine   e g   data placement strategies  and specialization of the satellites through the creation of indexes and data layouts tailored to given queries   The paper is organized as follows  In the next section we motivate Multimed by analyzing the behavior of PostgreSQL and MySQL when running on multicore machines  The architecture and design of Multimed are covered in section 3  while section 4 discusses in detail our experimental evaluation of Multimed  Section 6 discusses related work and section 7 concludes the paper  2  Motivation To explore the behavior of traditional architectures in more detail  we have performed extensive benchmarks over PostgreSQL and MySQL  open source databases that we can easily instrument and where we can map bottlenecks to concrete code sequences   Our analysis complements and con     rms the results of similar studies done on other database engines over a variety of multicore machines  Hardavellas 2007  Johnson 2009b  Papadopoulos 2008   The hardware con   guration and database settings used for running the following experiments are described in section 4 1  The values for L2 cache misses and context switches were measured using a runtime system pro   ler  OPro   le   2 1 Load interaction Conventional database engines assign threads to operations and optimize one query at a time  The execution plan for each query is built and optimized as if the query would run alone in the system  As a result  concurrent transactions can signi   cantly interfere with each other  This effect is minor in single CPU machines where real concurrency among threads is limited  In multicores  the larger number of hardware contexts leads to more transactions running in parallel which in turn ampli   es load interaction  0  200  400  600  800  1000  1200  1400  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  a  Normal Browsing mix  0  1000  2000  3000  4000  5000  6000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  b  Browsing without BestSellers  0  50  100  150  200  250  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 4 Cores PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores  c  BestSellers query only  0  10  20  30  40  50  60  70 Browsing Browsing  without  BestSellers BestSellers  Only L2 Data Cache Miss Ratio     4 Cores   100 Clients 4 Cores   1000 Clients 48 Cores   100 Clients 48 Cores   1000 Clients  d  PostgreSQL  L2 data cache miss ratio  0  20  40  60  80  100 Browsing Browsing  without  BestSellers BestSellers  Only PostgreSQL slock cache miss percentage     4 Cores   100 Clients 4 Cores   1000 Clients 48 Cores   100 Clients 48 Cores   1000 Clients  e  PostgreSQL  s lock cache misses  0  50  100  150  200  250  300  350 PerfectOverUnderPerfectOverUnderPerfectOverThread  yields to scheduler Transaction Mutex RW Shared Row Latch Exclusive Row Latch             load  48 Threads             load  24 Threads             load  12 Threads  f  MySQL  Context switches Figure 1  Current databases on multicore We have investigated load interaction in both PostgreSQL and MySQL using the Browsing mix of the TPC W Benchmark  see below for details on the experimental setup   PostgreSQL   s behavior with varying number of clients and cores is shown for the Browsing mix in    gure 1 a   for all other queries in the mix except BestSellers in    gure 1 b   and for the BestSellers query only in    gure 1 c   For the complete mix     gure 1 a    we observe a clear performance degradation with the number of cores  We traced the problem to the BestSellers query  an analytical query that is performing scans and aggregation functions over the three biggest tables in the database  On one hand the query locks a large amount of resources and  while doing this  causes a large amount of context switches  On the other hand all the concurrent queries have to wait until the BestSellers query releases the locked resources  When this query is removed from the mix     gure 1 b   the throughput increases by almost    ve times and now it actually improves with the number of cores  When running the BestSellers query alone     gure 1 c    we see a low throughput due to the interference among concurrently running queries and  again  low performance as the number of cores increases  The interesting aspect of this experiment is that BestSellers is a query and  as such  is not doing any updates  The negative load interaction it causes arises from the competition for resources  which becomes worse as the larger number of cores allows us to start more queries concurrently  Similar effects have been observed in MySQL  albeit for loads involving full table scans  Unterbrunner 2009   Full table scans require a lot of memory bandwidth and slow down any other concurrent operation  providing another example of negative load interaction that becomes worse as the number of cores increases  2 2 Contention One of the reasons why loads interact with each other is contention  Contention in databases is caused mainly by concurrent access to locks and synchronization primitives  To analyze this effect in more detail  we have pro   led PostgreSQL while running the BestSellers query  The individual run time for this query  running alone in the system  is on average less than 80ms  indicating that there are no limitations in terms of indexing and data organization  Figure 1 d  shows the L2 data cache misses for the full Browsing mix  the Browsing mix without the BestSellers and the BestSellers query alone  The L2 data cache miss ratio was computed using the expression below based on measured values for L2 cache misses  L2 cache    lls and L2 requests  using the CPU performance counters   We have done individual measurements for each CPU core  but as there are no signi   cant differences between the cores  we used the averaged values of the measured metrics  L2DC M iss Ratio   100    L2C ache Misses  L2C ache Fills   L2Requests With more clients and cores  we see a high increase in cache misses for the workloads containing the BestSellers query  We have traced this behavior to the    s lock     spin lock  function  which is used in PostgreSQL to control access to the shared buffers data structures  held in shared memory   Every time a lock can not be acquired  a context switch takes place  forcing an update of the L2 cache  Figure 1 e  shows that the time spent on the    s lock    function increases with both clients and cores  only when the BestSellers query is involved  We would expect to see an increase with the number of clients but not with more cores  Removing again the BestSellers query from the mix  we observe that it is indeed the one that causes PostgreSQL to waste CPU cycles on the    s lock    function as the number of cores increases  Finally  looking at the    s lock    while running only the BestSellers query we see that it dictates the behavior of the entire mix  The conclusion from these experiments is that  as the number of cores and clients increase  the contention on the shared buffers signi   cantly degrades performance  more memory leads to more data under contention  more cores just increase the contention  This problem that has also been observed by Boyd Wickizer  2010   The performance of MySQL for the Browsing mix with different number of cores and clients is shown in    gure 5  MySQL   s InnoDB storage engine acts as a queuing system  it has a    xed number of threads that process client requests  storage engine threads   If more client requests arrive than available threads  MySQL will buffer them until the previous ones have been answered  In this way MySQL is not affected by the number of clients but it shows the same pathological behavior as PostgreSQL with the number of cores  more cores result in lower throughput and higher response times  While running this experiment  we monitored the times a thread had to yield to the OS due to waits for a lock or a latch  Figure 1 f  presents the number of thread yields per transaction for different loads on the system  Running one storage engine thread for each CPU core available to MySQL  we looked at three scenarios  underload  a total of 12 clients   perfect load  same number of clients as storage engine threads  and over load  200 concurrent clients   Running on 12 cores  we see very few thread yields per transaction taking place  This indicates that for this degree of multi programming MySQL has no intrinsic problems  Adding extra cores and placing enough load as to fully utilize the storage engine threads  perfect load and over load scenarios   we see that the number of thread yields per transaction signi   cantly increases  We also observe that the queuing effect in the system does not add extra thread yields  With increasing cores  the contention of acquiring a mutex or a latch increases exponentially  Of the possible causes for the OS thread yields  we observe less than half are caused by the latches that MySQL   s InnoDB storage engine uses for row level locking  The rest are caused by mutexes that MySQL uses throughout its entire code  This implies that there is not a single locking bottleneck  but rather a problem with locking across the entire code base  making it dif   cult to change the system so that it does not become worse with the number of cores  In the case of the BestSellers query  MySQL does not show the same performance degradation issues due to the differences in engine architectures  MySQL has scalability problems with an increasing number of hardware contexts due to the synchronization primitives and contention over shared data structures  2 3 Our approach Load interaction is an intrinsic feature of existing database engines that can only become worse with multicore  Similarly     xing all synchronization problems in existing engines is a daunting task that probably requires major changes to the underlying architecture  The basic insight of Multimed is that we can alleviate the problems of load interaction and contention by separating the load and using the available cores as a pool of distributed resources rather than as a single parallel machine  Unlike existing work that focuses on optimizing the access time to shared data structures  Hardavellas 2007  Johnson 2009b  or aims at a complete redesign of the engine  Harizopoulos 2005  Unterbrunner 2009   Multimed does not require code modi   cations on the database engine  Instead  we use replicated engines each one of them running on a non overlapping subset of the cores  3  The Multimed System Multimed is a platform for running replicated database engines on multicore machines  It is independent of the database engine used  its main component being a middleware layer that coordinates the execution of transactions across the replicated database engines  The main roles of Multimed are   i  mapping database engines to hardware resources   ii  scheduling and routing transactions over the replicated engines and  iii  communicating with the client applications  3 1 Architectural overview From the outside  Multimed follows the conventional clientserver architecture of database engines  Multimed   s client component is implemented as a JDBC Type 3 Driver  Internally  Multimed     gure 2  implements a master slave replication strategy but offers a single system image  i e   the clients see a single consistent system  The master holds a primary copy of the data and is responsible for executing all updates  Queries  the read only part of the load  run on the satellites  The satellites are kept up to date by asynchronously propagating WriteSets from the master  To provide a consistent view  queries can be scheduled to run on a satellite node only after that satellite has done all the updates executed by the master prior to the beginning of the query Figure 2  A possible deployment of Multimed The main components of Multimed are the Communication component  the Dispatcher and the Computational Nodes  The Communication component implements an asynchronous server that allows Multimed to process a high number of concurrent requests  Upon receiving a transaction  Multimed routes the transaction to one of its Computational Nodes  each of which coordinates a database engine  The routing decision is taken by the Dispatcher subsystem  With this architecture  Multimed achieves several goals  First  updates do not interfere with read operations as the updates are executed in the master and reads on the satellites  second  the read only load can be separated across replicas so as to minimize the interference of heavy queries with the rest of the workload  3 2 How Multimed works We now brie   y describe how Multimed implements replication  which is done by adapting techniques of middleware based replication  Plattner 2004  to run in a multicore machine  In a later section we explore the optimizations that are possible in this context and are not available in network based systems  3 2 1 Replication model Multimed uses lazy replication  Gray 1996  between its master and satellite nodes but guarantees a consistent view to the clients  The master node is responsible for keeping a durable copy of the database which is guaranteed to hold the latest version of the data  All the update transactions are executed on the master node as well as any operation requiring special features such as stored procedures  triggers  or user de   ned functions  The satellite nodes hold replicas of the database  These replicas might not be completely up to date at a given moment but they are continuously fed with all the changes done at the master  A satellite node may hold a full or a partial replica of the database  Doing full replication has the advantage of not requiring knowledge of the data allocation for query routing  On the downside  full replication can incur both higher costs in keeping the satellites up to date due to larger update volumes  and lower performance because of memory contention across the replicas  In the experimental section we include an evaluation of partial replication but all the discussions on the architecture of Multimed are done on the basis of full replication to simplify the explanation  Each time an update transaction is committed  the master commits the transaction locally  The master propagates changes as a list of rows that have been modi   ed  A satellite enqueues these update messages and applies them in the same order as they were executed at the master node  Multimed enforces snapshot isolation as a consistency criterion  see  Daudjee 2006  for a description of snapshot isolation and other consistency options such as session consistency in this type of system   In snapshot isolation queries are guaranteed to see all changes that have been committed at the time the transaction started  a form of multiversion concurrency control found today in database engines such as Oracle  SQLServer  or PostgreSQL  When a query enters the system  the Dispatcher needs to decide where it can run the transaction  i e   to which node it should bind it  which may involve some small delay until a copy has all necessary updates  and if multiple options are available  which one to choose  Note that the master node is always capable of running any query without any delay and can be used as a way to minimize latency if that is an issue for particular queries  Within each database  we rely on the snapshot isolation consistency of the underlying database engine  This means that an update transaction will not interfere with read transactions  namely the update transaction is applied on a different    snapshot    of the database and once it is committed  the shadow version of the data is applied on the active one  In this way  Multimed can schedule queries on replicas at the same time they are being updated  3 2 2 WriteSet extraction and propagation In order to capture the changes caused by an update transaction  Multimed uses row level insert  delete and update triggers in the master database on the union of the tables replicated in all the satellites  The triggers    re every time a row is modi   ed and the old and new versions are stored in the context of the transaction  All the changed rows  with their previous and current versions  represent the WriteSet of a transaction  In our system  this mechanism is implemented using SQL triggers and server side functions  This is the only mechanism speci   c to the underlying database but it is a standard feature in today   s engines input   Connection con  Server Count Number scn WriteSet ws     con getWriteSet    if ws getStatementCount     0 then synchronized lock object con commit    ws setSCN scn atomicIncrement     foreach satellite sat do sat enqueue ws   end else con commit    Algorithm 1  WriteSet Propagation When an update transaction is committed on the master node  the WriteSet is extracted  by invoking a server side function   parsed and passed on to each satellite for enqueuing in its update queue  following algorithm 1  A total order is enforced by Multimed over the commit order of the transactions on the master node  This total order needs to be enforced so that it can be respected on the satellite nodes as well  This might be a performance bottleneck for the system  but as we show in the experimental section  the overhead induced by WriteSet extraction and by enforcing a total order over the commits of updates is quite small  In practice  Multimed introduces a small latency in starting a query  while waiting for a suitable replica  but it can execute many more queries in parallel and  often  the execution of each query is faster once started  Thus  the result is a net gain in performance  3 3 Multimed   s components The main components of Multimed are the computational nodes  the dispatcher  the system model and the client communication interface  3 3 1 Computational nodes A Computational Node is an abstraction over a set of hardware  CPUs  memory  etc   and software  database engine and stored data  connection pools  queues  etc  resources  Physically  it is responsible for forwarding queries and updates to its database engine  Each Computational Node runs in its own thread  It is in charge of  i  executing queries   ii  executing updates and  iii  returning results to the clients  Each transaction is bound to a single Computational Node  can be the master  which has the capability of handling all the requests that arrive in the context of a transaction  Multimed has one master Computational Node and any number of satellite Computational Nodes  Upon arrival  queries are assigned a timestamp and dispatched to the    rst satellite available that has all the updates committed up to that timestamp  thereby enforcing snapshot isolation   Satellite nodes do not need to have any durability guarantees  i e   do not need to write changes to disk   In the case of failure of a satellite  no data is lost  as everything is durably committed by the master Computational Node  3 3 2 Dispatcher The Multimed Dispatcher binds transactions to a Computational Node  It routes update transactions to the master node  leaving the read transactions to the satellites  The differentiation of the two types of transactions can be done based on the transaction   s readOnly property  from the JDBC API   The Dispatcher balances load by choosing the most lightly loaded satellite from among those that are able to handle the transaction  The ability of a satellite to handle a transaction is given by the freshness of the data it holds and by the actual data present  in the case of partial replication   The load can be the number of active transactions  the CPU usage  average run time on this node  etc  When no capable satellite is found  the Dispatcher waits until it can bind to a satellite with the correct update level or it may choose to bind the transaction to the master node  3 3 3 System model The system model describes the con   guration of all Computational Nodes  It is used by the Dispatcher when processing client requests  The System Model currently de   nes a static partitioning of the underlying software and hardware resources  dynamic partitioning is left for future work as it might involve recon   guring the underlying database   It is used at start time to obtain a logical and physical description of all the Computational Nodes and of the required connection settings to the underlying databases  It also describes the replication scheme in use  specifying what data is replicated where  the tables that are replicated  and where transactions need to be run  3 3 4 Communication component The communication subsystem  on the server side  has been implemented based on Apache Mina 2 0  Apache Mina   The communication interface is implemented as an asynchronous server using Java NIO libraries  Upon arrival  each client request is passed on to a Computational Node thread for processing  We have implemented the server component of the system as a non blocking message processing system so as to be able to support more concurrent client connections than existing engines  This is important to take advantage of the potential scalability of Multimed as often the management of client connections is a bottleneck in database engines  see the results for PostgreSQL above   3 4 System optimizations Multimed can be con   gured in many different ways and accepts a wide range of optimizations  In this paper we describe a selection to illustrate how Multimed can take advantage of multicore systems in ways that are not possible in conventional engines  On the communication side  the messages received by the server component from the JDBC Type 3 Driver are small  under 100 bytes   By default  multiple messages willbe packed together before being sent  based on Nagle   s algorithm  Peterson 2000    increasing the response time of a request  We disabled this by setting the TCP NODELAY option on the Java sockets  reducing the RTT for messages by a factor of 10 at the cost of a higher number of packets on the network  On the server side  all connections from the Computational Nodes to the database engines are done through JDBC Type 4 Drivers  native protocols  to ensure the best performance  Using our own connection pool increases performance as no wait times are incurred for creating freeing a database connection  At the Dispatcher level  the binding between an external client connection and an internal database connection is kept for as long as possible  This binding changes only when the JDBC readOnly property of the connection is modi   ed  For the underlying satellite node database engines  we can perform database engine speci   c tweaks  For instance  for the PostgreSQL satellites  we turned off the synchronous commit of transactions and increased the time until these reach the disk  Consequently  the PostgreSQL speci   c options like fsync  full page writes and synchronous commit were set to off  the commit delay was set to its maximum limit of 100  000  s  and the wal writer delay was set to 10  000ms  Turning off the synchronous commit of the satellites does not affect the system  since they are not required for durability  Similar optimizations can be done with MySQL although in the experiments we only include the delay writes option  In the experimental section  we consider three optimization levels  C0 implements full data replication on disk for all satellites  This is the na    ve approach  where we expect performance gains from the reduced contention on the database   s synchronization primitives  but also higher disk contention  C1 implements full data replication in main memory for all satellites  thereby reducing the disk contention  C2 implements partial or full data replication in main memory for the satellites and transaction routing at the Dispatcher  This approach uses far less memory than C1  but requires a priori knowledge of the workload to partition the data adequately  satellites will be specialized for running only given queries   For the case of the 20GB database used in our experiments  a specialized replica containing just the tables needed to run the BestSellers query needs only 5 6GB thus allowing us to increase the number of in memory satellite nodes  For CPU bound use cases this approach allows us to easily scale to a large number of satellite nodes  and effectively push the bottleneck to the maximum disk I O that the master database can use  4  Evaluation In this section we compare Multimed with conventional database engines running on multicore  We measure the throughput and response time of each system while running on a different number of cores  clients  and different database sizes  We also characterize the overhead and applicability of Multimed under different workloads  Aiming at a fair comparison between a traditional DBMS and Multimed  we used the TPC W benchmark  which allows us to quantify the behavior under different update loads  4 1 Setup All the experiments were carried out on a four way AMD Opteron Processor 6174 with 48 cores  128GB of RAM and two 146GB 15k RPM Seagate R Savvio R disks in RAID1  Each AMD Magny Cours CPU consists of two dies  with 6 cores per die  Each core has a local L1  128KB  and L2 cache  512KB   Each die has a shared L3 cache  12MB   The dies within a CPU are connected with two HyperTransport  HT  links between each other  each one of them having two additional HT links  For the experiments with three and    ve satellites  each satellite was allocated entirely within a CPU  respectively within a die  to avoid competition for the cache  In the experiments with ten satellites  partial replication was used  making the databases smaller  In this case  each satellite was allocated on four cores for a total of 3 satellites per socket  Two of these satellites are entirely within a die and the third spawns two dies within the same CPU  Due to the small size of the replicas  the point we want to make with partial replication   we have not encountered cache competition problems when satellites share the L3 cache  The hard disks in our machine prevented us from exploring more write intensive loads  In practice  network attached storage should be used  thereby allowing Multimed to support workloads with more updates  Nevertheless  the features and behavior of Multimed can be well studied in this hardware platform  A faster disk would only change at which point the the master hits the I O bottleneck  improving the performance of Multimed even further  The operating system used is a 64 bit Ubuntu 10 04 LTS Server  running PostgreSQL 8 3 7  MySQL 5 1 and Sun Java SDK 1 6  4 2 Benchmark The workload used is the TPC W Benchmark over datasets of 2GB and 20GB  Each run consists of having the clients connect to the database and issue queries and updates  as per the speci   cations of the TPC W mix being run  Clients issue queries for a time period of 30 minutes  without think times  Each experiment runs on a fresh copy of the database  so that dataset evolution does not affect the measurements  For consistent results  the memory and threading parameters of PostgreSQL and MySQL are    xed to the same values for both the standalone and Multimed systems  The clients are emulated by means of 10 physical machines  This way more than 1000 clients can load the target system without incurring overheads due to contention on the client side  Clients are implemented in Java and are used to 0  1000  2000  3000  4000  5000  6000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Transactions second Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  1000  2000  3000  4000  5000  6000  7000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  b  Throughput for 2GB database  0  1000  2000  3000  4000  5000  6000  7000  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 5  c  Throughput for 20GB database  0  200  400  600  800  1000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Response Time  msec  Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  5  10  15  20  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  e  Response time for 2GB database  0  5  10  15  20  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 5  f  Response time for 20GB database Figure 3  PostgreSQL  Standalone vs  Multimed  Browsing mix emit the workload as well as to measure the throughput and response time  The TPC W benchmark speci   es three workload mixes  TPC W Browsing  10  updates   TPC W Shopping  20  updates  and TPC W Ordering  50  updates   Out of these three  we focus on the Browsing and Shopping mixes  The Ordering mix is disk intensive and hits an I O bottleneck before any proper CPU usage is seen  The TPC W benchmark speci   es both an application and a database level  We implemented only the database level  as this is the point of interest for this work  Due to the lack of the application level  some features required for correctly implementing the benchmark had to be emulated at the database level  For example the shopping cart  which should reside in the web server   s session state  is present in our implementation as a table in the database  In order to limit the side effects of holding the shopping cart in the database  an upper bound is placed on the number of entries that it can hold  equal to the maximum number of concurrent clients  We have done extensive tests on Multimed  trying to    nd the optimal con   guration to use in the experiments  The number of cores on which the satellites and the master nodes are deployed can be adjusted  Also  the number of cores allocated for Multimed   s middleware code can be con   gured  In the experiments below we mention the number of satellites   S  and the optimization  C0 C2  that were used  4 3 PostgreSQL  Standalone vs  Multimed version This section compares the performance of PostgreSQL and Multimed running on top of PostgreSQL  4 3 1 Query intensive workload Figures 3 a  and 3 d  present the scalability of PostgreSQL compared to Multimed C1  in the case of the 2GB database  and 200 clients  The x axis shows the number of cores used by both Multimed and PostgreSQL  as well as the number of satellites coordinated by Multimed  Both the throughput     gure 3 a   and the response time     gure 3 d   show that the TPC W Browsing mix places a lot of pressure on standalone PostgreSQL  causing severe scalability problems with the number of cores  Multimed running on 4 cores  the master node on 4 cores  and each satellite on 4 cores scales up almost linearly to a total of 40 cores  equivalent of 8 satellites   The limit is reached when the disk I O bound is hit  all queries run extremely fast  leaving only update transactions in the system to run longer  and facing contention on the disk  The gap between the linear scalability line and Multimed   s performance is constant  being caused by the computational resources required by Multimed   s middleware  4 3 2 Increased update workload Figures 3 b  and 3 c  present the throughput of PostgreSQL  running on different number of cores  and of Multimed  running with different con   gurations   as the number of clients increases  Note that PostgreSQL has problems in 0  1000  2000  3000  4000  5000 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Transactions second Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  400 clients  0  500  1000  1500  2000  2500  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  b  Throughput for 2GB database  0  500  1000  1500  2000  2500  100 200 300 400 500 600 700 800 900 1000 Transactions second Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200  1400  1600 4 0 8 0 12 1  16 2 24 4 32 6 40 8 48 10 Response Time  msec  Nr  cores used   Satellites PostgreSQL Multimed C1 Linear scalability  d  Scalability response time  2GB  400 clients  0  2  4  6  8  10  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  e  Response time for 2GB database  0  2  4  6  8  10  100 200 300 400 500 600 700 800 900 1000 Response time  sec  Number of clients PostgreSQL 12 Cores PostgreSQL 24 Cores PostgreSQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 4  PostgreSQL  Standalone vs  Multimed  Shopping mix scaling with the number of clients issuing the workload  and its performance at 48 cores is lower than at 12  For both dataset sizes  Multimed  at all optimization levels  outperforms the standalone version of PostgreSQL  The C0 optimization level for Multimed shows higher error bars  as all satellites are going concurrently to disk  in order to persist updates  Switching to the C1 optimization level  we reduce the contention on disk  by using more main memory  We see an improvement of more than 1000 transactions per second between the na    ve C0 and optimized C1 versions of Multimed  Finally  switching to the less generic optimization level C2  Multimed accommodates more satellites in the available memory  and can take advantage of the available computational resources  until a disk I O limit is hit  Using the C2 optimization  the problem of load interaction is also solved by routing the    heavy     analytical  queries to different satellite nodes  of   oading the other nodes in the system  In all these experiments we have used static routing  Note the fact that Multimed retains a steady behavior with increasing number of concurrent clients  up to 1000   without exhibiting performance degradation  Looking at the corresponding response times  even under heavy load  Multimed   s response time is less than 1 second  indicating that Multimed is not only solving the problems of load interaction  but also the client handling limitations of PostgreSQL  For the Shopping mix  standalone PostgreSQL   s performance is slightly better than for the Browsing mix due to the reduced number of heavy queries  Figures 4 a  and 4 d  show that even in the case of the Shopping mix  PostgreSQL can not scale with the number of available cores  on the 2GB database  with 400 clients  Multimed scales up to 16 cores  2 satellites   at which point the disk becomes a bottleneck  Multimed   s performance stays    at with increasing cores  while that of PostgreSQL drops  Figures 4 b  and 4 c  show that PostgreSQL can not scale with the number of clients for this workload either  regardless of the database size  In the case of Multimed  for a small number of clients  all queries run very fast  leaving the updates to compete for the master node  Past 150 clients  the run time of queries increases and the contention on the master node is removed  allowing Multimed to better use the available satellites  We again observe that Multimed   s behavior is steady and predictable with increasing load  Using the C0 optimization level and for a low number of clients  Multimed performs worse than PostgreSQL  especially on the 20GB database  although it is more stable with the number of clients  With more updates in the system and with all of the satellites writing to disk  Multimed is blocked by I O  As in the previous case  the C1 optimization solves the problem  standard deviation is reduced and the throughput increases  The C2 optimization  at the same number of satellites  also gives the system a performance gain as fewer WriteSets need to be applied on the satellites  they run faster   Both in the case of a query intensive workload  Browsing mix  and in the case of increased update workload  Shopping 0  1000  2000  3000  4000  5000  6000 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Transactions second Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  500  1000  1500  2000  2500  3000  3500  4000  4500  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  b  Throughput for 2GB database  0  500  1000  1500  2000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Response time  msec  Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 10  e  Response time for 2GB database  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 5  MySQL  Standalone vs  Multimed  Browsing mix mix   PostgreSQL does not scale with the number of cores or with the number of clients  regardless of the database size  PostgreSQL   s inability to scale with the number of clients is due to the fact that for each new client a new process is spawned on the server  This might lead to the conclusion that the number of processes is far greater than what the operating system and the hardware can handle  This is disproved by Multimed  which can cope with 1000 clients in spite of the limitations of PostgreSQL  The problem in this case is not the large number of processes in the system  but rather the inability of a single PostgreSQL engine to handle high concurrency  Since Multimed splits the number of clients over a set of smaller sized satellites  it reduces the contention in each engine  resulting in a higher throughput 4 4 MySQL  Standalone vs  Multimed version In this section we compare standalone MySQL to Multimed running on top of MySQL computational nodes  For MySQL  we have done our experiments using its InnoDB storage engine  This engine is the most stable and used storage engine available for MySQL  However it has some peculiar characteristics   i  it acts as a queuing system  allowing just a    xed number of concurrent threads to operate over the data  storage engine threads    ii  it is slower than the PostgreSQL engine for disk operations  In all the results presented below  the number of cores available for MySQL is equal to the number of storage engine threads  Being a queuing system  MySQL will not show a degradation in throughput with the number of clients  but rather exhibits linear increase in response time  For this reason  the experiments for MySQL only go up to 400 clients  4 4 1 Query intensive workload Figures 5 a  and 5 d  present the ability of the standalone engine  and of Multimed running on top of it  to scale with the amount of computational resources  in the case of the 2GB database and 200 clients  The x axis  as before  indicates the total number of cores available for MySQL and Multimed  as well as the number of satellites coordinated by Multimed  Each satellite runs on 4 cores  In the case of the TPC W Browsing mix  we notice that MySQL does not scale with the number of cores  Figure 5 a  shows that MySQL performs best at 12 cores  Adding more cores increases contention and performance degrades  The same conclusion can be seen in the throughput and response time plots for both the 2GB and 20GB datasets     gures 5 b  and 5 c    that show the performance of MySQL  running on different number of cores  and of Multimed  running on different con   gurations  with increasing clients  Since the behavior is independent of the dataset  we conclude that the contention is not caused by a small dataset  but rather by the synchronization primitives  i e   mutexes  that are used by MySQL throughout its entire code  In contrast  Multimed scales with the number of cores  Figure 5 a  shows that on the 2GB dataset  Multimed scales up to 6 satellites  at which point the disk I O becomes the 0  1000  2000  3000  4000  5000  6000  7000 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Transactions second Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  a  Scalability throughput  2GB  200 clients  0  500  1000  1500  2000  2500  3000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  b  Throughput for 2GB database  0  500  1000  1500  2000  50 100 150 200 250 300 350 400 Transactions second Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  c  Throughput for 20GB database  0  200  400  600  800  1000  1200 8 0 12 1 16 2 24 4 32 6 40 8 18 10 Response time  msec  Nr  cores used   Satellites MySQL Multimed C1 Linear scalability  d  Scalability response time  2GB  200 clients  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  e  Response time for 2GB database  0  0 5  1  1 5  2  2 5  3  3 5  4  50 100 150 200 250 300 350 400 Response time  sec  Number of clients MySQL 12 Cores MySQL 24 Cores MySQL 48 Cores Multimed C0  S 3 Multimed C1  S 3 Multimed C2  S 3  f  Response time for 20GB database Figure 6  MySQL  Standalone vs  Multimed  Shopping mix bottleneck in the system  and the throughput and response times are    at  The fact that Multimed on top of PostgreSQL scaled in the same test up to 8 satellites corroborates the fact the PostgreSQL   s storage engine is faster than MySQL   s InnoDB for this workload  The three con   gurations that we have run for Multimed show that by replicating data  Multimed can outperform standalone MySQL by a factor of 2  before it reaches the disk I O bound  The C0 con   guration of Multimed shows a behavior similar to standalone MySQL   s best run  Removing this contention on disk from Multimed  by switching to its C1 con   guration  increases performance  The C2 optimization does not yield better performance than C1  The system is already disk bound and load interaction does not in   uence MySQL for this workload  To improve performance here  a faster disk or lower I O latency would be needed  4 4 2 Increased update workload The scalability plot     gure 6 a    shows that MySQL performs best at 8 cores  With more cores performance degrades  con   rming that contention is the bottleneck  not disk I O  Multimed scales up to 16 cores  at which point the throughput    attens con   rming that the disk becomes the bottleneck  Figures 6 b  and 6 c  show that on larger datasets data contention decreases  allowing standalone MySQL to perform better  On the 2GB database  Multimed brings an improvement of 3x  In the case of the 20GB database  Multimed achieves a 1 5x improvement  5  Discussion Multimed adapts techniques that are widely used in database clusters  As a database replication solution  Multimed inherits many of the characteristics of replicated systems and database engines  In this section we discuss such aspects to further clarify the effectiveness and scope of Multimed  5 1 Overhead The main overhead introduced by Multimed over a stand alone database is latency  Transactions are intercepted by Multimed before being forwarded either to the master or to a satellite  In the case the transactions go to a satellite  there might be further delay while waiting for a satellite with the correct snapshot  Multimed works because  for a wide range of database loads  such an increase in latency is easily compensated by the reduction in contention between queries and the increase in the resources available for executing each query  Although satellite databases in Multimed have fewer resources  they also have less to do  For the appropriate workloads  Multimed is faster because it separates loads across databases so that each can answer fewer queries faster than a large database can answer all the queries 5 2 Loads supported There is no database engine that is optimal for all loads  Stonebraker 2008   Multimed is a replication based solution and  hence  it has a limitation in terms of how many updates can be performed as all the updates need to be done at the master  Although this may appear a severe limitation  it is not so in the context of database applications  As the experiments above show  Multimed provides substantial performance improvements for the TPC W browsing and shopping mixes  For the ordering mix  with a higher rate of updates  Multimed offers similar performance as the single database since the bottleneck in both cases is the disk  Multimed can be used to linearly scale read dominated loads such as those found in business intelligence applications and data warehousing  For instance  it is possible to show linear scale up of Multimed by simply assigning more satellites to complex analytical queries  As a general rule  the more queries and the more complex the queries  the better for Multimed  Workloads with high update rates and without complex read operations are less suitable for Multimed     and indeed any primary copy replication approach     because the master becomes the bottleneck  regardless of why it becomes the bottleneck  CPU  memory  or disk   In cluster based replication  this problem is typically solved by simply assigning a larger machine to the master  Multimed can likewise be con     gured to mitigate this bottleneck with a larger allocation of resources  cores  memory  to the master  5 3 Con   guration Tuning and con   guring databases is a notoriously dif   cult problem  Some commercial database engines are known to provide thousands of tuning knobs  In fact  a big part of the impetus behind the autonomic computing initiative of a few years ago was driven by the need to automatically tune databases  Similarly  tuning Multimed requires knowledge of database loads  knowledge of the engines used  and quite a bit of experimentation to    nd the right settings for each deployment  The advantage of Multimed over a stand alone database is that the number of global tuning knobs is less as each element of the system needs to be tailored to a speci   c load  The master can be tuned for writes  the satellites for reads  It is even possible to con   gure Multimed so that a satellite answers only speci   c queries  for instance  particularly expensive or long running ones  and then optimize the data placement  indexes  and con   guration of that satellite for that particular type of query  In terms of the interaction with the operating system and the underlying architecture  Multimed can be con   gured using simple rules  allocate contiguous cores to the same satellites  restrict the memory available to each satellite to that next to the corresponding cores  prevent satellites from interfering with each other when accessing system resources  e g   cascading updates instead of updating all satellites at the same time   etc  Such rules are architecture speci   c but rather intuitive  Note as well that Multimed is intended to run in a database server  These are typically powerful machines with plenty of memory  often fast networks and even several network cards  and SAN NAS storage rather than local disks  The more main memory is available  the faster the I O  and the more cores  the more possibilities to tune Multimed to the application at hand and the bigger performance gains to be obtained from Multimed  5 4 Optimizations The version of Multimed presented in the paper does not include any sophisticated optimizations since the goal was to show that the basic concept works  There are  however  many possible optimizations over the basic sys</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Join Processing for Flash SSDs  Remembering Past Lessons ### Jaeyoung Do Univ  of Wisconsin Madison jae cs wisc edu Jignesh M  Patel Univ  of Wisconsin Madison jignesh cs wisc edu ABSTRACT Flash solid state drives  SSDs  provide an attractive alternative to traditional magnetic hard disk drives  HDDs  for DBMS applications  Naturally there is substantial interest in redesigning critical database internals  such as join algorithms  for    ash SSDs  However  we must carefully consider the lessons that we have learnt from over three decades of designing and tuning algorithms for magnetic HDD based systems  so that we continue to reuse techniques that worked for magnetic HDDs and also work with    ash SSDs  The focus of this paper is on recalling some of these lessons in the context of ad hoc join algorithms  Based on an actual implementation of four common ad hoc join algorithms on both a magnetic HDD and a    ash SSD  we show that many of the    surprising    results from magnetic HDD based join methods also hold for    ash SSDs  These results include the superiority of block nested loops join over sort merge join and Grace hash join in many cases  and the bene   ts of blocked I Os  In addition  we    nd that simply looking at the I O costs when designing new    ash SSD join algorithms can be problematic  as the CPU cost is often a bigger component of the total join cost with SSDs  We hope that these results provide insights and better starting points for researchers designing new join algorithms for    ash SSDs ###  1  INTRODUCTION Flash solid state drives  SSDs  are actively being considered as storage alternatives to replace or dramatically reduce the central role of magnetic hard disk drives  HDDs  as the main choice for storing persistent data  Jim Gray   s prediction of    Flash is disk  disk is tape  and tape is dead     7  is coming close to reality in many applications  Flash SSDs  which are made by packaging  NAND     ash chips  o   er several advantages over magnetic HDDs including faster random reads and lower power consumption  Moreover  as    ash densities continue to double as predicted in  9   and prices continue to drop  the appeal of    ash SSDs for DBMSs increases  In fact  vendors such as Fusion IO and HP sell    ashPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Proceedings of the Fifth International Workshop on Data Management on New Hardware  DaMoN 2009  June 28  2009  Providence  Rhode Island Copyright 2009 ACM 978 1 60558 701 1     10 00  based devices as I O accelerators for many data intensive workloads  The appeal of    ash SSDs is also attracting interest in redesigning various aspects of DBMS internals for    ash SSDs  One such aspect that is becoming attractive as a research topic is join processing algorithms  as it is well   known that joins can be expensive and can play a critical role in determining the overall performance of the DBMS  While such e   orts are well motivated  we want to approach a redesign of database query processing algorithms by clearly recalling the lessons that the community has learnt from over three decades of research in query processing algorithms  The focus of this paper is on recalling some of the important lessons that we have learnt about e   cient join processing in magnetic HDDs  and determining if these lessons also apply to joins using    ash SSDs  In addition  if previous techniques for tuning and improving the join performance also work for    ash SSDs  then it also changes what are interesting starting point for comparing new SSD based join algorithms  In the case of join algorithms  a lot is known about how to optimize joins with magnetic HDDs to use the available bu   er memory e   ectively  and to account for the characteristics of the I O subsystem  Speci   cally  the paper by Haas et al   8  derives detailed formulae for bu   er allocation for various phases of common join algorithms such as block nested loops join  sort merge join  Grace hash join  and hybrid hash join  Their results show that the right bu   er pool allocation strategy can have a huge impact     upto 400  improvements in some cases  Furthermore  the relative performance of the join algorithms changes once you optimize the bu   er allocations     block nested loops join is much more versatile  and Grace hash join is often not very competitive  The dangers of forgetting these lessons could lead to an incorrect starting point for comparing new    ash SSD join algorithms  For example  the comparison of RARE join  16  with Grace hash join  10  to show the superiority of the RARE join algorithm on    ash SSDs is potentially not the right starting point   It is possible that the RARE join is superior to the best magnetic HDD based join method when run over    ash SSDs  but this question has not been answered conclusively   As we show in this paper  in fact even block nested loops join far outperforms Grace hash join in most cases  on both magnetic HDDs and    ash SSDs  Cautiously  we note that we have only tried one speci   c magnetic HDD and one speci   c    ash SSD  but even this very    rst test produced interesting results   As part of future work  we want to look at wider range of hardware for both    ash SSDs and magnetic HDDs  The focus of this paper in on investigating four popular ad hoc join algorithms  namely block nested loops join  sortmerge join  Grace hash join  and hybrid hash join  on both    ash SSDs and magnetic HDDs  We start with the best bu   er allocation methods that are proposed in  8  for these join algorithms  and    rst ask the question  What changes for these algorithms as we replace a magnetic HDD with a    ash SSD  Then  we study the e   ect of changing the bu   er pool sizes and the page sizes and examine the impact of these changes on these join algorithms  Our results show that many of the techniques that were invented for joins on magnetic HDDs continue to hold for    ash SSDs  As an example  blocked I O is useful on both magnetic HDDs and    ash SSDs  though for di   erent reasons  In the case of magnetic HDDs  the use of blocked I O amortizes the cost of disk seeks and rotational delays  On the other hand  the bene   t of blocked I O with    ash SSDs comes from amortizing the latency associated with the software layers of    ash SSDs  and generating fewer erase operations when writing data  The remainder of this paper is organized as follows  In Section 2  we brie   y introduce the characteristics of    ash SSDs  Then we introduce the four classic join algorithms with appropriate assumptions and bu   er allocation strategies in Section 3  In Section 4  we explain and discuss the experimental results  After reviewing related work in Section 5  we conclude in Section 6  2  CHARACTERISTICS OF FLASH SSD Flash SSDs are based on NAND    ash memory chips and use a controller to provide a persistent block device interface  A    ash chip stores information in an array of memory cells  A chip is divided into a number of    ash blocks  and each    ash block contains several    ash pages  Each memory cell is set to 1 by default  To change the value to 0  the entire block has to be erased by setting it to 1  followed by selectively programming the desired cells to 0  Read and write operations are performed at the granularity of a    ash page  On the other hand  the time consuming erase operations can only be done at the level of a    ash block  Considering the typical size of a    ash page  4 KB  and a    ash block  64    ash pages   the erase before write constraint can significantly degrade write performance  In addition  most    ash chips only support 10 5    10 6 erase operations per    ash block  Therefore  erase operations should be distributed across the    ash blocks to prolong the service life of    ash chips  These kinds of constraints are handled by a software layer known as    ash translation layer  FTL   The major role of the FTL is to provide address mappings between the logical block addresses  LBAs  and    ash pages  The FTL maintains two kinds of data structures  A direct map from LBAs to    ash pages  and an inverse map for rebuilding the direct map during recovery  While the inverse map is stored on    ash  the direct map is stored on    ash and at least partially in RAM to support fast lookups  If the necessary portion of the direct map is not in RAM  it must be swapped in from    ash as required  While    ash SSDs have no mechanically moving parts  data access still incurs some latency  due to overheads associated with the FTL logic  However  latencies of    ash SSDs are typically much smaller than those of magnetic HDDs  4   3  JOINS In this section  we introduce four classic ad hoc join algorithms that we consider in this paper  namely  block nested loops join  sort merge join  Grace hash join  and hybrid hash join  The two relations being joined are denoted as R and S  We use  R    S  and B to denote the sizes of the relations and the bu   er pool size in pages  respectively  We also assume that  R     S   Each join algorithm needs some extra space to build and maintain speci   c data structures such as a hash table or a tournament tree  In order to model these structures  we use a multiplicative fudge factor  denoted as F  Next we brie   y describe each join algorithm  We also outline the bu   er allocation strategy for each join algorithm  The I O costs for writing the    nal results are omitted in the discussion below  as this cost is identical for all join methods  For the bu   er allocation strategy we directly use the recommendations by Haas et al   8   for magnetic HDDs   which shows that optimizing bu   er allocations can dramatically improve the performance of join algorithms  by 400  in some cases   3 1 Block Nested Loops Join Block nested loops join    rst logically splits the smaller relation R into same size chunks  For each chunk of R that is read  a hash table is built to e   ciently    nd matching pairs of tuples  Then  all of S is scanned  and the hash table is probed with the tuples  To model the additional space required to build a hash table for a chunk of R we use the fudge factor F  so a chunk of size  C   pages uses F C   pages in memory to store a hash table on C  The bu   er pool is simply divided into two spaces  one space  Iouter  is for an input bu   er with a hash table for R chunks  and another one  Iinner  is for an input bu   er to scan S  Note that reading R in chunks of size Iouter F     C    guarantees su   cient memory to build a hash table in memory for that chunk  5   3 2 Sort Merge Join Sort merge join starts by producing sorted runs of each R and S  After R and S are sorted into runs on disk  sortmerge join reads the runs of both relations and merges joins them  We use the tournament sort  a k a  heap sort  in the    rst pass  which produces runs that on average are twice the size of the memory used for the initial sorting  11   We also assume B   p F S  so that the sort merge join  which uses a tournament tree  can be executed in two passes  17   In the    rst pass  the bu   er pool is divided into three parts  an input bu   er  an output bu   er  and working space  WS  to maintain the tournament tree  During the second pass  the bu   er pool is split across all the runs of R and S as evenly as possible  3 3 Grace Hash Join Grace hash join has two phases  In the    rst phase  it reads each relation  applies a hash function to the input tuples  and hashes tuples into buckets that are written to disk  In the second phase  the    rst bucket of R is loaded into the bu   er pool  and a hash table is built on it  Then  the corresponding bucket of S is read and used to probe the hash table  Remaining buckets of R and S are handled in the same way iteratively  We assume B   p F R  to allow for a two phase Grace hash join  17  Join Algorithm First Phase Pass Second Phase Pass Block Nested Loops Join Iinner           y S  y S  B y  S      y S  y  S      y   Dl Dx Iouter   B     Iinner Sort Merge Join I   O             2z   4   B z   8     I         B NR NS     z    Dl Ds   F   R   S   Dl  B WS   B     I     O Grace Hash Join k        R F       R 2F 2 4B R F 2B     WS             F  R  k     O       B k 1     I   B     WS     I   B     k    O Hybrid Hash Join I   O      1 1     B    WS             F  R    WS k     k       F  R     B   I  B   I   O     I   B     WS     WS   B     I     k    O Table 1  Bu   er allocations for join algorithms from Haas et al   8   Ds  Dl  and Dx denote average seek time  latency  and page transfer time for magnetic HDDs  respectively There are two sections in the bu   er pool during the    rst partitioning phase  one input bu   er and an output bu   er for each of the k buckets  We subdivide the output bu   er as evenly as possible based on the number of buckets  and then give the remaining pages  if any  to the input bu   er  In the second phase  a portion of the bu   er pool  WS     is used for the i th bucket of R and its hash table  and the remaining pages are chosen as input bu   er pages to read S  3 4 Hybrid Hash Join As in the Grace hash join  there are two phases in this algorithm  assuming M   p F R   In the    rst phase  the relation R is read and hashed into buckets  Since a portion of the bu   er pool is reserved for an in memory hash bucket for R  this bucket of R is not written to a storage device while the other buckets are  Furthermore  as S is read and hashed  tuples of S matching with the in memory R bucket can be joined immediately  and need not be written to disk  The second phase is the same as Grace hash join  During the    rst phase  the bu   er pool is divided into three parts  one for the input  one for the output of k buckets excluding the in memory bucket  and the working space  WS  for the in memory bucket  The bu   er allocation scheme for the second phase is the same as Grace hash join  3 5 Buffer Allocation Table 1 shows the optimal bu   er allocations for the four join algorithms  for each phase of these algorithms  Note that block nested loops join does not distinguish between the di   erent passes  In this paper  we use the same bu   er allocation method for both    ash SSDs and magnetic HDDs  While these allocations may not be optimal for    ash SSDs  our goal here is to start with the best allocation strategy for magnetic HDDs and explore what happens if we simply use the same settings when replacing a magnetic HDD with a    ash SSD  Comments Values Magnetic HDD cost 129 99    0 36   GB  Magnetic HDD average seek time 12 ms Magnetic HDD latency 5 56 ms Magnetic HDD transfer rate 34 MB s Flash SSD cost 230 99    3 85   GB  Flash SSD latency 0 35 ms Flash SSD read transfer rate 120 MB s Flash SSD write transfer rate 80 MB s Page size 2 KB     32 KB Bu   er pool size 100 MB     600 MB Fudge Factor 1 2 orders table size 5 GB customer table size 730 MB Table 2  Device characteristics and parameter values 4  EXPERIMENTS In this section  we compare the performance of the join algorithms using the optimal bu   er allocations when using a magnetic HDD and a    ash SSD for storing the input data sets  Each data point presented here is the average over three runs  4 1 Experimental Setup We implemented a single thread and light weight database engine that uses the SQLite3  1  page format for storing relational data in heap    les  Each heap    le is stored as a    le in the operating system  and the average page utilization is 80   Our engine has a bu   er pool that allows us to control the allocation of pages to the di   erent components of the join algorithms  The engine has no concurrency control or recovery mechanisms  Our experiments were performed on a Dual Core 3 2GHz Intel Pentium machine with 1 GB of RAM running Red Hat100 200 300 400 500 600 0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 Join Time  sec  Buffer Size  MB  CPU time IO time                         B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H  a  Magnetic HDD 100 200 300 400 500 600 0 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 Join Time  sec  Buffer Size  MB  CPU time IO time                         B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H     B      N      L       S      M       G       H       H  b  Flash SSD Figure 1  Varying the size of the bu   er pool  8 KB page  blocked I O  Bu   er Pool Size 100 MB 200 MB 300 MB 400 MB 500 MB 600 MB Algorithms Join I O Join I O Join I O Join I O Join I O Join I O BNL 1 64X 2 86X 1 59X 2 62X 1 72X 3 04X 1 73X 2 87X 1 67X 2 79X 1 65X 2 61X SM 1 41X 1 81X 1 45X 2 05X 1 44X 2 06X 1 45X 2 08X 1 43X 2 04X 1 48X 2 20X GH 1 34X 1 54X 1 29X 1 59X 1 41X 1 62X 1 33X 1 62X 1 39x 1 77X 1 30X 1 55X HH 1 45X 1 77X 1 55X 1 90X 1 35X 1 51X 1 51X 1 89X 1 50x 1 78X 1 65X 2 09X Table 3  Speedups of total join times and I O times with    ash SSDs Enterprise 5  For the comparison  we used a 5400 RPM TOSHIBA 320 GB external HDD and a OCZ Core Series 60GB SATA II 2 5 inch    ash SSD  We used wall clock time as a measure of execution time  and calculated the I O time by subtracting the reported CPU time from the wall clock time  Since synchronous I Os were used for all tests  we assumed that there is no overlap between the I O and the computation  We also used direct I Os so that the database engine transfers data directly from to the bu   er pool bypassing the OS cache  so there is no prefetching and double bu   ering   With this setup all join numbers repeated here are    cold    numbers  4 2 Data Set and Join Query As our test query  we used a primary foreign key join between the TPC H  2  customer and the orders tables  generated with a scale factor of 30  The customer table contains 4 500 000 tuples  730 MB   and the orders table has 45 000 000  5 GB   Each tuple of both tables contains an unsigned 4 byte integer key  the customer key   and an average 130 and 90 bytes of padding for the customer and the orders tables respectively  The data for both tables were stored in random order in the corresponding heap    les  Characteristics of the magnetic HDD and the    ash SSD  and parameter values used in these experiments are shown in Table 2  4 3 Effect of Varying the Buffer Pool Size The e   ect of varying the bu   er pool size from 100 MB to 600 MB is shown in Figure 1  for both the magnetic HDD and the    ash SSD  We also used blocked I O to sequentially read and write multiple pages in each I O operation  The size of the I O block is calculated for each algorithm using the equations shown in Table 1  In Figure 1 error bars denote the minimum and the maximum measured I O times  across the three runs   Note that the error bars for the CPU times are omitted  as their variation is usually less than 1  of the total join time  Table 3 shows the speedup of the total join times and the I O times of the four join algorithms under di   erent bu   er pool sizes  The results in Table 3 show that replacing the magnetic HDD with the    ash SSD bene   ts all the join methods  The block nested loops join whose I O pattern is sequential reads shows the biggest performance improvement  with speedup factors between 1 59X to 1 73X   Interestingly  a case can be made that for sequential reads and writes  comparable or much higher speedups can be achieved with striped magnetic HDDs  for the same   cost  15    Other join algorithms also performed better on the    ash SSD compared to the magnetic HDD  with smaller speedup improvements than the block nested loops join  This is because the write transfer rate is slower than the read transfer rate on the    ash SSD  See Table 2   and unexpected erase operations might degrade write performance further Algorithms Sort Merge Join Grace Hash Join Hybrid Hash Join Bu   er Pool Size First Phase Second Phase First Phase Second Phase First Phase Second Phase 100 MB 1 52X 3 00X 1 34X 2 27X 1 57X 2 61X 200 MB 1 83X 2 81X 1 43X 2 19X 1 66X 3 09X 300 MB 1 86X 2 79X 1 47X 2 12X 1 34X 2 32X 400 MB 1 90X 2 63X 1 47X 2 13X 1 70X 2 91X 500 MB 1 81X 2 86X 1 59X 2 44X 1 63X 2 83X 600 MB 2 00X 2 89X 1 31X 2 68X 1 98X 2 84X Table 4  Speedups of I O times with    ash SSDs  broken down by the    rst and second phases 2 4 8 16 32 0 100 200 300 400 500 600 700 800 900 1000 1100 Join Time  sec  Page Size  KB  CPU time IO time                         B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H  a  Magnetic HDD 2 4 8 16 32 0 100 200 300 400 500 600 700 800 900 1000 1100 Join Time  sec  Page Size  KB  CPU time IO time                         B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H      B      N      L       S      M       G       H       H       H  b  Flash SSD Figure 2  Varying the page size  500 MB Bu   er Pool  blocked I O  As an example  di   erent I O speedups were achieved in the    rst and the second phases of the sort merge join as shown in Table 4  While the I O speedup of the second phase was between 2 63X and 3 0X due to faster random reads  the I O speedup in the    rst phase  that has sequential writes as the dominant I O pattern   was only between 1 52X and 2 0X  which reduced the overall speedup for sortmerge join  In the case of Grace hash join  all the phases were executed with lower I O speedups than those of the sort merge join  See Table 4   Note that the dominant I O pattern of Grace hash join is random writes in the    rst phase  followed by sequential reads in the second phase  While the I O speedup between 2 12X and 2 68X was observed for the second phase of Grace hash join  the I O speedup of its    rst phase was only between 1 31X and 1 59X due to expensive erase operations  This indicates that algorithms that stress random reads  and avoid random writes as much as possible are likely to see bigger improvements on    ash SSDs  over magnetic HDDs   While there is little variation in the I O times with the magnetic HDD  See the error bars in Figure 1 a  for the I O bars   we observed higher variations in the I O times with the    ash SSD  Figure 1 b    resulting from the varying write performance  Note that since random writes cause more erase operations than sequential writes  hash based joins show wider range of I O variations than sort merge join  On the other hand  there is little variation in the I O costs for block nested loops join regardless of the bu   er pool size  since it does not incur any writes  Another interesting observation that can be made here  Figure 1  is the relative I O performance between the sortmerge join and Grace hash join  Both have similar I O costs with the magnetic HDD  but sort merge join has lower I O costs with the    ash SSD  This is mainly due to the di   erent output patterns of both join methods  In the    rst phase of the joins  where both algorithms incur about 80  of the total I O cost  each writes intermediate results  sorted runs for sort merge join  and buckets for Grace hash join  to disk in di   erent ways  sort merge join incurs sequential writes as opposed to the random writes that are incurred by Grace hash join  While this di   erence in the output patterns has a substantial impact on the join performance with the    ash SSD because random write</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdnh09p3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#s09database_on_new_hw"/>
        <doc>Flash in a DBMS  Where and How  Manos Athanassoulis y Anastasia ### Ailamaki y Shimin Chen     Phillip B  Gibbons     Radu Stoica y y Ecole Polytechnique Fed   erale de Lausanne        Intel Labs Pittsburgh Abstract Over the past decade  new solid state storage technologies  with    ash being the most mature one  have become increasingly popular  Such technologies store data durably  and can alleviate many handicaps of hard disk drives  HDDs   Nonetheless  they have very different characteristics compared to HDDs  making it challenging to integrate such technologies into data intensive systems  such as database management systems  DBMS   that rely heavily on underlying storage behaviors  In this paper  we ask the question  Where and how will    ash be exploited in a DBMS  We describe techniques for making effective use of    ash in three contexts   i  as a log device for transaction processing on memory resident data   ii  as the main data store for transaction processing  and  iii  as an update cache for HDD resident data warehouses ###  1 Introduction For the past 40 years  hard disk drives  HDDs  have been the building blocks of storage systems  The mechanics of HDDs rotating platters dictate their performance limitations  latencies dominated by mechanical delays  seeks and rotational latencies   throughputs for random accesses much lower than sequential accesses  interference between multiple concurrent workloads further degrading performance  25   etc  Moreover  while CPU performance and DRAM memory bandwidth have increased exponentially for decades  and larger and deeper cache hierarchies have been increasingly successful in hiding main memory latencies  HDD performance falls further and further behind  As illustrated in Table 1  HDDs    random access latency and bandwidth have improved by only 3 5X since 1980  their sequential bandwidth lags far behind their capacity growth  and the ratio of sequential to random access throughput has increased 19 fold  New storage technologies offer the promise of overcoming the performance limitations of HDDs  Flash  phase change memory  PCM  and memristor are three such technologies  7   with    ash being the most mature  Flash memory is becoming the de facto storage medium for increasingly more applications  It started as a storage solution for small consumer devices two decades ago and has evolved into high end storage for performance sensitive enterprise applications  20  21   The absence of mechanical parts implies    ash is not limited by any seek or rotational delays  does not suffer mechanical failure  and consumes less power than HDDs  As highlighted in Table 1     ash based solid state drives  SSDs     ll in the latency and bandwidth gap left by HDDs  SSDs also provide a low ratio between sequential and random access throughput 1   The time to scan the entire device sequentially is much lower than modern HDDs  close to what it was in older HDDs  Copyright 2010 IEEE  Personal use of this material is permitted  However  permission to reprint republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists  or to reuse any copyrighted component of this work in other works must be obtained from the IEEE  Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 1 Random access throughput for SSDs is computed using a mix of reads and writes  If only reads are performed then sequential and random access throughputs are roughly equal  1Table 1  Comparison of Hard Disk Drives  1980  2010  and Flash Drives  2010  Device Capacity Cost Cost MB Random Access Random Access Sequential Access Sequential BW   Device Scan   Year  GB          Latency  ms  Bandwidth  MB s  Bandwidth  MB s  Random BW  s  HDD 1980 0 1 20000 200 28 0 28 1 2 4 3 83 HDD 2010 1000 300 0 0003 8 0 98 80 81 6 12500 SSD 2010 100 2000 0 02 0 026 300 700 2 3 143 Sources  Online documents and presentations  13   vendor websites and other sources  18   Flash devices  however  come with certain limitations of their own  There is an asymmetry between random reads and random writes  3  6  23   with the latter incurring a performance hit due to the speci   cs of the    ash technology  2  6   Newer SSDs  16  19  mitigate this performance hit  but random writes still have a negative impact on the future performance of the device  Interestingly  sequential writes not only offer good performance but in many cases repair device performance after extensive random writes  26   Finally     ash cells wear out after 10K   100K writes to the cell  The differences between HDDs and SSDs are particularly important in Database Management Systems  DBMS  because their components  query processing  query optimization  query evaluation  have been tuned for decades with the HDD characteristics in mind  Speci   cally  random accesses are considered slow  sequential accesses are preferred  and capacity is cheap  Thus  achieving successful integration of    ash requires revisiting DBMS design  Currently     ash usage in DBMS follows two trends  resulting either in    ash only systems or in hybrid    ash HDD systems  Flash only systems bene   t greatly from    ash friendly join algorithms  29   indexes  1  8  23  24   and data layout  this paper   Hybrid systems use    ash judiciously to cache either hot or incoming data or for speci   c operations  9  14   In this paper we describe techniques for using    ash to optimize data management in three different settings  covering DRAM resident  SSD resident  and HDD resident data stores  Each technique represents an example of how    ash can be used within current systems to address the limitations of HDDs  First  we consider transaction processing on a memory resident data store  and show how to use    ash for transactional logging  dramatically improving transaction throughput  Section 2 1   Second  a straightforward way to use    ash is to replace all the HDDs with SSDs without changing the DBMS software  In this scenario    ash addresses the performance limitations of HDDs but poses new challenges because excessive random writes degrade performance and wear out the    ash storage prematurely  To overcome these challenges  we present a technique that leaves a DBMS unchanged and yet avoids random writes on    ash  Section 2 2   Third  we show how    ash can be used as a performance booster for data warehouses stored primarily on HDDs  Speci   cally  we describe techniques for buffering data updates on    ash such that  i  for performance  queries process the HDD resident data without interference from concurrent updates and  ii  for correctness  the query results are adjusted on the    y to take into account the    ash resident updates  Section 3   Finally  we conclude by highlighting techniques and open problems for other promising uses of    ash in data management  Section 4   2 Ef   cient Transaction Processing Using Flash Devices In this section we describe how    ash can be used effectively in the context of online transaction processing  OLTP   We show that the absence of mechanical parts makes    ash an ef   cient logging device and that using SSDs as a drop in replacement for HDDs greatly bene   ts from    ash friendly data layout  2 1 Using Flash for Transactional Logging Synchronous transactional logging  in which log records are forced to stable media before a transaction commits  is the central mechanism for ensuring data persistency and recoverability in database systems  As DRAM capacity doubles every two years  an OLTP database that was considered    large    ten years ago can now    t into main memory  For example  a database running the TPCC benchmark with 30 million users requires less than 100GB space  which can easily    t into the memory of a server  64   128GB of memory   In contrast  2Figure 1  FlashLogging architecture  exploiting an array of    ash devices and an archival HDD for faster logging and recovery  0 5000 10000 15000 20000 25000 30000 35000 disk ideal 2f 2f 1d 2f 2f 1d 2f 2f 1d na  ve 1p 2p 4p 8p 10p flash A flash B flash C ssd ssd flashlogging NOTPM Figure 2  FlashLogging TPCC performance  synchronous logging requires writing to stable media  and therefore is becoming increasingly important to OLTP performance  By instrumenting a MySQL InnoDB running TPCC  we    nd that synchronous logging generates small sequential I O writes  Such write patterns are ill suited for an HDD because its platters continue to rotate between writes and hence each write incurs nearly a full rotational delay to append the next log entry  Because    ash supports small sequential writes well  we propose FlashLogging  10  to exploit    ash for synchronous logging  FlashLogging Design  Figure 1 illustrates the FlashLogging design  First  we exploit multiple    ash devices for good logging and recovery performance  We    nd that the conventional striping organization in disk arrays results in sub optimal behavior  such as request splitting or skipping  for synchronous logging  Because our goal is to optimize sequential writes during normal operations and sequential reads during recovery  we instead propose an unconventional array organization that only enforces that the LSNs  log sequence numbers  on an individual device are non decreasing  This gives the maximal    exibility for log request scheduling  Second  we observe that write latencies suffer from high variance due to management operations  such as erasures  in    ash devices  We detect such outlier writes and re issue them to other ready    ash devices  thus hiding the long latencies of outliers  Third  the    ash array can be implemented either with multiple low end USB    ash drives  or as multiple partitions on a single high end SSD  Finally  our solution can exploit an HDD as a near zerodelay archival disk  During normal processing     ash resident log data is    ushed to the HDD once it reaches a prede   ned size  e g   32KB   Logging performance is improved because the HDD can also serve write requests when all the    ash drives are busy  Performance Evaluation  We replace the logging subsystem in MySQL InnoDB with FlashLogging  Figure 2 reports TPCC throughput in new order transactions per minute  NOTPM   comparing 14 con   gurations     Disk    represents logging on a 10k rpm HDD  while    ideal    enables the write cache in    disk     violating correctness  so that small synchronous writes achieve almost ideal latency  We evaluate three low end USB    ash drives  A  B  and C  from different vendors     2f    uses two identical    ash drives  and    2f 1d    is    2f    plus an archival HDD  We also employ a single high end SSD either directly with the original logging system     naive      or with FlashLogging while using multiple SSD partitions  1   10 partitions  as multiple virtual    ash drives  From Figure 2  we see that  i  FlashLogging achieves up to 5 7X improvements over traditional  HDD based  logging  and obtains up to 98 6  of the ideal performance   ii  the optional archival HDD brings signi   cant bene   ts   iii  while replacing the HDD with an SSD immediately improves TPCC throughput by 3X  FlashLogging further exploits the SSD   s inner parallelism to achieve an additional 1 4X improvement  and  iv  when compared to the high end SSD  multiple low end USB    ash drives achieve comparable or better performance at much lower price  2 2 Transparent Flash friendly Data Layout Using    ash as persistent storage medium for a DBMS often leads to excessive random writes  which impact    ash performance and its predictability  In Figure 3 a  we quantify the impact of continuous 4K random writes 3 0  50  100  150  200  250  300  350  0 20000 40000 60000 80000 100000 Time  s  Throughput  MiB  Average over 1s Moving average  0  100  200  300  400  500  0 1000 2000 3000 4000 5000 Time  s  Throughput  MiB  Average over 1s Moving average  a  Random writes  b  Random read write mix  50  50   using A P Figure 3  Experiments with a FusionIO SSD as  a  drop in replacement of HDDs and  b  using Append Pack  on FusionIO  16   a high end SSD  We    nd that the sustained throughout is about 16  of the advertised random throughput  At the same time  sequential writes can    x random read performance  26   Transforming Temporal Locality To Spatial Locality  Viewing SSDs as append logs helps us exploit the good sequential write bandwidth when writing  Consequently  temporal locality is transformed to spatial locality and thus sequential reads are transformed to random reads  This transformation helps overall performance  because unlike HDDs     ash offers virtually the same random and sequential read performance  Append Pack  A P  Design  The A P data layout  26  treats the entire    ash device as a log  The dirty pages that come out of the buffer pool to be stored persistently are appended in the    ash device and an index is kept in memory maintaining the mapping between the database page id and the location in the log  The A P layer exports to the overlying device a transparent block device abstraction  We implement a log structure data layout algorithm that buffers the dirty pages and writes them in blocks equal to the erase block size in order to optimize write performance  A P organizes  as well  a second log structured region of the device that is used for the packing part of the algorithm  During the operation of the system  some logical pages are appended more than once  i e   the pages are updated  having multiple versions in the log   In order to keep track of the most recent version of the data  we invalidate the old entry of the page in the in memory data structure  When the log is close to full with appended pages  we begin packing  that is  we move valid pages from the beginning of the log to the second log structure assuming these pages are not updated for a long period  cold pages   The premise is that any  write  hot page should be invalidated and all remaining pages are assumed to be cold  The cold pages are appended in a second log structure to ensure good sequential performance and  thus  fast packing of the device  Experimentation and Evaluation  We implement A P as a standalone dynamic linked library which can be used by any system  The A P library offers the typical pwrite and pread functions but manipulates writes as described above  We experimented with a PCIe Fusion ioDrive card  16   The    ash card offers 160GB capacity  and can sustain up to 700MB s read bandwidth and up to 350MB s sequential write bandwidth  Serving a TPCC like workload using the A P design  we maintain stable performance  in a read write mix  achieving the max that the device could offer  400MB s throughput as a result of combining reads and writes   as shown in Figure 3 b   When compared to the performance of a TPCC system on the same device but without A P  we achieved speedups up to 9X  26   3 Flash enabled Online Updates in Data Warehouses In this section  we investigate the use of    ash storage as a performance booster for data warehouses  DW  stored primarily on HDDs  because for the foreseeable future  HDDs will remain much cheaper but slower than SSDs  We focus on how to minimize the interference between updates and queries in DWs  While traditional DWs allow only of   ine updates at night  the need for 24x7 operations in global markets and the data freshness requirement of online and other quickly reacting businesses make concurrent online updates 4Run scan Run scan Run scan Merge updates Mem scan Table range scan pages M pages Main memory M Merge data updates  main data  Disks updates Incoming SSD  updates  oldest newest Materialized sorted runs  each with run index  a  MaSM algorithm using 2M memory  b  TPCH performance with online updates Figure 4  MaSM design and performance   M          SSD     increasingly desirable  Unfortunately  the conventional approach of performing updates in place can greatly disturb the disk friendly access patterns  mainly  sequential scans  of large analysis queries  thereby slowing down TPCH queries by 1 5   4 0X in a row store DW and by 1 2   4 0X in a column store DW  4   Recent work studied differential updates for addressing this challenge in column store DWs  17  27   The basic idea is  i  to cache incoming updates in an in memory buffer   </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>Privacy Aware Data Management in Information Networks ### Michael Hay Cornell University Ithaca  NY mhay cs cornell edu Kun Liu Yahoo  Labs Santa Clara  CA kun yahoo inc com Gerome Miklau U  of Massachusetts Amherst Amherst  MA miklau cs umass edu Jian Pei Simon Fraser University Burnaby  BC Canada jpei cs sfu ca Evimaria Terzi Boston University Boston  MA evimaria cs bu edu ABSTRACT The proliferation of information networks  as a means of sharing information  has raised privacy concerns for enterprises who manage such networks and for individual users that participate in such networks  For enterprises  the main challenge is to satisfy two competing goals  releasing network data for useful data analysis and also preserving the identities or sensitive relationships of the individuals participating in the network  Individual users  on the other hand  require personalized methods that increase their awareness of the visibility of their private information  This tutorial provides a systematic survey of the problems and state of the art methods related to both enterprise and personalized privacy in information networks  The tutorial discusses privacy threats  privacy attacks  and privacypreserving mechanisms tailored speci   cally to network data  Categories and Subject Descriptors H 2 0  DATABASE MANAGEMENT   Security  integrity  and protection General Terms Algorithms  Security Keywords Networks  Privacy  Anonymization  Di   erential Privacy ### 1  INTRODUCTION A network dataset is a graph representing a set of entities and the connections between them  Network data can describe a variety of domains  a social network might describe individuals connected by friendships  an information network might describe a set of articles connected by ciPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  tations  a communication network might describe Internet hosts related by tra   c    ows  The ability for enterprises to collect network data has increased rapidly  creating the possibility for a wide range of compelling data analysis tasks  studying disease transmission  measuring a publication   s in   uence  evaluating a network   s resiliency to faults and attacks  etc  While members of the enterprise may be able to perform these analyses  they often wish to enlist outside experts  In addition  the ability to disseminate network data  and or release the results of analyses  supports experimental repeatability and advances scienti   c investigation  Unfortunately  access to network data is extremely constrained because many networks contain sensitive information about their participants  The    rst topic in this tutorial focuses on privately managing enterprise network data  We investigate the problem of limiting the disclosure of sensitive information while publishing network data sets for useful analysis  or alternatively  releasing the results of network analyses  At the same time  individuals increasingly participate in online information networks  complex systems in which their personal information and interactions with others are recorded and displayed publicly  The second part of the tutorial identi   es the privacy risks to individuals due to these public networked interactions  We focus our discussion on measures for quantifying users    privacy risks as well as mechanisms for helping them identify appropriate privacy settings  2  TUTORIAL OUTLINE Our tutorial is organized as follows  2 1 Information network data The tutorial begins with a brief introduction to network data that includes social network data  e g   Facebook  LinkedIn   instant messenger networks  collaboration networks  etc  We review examples of large scale networks currently being collected and analyzed  We provide examples of key analysis tasks as well as the nature of the sensitive information contained in network data sets  2 2 Threats and attacks for network data Because network analysis can be performed in the absence of entity identi   ers  e g   name  social security number   a natural strategy for protecting sensitive information is to replace identifying attributes with synthetic identi   ers  We refer to this procedure as naive anonymization  This com  1201mon practice attempts to protect sensitive information by breaking the association between the real world identity and the sensitive data  We review a number of attacks on naively anonymized network data which can re identify nodes  disclose edges between nodes  or expose properties of nodes  e g   node features   These attacks include  matching attacks  which use external knowledge of node features  20  14  39  27   injection attacks  which alter the network prior to publication  1   and auxiliary network attacks  which use publicly available networks as an external information source  25   2 3 Publishing networks privately For enterprises that wish to publish their network data without revealing sensitive information  the above attacks demonstrate that simply removing identi   ers prior to publication fails to protect privacy  Thus  enterprises must consider more complex transformations of the data  An active area of research has focused on designing algorithms that transform network data so that it is safe for publication  These algorithms have two primary objectives  First  the transformations should protect privacy  which is typically demonstrated by proving that the transformed network resists certain attacks  Second  the utility of the data should be preserved by the transformation   i e   salient features of the network should be minimally distorted  While the privacy objective makes some distortion inevitable  most algorithms are designed to minimize distortion  measured in various ways  depending on the algorithm   In most cases  utility is not provably guaranteed but rather assessed empirically  for instance through a comparison of the transformed network to the original in terms of a measure of graph distance  or in terms of the di   erence in various network statistics  such as average shortest path lengths  clustering coef     cient and degree distribution  In addition to protecting privacy and preserving utility  algorithm runtime and scalability are also important considerations  One of the    rst algorithms proposed was that of Liu and Terzi  20   which transforms the network through edge insertions  Edges are added until nodes cannot be distinguished by their degree  speci   cally  for each node  there are at least k   1 other nodes that share its degree  This prevents an adversary with knowledge of node degree from re identifying a target node beyond a set of k candidates   The transformed network may also resist attacks from adversaries with richer auxiliary information  but the algorithm provides no formal guarantee   To preserve utility  the algorithm attempts to    nd the minimal set of edge insertions necessary to achieve the privacy objective  In recent years  many other algorithms have been proposed  cf  surveys  13  32  38    They can be organized based on two key design decisions  the kind of data transformation and the privacy objective  Algorithms transform the network using one of several kinds of alteration  directed alterations transform the network through addition and deletion of edges  6  20  27  39   network generalization summarizes the network data in terms of node groups  5  7  8  14   random alteration transforms the network stochastically via random edge additions  deletions  or rewirings  15  22  31  34   There is also work comparing di   erent alteration strategies  33   While a common privacy objective is preventing re identi   cation  5  6  14  20  27  39   other work seeks to prevent the disclosure of sensitive informa
</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>TrustedDB  A Trusted Hardware based Outsourced Database Engine ### Sumeet Bajaj Stony Brook Computer Science Stony Brook  New York  USA sbajaj cs stonybrook edu Radu Sion Stony Brook Computer Science Stony Brook  New York  USA sion cs stonybrook edu ABSTRACT TrustedDB  11  is an outsourced database prototype that allows clients to execute SQL queries with privacy and under regulatory compliance constraints without having to trust the service provider  TrustedDB achieves this by leveraging server hosted tamper proof trusted hardware in critical query processing stages  TrustedDB does not limit the query expressiveness of supported queries  And  despite the cost overhead and performance limitations of trusted hardware  the costs per query are orders of magnitude lower than any  existing or  potential future software only mechanisms  In this demo we will showcase TrustedDB in action and discuss its architecture ### 1  INTRODUCTION Virtually all major    cloud    providers today o   er a database service of some kind as part of their overall solution  Numerous startups also feature more targeted data management and or database platforms  Yet  signi   cant challenges lie in the path of large scale adoption  Such services often require their customers to inherently trust the provider with full access to the outsourced datasets  But numerous instances of illicit insider behavior or data leaks have left clients reluctant to place sensitive data under the control of a remote  third party provider  without practical assurances of privacy and con   dentiality     especially in business  healthcare and government  Most of the existing research e   orts have addressed such outsourcing security aspects by encrypting the data before outsourcing  Once encrypted however  inherent limitations in the types of primitive operations that can be performed on encrypted data lead to fundamental expressiveness and practicality constraints  Recent theoretical cryptography results provide hope by proving the existence of universal homomorphisms  i e   encryption mechanisms that allow computation of arbitrary functions without decrypting the inputs  6   Unfortunately actual instances of such mechanisms seem to be decades Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  Articles from this volume were invited to present their results at The 37th International Conference on Very Large Data Bases  August 29th   September 3rd 2011  Seattle  Washington  Proceedings of the VLDB Endowment  Vol  4  No  12 Copyright 2011 VLDB Endowment 2150 8097 11 08      10 00  10 5 10 10 10 15 10 20 10 25 10 30 10 0 10 1 10 2 10 3 10 4 10 5 10 6 10 7 10 8 10 9 10 10 10 11 10 12 Cost  picocents  Database size  items  Cryptography based  SELECT query  Cryptography based  JOIN query  SCPU  SELECT query  SCPU  JOIN query  Figure 1  The SCPU is 1 2 orders of magnitude cheaper than deploying cryptography  logarithmic   away from being practical  7   TrustedDB on the other hand utilizes secure  tamper resistant hardware such as the IBM 4764 5  3  4  cryptographic coprocessors deployed on the service provider   s side to implement a complete SQL database processing engine  The TrustedDB design provides strong data con   dentiality assurances  Moreover  it does not limit query expressiveness  2  THE CASE FOR TRUSTED HARDWARE A cost based empirical comparison of solutions for query processing using cryptography and trusted hardware  11   selected results in Figure 1  1   shows a 2  orders of magnitude cost advantage of using trusted hardware over cryptography based mechanisms  This is so because cryptographic overheads  for cryptography that allows some processing by the server  are extremely high even for simple operations  a fact rooted not in cipher implementation ine   ciencies but rather in fundamental cryptographic hardness assumptions and constructs  such as trapdoor functions     the cheapest we have so far being at least as expensive as modular multiplication  9    This is unlikely to change anytime soon  none of the current primitives have  in the past half century   Tamper resistant designs provide a secure execution environment for applications  thereby avoiding the need to use expensive cryptographic operations  However  they are signi   cantly constrained in both computational ability and memory capacity which makes implementing fully featured database solutions using secure coprocessors  SCPUs  very 1 1 US picocent   10    14 USD 1359Figure 2  TrustedDB architecture  challenging  TrustedDB overcomes these limitations by utilizing common unsecured server resources to the maximum extent possible  For example  TrustedDB enables the SCPU to transparently access external storage while preserving data con   dentiality with on the    y encryption  This eliminates the limitations on the size of databases that can be supported  Moreover  client queries are pre processed to identify sensitive components to be run inside the SCPU  Non sensitive operations are o    loaded to the untrusted host server  This greatly improves performance and reduces the cost of transactions  3  ARCHITECTURE TrustedDB is built around a set of core components  Figure 2  including a request handler  a processing agent and communication conduit  a query parser  a paging module  a query dispatch module  a cryptography library  and two database engines  While presenting a detailed architectural blueprint is not possible in this space  in the following we discuss some of the key elements and challenges faced in designing and building TrustedDB  3 1 Outline Challenges  The IBM 4764 001 SCPU presents signi     cant challenges in designing and deploying custom code to be run within its enclosure  For strong security  the underlying hardware code as well as the OS are embedded and no hooks are possible e g   to augment virtual memory and paging mechanisms  We were faced with the choice of having to provide virtual memory and paging in user land  speci   cally inside the query processor as well as all the support software  The embedded Linux OS is a Motorola PowerPC 405 port with fully stripped down libraries to the bare minimum required to support the IBM cryptography codebase and nothing else  This constituted a signi     cant hurdle  as cross compilation became a complex task of mixing native logic with custom ported functionality  The SCPU communicates with the outside world synchronously through    xed sized messages exchanged over the PCI X bus in exact sequences  Interfacing such a synchronous channel with the communication model of the query processors and associated paging components required the development of the TrustedDB Paging Module  The SCPU   s cryptographic hardware engine features a set of latencies that e   ectively crippled the ability to run for highly interactive mechanisms manipulating small amounts of data  e g   32 bit integers   To handle this speci   c case we ended up porting several cryptographic primitives to be run on the SCPU   s main processor instead  and thus eliminate the hardware latencies for small data items  Space constraints prevent the discussion of the numerous other encountered challenges  Overview  To remove SCPU related storage limitations  the outsourced data is stored at the host provider   s site  Query processing engines are run on both the server and in the SCPU  Attributes in the database are classi   ed as being either public or private  Private attributes are encrypted and can only be decrypted by the client or by the SCPU  Since the entire database resides outside the SCPU  its size is not bound by SCPU memory limitations  Pages that need to be accessed by the SCPU side query processing engine are pulled in on demand by the Paging Module  Query execution entails a set of stages   0  In the    rst stage a client de   nes </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>Differentially Private Data Cubes  Optimizing Noise Sources and Consistency ### Bolin Ding 1 Marianne Winslett 2 1 Jiawei Han 1 Zhenhui Li 1 1 Department of Computer Science  University of Illinois at Urbana Champaign  IL  USA 2 Advanced Digital Sciences Center  Singapore  bding3  winslett  hanj  zli28  uiuc edu ABSTRACT Data cubes play an essential role in data analysis and decision support  In a data cube  data from a fact table is aggregated on subsets of the table   s dimensions  forming a collection of smaller tables called cuboids  When the fact table includes sensitive data such as salary or diagnosis  publishing even a subset of its cuboids may compromise individuals    privacy  In this paper  we address this problem using differential privacy  DP   which provides provable privacy guarantees for individuals by adding noise to query answers  We choose an initial subset of cuboids to compute directly from the fact table  injecting DP noise as usual  and then compute the remaining cuboids from the initial set  Given a    xed privacy guarantee  we show that it is NP hard to choose the initial set of cuboids so that the maximal noise over all published cuboids is minimized  or so that the number of cuboids with noise below a given threshold  precise cuboids  is maximized  We provide an ef   cient procedure with running time polynomial in the number of cuboids to select the initial set of cuboids  such that the maximal noise in all published cuboids will be within a factor  ln  L    1  2 of the optimal  where  L  is the number of cuboids to be published  or the number of precise cuboids will be within a factor  1     1 e  of the optimal  We also show how to enforce consistency in the published cuboids while simultaneously improving their utility  reducing error   In an empirical evaluation on real and synthetic data  we report the amounts of error of different publishing algorithms  and show that our approaches outperform baselines signi   cantly  Categories and Subject Descriptors H 2 8  DATABASE MANAGEMENT   Database Applications    Data mining  H 2 7  DATABASE MANAGEMENT   Database Administration   Security  integrity  and protection General Terms Algorithms  Security  Theory Keywords OLAP  data cube  differential privacy  private data analysis Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00 ###  1  INTRODUCTION Data cubes play an essential role in multidimensional data analysis and fast OLAP operations  Often the underlying data is sensitive  and publishing all or part of the cube may endanger the privacy of individuals  For example  privacy concerns prevent Singapore   s Ministry of Health  MOH  from performing wide scale monitoring for adverse drug reactions among Singapore   s three main ethnic groups  none of which are typically included in pharmaceutical companies    drug trials  Privacy concerns also limit MOH   s published health summary tables to extremely coarse categories  reducing their utility for policy planning  Institutional Review Board  IRB  approval is now required for access to most high level summary tables from studies funded by the US National Institutes of Health  making it very hard to leverage results from past studies to plan new studies  In these and many other scenarios  society can greatly bene   t from the publication of detailed  high utility data cubes that also preserve individuals    privacy  The data cube of a fact table consists of cells and cuboids  A cell aggregates the rows in the fact table that match on certain dimensions  The fact table in Figure 1 a  has three dimensions  to be aggregated with count measure  As in Figures 1 b  1 d   a cuboid can be viewed as the projection of a fact table on a subset of dimensions  producing a set of cells with associated aggregate measures  With background knowledge  an adversary can infer sensitive information about an individual from a published cube  16  21  35   For example  in the data cube in Figure 1  if we know that Alice is aged 31 40 and is in the table  the count in cuboid  Age  Salary  tells us her salary is 50 200k  If we know Bob is in the table and is aged 21 30  we learn there is a 75  chance that his salary is 10  50k and a 25  chance it is 50 200k  If we also know that Carl  aged 21 30 and with salary 50 200k  is in the table  then the values of count in cuboid  Age  Salary  tell us Bob   s salary is 10 50k  Even publishing large actual aggregate counts is still not safe  if an adversary has enough background knowledge  For example  suppose there are 100 individuals in a fact table  Sex Age  Salary   and we publish two cells            10 50k  and            50 200k   both with count equal to 50  Suppose the adversary knows everyone   s salary except Bob   s  if 49 people have salary 10 50k and 50 have 50 200k  then s he can infer that Bob   s salary is 10 50k  We apply the notion of   differential privacy  10   DP or   DP for short in the rest of this paper  in data cube publishing  Compared to previous techniques for privacy preserving data publishing  see  2  15  for surveys   DP makes very conservative assumptions about the adversary   s background knowledge  The privacy guarantee it provides is independent of any background knowledge the adversary may have about the database  In particular  a publishing algorithm satisfying DP protects the privacy of any individual row in the database even if the adversary knows every other row  10  Sex Age Salary F 21 30 10 50k F 21 30 10 50k F 31 40 50 200k F 41 50 500k  M 21 30 10 50k M 21 30 50 200k M 31 40 50 200k M 60  500k   a  Fact Table T Sex Age Salary c         0 10k 0         10 50k 3         50 200k 3                      b  Cuboid  Salary  Sex Age Salary c F 21 30 0 10k 0 F 21 30 10 50k 2                          c  Cuboid  Sex Age  Salary  Sex Age Salary c     21 30 0 10k 0     21 30 10 50k 3     21 30 50 200k 1     21 30 200 500k 0     21 30 500k  0     31 40 0 10k 0     31 40 10 50k 0     31 40 50 200k 2     31 40 200 500k 0     31 40 500k  0                        d  Cuboid  Age  Salary  Figure 1  Fact Table and count Data Cube Informally  DP guarantees that the presence absence or speci   c value of any particular individual   s record has a negligible impact on the likelihood that a particular result is returned to a query  Thus an adversary cannot make meaningful inferences about any one individual   s record values  or even whether the record was present  One way to achieve DP is to add random noise to query results  10   The noise is carefully calibrated to the query   s sensitivity  which measures the total change of the query output under a small change of the input  As the variance of the noise increases  the privacy guarantee becomes stronger  but the utility of the result drops  To publish an   DP data cube over d dimensions  there are two baselines   i  We can compute the count measure for each cuboid from the fact table  and then add noise to each cell  As changing one row in the table affects 2 d cells  according to  12   each cell needs Laplace noise Lap 2 d      which destroys the utility of the cube unless d is small  The same idea is applied in  4  to publish a set of marginals of a contingency table   ii  If we only compute count for the base cuboid   Sex  Age  Salary  in Figure 1 c   from the fact table  Lap 1    suf   ces  We compute the other cuboids from the noisy base cuboid to ensure DP  However  noise in highlevel cuboids  such as  Salary   will be magni   ed signi   cantly  and utility will be low  This idea can be applied to universal histograms  but noise accumulation also makes them ineffective there  19   Another possible way is to treat each cell in a cuboid as a query  and apply methods in  23  to answer a workload of count queries while ensuring DP  But this approach is not practical in our context  as its running time space is at least quadratic in the number of cells  If we roll up measures across DP cuboids  the sums may not match the totals recorded in other cuboids  According to a Microsoft user study  1   users are likely to accept these kinds of small inconsistencies if they trust the original data and understand why the inconsistencies are present  However  if the users do not trust the original data  they may interpret the inconsistencies as evidence of bad data  So it is desirable to enforce a requirement for correct roll up totals across dimensions in the DP cuboids to be published  Consistency also boosts accuracy of DP data publishing in some cases  e g   in answering one dimensional range queries  19   Contributions  We study how to publish all or part of a data cube for a given fact table  while ensuring   differential privacy and limiting the variance of the noise added to the cube  We propose a general noise control framework in which a subset Lpre of cuboids is computed from the fact table  plus random noise  The remaining cuboids are computed directly from those in Lpre  which is the    source of noise     When Lpre is larger  each of its members requires more noise  but the cuboids computed from Lpre accumulate less noise  So a clever selection of Lpre can reduce the overall noise  We de   ne two publishing scenarios that    t the needs of the Ministry of Health  In the    rst scenario  MOH identi   es a set of cuboids which must be released  and the goal is to minimize the max noise in the cuboids  In the second scenario  MOH has a large body of cross tabulations that can be useful for urban planners and the medical community  A weighting function indicates the importance of releasing each cuboid  The question is  which of these cuboids can be released in a DP manner  while respecting a given noise variance bound for each cell  called precise cuboids    the goal is to maximize the sum of the weights of the released precise cuboids  We formalize these two optimization problems for the selection of Lpre  prove that they are NP hard  and give ef   cient algorithms with provable approximation guarantees  For the    rst problem  the max noise variance in all published cuboids will be within a factor  ln  L    1  2 of optimal  where  L  is the number of cuboids to be published  and for the second  the number weight of precise cuboids will be within a factor  1     1 e  of optimal  We also show how to enforce consistency over a DP data cube by computing a consistent measure from the noisy measure released by a DP algorithm  We minimize the L p distance between the consistent measure and the noisy measure  The revised data cube is still DP  as we do not revisit the fact table when enforcing consistency  The consistency enforcing techniques in  4  are similar to our L    version  but our L 1 version yields a much better theoretical bound on error than the L    version  We show that in the L 2 version  the consistent measure can be computed ef   ciently  and provide better utility than the original inconsistent noisy measure  Organization  Section 2 provides background for data cubes and DP  plus our noise control goals  Section 3 gives our noise control publishing framework and formalizes the optimization problems for noise control  Section 4 gives approximation algorithms for these problems  Section 5 shows how to enforce consistency across cuboids  Experimental results are reported in Section 6  followed by discussion and extension of our techniques in Section 7  and related work in Section 8  Proofs of all theorems are in the Appendix  2  BACKGROUND AND PROBLEM Data Cubes Consider a fact table T with d nominal dimensions A    A1  A2           Ad   We also use Ai to denote the set of all possible values for the i th dimension  and  Ai  to denote the number of distinct values  i e   cardinality   For a row r     T   r i  is r   s value for Ai  The data cube of T consists of cells and cuboids  A cell a takes the form  a 1   a 2            a d    where a i       Ai            denotes the i th dimension   s value for this cell  A cell is associated with certain aggregate measure of interest  In this paper  we    rst focus on the count measure c a  and discuss how to handle other measures in Section 7 3  c a  is the number of rows r in T that are aggregated in cell a  with the same values on non     dimensions of a   c a      r     T      1     i     d  r i    a i      a i           Cell a is an m dim cell if exactly m of a 1   a 2            a d  are not      An m dim cuboid C is speci   ed by m dimensions  C     Ai1   Ai2             Aim   The cuboid C consists of all m dim cells a such that    1     k     m  a ik      Aik   and C can be interpreted as the projection of T on the set of dimensions  C   The d dim cuboid is called the base cuboid and the cells in it are base cells  For two cuboids C1 and C2  if  C1       C2   denoted as C1   C2   then  measures of cells in  C1 can be computed from C2  C1 is said to be an ancestor of C2  and C2 is a descendant of C1  Let Lall denote the set of all cuboids  Clearly   Lall      forms a lattice EX A M P L  E 2 1  Fact table T in Table 1 has three dimensions  Sex    M  F   Age    0 10  11 20  21 30  31 40  41 50  51  60  60    and Salary    0 10k  10 50k  50 200k  200 500k  500k    Figure 2 a  shows the lattice of cuboids of T   As  Salary       Age  Salary   cuboid  Salary  can be computed from cuboid  Age  Salary   For example  to compute cell            10 50k   we aggregate cells       0 10  10 50k                  60   10 50k     Differential Privacy Differential privacy  DP for short in the rest of this paper  is based on the concept of neighbors  Two tables T1 and T2 are neighbors if they differ by at most one row  i e     T1     T2       T2     T1     1  1 Let nbrs T   be the neighbors of T   and TB be the set of all possible table instances with dimensions A1           Ad  Let F   TB     R n be a function that produces a vector of length n from a table instance  In our context  F computes the set of cuboids we select  De   nition 1   Differential Privacy  10   A randomized algorithm K is   differentially private if for any two neighboring tables T1 and T2 and any subset S of the output of K  Pr  K T1      S      exp       Pr  K T2      S    where the probability is taken over the randomness of K  Consider an individual   s concern about the privacy of her his record r  When the publisher speci   es a small    De   nition 1 ensures that the inclusion or exclusion of r in the fact table makes a negligible difference in the chances of K returning any particular answer  De   nition 2   Sensitivity  10   The L1 sensitivity of F is  S F     max    T1 T2   TB   T1   nbrs T2    F T1      F T2   1  where   x     y  1     1   i   n  xi     yi  is the L 1 distance between two n dimensional vectors x    x1           xn  and y    y1           yn   Let Lap     denote a sample Y taken from a zero mean Laplace distribution with probability density function f x    1 2   e    x       Here  E  Y     0 and variance Var  Y     2   2   We write  Lap      n to denote a vector of n independent random samples Lap      TH E  O R E  M 1        12     Let F be a query sequence of length n  The randomized algorithm that takes as input database T and output F   T     F T      Lap S F      n is   differentially private  Problem Description Given a d dimensional fact table T   we aim to publish a subset L of all T    s cuboids Lall with measure   c      a noisy version of the count measure c      using an algorithm K that ensures   differential privacy  In particular  for any cell a in a cuboid in L  we want to publish a noisy count measure   c a  using the DP algorithm K  Measuring Noise  We measure the utility of an algorithm by the variance of the noisy measure it publishes  As we apply Laplace mechanism in Theorem 1  we will show noisy measure   c     published by our algorithms is unbiased  i e   the expectation E    c a     c a   So for one cell  the variance Var    c a   is equal to the expected squared error  i e   Var    c a     E      c a      c a   2     and we use it to measure the noise error in   c a   Similarly  for any set P of cells  we use Var    a   P   c a    to measure the noise error  1 Some DP papers use a slightly different de   nition  T1 and T2 are neighbors if they have the same cardinality and T1 can be obtained from T2 by replacing one row    indistinguishable  12    Our algorithms also work with this de   nition  if we double the noise  C110 C101 C011 C111 C100 C010 C001 C000  Sex  Age  Salary   Sex  Age   Sex  Salary   Age  Salary   Age   Salary       Sex   a  Lattice Structure of Cuboids C110 C101 C011 C111 C100 C010 C001 C000 7 2 2 7 5 5 7 2 5 2 7 5  b  Variance Magni   cation Figure 2  Lattice of cuboids  Variance magni   cation of noise Noise Control Objectives  We aim to control the noise in the published cuboids in one of the following two ways   i   Bounding max noise  Minimize the maximal variance over all cells  maxa Var    c a    in the published cuboids   ii   Publishing as many as possible  Given a threshold   0  a cuboid C is said to be precise if Var    c a         0 for all cells a in C  Maximize the number of precise cuboids in all published ones  Enforcing Consistency  When the consistency is required  we show how to take noisy measure   c     as input and alter it into measure   c     to    t consistency constraints that the cuboids should sum up correctly  Our consistency enforcing algorithm takes only   c     as input  not touching the fact table T    and thus from the composition property of differential privacy  28   it is also   DP  The output consistent measure   c     should be as close to   c     and c     as possible  The consistency constraints will be formulated in Section 5  3  NOISE CONTROL FRAMEWORK We    rst present two straw man approaches  Kall and Kbase  followed by our noise optimizing approach Kpart  Straw Man Release Algorithms Kall and Kbase One option  Kall   is to compute the exact measure c a  for each cell a in each cuboid in L  and add noise to each cell independently  including empty cells  Formally  let Fall T   be the vector containing measure c a  for each cell a of each cuboid in L  Fall has sensitivity  L   De   nition 2   since a row in T contributes 1 to exactly one cell in each cuboid in L  Kall adds noise drawn from Lap  L     to each cell     c a    c a    Lap  L       and publishes the resulting vector  The noise variance in Kall is high  With 8 dimensions   L  can reach 2 8   making Var    c a     2    2 16    2   We will discuss the relationship between Kall and related work  4  36  in Section 8  TH E  O R E  M 2  Kall is   differentially private  For any cell a to be published  E    c a     c a  and Var    c a     2 L  2    2   Another option  Kbase  computes only the cells in the d dim cuboid  i e   the base cuboid  directly from T   and adds noise into them  Kbase computes the measures of the remaining cells to be released by aggregating the noisy measures of the base cells  The vector Fbase T   has each entry as the measure c a  of a cell in the base cuboid  and thus its sensitivity is 1  therefore  publishing   c a    c a    Lap 1    for each base cell preserves   differential privacy  When Kbase computes higher level cuboids from Fbase T    we  do not touch T   so   DP is preserved  However  the noise variance grows as we aggregate more base cells for cells in higher levels  TH E  O R E  M 3  Kbase is   differentially private  For any published cell a  E    c a     c a   If a is in a cuboid with dimensions  C   then   c a  has noise variance Var    c a     2   Aj     C   Aj     2  EX A M P L  E 3 1  For the fact table T in Figure 1 a   Figure 2 a  shows the lattice of cuboids under the relationship    Three dimensions Sex  Age  and Salary have cardinality 2  7  and 5  respectively  Each cuboid is labeled as Cx1x2x3   where xi   1 iff the i th dimension is in this cuboid  For example  C011 is the cuboid  Age  Salary   The label on an edge from C to C   in Figure 2 b  is the variance magni   cation ratio when cells in cuboid C are aggregated to compute the cells in C     For example  if C011 is computed from C111  the noise variance doubles  since the dimension Sex has 2 distinct values   each cell in C011 is the aggregation of 2 cells in C111  If C001 is computed from C111  the noise variance is magni     ed 2    7   14 times  since 14 cells in C111 form one in C001  Suppose we want to publish all the cuboids in Figure 2 a   Using Kall   we add Laplace noise Lap 8    to each cell  giving noise variance 2    64   2   128   2 for one cell  Using Kbase  we add Laplace noise Lap 1    to each cell in C111 and then aggregate its cells  Each cell in C100 is built from 7    5 cells in C111  with noise variance 35   2   2   70   2   A cell in C000 is built from 7   5   2 cells in C111  with noise variance 70    2   2   140   2   A better approach is to compute cuboids C111  C110  C101  and C100 from T   adding Laplace noise Lap 4    to each  the sensitivity is 4   We compute the other cuboids from these  Then the noise variance in C100 is 32   2 and in C000 is 2    32   2   64   2  aggregating 2 cells of C100   As shown in Example 3 1  the more cuboids we compute directly from the table T   the higher their sensitivity is  and so the more noise they require  but the less noise is accumulated when other cuboids are computed from them  Kall and Kbase represent the extremes of computing as many or as few cuboids as possible directly from T   There must be a strategy between Kall and Kbase that gives better bounds for the noise variance of released cuboids  A Better Algorithm Kpart To publish a set L     Lall of cuboids  Kpart chooses which cuboids  denoted as Lpre  to compute directly from the fact table T   in a manner that reduces the overall noise  Lpost   L     Lpre includes all the other cuboids in L  We do not require Lpre     L  1   Noise Sources  For each cell a of cuboids in Lpre  Kpart computes c a  from T and releases   c a    c a    Lap s     where  the sensitivity s    Lpre   Note that Lpre is selected by our algorithms in Section 4 1 s t  all cuboids in L can be computed from Lpre  2   Aggregation  For each cuboid C     Lpost  Kpart selects a descendant cuboid C     from Lpre s t  C       C  and computes   c a  for each cell a     C by aggregating the noisy measure of cells in C       We discuss how to pick C     as follows  The measure   c a  output by Kpart is an unbiased estimator of c a  for every cell a  i e   E    c a     c a   For a cell a in cuboid C       Lpre  Var    c a     2s 2    2  s    Lpre    Suppose cell a in C     Lpost is computed from C       Lpre by aggregating on dimensions  C          C     Ak1           Akq    the variance magni   cation is de   ned as mag C  C         1   i   q  Aki    So the noise variance is Var    c a     mag C C        2s 2    2    1  Let mag C  C    1  If C cannot be computed from C    C   C      let mag C  C            We should compute the cells in C     Lpost from the cuboid C         Lpre for which mag C  C       is minimal  i e   mag C  C          minC    Lpre mag C  C      Let noise C  Lpre     min C    Lpre mag C C        2s 2    2  2  be the smallest possible noise variance when computing C from a single cuboid in Lpre  For C       Lpre  noise C     Lpre    2s 2    2   TH E  O R E  M 4  Algorithm Kpart is   differentially private  For any cuboid cell a to be released  E    c a     c a   EX A M P L  E 3 2  Consider the data cube in Example 2 1 and release algorithm Kpart with Lpre    C111  C110  C101  C100   C000 can be computed from C100 with noise variance 2    32   2   since mag C000  C100     2  or from C110 with noise variance 14    32   2   448   2   since mag C000  C110    2    7   14  So Kpart computes C000 from C100  and noise C000  Lpre    64   2   Kpart shares some similarities to the matrix mechanism in  23   Kpart chooses cuboids Lpre to compute L  while  23  chooses a set of queries to answer a given workload of count queries  If we adapt the matrix mechanism for our problem by treating a cell as a count query  we need to manipulate matrices of sizes equal to the number of cells  e g   matrices with 10 6    10 6 entries for a moderate data cube with 10 6 cells   So  23  is not applicable in our context  Ef   cient Implementation of Kpart  Suppose Lpre is given  A naive way to compute DP cuboids in L is to    rst inject noise into each cuboid in Lpre  Then for each C     L  query its descendant C         Lpre  C   C        and aggregate cells in C     to compute cells in C  We can compute DP cuboids in L more ef   ciently with fewer cell queries  Suppose C   C      C       noise is injected into C         Lpre for ensuring DP  and C    is computed from C       Then computing C from C     is equivalent to computing it from C      with DP ensured  So after noise is injected into all cuboids in Lpre  DP cuboids in L can be computed in a level by level way  i e   computing idim cuboids from  i   1  dim cuboids  The total running time is O N d 2    where N     j   Aj     1  is the total number of cells  Problems of Optimizing Noise Source  Choosing Lpre We formalize two versions of the problem of choosing Lpre with different goals  given table T and set L of cuboids to be published  Problem 1    BO U N D MA X VA R I A N C E  Choose a set Lpre of cuboids s t  all cuboids in L can be computed from Lpre and the max noise noise Lpre    maxC   L noise C  Lpre  is minimized  Problem 2    PU B L  I  S H MO S T  Given threshold   0 and cuboid weighting w      choose Lpre s t    C   L  noise C Lpre      0 w C  is maximized  In other words  maximize the weight of the cuboids computed from Lpre with noise variance no more than   0  TH E  O R E  M 5  BO U N D MA X VA R I A N C E and PU B L  I  S H MO S T are both NP hard  where  L  is the input size  The proof uses a non trivial reduction from the VE RT E X COVER problem in degree 3 graphs  Details are in the appendix  Note that Kall and Kbase are special cases of Kpart by letting Lpre   L and Lpre    the base cuboid   respectively  We will introduce two algorithms that choose Lpre carefully so that Kpart is better than Kall and Kbase w r t  objectives in Problems 1 and 2  4  OPTIMIZING THE SOURCE OF NOISE A brute force approach for Problems 1 and 2   enumerating all possible choices of Lpre  all possible cuboid subsets    takes O 2 2 d   time  which is not practical even for d   5  Due to the hardness result in Theorem 5  we introduce approximation algorithms for these two problems  with running time polynomial in 2 d   4 1 Bounding the Maximum Variance We now present a  ln  L   1  2  approximation algorithm for the BO U N D MA X VA R I A N C E problem  with running time polynomialin  L  and 2 d   First  suppose we have an ef   cient algorithm FE AS I  B  L  E for subproblem FE A S I B I L I T Y L       s   for a    xed    and s  is there a set of s cuboids Lpre s t  for all C     L  noise C  Lpre          Let FE A S I B L E L       s  return Lpre if a solution exists  and    NO    otherwise  Then to solve Problem 1  we can    nd the minimum    such that for some s  FE A S I B L E L       s  returns a feasible solution  This    can be found using binary search instead of guessing all possible values  Algorithm 1 provides the details  1    L     0    R     2 L  2    2  or   R     2    4 d    2 if  L    2 d    2  while    L       R    1   2 do 3            L     R  2  4  if FE A S  I  B L E L       s    NO for all s   1             L  then   L         else   R         5  return            R and the solution found by FE A S  I  B L E L           s   FE A S  I  B L E L       s  6  Compute coverage cov C  for each cuboid based on    and s  7  R          COV          8  repeat the following two steps s times 9  Select a cuboid C  such that  cov C        COV   is maximized 10  R     R      C     COV      COV     cov C     11  if COV   L then return R as Lpre  12  else return NO  Algorithm 1  Algorithm for BO U N D MA X VA R I A N C E problem Theorem 5 says that BO U N D MA X VA R I A N C E is NP hard  so there cannot be an ef   cient exact algorithm for the subproblem FE A S I B I L I T Y  We provide a greedy algorithm  analogous to that for SE T COVER  that achieves the promised approximation guarantee  ln  L    1  2 for the BO U N D MA X VA R I A N C E problem  Using  2   we rewrite FE A S I B I L I T Y   s noise constraint as  noise C  Lpre             min C    Lpre mag C  C             2 2s2    3  For    xed        and s  cuboid C   covers cuboid C if C   C   and mag C  C             2 2s2   De   ne C      s coverage to be  cov C            s     C     L   C   C     mag C  C             2 2s2     4  For simplicity  we write cov C     for cov C             s  when        and s are    xed  The following lemma is from  3  and  4   LE  M M A 1  noise C  Lpre         if and only if there exists C       Lpre such that C     cov C              Lpre    By Lemma 1  FE A S I B I L I T Y L       s  reduces to    nding s cuboids that cover all cuboids in L  To solve this problem  we employ the greedy algorithm for the SE T COVER problem  in each of s iterations  we add to Lpre the cuboid that covers the maximum number of not yet covered members of L  The greedy algorithm can    nd at most  ln  L    1 s     cuboids to cover all cuboids in L if the minimum covering set has size at least s       This algorithm  denoted as FE A S I B L E L       s   appears in lines 6 12 of Algorithm 1  TH E  O R E  M 6  Algorithm 1    nds an  ln  L  1  2  approximation to Problem 1 in O min 3 d   2 d  L   L  log  L   time  Moreover  using the solution Lpre produced by Algorithm 1  Kpart is at least as good as Kall and Kbase   in terms of the objective in Problem 1  Algorithm 1   s running time is polynomial in 2 d and  L   and it provides a logarithmic approximation  Note that parameter d is not very large in most applications  but  L  can be as large as 2 d   EX A M P L  E 4 1  Suppose we want to publish all cuboids in Figure 2 a   L   Lall      When FE A S I B L E     is called with      80   2 and s   2  from  4   C   covers C iff C   C   and mag C  C         10  mag C  C     can be computed from Figure 2 b  by multiplying the edge weights on the path from C   to C  In the    rst iteration of lines 8 10  Algorithm 1 chooses C111 for cov C111     C111  C110  C010  C101  C011  covers the most cuboids in L  and puts C111 into R  In the second iteration  Algorithm 1 chooses C101 because cov C101     C101  C100  C001  C000  which covers three  the most  cuboids not covered by C111 yet  C111 and C101 cover all the cuboids  so FE A S I B L E    returns Lpre    C111  C101   The binary search in Algorithm 1 determines that 64   2 is the smallest value of    for which FE A S I B L E    nds a feasible Lpre for some s  In that iteration  for s   4  FE A S I B L E returns Lpre    C111  C110  C101  C100   There  we have cov C111     C111  C011   cov C110     C110  C010   cov C101     C101  C001   and cov C100     C100  C000   4 2 Publishing as Many as Possible We now present a  1     1 e  approximation algorithm for the PU B L  I  S H MO S T problem  with running time polynomial in  L  and 2 d   Given threshold   0  assuming the optimal solution has s cuboids  from Lemma 1  PU B L  I  S H MO S T is equivalent to    nding a set of s cuboids Lpre s t  the weighted sum of the cuboids in L that they cover  with        0 and the    xed s  is as high  as  possible   We will consider values up to  L  for s  and apply the greedy algorithm for the MA X I M U M COVERAGE problem for each choice of s  As outlined in Algorithm 2  initially R and C OV are empty  In each iteration  we    nd the cuboid for which the total weight of the not previously covered cuboids it covers in L is maximal  We put this cuboid into R and put the newly covered cuboids into C OV  After repeating this s times   R    s and C OV is the set of cuboids with noise variance no more than   0 if computed from R  At the end  pick the best R over all choices of s as Lpre  If some cuboids in L cannot be computed from Lpre but we desire to publish them  we can simply add one more cuboid  the base cuboid  into Lpre  1  for s   1  2           L  do Rs     GR E E DYCOVER L    0  s   2  return the best Rs as Lpre  GR E E DYCOVER L    0  s  3  Compute coverage cov C  for each cuboid based on   0 and s  4  R          COV          5  repeat the following steps s times 6  Select a cuboid C  such that  cov C        COV   is maximized  or   C   cov C      COV w C  is maximized  7  R     R      C     COV      COV     cov C     8  return R  Algorithm 2  Algorithm for PU B L  I  S H MO S T TH E  O R E  M 7  Algorithm 2    nds a  1     1 e  approximation to Problem 2 in O min 3 d   2 d  L  d L   time  Moreover  using the solution Lpre produced by Algorithm 2  Kpart is as least as good as Kall and Kbase  in terms of the objective in Problem 2  EX A M P L  E 4 2  Consider the effect of Algorithm 2 on Example 4 1  with   0   40   2 and all cuboids to be published with equal weights  For s   2  when we call GR E E DYCOVER L 40   2   2    C111  C101  is returned as R in line 8  Since no other value of s covers more cuboids   C111  C101  is    nally returned as Lpre  5  ENFORCING CONSISTENCY Suppose Lpre    C1  C2   where  C1     A1  A2  A3  and  C2     A1  A2  A4   Consider cuboid  C     A1  A2  to be published  C can be computed from either C1 or C2  Since we add noise to C1 and C2 independently  the two computations of C will probably yield different results  If we revise Kpart by letting C bethe weighted  based on variance  average of the two results  then C will not be an exact roll up of either C1 or C2  though it will be unbiased  Such inconsistency occurs in data cubes released by both Kall and Kpart  as long as Lpre contains more than one cuboid  In this section  we introduce systematic approaches to enforce consistency across released cuboids  The basic idea is as follows  Consider the noisy measures   c     of cells in each cuboid C     Lpre released by Kpart  where Lpre is selected by either Algorithm 1 or Algorithm 2  Recall that  in Kall   Lpre is the set of all cuboids to be published  We construct consistent measure   c     from   c     so that the consistency constraints are enforced and the distance between   c     and   c     is minimized  Since the algorithm takes only   c     as the input and does not touch the real count measure c     or the fact table T     DP is automatically preserved  Consistency Constraints and Distance Measures Clearly  there is no inconsistency if we compute measures of all cells from the base cells  i e   d dim cells   For a cell a  let Base a  be the set of all base cells  each of which aggregates a disjoint subset of rows that are contained in a  Formally  a       Base a  if and only if for any dimension Ai  a i         implies a i    a    i   So we enforce consistency constraints for measure   c     as follows    a    Base a    c a         c a       cells a   5  We seek the choice of   c     that satis   es consistency constraints in  5  and is as close as possible to   c      but can be computed without reexamining c      We use L p distance  p     1  to measure how close   c     and   c     are  Let Epre be the set of cells in cuboids belonging to Lpre  Treat   c     and   c     as vectors in R Epre   and  the L p distance between is     c           c      p     a   Epre     c a        c a   p   1 p   Finding   c     to minimize     c           c      p subject to  5  can be viewed as a least norm problem  From classic results from convex optimization  7   it can be solved ef   ciently  at least in theory  We will prove that the utility of optimal solutions for L 1   L 2   and L    distances satis   es certain statistical guarantees  More importantly  classic algorithms do not work in our context because the number of variables involved in  5  is equal to the number of cells  which is huge  We provide a practical and theoretically sound algorithm that minimizes the L 2 distance in time linear in the number of cells  5 1 Minimizing L    and L 1 Distance We    rst consider minimizing the L    distance  which is essentially minimizing the maximal difference between   c a  and   c a  for any cell in Epre  i e       c         c            maxa   Epre    c a      c a    Equivalently  we solve for   c     in the following linear program  minimize z  6  s t     c a        c a       z       cells a     Epre    a    Base a    c a         c a       cells a     Epre   4  considers a similar consistency enforcing scheme  but injects noise into all cuboids instead of the carefully selected subset of cuboids Lpre  We can bound the error of   c     as follows  TH E  O R E  M 8   Generalized Theorem 8 in  4   For   c     obtained by solving  6   with probability at least 1         where       Epre  e   2     a   Epre    c a      c a        Epre  Lpre    2 log  Epre        Epre  Lpre        A linear program can also be used to minimize the L 1 distance  As     c         c      1     a    c a      c a    by introducing an auxiliary variable za for each cell a with constraint    za       c a      c a      za  minimizing     c           c      1 while enforcing consistency in   c     is equivalent to the following linear program  minimize   a   Epre za  7  s t     c a        c a       za      cell a     Epre    a    Base a    c a         c a       cell a     Epre  TH E  O R E  M 9  For   c     obtained by solving  7   if       1  with probability at least 1         where           2e   2   1    Epre        2     a   Epre    c a      c a        Epre  Lpre        For a data cube where the cardinality of every dimension is two  Theorem 8 in  4  yields a bound similar to that of our Theorem 8  by discarding the integrality constraint  i e   counts are integers   Theorem 9 mirrors Theorem 8  but replaces the upper tail       Epre  e   with      2e   2   1    Epre    As  Epre  is usually large  linear program  7  and Theorem 9 give a much better bound on the average error in   c     than linear program  6  and Theorem 8  To enforce the integrality constraint for   c     obtained from linear program  6  or  7   we can simply round   c a  for each cell to the nearest integer  which will replace the error bound  Epre  Lpre       with  Epre  Lpre          Epre  in both Theorems 8 and 9  5 2 Minimizing L 2 Distance Now let   s focus on minimizing the L 2 distance  i e   the sum of squared differences between   c a  and   c a  for all cells  This problem has a very ef   cient solution with good statistical guarantees  minimize   a   Epre    c a        c a   2  8  s t    a    Base a    c a         c a       cell a     Lpre  Program  8  can be viewed as a least L 2  norm problem  Its unique solution   c      called the least square solution  can be found using linear algebra  7   The classical method needs to compute multiplication inversion of M    M matrices  where M     j  Aj   is the total number of base cells  Since M is typically larger than 10 6   the classical method is inef   cient in our context  Fortunately  we can derive the optimal solution   c     much more ef   ciently by utilizing the structure of data cubes  as follows  We de   ne Ancs a     to be the set of ancestor cells of cell a     a     Ancs a     if and only if a is in an ancestor of the cuboid containing a   and has the same value as a   on all non     dimensions  formally  a     Ancs a     if and only if for any dimension Ai  a i         implies a i    a    i   If a   is a base cell  a     Ancs a         a       Base a   For a cell a and a cuboid C  let a C  be a   s values on dimensions of  C   Suppose  C     Ai1            Aik    a C     a i1            a ik    For two cuboids C1 and C2  let C1     C2 be the cuboid with dimensions  C1       C2  and C1     C2 with dimensions  C1       C2   We provide the following two stage algorithm to compute the consistent measure   c     that is the optimal solution to program  8   1  Bottom up stage  We    rst compute obs a      for every cell a    in a cuboid C    that can be computed from Lpre  Let obs a          a    Base a       a   Epre   Ancs a      c a    9 All AllC BMax BMaxC PMost PMostC Base  50  100  200  400  800  1600  3200  6400  12800 0 25 0 5 1 1 5 2 Privacy Parameter     a  Max Cuboid Error  20  40  80  160  320  640  1280 0 25 0 5 1 1 5 2 Privacy Parameter     b  Average Cuboid Error Figure 3  Varying privacy parameter   A cuboid C     Lpre is maximal in Lpre if there is no C       Lpre s t  C     C     In all maximal cuboids C    in Lpre  obs a      can be computed as in Formula  9   In all the other cuboids that can be computed from Lpre  obs a      can be computed recursively as follows  suppose a        C    and C    can be computed from Lpre  there must be some cuboid C with dimensionality dim C        1 s t  either C     Lpre or C can be computed from Lpre  then  obs a          a  a   C a C     a     C     obs a    10  2  Top down stage  After obs a      is computed for every possible cell a      consider another quantity est a      for each cell a        C      est a          C   Lpre   b  b    C   C      b C   C     a     C   C     deg C     C       c b    11  where deg C      Ai   A    C   Ai  and A is the set of all dimensions and  Ai  is the cardinality of dimension Ai  Suppose obs       s are computed in the    rst stage as constants and   c       s are variables  Solving the equations est a         obs a       we can compute   c       s in a top down manner  from ancestors to descendants  as follows  ratio C          C  C   Lpre  C    C deg C     C        12  aux a          C  C   Lpre  C    C   b  b    C   C      b C   C     a     C   C     deg C     C       c b    13    c a        1 ratio C       obs a          aux a           14  The above approach can be also applied to Kall because Kall is a special case of Kpart obtained by setting Lpre   L  Note that this approach generalizes the consistency enforcing scheme in  19  from a tree like structure  hierarchy of intervals  to a lattice  The method in  19  cannot be directly applied here  Optimality  We can prove   c     obtained above is not only consistent but also an unbiased estimator of c      Also    c     is optimal in the sense that no other linear unbiased estimator of c     obtained from   c     has smaller variance  i e   smaller expected squared error  TH E  O R E  M 10   i  The above algorithm correctly computes a value for   c     that is consistent and solves the L 2 minimization problem  8    ii  The above algorithm requires O N d 2  d Lpre    time to compute   c     for all N cells   iii  For any cell of cuboids in Lpre    c a  is an unbiased estimator of c a   i e   E    c a     c a    iv  For any cell a    c a  has the smallest variance among any linear unbiased estimator  e g     c a   of c a  obtained from   c       v  For any set P of cells    a   P   c a  has the smallest variance among any linear unbiased estimator of   a   P c a  obtained from   c       0  100  200  300  400  500  600  0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 Error ratio    0        PMost Max Error PMostC Max Error PMost Avg Error PMostC Avg Error  120  140  160  180  200  220  240  260  0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 ratio    0        total   of cuboids   of precise cuboids Figure 4  Varying   0 in Algorithm 2  for PMost and PMostC  6  EXPERIMENTS We evaluate and compare seven proposed techniques on both real and synthetic datasets  The    rst two techniques are Kall and Kbase  which were de   ned at the beginning of Section 3  We denote them as All and Base  respectively  in this section  Another two are based on our generalized framework Kpart  One of these has the objective of bounding the max noise variance  Problem 1  and is denoted by BMax  Kpart   Algorithm 1   The other one has the objective of maximizing the number of cuboids with noise variance no more than a given variance threshold   0  Problem 2  and is denoted by PMost  Kpart   Algorithm 2   The last three techniques involve applying the methods in Section 5 2 to enforce consistency in the data cubes published by All  Lpre   L   BMax  and PMost  The resulting three techniques are denoted AllC  BMaxC  and PMostC  respectively  The LP based techniques in Section 5 1 are not practical for large tables  All seven algorithms are coded in C   and evaluated on an 8GB 64 bit 2 40GHz PC  Real dataset  We use the Adult dataset from the UCI Machine Learning Repository  http   archive ics uci edu ml   with census information  It contains 32 561 rows with 8 categorical dimensions  workclass  cardinality 9   education  16   marital status  7   occupation  15   relationship  6   race  5   sex  2   and salary  2   Synthetic dataset  To generate the fact table  we    rst    x the number of dimensions and the dimension cardinalities  Then we generate each row independently  with each of its column values drawn uniformly and independently from the domain of its dimension  Error measurement  We compute the error as the absolute difference between the real count measure computed directly from the fact table and the noisy measure released by one of the seven   DP algorithms  The cuboid error is the average error for all cells in this cuboid  We report both the max cuboid error and the average cuboid error among all published cuboids  We release L   all cuboids and evaluate all algorithms on the same set of published cuboids  This is the hardest test for all algorithms  as less noise will be needed if L omits some cuboids  Exp I  Varying privacy parameter    Using the Adult dataset  we vary privacy parameter   from 0 25 to 2  Smaller   implies more privacy and thus more noise  Recall Algorithm 2  for PMost  takes a variance threshold   0 in the input  here  we set   0   0 5       where      is the variance bound found by Algorithm 1  for BMax   Figure 3 uses a logarithmic scale to show the max average error in the released cuboids  As the inconsistent version of an approach always has at least as much error as the consistent version  the two versions  All AllC  BMax BMaxC  and PMost PMostC  are stacked together on a single bar in each histogram in Section 6  BMax and PMost always incur much less error than the two baselines Base and All  BMax is better than PMost for bounding the max error  while PMost is better than BMax for bounding the average error  Base performs the worst in terms of the max error  because only the base cuboid is computed from the table  and the noise is magni   ed signi   cantly when cuboids at higher levels are computed by aggregating cells in the base cuboid  All is the worstAll AllC BMax BMaxC PMost PMostC Base  20  40  80  160  320  640  1280  2560 0 1 2 3 4 5 6 7 8 m  Dimensionality of Cuboids Figure 5  Max cuboid error in different cuboids as dimensionality varies  when all cuboids must be released in terms of the average error  for the large amount of noise initially injected in each cuboid  Error decreases as   increases  As suggested by Theorem 10  iv   v   our consistency enforcing techniques tend to reduce error  since the variance of the consistent measure   c     is no larger than that of the inconsistent   c      So AllC BMaxC PMostC reduces error by 30  50   compared to All BMax PMost  while providing the same privacy guarantee  Exp II  Varying variance threshold   0  Note that performance of PMost and PMostC depends on the input threshold   0 in Problem 2  as   0 determines Lpre  Fix     1  Given the variance bound      found by Algorithm 1  we vary   0 from 0 1     to 0 9       Figure 4 shows how   0 affects Algorithm 2 on the Adult dataset  As   0 increases  we have more precise cuboids  i e   the released cuboids with noise variance no more than   0  When   0          all 2 8   256 cuboids are precise  and PMost PMostC is equivalent to BMax BMaxC  as the same Lpre is used  But the max average error of PMost and PMostC does not increase monotonically with   0  both   0        and   0   0 5     are local optimums for minimizing errors  In the remaining experiments  we set   0   0 5       Exp III  Noise in different cuboids  Figure 5 shows the max error in cuboids of different dimensionality on Adult  for     1  Recall in an m dim cuboid  a cell has non     values on exactly m dimensions  As m decreases  a cell in an m dim cuboid aggregates more base cells  So Base aggregates more noise from base cells as m drops  and performs the worst for m   3  BMaxC is the best when 0   m    5  and its max error changes little for 0   m    7  The consistency enforcing techniques  AllC BMaxC PMostC  are very effective for small m  reducing error by up to 70   Exp IV  Varying number of dimensions  We create two more dimensions  each of which has cardinality 4  on the Adult dataset  and generate their values randomly for each row  We consider the fact table with the    rst 6 to all the 10 dimensions  Fix     1  We report the max average error of the seven approaches in Figure 6  As the number of dimensions increases  BMaxC and PMostC are always the best two  and the error in both All and Base is much larger and increases faster than the error in others  Exp V  Varying cardinality of dimensions  We generate synthetic tables with 7 dimensions  each of which has the same cardinality  We vary the cardinality from 2 to 10  and generate a table with 10 8 rows randomly for each  We report the error of each approach in Figure 7  for     1  The performance of Base deteriorates quickly as the cardinality increases  because more cells in the base cuboid need to be aggregated  The performance of All does not change much  for its performance is mainly determined by the number of dimensions  which determines the total number of cuboids  Again  BMaxC and PMostC perform best in most cases  Exp VI  Ef   ciency  Consider a data cube with 5 10 dimensions on the Adult dataset  with two more synthetic dimensions as in ExpIV   The overall DP data cube publishing time is reported in Figure 8  which can be decomposed as follows  Recall Algorithms 1 2  80  160  320  640  1280  2560  5120  10240 6 7 8 9 10 Dimensionality of Fact Table  a  Max cuboid error  40  80  160  320  640  1280 6 7 8 9 10 Dimensionality of Fact Table  b  Average cuboid error Figure 6  Varying dimensionality of fact table  20  40  80  160  320  640  1280  2560  3840 2 4 6 8 10 Cardinality of Dimensions  a  Max Cuboid Error  20  40  80  160 2 4 6 8 10 Cardinality of Dimensions  b  Average Cuboid Error Figure 7  Varying cardinality of dimensions  7 dimensions  select Lpre for BMax BMaxC and PMost PMostC  respectively  The running time of Algorithms 1 2 is reported in Figure 9 a   For different choices of Lpre on the 8 dim table  the time needed for noise injection  Kpart  and consistency enforcement  the method in Section 5 2  in all cuboids is reported in Figure 9 b   From Figure 9 a   although the running time of Algorithms 1 2 is polynomial in 2 d   they are not the bottleneck  Their running time is always a very small portion of the overall DP data cube publishing  From Figure 9 b   it is shown that the consistency enforcement time increases linearly with  Lpre   as predicted by Theorem 10  ii   The time for noise injection decreases as  Lpre  increases  This is because when more cuboids are initially injected with noise  less aggregation of noise occurs later on  From Figure 8  using consistency enforcement  AllC is especially expensive  because  Lpre    2 d in AllC  BMaxC and PMostC  although using consistency enforcement  usually only need 3  10  time of AllC  for they use smaller Lpre   s  For all approaches  the publishing time increases exponentially with the dimensionality  mainly because the total number of cells increases exponentially   1  3  10  40  160  640  2560  10240 5  2x10 5   6  9x10 5   7  3x10 6   8  8x10 6   9  4x10 7  10  2x10 8   Dimensionality of Fact Table  and Number of Cells  Figure 8  DP data cube publishing time  in seconds  Summary  Both All and Base are sensitive to the dimensionality of the fact table  and Base is also sensitive to cardinalities of dimensions  All usually has a large average error  as a large amount of noise is injected into all cuboids  Base has a large max error  because noise is aggregated from the base cells  and that is why Base incurs small average errors in the cuboids close to the base cuboid  BMaxC and PMostC are the best most of the time  All BMax PMost run much faster than AllC BMaxC PMostC  But with our consistency enforcement  AllC BMaxC PMostC reduce error in All BMax PMost  respectively  by typically 30   70  or more  and ensure that the published data is consistent  The error of BMaxC and PMostC is usually only 5  30  of 0 001  0 01  0 1  1  5  5 6 7 8 9 10 Number of Dimensions Select L pre  for BMax  Alg  1  Select L pre  for PMost Alg  2   a  Time  in seconds   1  2  4  8  16  32  64  128  256  3 5 12 20 64 256  L pre   Noise injection Enforce consistency  b  Time  in seconds  Figure 9  Ef   ciency of different algorithms the error incurred by All  and 20  50  of the error of AllC  Note that the y axis in our    gures is always in logarithmic scale  Because of our consistency enforcing method  the error of AllC is sometimes comparable to BMax BMaxC and PMost PMostC  when the dimensionality of the fact table is low  However  AllC is very expensive because  Lpre  in AllC is equal to 2 d  recall Theorem 10 for the running time of our consistency enforcement   When there are more than    ve dimensions  AllC   s publishing time is 6 10 times larger than BMaxC   s    10 times for ten dimensions   and 10 40 times larger than PMostC   s    40 times for ten dimensions   7  DISCUSSION AND EXTENSIONS 7 1 Relative Error vs  Absolute Error The amount of noise Kpart adds to DP cuboids is independent of the number of rows and speci   c data in the fact table  Instead  the selection of Lpre and the amount of noise depend only on which cuboids are to be published  the number of dimensions  and their cardinalities  So our expected absolute error is    xed if the structure of the fact table is    xed  no matter how many rows there are  This feature of our approaches is also true in DP based frameworks for different publishing tasks  4  19  23  36   The implication is that the expected relative error cannot be bounded in general  Because  with the expected absolute error    xed  some cells may have very small values of the count measure  e g   1   while some have very large values  e g   10 3    That is also why we report absolute error in Section 6  On the other hand  the advantage is  for a particular cell  it has less relative error if it aggregates more rows  7 2 Further Reduction of Noise We can generalize our noise control framework Kpart by adding different amounts of noise to different cuboids in Lpre to further reduce noise  Suppose Lpre    C1           Cs  and noise Lap   i  is injected into each cell in Ci  By the composition property of DP  28   we claim that if  s i 1 1   i      then publishing Lpre is   DP  Finding the optimal Lpre and parameters    i  to achieve the goals in Problems 1 and 2 is hard  but Algorithms 1 and 2 can be modi   ed to provide approximate solutions  Here  privacy parameter   can be interpreted as the cost budget in WE I G H T E D SE T COVER  To enforce consistency and yield an optimality result similar to Theorem 10  we can solve a weighted version of the least squares problem in Section 5 2  Our experiments show this reduces absolute error in BMaxC PMostC by 10   We omit the details here  7 3 Extension to Other Measures Our techniques for the count measure can be extended to other two basic measures sum and avg  sum can be considered as a generalized count measure  where each row in the fact table is associated with a value instead of just 0 1  Compared to count  the sensitivity of a publishing function for sum is magni   ed    times  where    is the range of possible values for any individual fact table tuple  Thus our techniques for count can be applied to sum  with the noise variance magni   ed   2 times  To handle the average measure avg  we can compute two DP data cubes  partitioning the privacy budget across them   one with sum measure and one with count measure  from the same fact table  The avg measure of a cell can be computed from the two cubes  without violating DP  7 4 Handling Larger Data Cubes Our approaches introduced so far are applicable for mid size data cubes  e g   with     12 dimensions or     10 9 cells  Since the current framework needs to store all cells for consistency enforcement  it cannot handle data cubes with too many cells  For even larger data cubes  e g   with     20 dimensions and     2 20 cuboids   it is unnecessary to publish all cuboids at one time  as typical users are likely to query only a very small portion of them  Also  it is impossible to publish all cuboids while ensuring DP  as the huge amount of noise will make the result meaningless  So we outline an online version of our approach as follows  Initially  Lpre       and we have certain amount   of privacy budget  When a cuboid query C comes  if C      Lpre and C can be computed from some DP cuboid C         Lpre  there are two choices  a  compute C from C       with error in C     magni   ed in C  or b  compute real cuboid C using high dimensional OLAP techniques like  25   inject noise into C to obtain a DP cuboid  insert C into Lpre  and deduct a certain amount of privacy budget from    If C      Lpre but C cannot be computed from any DP cuboid C         Lpre  we have to follow b  above  If C     Lpre or C used to be queried  we can directly output the old DP cuboid C  After we run out of privacy budget    to ensure   DP  we cannot create new DP cuboids in Lpre any more and may be unable to answer new queries  How to distribute the privacy budget online so that more queries can be answered with less error is interesting future work  8  RELATED WORK Since   differential privacy  DP   12  was introduced  many techniques have been proposed for different data publishing and analysis tasks  refer to  10  11  for a comprehensive review   For example  the notion of DP has been applied to releasing query and click histograms from search logs  18  22   recommender systems  29   publishing commuting patterns  27   publishing results of machine learning  6  8  20   clustering  13  30   decision tree  14   mining frequent patterns  5   and aggregating distributed time series  31   For a single counting query  one method to ensure DP  12  has been shown to be optimal under a certain utility model  17   Recent works  36  19  23  on differentially private count queries over histograms are related to our work  Xiao et al   36  propose to use the Haar wavelet for range count queries  Hay et al   19  propose an approach based on a hierarchy of intervals  Li et al   23  propose a general framework which supports answering a given workload of count queries  and consider the problem of    nding optimal strategies for a workload  While  36  and  19  can be uni   ed in the framework in  23   the speci   c algorithms given in  36  19  are more ef   cient than the matrix multiplication used in  23   Xiao et al  extend their wavelet approach to nominal attributes and multidimensional count queries  Their extended approach can be applied in our problem to achieve the same noise bounds as our Kall  refer to Theorem 3 in  36  and Theorem 2 in this work   In general  our Kpart will add less noise  as shown in Theorems 6 7  Barak et al   4  show how to publish a set of marginals of a contingency table while ensuring DP  One of their two approaches is similar to Kall   add noise to all the marginals to be published  Their LP rounding method minimizes the L    distance while enforcing consistency and integrality and removing negative numbers in the publishing  Our Theorem 9 shows that minimizing the L 1 distanceyields a much better theoretical bound on error  The other approach in  4  is similar  but moves to the Fourier domain at    rst  Unlike our work  they do not optimize the publishing strategy  i e   the selection of Lpre   Moreover  the number of variables in the LP equals the number of cells  often   10 6 in our experiments   So LP based methods can only handle data cubes with small numbers of cells  Agrawal et al   3  study how to support OLAP queries while ensuring a limited form of    1    2  privacy  by randomizing each entry in the fact table with a constant probability  An OLAP query on the fact table can be answered from the perturbed table within roughly    dataset   4      1    2  privacy is in general not as strong as   differential privacy  Also  the error incurred by this method  3  depends on the dataset size  in our framework  the amount of noise to be added is data independent  only determined by the number of cuboids to be published and the structure of the fact table  Differential privacy provides much stronger privacy guarantees than other privacy concepts based on deterministic algorithms do  such as k anonymity  32  and its extension l diversity  26  and tcloseness  24    34  studies how to specify authorization and control inferences for OLAP queries in the data cube  9  CONCLUSIONS We have introduced a general noise control framework to release all or part of a data cube in an   differentially private way  To reduce the noise in the released cuboids  we choose certain cuboids Lpre to compute directly from the fact table  and add noise to make them differentially private  All the other cuboids are computed directly from the cuboids in Lpre  without adding additional noise or revisiting the fact table  We modeled the selection of Lpre as two optimization problems  proved them NP hard  and proposed approximation algorithms  To ensure consistency across different rollups of released cuboids  we proposed consistency enforcing techniques that have the side bene   t of reducing noise  10  ACKNOWLEDGMENTS We thank the reviewers for their insightful suggestions that immensely improved the paper  This work was supported in part by NSF IIS 09 05215  U S  Air Force Of   ce of Scienti   c Research MURI award FA9550 08 1 0265  the U S  Army Research Laboratory under Cooperative Agreement Number W911NF 09 2 0053  NS CTA   A STAR SERC grant 1021580074  and a grant from Boeing  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the of   cial policies  either expressed or implied  of the Army Research Laboratory or the U S  Government  The U S  Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon  11  REFERENCES  1  www cs cmu edu  compthink mindswaps oct07 difpriv ppt  2007   2  N  R  Adam and J  C  Wortmann  Security control methods for statistical databases  A comparative study  ACM Comput  Surv   21 4  515   556  1989   3  R  Agrawal  R  Srikant  and D  Thomas  Privacy preserving OLAP  In SIGMOD  pages 251   262  2005   4  B  Barak  K  Chaudhuri  C  Dwork  S  Kale  F  McSherry  and K  Talwar  Privacy  accuracy  and consistency too  a holistic solution to contingency table release  In PODS  pages 273   282  2007   5  R  Bhaskar  S  Laxman  A  Smith  and A  Thakurta  Discovering frequent patterns in sensitive data  In KDD  pages 503   512  2010   6  A  Blum  K  Ligett  and A  Roth  A learning theory approach to non interactive database privacy  In STOC  pages 609   618  2008   7  S  Boyd and L  Vandenberghe  Convex Optimization  Cambridge Univ  Press  2004   8  K  Chaudhuri and C  Monteleoni  Privacy preserving logistic regression  In NIPS  pages 289   296  2008   9  D  P  Dubhashi and A  Panconesi  Concentration of Measure for the Analysis of Randomized Algorithms  Cambridge Univ  Press  2009   10  C  Dwork  Differential privacy  A survey of results  In TAMC  pages 1   19  2008   11  C  Dwork  The differential privacy frontier  extended abstract   In TCC  pages 496   502  2009   12  C  Dwork  F  McSherry  K  Nissim  and A  Smith  Calibrating noise to sensitivity in private data analysis  In TCC  pages 265   284  2006   13  D  Feldman  A  Fiat  H  Kaplan  and K  Nissim  Private coresets  In STOC  pages 361   370  2009   14  A  Friedman and A  Schuster  Data mining with differential privacy  In KDD  pages 493   502  2010   15  B  C  M  Fung  K  Wang  R  Chen  and P  S  Yu  Privacy preserving data publishing  A survey on recent developments  ACM Comput  Surv   42 4   2010   16  S  R  Ganta  S  P  Kasiviswanathan  and A  Smith  Composition attacks and auxiliary information in data privacy  In KDD  pages 265   273  2008   17  A  Ghosh  T  Roughgarden  and M  Sundararajan  Universally utility maximizing privacy mechanisms  In STOC  pages 351   360  2009   18  M  G  tz  A  Machanavajjhala  G  Wang  X  Xiao  and J  Gehrke  Publishing search logs   a comparative study of privacy guarantees  TKDE  2011   19  M  Hay  V  Rastogi  G  Miklau  and D  Suciu  Boosting the accuracy of differentially private queries through consistency  In PVLDB  pages 1021   1032  2010   20  S  P  Kasiviswanathan  H  K  Lee  K  Nissim  S  Raskhodnikova  and A  Smith  What can we learn privately  In FOCS  pages 531   540  2008   21  D  Kifer  Attacks on privacy and deFinetti   s theorem  In SIGMOD  pages 127   138  2009   22  A  Korolova  K  Kenthapadi  N  Mishra  and A  Ntoulas  Releasing search queries and clicks privately  In WWW  pages 171   180  2009   23  C  Li  M  Hay  V  Rastogi  G  Miklau  and A  McGregor  Optimizing histogram queries under differential privacy  In PODS  pages 123   134  2010   24  N  Li  T  Li  and S  Venkatasubramanian  t closeness  Privacy beyond k anonymity and l diversity  In ICDE  pages 106   115  2007   25  X  Li  J  Han  and H  Gonzalez  High dimensional OLAP  A minimal cubing approach  In VLDB  pages 528   539  2004   26  A  Machanavajjhala  J  Gehrke  D  Kifer  and M  Venkitasubramaniam  l diversity  Privacy beyond k anonymity  In ICDE  page 24  2006   27  A  Machanavajjhala  D  Kifer  J  M  Abowd  J  Gehrke  and L  Vilhuber  Privacy  Theory meets practice on the map  In ICDE  pages 277   286  2008   28  F  McSherry  Privacy integrated queries  an extensible platform for privacy preserving data analysis  In SIGMOD  pages 19   30  2009   29  F  McSherry and I  Mironov  Differentially private recommender systems  building privacy into the Net   ix prize contenders  In KDD  pages 627   636  2009   30  K  Nissim  S  Raskhodnikova  and A  Smith  Smooth sensitivity and sampling in private data analysis  In STOC  pages 75   84  2007   31  V  Rastogi and S  Nath  Differentially private aggregation of distributed time series with transformation and encryption  In SIGMOD  pages 735   746  2010   32  P  Samarati and L  Sweeney  Generalizing data to provide anonymity when disclosing information  abstract   In PODS  page 188  1998   33  S  D  Silvey  Statistical Inference  Chapman Hall  1975   34  L  Wang  S  Jajodia  and D  Wijesekera  Preserving privacy in on line analytical processing data cubes  In Secure Data Management in Decentralized Systems  pages 355   380  2007   35  R  C  W  Wong  A  W  C  Fu  K  Wang  and J  Pei  Minimality attack in privacy preserving data publishing  In VLDB  pages 543   554  2007   36  X  Xiao  G  Wang  and J  Gehrke  Differential privacy via wavelet transforms  In ICDE  pages 225   236  2010 APPENDIX  Proofs Proof of Theorem 2  The vector Fall T   has sensitivity S Fall     L   So from Theorem 1  Kall is   DP  As the noise is Lap  L      we have E    c a     c a  and Var    c a     2 L    2     Proof of Theorem 3  The sensitivity S Fbase    1  So from Theorem 1  publishing Fbase T    Lap 1     is   DP  For any other cell a not in the base cuboid  the computation of   c a  takes the released base cuboid as input  so from the composition property  28   is    DP  For a cell a in a cuboid C  it aggregates   Aj     C   Aj   cells  a     in the base cuboid  Then E    c a       a  E    c a          a  c a       c a   And since noise is generated independently  we have the variance Var    c a       a  Var    c a            Aj     C   Aj        2   2     Proof of Theorem 4  Consider measures  c a   for all cells in the s selected cuboids in Lpre  the sensitivity of publishing Lpre is s    Lpre   So by Theorem 1 and the composition property of DP  adding a noise Lap s    to each cell in Lpre and computing L from Lpre is   DP  E    c a     c a  is also from the linearity of expectation    Proof of Theorem 5  To complete the proof  we reduce the problem VE RT E X COVER I N  D E  G R E  E  3 G R A P H S to the BO U N D MA X VA R I A N C E problem  Then the hardness of the PU B L  I  S H MO S T problem follows  Following is an instance of the VE RT E X COVER problem in a degree 3 graph  where the degree of a vertex is bounded by 3   Given a degree 3  undirected  graph G V   E  where  V     n  decide if G has a vertex cover V       V with at most m    n  vertices  V       V is said to be a vertex cover of G iff for any edge uv     E  we have either u     V   or v     V     Abusing the notations a bit  let cov v  be the edges incident on a vertex v  then we want to decide if there is V       V such that    v   V   cov v    E and  V         m  We can assume the degree of any vertex in V is larger than 1  since a degree 1 vertex will be never chosen into a minimum vertex cover  And since there is at least one vertex with degree 3  otherwise G can be decomposed into cycles which are the trivial case for the VE RT E X COVER problem   we have  E    2n 2   n  Construct an instance of the BO U N D MA X VA R I A N C E problem from the above instance of VE RT E X COVER problem  accordingly   i  For each edge e     E  create a dimension Ae with cardinality  Ae    2  and a 1 dim cuboid  C 1 e      Ae    ii  Create 3n distinct dimensions B1           B3n  each of cardinality 2  For each of them  create a 1 dim cuboid  C 1 i      Bi    iii  For each vertex v     V   create a 3 dim cuboid  C 3 v      Ae1   Ae2   Ae3   for each edge ei incident on v  Note that a vertex may have less than 3 edges incident on it  in this case  we create one or two new distinct dimensions with cardinality 2  and include them in  C 3 v    So we create n 3 dim cuboids here   iv  Create another n 3 dim cuboids C 3 x   s  each cuboid  C 3 x     Bx1   Bx2   Bx3    where each Bxi    s are distinct dimensions with cardinality 2 created in  ii    v  For each 3 dim cuboid   C 3 v      Ae1   Ae2   Ae3   or  C 3 x     Bx1   Bx2   Bx3    create a new dimension Dv or Dx  respectively  with cardinality 4  and a 4 dim cuboid  C 4 v      Ae1   Ae2   Ae3   Dv  or  C 4 x     Bx1   Bx2   Bx3   Dx   respectively  So in total  we have 2n 4 dim cuboids created here  In this instance of the BO U N D MA X VA R I A N C E problem  we want to publish all the 1 dim  3 dim  and 4 dim cuboids in  i   v   denoted by L  and to decide if we can select a set of cuboids Lpre such that the max noise variance in releasing L from Lpre using Kpart is at most 8 3n   m  2    2  noise Lpre      8 3n   m  2    2    We prove the VE RT E X COVER instance is YES if and only if the BO U N D MA X VA R I A N C E instance is YES  to complete our proof  Due to the space limit  details are deferred to the full version    Remark  The main dif   culty in the reduction is the lattice structure of cuboids  And  how to prove disprove the NP hardness when the number of dimensions d is bounded  d   O log  L    and how to prove the hardness of approximation are interesting open questions  Proof of Lemma 1  The proof is from the de   nition  Let s    Lpre   If noise C  Lpre          from Equation  2   there is a C       Lpre s t  mag C  C             2 2s2   Thus from Equation  4   C     cov C             s   The converse direction is similar    Proof of Theorem 6  Approximation Ratio  Suppose the optimal solution is          i e   there are s     cuboids Lpre s t  for any C     L  noise C  Lpre               From Lemma 1  equivalently  there are s     cuboids C     1            C     s    s t   s     i 1 cov C     i               s         L  And  from Equation  4   for any cuboid C and      0  cov C     2               s         cov C               s        Suppose in line 4 of Algorithm 1  we have         2        and s     s       In the following part  we will prove when      ln  L    1  the algorithm FE A S I B L E L       s  can    nd s cuboids C   1           C   s  s t   s i 1 cov C   i           s    L  which implies that  with Lpre    C   1             C   s   maxC   L noise C  Lpre              2          So we can conclude that Algorithm 1    nds an  ln  L    1  2  approximation  Since for any cuboid C  cov C         s    cov C              s       for the selection of    and p above  we write both of them as cov C   Now we only need to prove  if there exists s     cuboids C     1            C     s    such that  s     i 1 cov C     i     L  the algorithm FE A S I B L E    nds s     s     cuboids C   1           C   s  in this order  s t   s i 1 cov C   i     L  We apply the analysis of the greedy SE T COVER algorithm here  Let li be the number of cuboids in L uncovered by  C   1 </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp4 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdpsp4">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_privacy_security"/>
        <doc>iReduct  Differential Privacy with Reduced Relative Errors ### Xiaokui Xiao       Gabriel Bender       Michael Hay       Johannes Gehrke         School of Computer Engineering Nanyang Technological University Singapore xkxiao ntu edu sg    Department of Computer Science Cornell University Ithaca  NY  USA  gbender mhay johannes  cs cornell edu ABSTRACT Prior work in differential privacy has produced techniques for answering aggregate queries over sensitive data in a privacypreserving way  These techniques achieve privacy by adding noise to the query answers  Their objective is typically to minimize absolute errors while satisfying differential privacy  Thus  query answers are injected with noise whose scale is independent of whether the answers are large or small  The noisy results for queries whose true answers are small therefore tend to be dominated by noise  which leads to inferior data utility  This paper introduces iReduct  a differentially private algorithm for computing answers with reduced relative errors  The basic idea of iReduct is to inject different amounts of noise to different query results  so that smaller  larger  values are more likely to be injected with less  more  noise  The algorithm is based on a novel resampling technique that employs correlated noise to improve data utility  Performance is evaluated on an instantiation of iReduct that generates marginals  i e   projections of multi dimensional histograms onto subsets of their attributes  Experiments on real data demonstrate the effectiveness of our solution  Categories and Subject Descriptors H 2 0  DATABASE MANAGEMENT   Security  integrity  and protection General Terms Algorithms  Security Keywords Privacy  Differential Privacy ### 1  INTRODUCTION Disseminating aggregate statistics of private data has much bene   t to the public  For example  census bureaus already publish aggregate information about census data to improve decisions about the allocation of funding  hospitals could release aggregate information about their patients for medical research  However  since Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  those datasets contain private information  it is important to ensure that the published statistics do not leak sensitive information about any individual who contributed data  Methods for protecting against such information leakage have been the subject of research for decades  The current state of the art paradigm for privacy preserving data publishing is differential privacy  Differential privacy requires that the aggregate statistics reported by a data publisher should be perturbed by a randomized algorithm G  so that the output of G remains roughly the same even if any single tuple in the input data is arbitrarily modi   ed  This ensures that given the output of G  an adversary will not be able to infer much about any single tuple in the input  and thus privacy is protected  In this paper  we consider the setting in which the goal is to publish answers to a batch set of queries  each of which maps the input dataset to a real number  4 8 16 17 21 23 32   The pioneering work by Dwork et al   8  showed that it is possible to achieve differential privacy by adding i i d  random noise to each query answer if the noise is sampled from a Laplace distribution where the scale of the noise is appropriately calibrated to the query set  In recent years  there has been much research on developing new differentially private methods with improved accuracy  4 16 17 21 32   Such work has generally focused on reducing the absolute error of the queries  and thus the amount of noise injected into a query is independent of the actual magnitude of the query answer  In many applications  however  the error that can be tolerated is relative to the magnitude of the query answer  larger answers can tolerate more noise  For example  suppose that a hospital performs queries q1 and q2 on a patient record database in order to obtain counts of two different but potentially overlapping medical conditions  Suppose the    rst condition is relatively rare  so that the exact answer for q1 is 10  while the second is common  so that the exact answer for q2 is 10000  In that case  the noisy answer to q1 might be dominated by noise  even though the result of q2 would be quite accurate  Other applications where relative errors are important include learning classi   cation models  12   selectivity estimation  13   and approximate query processing  29   Our Contributions  This paper introduces several techniques that automatically adjust the scale of the noise to reduce relative errors while still ensuring differential privacy  Our main contribution is the iReduct algorithm  Section 4   In a nutshell  iReduct initially obtains very rough estimates of the query answers and subsequently uses this information to iteratively re   ne its estimates with the goal of minimizing relative errors  Ordinarily  iterative resampling would incur a considerable privacy    cost    because each noisy answer to the same query leaks additional information  We avoid this cost by using an innovative resampling function we call NoiseDown  The key to NoiseDown is that it resamples conditioned onprevious samples  generating a sequence of correlated estimates of successively reduced noise magnitude  We prove that the privacy cost of the sequence is equivalent to the cost of sampling a single Laplace random variable having the same noise magnitude as the last estimate in the sequence produced by NoiseDown  To demonstrate the application of iReduct  we present an instantiation of iReduct for generating marginals  i e   the projections of a multi dimensional histogram onto various subsets of its attributes  Section 5   With extensive experiments on real data  Section 6   we show that iReduct has lower relative errors than existing solutions and other baseline techniques introduced in this paper  In addition  we demonstrate the practical utility of optimizing for relative errors  showing that it yields more accurate machine learning classi   ers  2  PRELIMINARIES 2 1 Problem Formulation Let T be a dataset  and Q    q1          qm  be a sequence of m queries on T   each of which maps T to a real number  We aim to publish the results of all queries in Q on T using an algorithm that satis   es   differential privacy  a notion of privacy de   ned based on the concept of neighboring datasets  DEFINITION 1  NE I G H B O R I N G DATAS ETS   Two datasets T1 and T2 are neighboring datasets if they have the same cardinality but differ in one tuple    DEFINITION 2    DIFFERENTIAL PR I VAC Y   9       A randomized algorithm G satis   es   differential privacy  if for any output O of G and any neighboring datasets T1 and T2  P r  G T1    O      e      P r  G T2    O      We measure the quality of the published query results by their relative errors  In particular  the relative error of a published numerical answer r     with respect to the original answer r is de   ned as err r          r         r  max r         1  where    is a user speci   ed constant  referred to as the sanity bound  used to mitigate the effect of excessively small query results  This de   nition of relative error follows from previous work  13 29   For ease of exposition  we assume that all queries in Q have the same sanity bound  but our techniques can be easily extended to the case when the sanity bound varies from query to query  Table 1 summarizes the notations that will be frequently used in this paper  2 2 Differential Privacy via Laplace Noise Dwork et al   9  show that   differential privacy can be achieved by adding i i d  noise to the result of each query in Q  Speci   cally  the noise is sampled from the Laplace distribution which has the following probability density function  p d f   P r      x    1 2   e    x             2  where    is the mean of the distribution  we assume      0 unless stated otherwise  and     referred to as the noise scale  is a parameter that controls the degree of privacy protection  A random variable that follows the Laplace distribution has a standard deviation of     2       and an expected absolute deviation of     ie  E                   Symbol Description q T  the result of a query q on a dataset T Q a given sequence of m queries  q1            qm     a sequence of m noise scales    1            m     the sanity bound for the queries in Q S Q  the sensitivity of Q GS Q      the generalized sensitivity of Q with respect to    x an arbitrary real number yi a random variable such that yi     x follows a Laplace distribution         min x  y     1  f the conditional probability density function of y   given x  y      and        as de   ned in Equation 6 Table 1  Frequently Used Notations The resulting privacy depends both on the scale of the noise and the sensitivity of Q  Informally  sensitivity measures the maximum change in query answers due to changing a single tuple in the database  DEFINITION 3   SE N S I T I V I T Y   9       The sensitivity of a sequence Q of queries is de   ned as S Q     max T1 T2  m i 1  qi T1      qi T2    3  where T1 and T2 are any neighboring datasets    Dwork et al  prove that adding Laplace noise of scale    leads to  S Q      differential privacy  PROPOSITION 1   PR I VAC Y FROM LA P L AC E NO I S E   9       Let Q be a sequence of queries and G be an algorithm that adds i i d  noise to the result of each query in Q  such that the noise follows a Laplace distribution of scale     Then  G satis   es  S Q      differential privacy    3  FIRST CUT SOLUTIONS Dwork et al    s method  described above  may have high relative errors because it adds noise of    xed scale to every query answer regardless of whether the answer is large or small  Thus  queries with small answers have much higher expected relative errors than queries with large answers  In this paper  we propose several techniques that calibrate the noise scale to reduce relative errors  This section describes two approaches  The    rst is a simple idea that achieves uniform expected relative errors but fails to satisfy differential privacy  while the second is a    rst attempt at a differentially private solution  Before presenting the strategies  we introduce an extension to Dwork et al    s method that adds unequal noise to query answers  adapted from  32    Let         1            m  be a sequence of positive real numbers such that qi will be answered with noise scale   i for i      1  m   We extend the notion of sensitivity to a sequence of queries Q and a corresponding sequence of noise scales     DEFINITION 4  GE N E R A L I Z E D SE N S I T I V I T Y   3 2       Let Q be a sequence of m queries  q1           qm   and    be a sequence of m positive constants    1             m   The generalized sensitivity of Q with respect to    is de   ned as GS Q        max T1 T2  m i 1   1   i     qi T1      qi T2        4  where T1 and T2 are any neighboring datasets   The following proposition states that adding Laplace noise with unequal scale leads to GS Q      differential privacy  PROPOSITION 2   PR I VAC Y FROM UN E QUA L NO I S E   3 2       Let Q    q1           qm  be a sequence of m queries and         1             m  be a sequence of m positive real numbers  Let G be an algorithm that adds independent noise to the result of each query qi in Q  i      1  m    such that the noise follows a Laplace distribution of scale   i  Then  G satis   es GS Q      differential privacy    For convenience  we use LaplaceNoise T    Q      to denote the above algorithm  On input T    Q      it returns a sequence of noisy answers Y    y1          ym   where yi   qi T       i and   i is a sample from a Laplace distribution of scale   i for i      1  m   3 1 Proportional Laplace Noise To reduce relative errors  a natural but ultimately    awed approach is to set the scale of the noise to be proportional to the actual query answer   Extremely small answers are problematic  so we can set the noise scale to be the maximum of qi T   and the sanity bound      We refer to this strategy as Proportional  The expected relative error for query qi is   i max qi T         and so setting   i   c max qi T        for some constant c ensures that all query answers have equal expected relative errors  thereby minimizing the worst case relative error  Unfortunately Proportional is    awed because it violates differential privacy  This is because the scale of the noise depends on the input  As a consequence  two neighboring datasets may have different noise scales for the same query  and hence  some outputs may become considerably more likely on one dataset than the other  The following example illustrates the privacy defect of Proportional  EX A M P L E 1  Let      1 and     1  Suppose that we have a census dataset that reports the ages of a population consisting of 5 individuals  T1    42  17  35  19  55   We have two queries  q1 asks for the number of teenagers  ages 13     19   and q2 asks for the number of people under the age of retirement  ages under 65   Since we have q1 T1    2 and q2 T2    5  the Proportional algorithm would set         1    2  such that   1   2c and   2   5c for some constant c  It can be veri   ed that c   7 10 achieves GS Q           Thus  we set   1   2c   1 4 and   2   5c   3 5  Now consider a neighboring dataset T2 in which one of the teenagers has been replaced with a 20 year old  T2    42  17  35  20  55   Compared to T1  the answer on T2 for q1 is smaller by 1 and q2 is unchanged  and accordingly Proportional sets the scales so that   1   c and   2   5c  It can be veri   ed that c   6 5 achieves GS Q          and so   1   1 2 and   2   6  Thus  the expected error of q1 is higher on T1 than on T2  If we consider the probability of an output that gives a highly inaccurate answer for the    rst query  such as y1   102 and y2   5  we can see it is much more likely on T1 than T2  P r Proportional outputs y1  y2 given T1  P r Proportional outputs y1  y2 given T2    exp     y1     q1 T1   1 4     exp     y2     q2 T1   3 5  exp     y1     q1 T2   1 2     exp     y2     q2 T2   6    exp    100 1 4   101 1 2    exp 535 42    exp     This demonstrates that Proportional violates differential privacy    Algorithm TwoPhase  T  Q       1   2  1  let m    Q  and qi be the i th  i      1  m   query in Q 2  initialize         1             m  such that   i   S Q   1 3  Y   LaplaceNoise T    Q      4        Rescale Y            2  5  if GS Q             2 6  Y     LaplaceNoise T    Q        7  for any i      1  m  8  yi        2 i    yi      2 i    y   i       2 i       2 i   9  return Y Figure 1  The TwoPhase Algorithm 3 2 Two Phase Noise Injection To overcome the drawback of Proportional  a natural idea is to ensure that the scale of the noise is decided in a differentially private manner  Towards this end  we may    rst apply LaplaceNoise to compute a noisy answer for each query in Q  and then use the noisy answers  instead of the exact answers  to determine the noise scale  This motivates the TwoPhase algorithm illustrated in Figure 1  TwoPhase takes as input    ve parameters  a dataset T   a sequence Q of queries  a sanity bound     and two positive real numbers  1 and  2  Its output is a sequence Y    y1          ym  such that yi is a noisy answer for each query qi  The algorithm is called TwoPhase because it interacts with the private data twice  In the    rst phase  Lines 1 3   it uses LaplaceNoise to compute noisy query answers  such that the scale of noise for all queries is equal to S Q   1  In the second phase  Lines 4 8   the noise scales are adjusted  based on the output of the    rst phase  and a second sequence of noisy answers is obtained  Intuitively  we would like to rescale the noise so that it is reduced for queries that had small noisy answers in the    rst phase  This could be done in a manner similar to Proportional where the noisy answer yi is used instead of the private answer qi T     i e   set      i proportional to max yi       However  it is possible to get even lower relative errors by exploiting the structure of the query sequence Q  For now  we will treat this as a    black box     referred to as Rescale on Line 4  and we will instantiate it when we consider speci   c applications in Section 5  Using the adjusted noise scales        TwoPhase regenerates a noisy result y   i for each query qi  Line 6   and then estimates the    nal answer as a weighted average between the two noisy answers for each query  Line 7   Speci   cally  the    nal answer for qi equals      2 i   yi     2 i   y   i       2 i      2 i    which is an estimate of qi T   with the minimum variance among all unbiased estimators of qi T   given yi and y   i  this can be proved using a Lagrange multiplier and the fact that yi and y   i have variance 2   2 i and 2    2 i   respectively   The following proposition states that TwoPhase ensures    differential privacy  PROPOSITION 3   PR I VAC Y  O F TwoPhase    TwoPhase ensures   differential privacy when its input parameters  1 and  2 satisfy  1    2          PROOF  TwoPhase only interacts with the private data T through two invocations of the LaplaceNoise mechanism  The    rst invocation satis   es  1 differential privacy  which follows from Proposition 1 and the fact that   i   S Q   1 for i      1  m   Before the second invocation  the algorithm checks that the generalized sensitivity is at most  2  therefore by Proposition 2 it satis   es  2 differential privacy  Finally  differentially private algorithms compose  the sequential application of algorithms  Gi   each satisfying  i differential privacy  yields     i  i  differential privacy  24   Therefore TwoPhase satis   es   1    2  differential privacy TwoPhase differs from Proportional in that the noise scale is decided based on noisy answers rather than the correct  but private   answers  While this is an improvement over Proportional from a privacy perspective  TwoPhase has two principal limitations in terms of utility  The    rst obvious issue is that errors in the    rst phase can lead to mis calibrated noise in the second phase  For example  if we have two queries q1 and q2 with q1 T     q2 T    the    rst phase may generate y1 and y2 such that y1   y2  In that case  the second phase of TwoPhase would incorrectly reduce the noise for q2  The second  more subtle issue is that given the requirement that  1    2       for a    xed    it is unclear how to set  1 and  2 so that the expected relative error is minimized  There is a tradeoff  If  1 is too small  the answers in the    rst phase will be inaccurate and the noise will be mis calibrated in the second phase  possibly leading to high relative errors for some queries  Although increasing  1 makes it more likely that the noise scale in the second phase will be appropriately calibrated  the answers in the second phase will be less accurate overall as the noise scale of all queries increases with decreasing  2  In general   1 and  2 must be chosen to strive a balance between the effectiveness of the    rst and second phases  which may be challenging without prior knowledge of the data distribution  In the next section  we will remedy this de   ciency with a method that does not require user inputs on the allocation of privacy budgets  4  ITERATIVE NOISE REDUCTION This section presents iReduct  iterative noise reduction   an improvement over the TwoPhase algorithm discussed in Section 3  The core of iReduct is an iterative process that adaptively adjusts the amount of noise injected into each query answer  iReduct begins by producing a noisy answer yi for each query qi     Q by adding Laplace noise of relatively large scale  In each subsequent iteration  iReduct    rst identi   es a set Q    of queries whose noisy answers are small and may therefore have high relative error  Next  iReduct resamples a noisy answer for each query in Q     reducing the noise scale of each query by a constant  This iterative process is repeated until iReduct cannot decrease the noise scale of any answer without violating differential privacy  Intuitively  iReduct optimizes the relative errors of the queries because it gives queries with smaller answers more opportunities for noise reduction  The aforementioned iterative process is built upon an algorithm called NoiseDown which takes as input a noisy result yi and outputs a new version of yi with reduced noise scale  We will introduce NoiseDown in Sections 4 1 and 4 2 below and present the details of iReduct in Section 4 3  4 1 Rationale of NoiseDown At a high level  given a query q and a dataset T   iReduct estimates q T   by iteratively invoking NoiseDown to generate noisy answers to q with reduced noise scales  The properties of the NoiseDown function ensure that an adversary who sees the entire sequence of noisy answers can infer no more about the dataset T than an adversary who sees only the last answer in the sequence  For ease of exposition  we focus on executing a single invocation of NoiseDown  Let Y  Y     be a Laplace random variable with mean value q T   and scale             such that            Intuitively  Y represents the noisy answer to q obtained in one iteration  and Y   corresponds to the noisy estimate generated in the next iteration  Given Y   the simplest approach to generating Y   is to add to the true answer q T   fresh Laplace noise that is independent of Y and has scale        This approach  however  incurs considerable privacy cost  For example  let q be a count query  such that for any two neighboring datasets T1 and T2  we have q T1      q T2          1  0  1   We will show that an algorithm that publishes independent samples Y   and Y in this scenario can satisfy   differential privacy only if       1       1      In other words  the privacy    cost    of publishing independent estimates of Y   and Y is 1       1      For any neighboring datasets T1 and T2  let q be a count query such that q T1    c and q T2    c   1 for some integer constant c  Then for any noisy answers y  y     c  we have P r  Y     y     Y   y   T   T1  P r  Y     y     Y   y   T   T2    P r  Y     y     Y   y   q T     c  P r  Y     y     Y   y   q T     c   1    1 2    exp      c     y               1 2   exp      c     y      1 2    exp      c   1     y              1 2   exp      c   1     y        exp   1        c   1     y          c     y         1      c   1     y       c     y       exp   1        1        In contrast  the privacy cost of publishing Y   alone is 1        That is  we pay an extra cost of 1    for sampling Y   even though the sample is discarded once we generate Y  1   Intuitively  the reason for this excess privacy cost is that both Y and Y   leak information about the state of the dataset T   Even though Y is less accurate than Y     an adversary who knows Y in addition to Y   has more information than an adversary that knows only Y     because the two samples are independent  The problem  then  is that the sampling process for Y   does not depend on the previously sampled value of Y   Therefore  instead of sampling Y   from a fresh Laplace distribution  we want to sample Y   from a distribution that depends on Y   Our aim is to do so in a way such that Y provides no new information about the dataset once the value of Y   is known  We can formalize this objective as follows  From an adversary   s perspective  the dataset T is unknown and can be modeled as a random variable  The adversary tries to infer the contents of the dataset by looking at Y   and Y   For any possible dataset T1  we want the following property to hold  P r   T   T1   Y   y  Y     y       P r T   T1   Y     y      5  To see how this restriction allows us to use the privacy budget more ef   ciently  let us again consider any two neighboring datasets T1 and T2  Observe that when Equation 5 is satis   ed  we can apply Bayes    rule  twice  to show that  P r Y     y     Y   y   T   Ti    P r T   Ti   Y     y     Y   y     P r Y     y     Y   y  P r T   Ti    P r T   Ti   Y     y        P r Y     y     Y   y  P r T   Ti    P r Y     y     T   Ti P r T   Ti  P r Y     y        P r Y     y     Y   y  P r T   Ti    P r Y     y     T   Ti     P r Y   y   Y     y     where the term P r Y   y   Y     y     does not depend on the value of T   1 Rather than discarding Y   one could combine Y and Y   to derive an estimate of q T    as done in the TwoPhase algorithm  see Line 8 in Figure 1   This does reduce the expected error  but it still has excess privacy cost compared to NoiseDown  as explained in Appendix A This allows us to derive an upper bound on the privacy cost of an algorithm that outputs Y followed by Y     Let us again consider a count query q such that q T1      q T2          1  0  1   Let Y   be a random variable that  i  follows a Laplace distribution with mean q T   and scale      but  ii  is generated in a way that depends on the observed value for Y   If these criteria are satis   ed and Equation 5 holds  it follows that  P r Y     y     Y   y   T   T1  P r Y     y     Y   y   T   T2    P r Y     y     T   T1     P r Y   y   Y     y     P r Y     y     T   T2     P r Y   y   Y     y       P r Y     y     T   T1  P r Y     y     T   T2      exp 1        The last step works because of our assumption that Y   follows a Laplace distribution with scale        The above inequality implies that obtaining correlated samples Y   and Y now incurs a total privacy cost of just 1        i e   no privacy budget is wasted on Y   In summary  if Y   follows a Laplace distribution but is sampled from a distribution that depends on Y   and if Equation 5 is satis     ed  then we can perform the desired resampling procedure without incurring any loss in the privacy budget  We now de   ne the conditional probability distribution of Y     DEFINITION 5  NO I S E DOWN DI S T R I BU T I O N   The Noise Down distribution is a conditional probability distribution on Y   given that Y   y  It is de   ned by the following conditional probability distribution function  p d f    Let              be    xed parameters  The conditional p d f  of Y   given that Y   y is de   ned as  f           y    Y   y              exp        y                exp        y                             y     y   6  where               y     y    1 4      1 cosh   1           1      2 cosh   1          exp        y   y              exp        y   y      1           exp        y   y    1           7  and cosh     denotes the hyperbolic cosine function  i e   cosh z     e z   e   z   2 for any z  Theorem 1 shows that this conditional distribution has the desired properties  namely   i  Y   follows a Laplace distribution and  ii  releasing Y in addition to Y   leaks no additional information  TH E O R E M 1   PROPERTIES OF NO I S E DOWN    Let Y be a random variable that follows a Laplace distribution with mean q T   and scale     Let Y   be a random variable drawn from a Noise Down distribution conditioned on the value of Y with parameters              such that      q T   and            Then  Y   follows a Laplace distribution with mean q T   and scale        Further  Y   and Y satisfy Equation 5 for any values of y     y  and T1  Theorem 1 provides the theoretical basis for the iterative resampling that is key to iReduct  The next section describes an algorithm for sampling from the Noise Down distribution  4 2 Implementing NoiseDown To describe an algorithm for sampling from the Noise Down distribution  Equation 6   it is suf   cient to    x Y   y for some real constant y and    x parameters         and        Let f   R      0  1  be 0 001 0 01 0 1 1    y   1 y y   1 y    f y     Figure 2  Illustration of f de   ned by f y       f           y     Y   y   i e   the conditional p d f  from Equation 6  We now describe an algorithm for sampling from the probability distribution de   ned by f  In the following derivation  we focus on the case where        y  we will describe later the modi   cations that are necessary to handle the case where       y  Let      min     y     1   By Equation 6  for any y           f y       e y                          y     y               exp               y               where               y     y    e y          1 4      1 cosh   1           1      2 cosh   1          exp     y          exp   1   y          exp     1   y           Note that the function    above is as de   ned in Equation 7 but takes a simpli   ed form given y          As     y      and      are all given  f y         exp y          y        holds  Similarly  it can be veri   ed that 1     y            y     1   f y         exp y           y            2     y        y   1         f y         exp    y             y         For example  Figure 2 illustrates f with the y axis in log scale  In summary  f conforms to an exponential distribution on each of the following intervals                     y     1   and  y   1         When the probability mass of f on each of the three intervals is known  it is straightforward to generate random variables that follow f on those intervals  Let   1               f y    dy       2     y   1    f y    dy     and   3          y 1 f y    dy     It can be shown that   1          cosh  1           cosh  1           exp     1       1                     2                cosh  1           1      8    2         cosh  1            e 1          e 1          1     e   1         1      4                   cosh  1           1        1     exp     1         1                 y   1        9    3           cosh  1           cosh  1           exp       y   1              y 1      2                cosh  1           1      10  For the remaining interval  y     1  y   1  on which f has a complex form  we resort to a standard importance sampling approach  Speci   cally  we    rst generate a random sample y   that is uniformly distributed in  y     1  y   1   After that  we toss a coin that comesAlgorithm NoiseDown      y             1  initialize a boolean variable invert   true 2  if      y 3  set             y      y  and invert   false 4       min     y     1  5  let   1    2    3  and    be as de   ned in Eqn  8  9  10  and 11 6  generate a random variable u uniformly distributed in  0  1  7  if u      0    1  8  generate a random variable y                    such that P r y     y         exp y           y        9  else if u        1    1     2  10  generate a random variable y            y     1  such that P r y     y         exp y           y           11  else if u      1       3  1  12  generate a random variable y        y   1        such that P r y     y         exp    y             y        13  else 14  while true 15  generate a random variable y   uniformly distributed in  y     1  y   1  16  generate a random variable u   uniformly distributed in  0  1  17  if u       f y        then break 18  if invert   true then return y     otherwise  return    y   Figure 3  The NoiseDown Algorithm up heads with a probability f y         where      1 2       cosh   1           exp       1      cosh   1           1    exp   y               max 0  y            1           11  If the coin shows a tail  we resample y   from a uniform distribution on  y     1  y   1   and we toss the coin again  This process is repeated until the coin shows a head  at which time we return y   as a sample from f  The following proposition proves the correctness of our sampling approach  PROPOSITION 4  NO I S E DOWN SA M P L I N G   Given        y  we have f y          for any y        y     1  y   1     So far  we have focused on the case where        y  For the case when       y  we    rst set             y      y  and then generate y   using the method described above  After that  we set y        y   before returning it as a sample from f  The correctness of this method follows from the following property of the Noise Down distribution f           y    Y   y   as de   ned Equation 6   f           y    Y   y    f                 y    Y      y  for any                y     y  As a summary  Figure 3 shows the pseudocode of the NoiseDown function that takes as input     y           and returns a sample from the distribution de   ned in Equation 6  4 3 The iReduct Algorithm We are now ready to present the iReduct algorithm  as illustrated in Figure 4  It takes as input a dataset T   a sequence Q of queries on T   a sanity bound     and three positive real numbers      max  and        iReduct starts by initializing a variable   i for each query qi     Q  i      1  m    setting   i     max  Lines 1 2   The userspeci   ed parameter   max is a large constant that corresponds to the greatest amount of Laplace noise that a user is willing to accept in any query answer returned by iReduct  For example  if Q is a sequence of count queries on T then a user may set   max to 10  of the number of tuples in the dataset T   Algorithm iReduct  T  Q           max         1  let m    Q  and qi be the i th  i      1  m   query in Q 2  initialize         1             m   such that   i     max 3  if GS Q          then return     4  Y   LaplaceNoise T    Q      5  let Q    Q 6  while Q         7  Q      PickQueries Q    Y           8  for each i      1  m  9  if qi     Q    then   i     i           10  if GS Q            11  for each i      1  m  12  if qi     Q    then yi   NoiseDown   qi T   yi    i            i   13  else 14  for each i      1  m  15  if qi     Q    then   i     i         16  Q    Q    Q    17  return Y Figure 4  The iReduct Algorithm As a second step  iReduct checks whether the conservative setting of the noise scale guarantees   differential privacy  Line 3   This is done by measuring the generalized sensitivity of the query sequence given noise scales     If the generalized sensitivity exceeds    iReduct returns an empty set to indicate that the results of Q cannot be released without adding excessive amounts of noise to the queries  Otherwise  iReduct generates a noisy result yi for each query qi     Q by applying Laplace noise of scale   i  Line 4   Given Y   iReduct iteratively applies NoiseDown to adjust the noise in yi so that noise magnitude is reduced for queries that appear to have high relative errors  Lines 5 16   In each iteration  iReduct    rst identi   es a set Q    of queries  Line 7  and then tries to decrease the noise scales of the queries in Q    by a constant        Lines 8 16   The selection of Q    is performed by an applicationspeci   c function that we refer to as PickQueries  An instantiation of PickQueries is given in Section 5 3  but in general  any algorithm can be applied  so long as the algorithm utilizes only the sanity bound     the noisy queries answers seen so far  and the queries    noise scales  and does not rely on the true answer qi T   of any query qi  This requirement ensures that the selection of Q    does not reveal any private information beyond what has been disclosed by the noisy results generated so far  For example  if we aim to minimize the maximum relative error of the query results  we may implement PickQueries as a function that returns the query qi that maximizes   i max yi       i e   the query whose noise scale is likely to be large with respect to its actual result  Once Q    is selected and the scales   i have been adjusted accordingly  iReduct checks whether the revised scales are permissible given the privacy budget  This is done by measuring the generalized sensitivity given the revised scales  Line 10   If the revised scales are permissible  iReduct applies NoiseDown to reduce the noise scale of each qi in Q    by        Lines 11 12   Otherwise  iReduct reverts the changes in    and removes the queries in Q    from its working set  Line 13 16   This iterative process is repeated until no more queries remain in the working set  which indicates that iReduct cannot    nd a subset of queries whose noise can be reduced without violating the privacy constraint  At this point  it outputs the noisy answers Y   TH E O R E M 2   PR I VAC Y  O F iReduct    iReduct ensures       differential privacy whenever its input parameter   satis   es              5  CASE STUDY  PRIVATE MARGINALS In this section  we present an instantiation of the TwoPhase and iReduct algorithms for generating privacy preserving marginals  Section 5 1 describes the problem and discusses existing solutions and Sections 5 2 and 5 3 describe the instantiations of TwoPhase and iReduct  5 1 Motivation and Existing Solution A marginal is a table of counts that summarize a dataset along some dimensions  Marginals are widely used by the U S  Census Bureau and other federal agencies to release statistics about the U S  population  More formally  a marginal M of a dataset T is a table of counts that corresponds to a subset A of the attributes in  T   If A contains k attributes A1  A2          Ak  then M comprises k i 1  Ai  counts  where  Ai  denotes the number of values in the domain of Ai  Each count in M pertains to a point  v1  v2           vk  in the k dimensional space A1  A2            Ak  and the count equals the number of tuples whose value on Ai is vi  i      1  k    For example  Table 2 illustrates a dataset T that contains three attributes  Age   Marital  Status  and Gender  Table 3 shows a marginal of T on  Status  Gender   In general  a marginal de   ned over a set of k attributes is referred to as a k dimensional marginal  While a dataset with k attributes can be equivalently represented as a single k dimensional marginal  such a marginal is likely very sparse  i e   all counts near zero   and thus will not be able to tolerate the random noise added for privacy  Instead  we therefore publish a setMof low dimensional marginals  each of which is a projection onto j dimensions for some small j   k  This is common practice at statistical agencies  and is consistent with prior work on differentially private marginal release  1   We can publish M in a   differentially private manner as long as we add Laplace noise of scale 2     M    to each count in every marginal  This is because changing a record affects only two counts in each marginal  each count would be offset by one   thus the sensitivity of the marginals is 2   M   i e   Laplace noise of scale 2     M    suf   ces for privacy  as shown in Proposition 1  However  adding an equal amount of noise to each marginal in M may lead to sub optimal results  For example  suppose that M contains two marginals M1 and M2  such that M1  M2  has a large  small  number of counts  all of which are small  large   If identical amount of noise is injected to M1 and M2  then the noisy counts in M1 would have much higher relative errors than the noisy counts in M2  Intuitively  a better solution is to apply smaller  larger  amount of noise to M1  M2   so as to balance the quality of M1 and M2 without degrading their overall privacy guarantee  We measure the utility of a set of noisy marginals in terms of minimizing overall error  as de   ned next  Each marginal Mi     M is represented as a sequence of queries  qi1           qi Mi    where qij denotes the j th query in Mi for i      1   M   and j      1   Mi    Let M    i denote a noisy version of marginal Mi and let yij denote to the noisy answer to qij for i      1   M   and j      1   Mi    DEFINITION 6  OV E R A L L ER RO R  O F MA R G I NA L S   The overall error of a set of noisy marginals  M    1           M     M    is de   ned as 1  M       M  i 1 1  M    i       M    i    j 1  yij     qij  T    max     qij  T      We next describe how we instantiate TwoPhase and iReduct with this utility goal in mind  The query sequence input to both algorithms is simply the concatenation of the Age Status Gender 23 Single M 25 Single F 35 Married F 37 Married F 85 Widowed F Table 2  A dataset T Gender Status M F Single 1 1 Married 0 2 Divorced 0 0 Widowed 0 1 Table 3  A marginal of T individual query sequences for each marginal  i e   Q    q11          q1 M1            q M 1           q M  M M       5 2 Instantiation of TwoPhase To instantiate TwoPhase  we must describe the implementation of the Rescale subroutine that was treated as a    black box    in Section 3 2  Let us    rst consider an ideal scenario where the    rst phase of TwoPhase returns the exact count of every query qij   In this case  we know that if we added Laplace noise of scale   i to every count in a marginal Mi  then each count qij  T   in Mi would have an expected relative error of   i max     qij  T      Recall that the expected absolute deviation of a Laplace variable equals its scale   The expected average relative error in Mi would therefore be   i  Mi        Mi  j 1 1 max     qij  T        Consequently  to minimize the expected overall error of the marginals  it suf   ces to ensure that   Mi     i  Mi       Mi  j 1 1  max     qij  T      is minimized  subject to the privacy constraint that the marginals should ensure   differential privacy  i e     M  i 1 2   i        Using a Lagrange multiplier  it can be shown that   i should be proportional to    Mi      Mi  j 1 1  max     qij  T      We refer to this optimal strategy for deciding noise scale as the Oracle method  Oracle is similar to the Proportional algorithm described in Section 3 1  but sets its   i values to minimize average rather than worst case relative error  Rescale sets the noise scale similarly to how it is set by Oracle except that it uses the noisy counts to approximate the exact counts  To be precise    i is proportional to    Mi     Mi  j 1 max     yij    where yij denotes the noisy answer produced by the    rst phase of TwoPhase  5 3 Instantiation of iReduct Before we can use iReduct to publish marginals  we need to instantiate two of its subroutines   i  a method for computing the generalized sensitivity of a set of weighted marginal queries and  ii  an implementation of the PickQueries black box  discussed in Section 4 3  that chooses a set Q    of marginal queries for noise reduction  In practice  the sensitivity of a set of marginals depends only on the smallest noise scale in each marginal  and so we gain the best tradeoff between low sensitivity and high utility by always selecting the same noise scale for every count in a given marginal  We maintain the invariant by  i  initially setting all marginal counts to have the same noise scale   max and  ii  having PickQueries always pass to NoiseDown the queries corresponding to a single marginal M    i in which the noise scale for each cell in M    i is reduced by the same constant        As the counts in the same marginal always have identical noise scale  we can easily derive the generalized sensitivity of the marginal queries as follows  Assume that  in the beginning of a certain iteration  the counts in M    i  i      1   M    have noise scale   i Age Gender Marital Status State Birth Place Race Education Occupation Class of Worker Brazil 101 2 4 26 29 5 5 512 4 US 92 2 4 51 52 14 5 477 4 Table 4  Sizes of Attribute Domains It can be veri   ed that the generalized sensitivity of the marginals is g       i    1  M   2   i  12  Consider that we apply NoiseDown on a noisy marginal M    j  j      1   M     The noise scale of the queries in M    j would become   j            in which case the generalized sensitivity would become  g     2   j               i    1  M     i   j 2   i    13  Recall that  in each iteration of iReduct  we aim to ensure that the generalized sensitivity of the queries does not exceed    Therefore  we can apply NoiseDown on a marginal M    j only if g          We now discuss our implementation of the PickQueries function  Notice that when we apply NoiseDown to a noisy marginal M    j  j      1   M     the expected errors of the estimated counts for that marginal decrease  due to the decrease in the noise scale of M    j   but the privacy guarantee degrades  Ideally  running NoiseDown on the selected marginal should lead to a large drop in the overall error and a small increase in the privacy overhead  To identify good candidates  we adopt the following methods to quantify the changes in the overall error and privacy overhead that are incurred by invoking NoiseDown on a marginal M    j   First  recall that when we employ NoiseDown to reduce the noise scale   j of M    j by a constant        the generalized sensitivity of the noisy marginals increases from g   to g    where g   and g   are as de   ned in Equations 12 and 13  In light of this  we quantify the cost of privacy entailed by applying NoiseDown on M    j as g       g     2   j               2   j    14  Second  for each noisy count y in M    j   we estimate its relative error as   j  max y       where    is the sanity bound  In other words  we use y to approximate the true count  and we estimate the absolute error of y as the expected absolute deviation   j of the Laplace noise added in y  Accordingly  the average relative error of M    j is estimated as   j      y   M    j 1  max y       Following the same rationale  we estimate the average relative error of M    j after the application of NoiseDown as    j                 y   M    j 1  max y       Naturally  the decrease in the overall error of the marginals  resulted from invoking NoiseDown on M    j   is quanti   ed as 1  M         j      y   M    j 1 max y             j                 y   M    j 1 max y                 M       y   M    j 1 max y       15  Given Equations 14 and 15  in each iteration of iReduct  we heuristically choose the marginal M    j that maximizes          M       y   M    j 1 max y          2   j               2   j     That is  we select the marginal that maximizes the ratio between the estimated decrease in overall error and the estimated increase in privacy cost  0 0 02 0 04 0 06 0 08 0 1 0 12 0 14 0 0 1 0 2 0 3 0 4 0 5 0 6    1       overall error TwoPhase 0 0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 0 1 0 2 0 3 0 4 0 5 0 6    1       overall error TwoPhase  a  Brazil  b  USA Figure 5  Overall Error vs   1    1D Marginals  6  EXPERIMENTS We evaluate the accuracy of the proposed algorithms on three tasks  estimating all one way marginals  Section 6 3   estimating all two way marginals  Section 6 4   and learning a Naive Bayes classi   er  Section 6 5   6 1 Experimental Settings We use two datasets 2 that are composed of census records collected from Brazil and the US respectively  Each dataset contains nine attributes  whose domain sizes are as illustrated in Table 4  The Brazil dataset consists of nearly 10 million tuples  while the US dataset has around 14 million records  We use iReduct to generate noisy marginals from each dataset  and we compare the performance of iReduct against four alternate methods  The    rst method is the Oracle algorithm presented in Section 5 2  which utilizes the exact counts in the marginals to decide the noise scale of each marginal in a manner that minimizes the expected overall error  Although Oracle does not conform to    differential privacy  it provides a lower bound on the error incurred by any member of a large class of   differential privacy algorithms  The second and third methods are the TwoPhase and iResamp algorithms presented in Section 3 and Appendix A respectively  The    nal technique  henceforth referred to as Dwork  is Dwork et al    s method  Section 2 2   which adds an equal amount of noise to each marginal  We measure the quality of noisy marginals by their overall errors  as de   ned in Section 5 1  In every experiment  we run each algorithm 10 times  and we report the mean of the overall errors incurred by the algorithm  Among the input parameters of iReduct  we    x   max    T   10 and          T   10 6   where  T   denotes the number of tuples in the dataset  That is  iReduct starts by adding Laplace noise of scale  T   10 to each marginal and  in each iteration  it reduces the noise scale of a selected marginal by  T   10 6   All of our experiments are run on a computer with a 2 66GHz CPU and 24GB memory  6 2 Calibrating TwoPhase As was discussed in Section 3 2  the performance of the TwoPhase algorithm depends on how the    xed privacy budget is allocated across its two phases  This means that before we can use 2 Both datasets are available online as part of the Integrated Public Use Microdata Series  25  iReduct Oracle TwoPhase iResamp Dwork 0 1 1 0 2 0 4 0 6 0 8 1       10  2   overall error 0 1 1 0 2 0 4 0 6 0 8 1       10  2   overall error  a  Brazil  b  USA Figure 6  Overall Error vs     1D Marginals  iReduct Oracle TwoPhase iResamp Dwork 0 0 05 0 1 0 15 0 2 0 25 0 2 0 4 0 6 0 8 1       T     10  4   overall error 0 0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 08 0 09 0 2 0 4 0 6 0 8 1       T     10  4   overall error  a  Brazil  b  USA Figure 7  Overall Error vs      1D Marginals  the TwoPhase algorithm  we must decide how to set the values of  1 and  2 to optimize the quality of the noisy marginals  To determine the appropriate setting  we    x     0 01 and measure the overall error of TwoPhase when  1 varies and  2          1  Figure 5 illustrates the overall error on the set of one dimensional marginals as a function of  1    As  1   increases  the overall error of TwoPhase decreases until it reaches a minimum and then increases monotonically  When  1 is too small  the    rst step of TwoPhase is unable to generate accurate estimates of the marginal counts  This renders the second step of TwoPhase less effective  since it relies on the estimates from the    rst step to determine the noise scale for each marginal  On the other hand  making  1 too large causes  2 to becomes small and forces the second step to inject large amounts of noise into all of the marginal counts  The utility of the noisy marginals is optimized only when  1   strikes a good balance between the effectiveness of the    rst and second steps  As shown in Figure 5  the overall error of TwoPhase hits a sweet spot when  1      0 06   0 08    We set  1   0 07  in the experiments that follow  6 3 Results on 1D Marginals We compare the overall error incurred when each of the    ve algorithms described above is used to produce differentially private estimates of the one dimensional marginals of the Brazil and USA census data  We vary both   and     Figure 6 shows the overall error of each algorithm as a function of    with       xed to 10   4     T    iReduct yields error values that are almost equal to those of Oracle  which indicates that its performance is close to optimal  TwoPhase performs worse than iReduct in all cases  but it still consistently outperforms the other methods  The overall error of iResamp is comparable to that of Dwork  Figure 7 illustrates the overall error of each method as a function of the sanity bound    when     0 01  The relative performance of each method is the same as in Figure 6  The overall errors of all algorithms decrease as    increases  since a larger    leads to lower relative errors for marginal counts smaller than     see Equation 1   iReduct Oracle TwoPhase iResamp Dwork 0 0 2 0 4 0 6 0 8 1 1 2 1 4 1 6 0 2 0 4 0 6 0 8 1       10  2   overall error 0 0 2 0 4 0 6 0 8 1 1 2 0 2 0 4 0 6 0 8 1       10  2   overall error  a  Brazil  b  USA Figure 8  Overall Error vs     2D Marginals  iReduct Oracle TwoPhase iResamp Dwork 0 0 2 0 4 0 6 0 8 1 1 2 0 2 0 4 0 6 0 8 1       T     10  4   overall error 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 2 0 4 0 6 0 8 1       T     10  4   overall error  a  Brazil  b  USA Figure 9  Overall Error vs      2D Marginals  In the aforementioned experiments  every method but iReduct needs only a few linear scans of the marginal counts to generate its output  and therefore incurs negligible computation cost  In contrast  the inner loop of iReduct is executed O   max        times  and the iReduct takes around 5 seconds to output a set of marginal counts  The relative high computational overhead of iReduct is justi   ed by the fact that it provides improved data utility over the other methods  6 4 Results on 2D Marginals In the second sets of experiments  we consider the task of generating all two dimensional marginals of the datasets  For the input parameters of TwoPhase  we set  1     0 025 based on an experiment similar to that described in Section 6 2  Figure 8 shows the overall error of each method when      10   4     T   and   varies  The overall error of iReduct is almost the same as that of Oracle  TwoPhase is outperformed by iReduct  but its overall error is consistently lower than that of iResamp or Dwork  Interestingly  the performance gaps among iReduct  TwoPhase  and Dwork are not as substantial as the case for one dimensional marginals  The reason is that a large fraction of the two dimensional marginals are sparse  As a consequence  iReduct and TwoPhase apply roughly equal amounts of noise to those marginals in order to reduce overall error  The noise scale of the marginals is therefore relatively uniform  the allocation of noise scale selected by iReduct or TwoPhase does not differ too much from the allocation used by Dwork  This explains why the improvement of iReduct and TwoPhase over Dwork is less signi   cant  Figure 9 illustrates the overall error of each algorithm as a function of     with     0 1  The relative performance of each technique remains the same as in Figure 8  Regarding computation cost  iReduct requires around 15 minutes to generate each set of two dimensional marginals  This running time is considerably longer than for one dimensional marginals because iReduct must handle more marginal counts in the twodimensional case iReduct Oracle TwoPhase iResamp Dwork 0 1 2 3 4 5 6 0 1 0 2 0 4 0 7 1       10  2   overall error 0 0 5 1 1 5 2 2 5 3 3 5 0 1 0 2 0 4 0 7 1       10  2   overall error  a  Brazil  b  USA Figure 10  Overall Error vs     Marginals for Classi   er  6 5 Results on Naive Bayes Classi   er Our last set of experiments demonstrates that relative error is an important metric for real world analytic tasks  In these experiments  we consider the task of constructing a Naive Bayes classi   er from each dataset  We use Education as the class variable and the remaining attributes as feature variables  The construction of such a classi   er requires 9 marginals  a one dimensional marginal on Education and 8 two dimensional marginals  each of which contains Education along with another attribute  We apply each algorithm to generate noisy versions of the 9 marginals  and we measure the accuracy of the classi   er built from the noisy data 3   For robustness of measurements  we adopt 10 fold crossvalidation in evaluating the accuracy of classi   cation  That is  we    rst randomly divide the dataset into 10 equal size subsets  Next  we take the union of 9 subsets as the training set and use the remaining subset for validation  This process is repeated 10 times in total  using each subset for validation exactly once  We report  i  the average overall error of the 10 sets of noisy marginals generated from the 10 training datasets and  ii  the average accuracy of the classi   ers built from the 10 sets of noisy marginals  Figure 10 illustrates the overall error incurred by each algorithm when      10   4     T   and   varies   The input parameter  1 of TwoPhase is set to 0 03 based on an experiment similar to that described in Section 6 2   Again  iReduct and Oracle entail the smallest errors in all cases  followed by TwoPhase  In addition  iResamp consistently incurs higher error than Dwork does  Figure 11 shows the accuracy of the classi   ers built upon the output of each algorithm  The dashed line in the    gure illustrates the accuracy of a classi   er constructed from a set of marginals without any injected noise  Observe that methods that incur lower relative errors lead to more accurate classi   ers  7  RELATED WORK There is a plethora of techniques  1   5  5  9   12  14  15  17   20  22  26   28  32  for enforcing   differential privacy in the publication of various types of data  such as relational tables  9 17 21 32   search logs  15 20   data mining results  12 23   and so on  None of these techniques optimizes the relative errors of the published data  instead  they optimize either  i  the variance of the noisy results or  ii  some application speci   c metric such as the accuracy of classi   cation  12   Below  we will discuss several pieces of work that are closely related to  but different from  ours  Barak et al   1  devise a technique for publishing marginals of a given dataset  Their objective  however  is not to improve the accuracy of the released marginal counts  Instead  their technique is de  3 Following previous work  6   we postprocess each noisy marginal cell y by setting y   max y   1  1  before constructing the classi   er from the marginals  iReduct Two Phase iResamp Dwor</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web"/>
        <doc>Facet Discovery for Structured Web Search  A Query log Mining Approach ### Jeffrey Pound   University of Waterloo Waterloo  Canada jpound cs uwaterloo ca Stelios Paparizos Microsoft Research Mountain View  CA  USA steliosp microsoft com Panayiotis Tsaparas Microsoft Research Mountain View  CA  USA panats microsoft com ABSTRACT In recent years  there has been a strong trend of incorporating results from structured data sources into keyword based web search systems such as Bing or Amazon  When presenting structured data  facets are a powerful tool for navigating  re   ning  and grouping the results  For a given structured data source  a fundamental problem in supporting faceted search is    nding an ordered selection of attributes and values that will populate the facets  This creates two sets of challenges  First  because of the limited screen real estate  it is important that the top facets best match the anticipated user intent  Second  the huge scale of available data to such engines demands an automated unsupervised solution  In this paper  we model the user faceted search behavior using the intersection of web query logs with existing structured data  Since web queries are formulated as free text queries  a challenge in our approach is the inherent ambiguity in mapping keywords to the different possible attributes of a given entity type  We present an automated solution that elicits user preferences on attributes and values  employing different disambiguation techniques ranging from simple keyword matching  to more sophisticated probabilistic models  We demonstrate experimentally the scalability of our solution by running it on over a thousand categories of diverse entity types and measure the facet quality with a real user study  Categories and Subject Descriptors  H 2 8  Database Management   Data Mining General Terms  Algorithms  Experimentation  Human Factors  Keywords  Facets  Faceted search  Structured web search ###  1  INTRODUCTION In recent years  search engines like Google or Bing have evolved to include in their results information from structured data sources along with text documents  Furthermore  there are specialized search verticals like Amazon that rely on an even greater variety of structured data sources for their results  Structured data have the ad   Work done while at Microsoft Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  vantage of having rich meta data  allowing for an improved search experience compared to just keyword search  Faceted search is a common interaction paradigm that utilizes structured information to provide a user friendly alternative to keyword search  In this paradigm  facets are used as summarized data axes that allow the users to pivot on the information shown and create a different or more selective result set  If we abstract the structured data as typed entities grouped together into logical tables  one per entity type  A table facet corresponds to a table attribute and underlying attribute values  Faceted search is enabled via a facet system which exposes an ordered list of attributes and an ordered list of attribute values  For example  for a table of digital cameras  a facet could be a popular attribute such as brand and prominent values such as    Canon        Nikon     and    Sony     Other facets could be megapixel resolution  color  size and corresponding values per attribute  Exposing them into a facet system  allows the user to quickly zoom into the appropriate camera subset by selecting characteristics of interest  Similar examples exist in other domains with structured data  such as automobiles or movies  It is clear that the user experience for structured web search can bene   t from facets  However for an engine to support facets effectively it needs to address two groups of challenges   a  Given the limited screen real estate and the large number of possible facets to consider  we need to select the top k most important facets  where k is usually a small number  Facet importance in this case can be measured by the utility of a facet towards a user   s anticipated action like a pivot or re   nement  Since an entity type can have over one hundred attributes  the challenge becomes    nding the few most important attributes  and attribute values with the maximum anticipated utility   b  There is a huge number of structured data sources currently available to engines like Google or Amazon  If we abstract the data as de normalized entity type tables  there are thousands of such tables to consider  So a solution that    nds facets from these tables must be fully automated to scale appropriately  The best method to learn the utility of attributes for users would be to consume user produced signals  Such an ideal signal would be produced if all possible attributes for all possible entity type tables are shown and then the system observes and records which ones are being selected by an extensive user study  Unfortunately  this is not feasible due to the scale of data and the number of required users for statistical signi   cance  Instead  common practice  8  10  has been to show attributes selected manually by experts with values populated from sampling the results  It is debatable how well a domain expert can match the average user intent  but regardless  the scale of structured data available to web engines makes it impossible to go this route with consistently good quality across all tables  As a result  existing approaches  3  22  rely on automatically learningfacets from the structured data  However  such approaches do not capture the true user intent since they focus on the structured data without incorporating user signals  In this work we use web query logs intersected with the structured data to model the facet utility  Our intuition is that query logs act as a proxy for the exploration process of the users when navigating through a structured domain  We assume that users issue queries to re   ne or alter their results into an appropriate consideration set in a similar manner they would have used facets if there were available  The attributes that appear frequently in the query logs are the ones that users would use to narrow down their search  and thus they are appropriate for facets  The values of these attributes that appear frequently in the queries are good values to populate the facets  This formulation has the advantage that it can lead to a scalable automated solution that captures the real user needs better than the opinion of an expert  or the just the statistics of the structured data  Additionally  it adapts to the needs and preferences of the users as they change over time  It is worth mentioning that trying to match the user intent for attribute importance has been studied in the past  see  7  21   with the application domain being either attribute ranking or product visibility  Although the high level principle is similar  the underlying models and corresponding solutions are different from those for discovering the facet utility  Using the web query logs to    nd frequently queried values and attributes would be easy if the attribute value pairs were clearly speci   ed in the queries as in SQL statements  However  although the data is structured  web queries are free text keywords  Finding the frequent attributes poses two hard challenges due to the inherent ambiguity of translating the keywords to attribute value pairs   a  Mapping the query to the appropriate structured source     e g   map the keyword    apple    to either mp3 players  computers  or fruit  and  b  identifying and disambiguating the occurrence of attributes in the query amongst attributes of the same entity type     e g   for the table televisions  disambiguate the keyword    30inch    between the diagonal screen size  height or width of a television  or for the table watches  map the keyword    gold    to the color or material of a watch  In this paper  we assume that the    rst problem  understanding if a query targets a source of structured data and which source in particular  has been addressed from existing literature  such as  13  19  24   We use the existing work to    nd the appropriate target data source  We focus on the second problem  disambiguating between attributes within an entity type domain  Using appropriate disambiguation  we learn the most suitable facet attributes and values  Furthermore  we also discuss how data statistics can be used to help with the disambiguation process by being tolerant to data noise and imperfections  and also which data statistics indicate good facets  Our contributions are summarized as follows   i  We formulate the problem of query log based facet attribute and value selection  presented in Section 2   ii  We introduce different attribute value identi   cation and disambiguation techniques in Section 3  which we extend for computing facet values  including context dependent ones  in Section 4   iii  We explore the effect of appropriate data statistics in facet selection  presented in Section 5  And  iv   we perform a detailed experimental study on a large scale data set with a thorough evaluation on the quality of the results from real users  presented in Section 6  where we discuss the qualitative difference between our techniques as well as show our best approach outperforming state of the industry systems  We conclude with related work in Section 7 and    nal thoughts in Section 8  2  FACET SELECTION In this section we formulate our problem  and provide the necessary de   nitions for the remainder of the paper  2 1 Problem Formulation We assume that structured data are organized as a collection of tables T   fT1  T2          T  g 1   A table T is a set of related entities de   ned over a set of attributes  T A   fT A1  T A2          T A g  We use T A V to denote the domain of the attribute T A  We omit the pre   x T and use A  whenever the table is unambiguous  Given a table T we de   ne a facet system F   hF1       Fki for table T as an ordered list of k facets  where k is is the facet budget  A facet Fi   fAi hAi v1       Ai vmi ig consists of an attribute Ai and an ordered list of mi values  where mi is the value budget for attribute Ai  For the following we use AF  or F A  to denote the set of attributes of F  and A VF to denote the set of values for attribute A 2 AF in facet system F  We say that a facet system supports an attribute A if A 2 AF   and we de   ne the attribute support function as follows  ASUP F  A      1  if F supports A 0  otherwise  1  We also say that a facet attribute Af 2 AF supports value v if v 2 Af  VF   and we de   ne the attribute value support function as follows  VSUP Af   v      1  if Af supports v 0  otherwise  2  A facet system is exposed through the user interface  and it enables the user to navigate through the data in the table T by selecting values for the different attributes  The design and analysis of faceted user interfaces is an active research area in the HCI community  10   In any faceted search system  the facet selection is paramount to the success of the system  The order of facets is also important since it determines how the facets will be displayed  with more important facets being displayed more prominently  The goal is to maximize the user engagement and satisfaction  and the i th facet corresponds to the i th best attribute for this task  A similar reasoning applies also to the ordering of the facet values within a facet  However  for a selected facet attribute there is usually a greater    exibility in the presentation of values  drop down menus  slider bars  text boxes  making the value selection and ordering less critical  We thus consider the facet attribute ordering problem and the facet attribute value ordering problem separately  with the former being more important than the latter  In this work we are interested in constructing a facet system using web search queries  We assume that we have a query log Q   fq1       qng  consisting of queries posed to a generic web search engine  For each query qi we also have a weight wi denoting the importance of the query  e g   the number of times that the query appears in the query log  Although queries are posed to the search engine  it is common that users make queries that are targeted to structured data  For example  the query    canon powershot    can be interpreted as targeting a table with structured data about digital cameras  For a given table T let QT   Q denote the subset of queries that target table T  We will drop the subscript T whenever the table is unambiguous  We assume that each query q 2 QT speci   es a set of attributevalue pairs AVq   fAVig of the form AVi    Ai  Ai vj    For example  the query    canon powershot    speci   es the value    canon    for attribute brand  and the value    powershot    for attribute product line  We can thus map the query q     canon powershot    to the set Aq    brand    canon      product line    powershot       We use Aq 1 The organization of data into tables is purely conceptual and orthogonal to the underlying storage layer  the data can be physically stored in XML    les  relational tables  retrieved from remote web services  etc  Our assumption is that a mapping between the storage layer and the    schema    of table collection T has been de   ned  a  Queries to discrete attribute value pairs  b  Queries to attribute value pairs through tagged tokens Figure 1  Query to Attribute Value Pairs Mappings to denote the set of attributes speci   ed in the query  Figure 1 a  depicts graphically an example  Note that we only assume that such a mapping exists  obtaining it from the web log is a challenging problem as we elaborate below  We now de   ne the attribute utility of a facet system F  making use of the attribute support function de   ned in Equation 1  DEFINITION 1  ATTRIBUTE UTILITY   Given a facet system F  and a query q  we de   ne the attribute utility of F for q as Ua F  q    X A2Aq ASUP F  A  the number of attributes in Aq supported by F  The attribute utility of F for query log Q is de   ned as Ua F  Q    X q2Q wqUa F  q  where wq denotes the weight of query q  We can similarly de   ne the attribute value utility of a facet attribute Af   making use of the attribute value support function de     ned in Equation 2  DEFINITION 2  ATTRIBUTE VALUE UTILITY   Given a facet attribute Af and a query q  we de   ne the attribute value utility of Af for q as Uv Af   q    X  A v 2AVq A Af VSUP Af   v  the number of values in AVq supported by Af   The attribute value utility of Af for query log Q is de   ned as Uv Af   Q    X q2Q wqUv Af   q  where wq denotes the weight of query q  As we have already argued  queries act as a proxy for the exploration process of the users over the structured data  Users include in their queries attributes over which they want to dissect the table  A good faceted system should anticipate the information need of the user and select attributes  and values  that maximize the utility to the users  We thus have the following de   nition of the query based facet selection problems  PROBLEM 1  QUERY BASED FACET ATTRIBUTE SELECTION   Given a table T  and a query log QT over table T  create an ordering L T A  of the attributes T A such that for any k   1  a facet system F with attribute set F A   Lk T A      the top k attributes according to ordering L     has maximum attribute utility Ua F  QT   over all facet systems with attribute set of size k  PROBLEM 2  QUERY BASED FACET VALUE SELECTION   Given a table T  an attribute A 2 T A  and a query log QT over table T  create an ordering L A V  of the values in A V such that for any m   1  a facet system F that contains A with attribute value set A VF   Lm A V      the top m attributes according to ordering L     has maximum attribute value utility Uv A  QT   over all facet systems that contain A with attribute value set of size m  If we are given the mapping AVq from queries to attribute value pairs  as in Figure 1 a  then the facet selection problems we de     ned above are easy to solve  This is the case for example with structured query logs  e g  SQL   where the user speci   es explicitly the attributes and values they are interested in  Then  it is straightforward to see that the optimal solution for Problem 1 is to order the attributes according to their popularity in the query log  The popularity FQ Ai  of attribute Ai over query log Q is computed as FQ Ai    X q2Q X  Ai v 2AVq wq  3  The popularity FQ Ai  is thus the number of attribute value pairs in Q that contain attribute Ai weighted by the weight of the query in which they appear  Similarly  the optimal solution for Problem 2 is to order the values within attribute Ai according to their popularity in the query log  The popularity of a value vj of attribute Ai is computed as FQ Ai vj     X q2Q X  Ai vj  2AVq wq  4  the number of attribute value pairs in the query log  where Ai is the attribute and vj is the value  Unfortunately  in the case of web query logs this mapping is not given to us and we need to discover it from the data  We elaborate on this challenge in the following section  2 2 Mapping Web Queries to Structured Data In order to extract attribute value pairs from the queries in the unstructured web query log  we need to address two issues   a  for every table T  identify the queries in Q that target table T  i e   determine the set QT   and  b  identify and disambiguate the attribute occurrences in the queries  As previously discussed  we use existing work  13  24  to solve  a  and instead focus on the latter problem  Given a query q 2 QT we now need to identify the occurrences of the attributes in T A in the query q  We assume a tagging process TAGGER that performs this task  De   ne a token to be a string consisting of one or more contiguous words  and let Lq denote the set of all tokens in q  The TAGGER produces all tokens t 2 Lq  and matches them with attributes in T A  producing attribute tokenFigure 2  Fraction of tokens mapped to varying num of attributes  pairs  AT    A  t   For a categorical attribute Ac 2 T A  the TAGGER outputs an attribute token pair AT    Ac  t   if the token t appears in Ac V  Approximate  or synonymous matching is also possible as an extension of the same process  although discussing it in detail is outside the scope of this paper  For a numerical attribute An with unit u  e g   inches  Kg  or x zoom   the TAGGER produces an attribute token pair AT    An  t  for every token t that consists of a number followed by a unit u  For example  the tagger output for the query    5x nikon    over the digital cameras table is f digital zoom     5x       optical zoom     5x       brand     nikon    g  For a query q we use AT q   f A  t g   T A   Lq to denote the set of attribute token pairs output by the TAGGER   Note that attribute token pairs are different from attribute value pairs  since a token may not be a value in the table T  This is the case with synonyms and approximate matches  as well as with numeric attributes  For example  in the query    30 inch lcd TV     the token    TV    can match the data value    Television     and the token    30 inch    can be mapped to diagonal screen size even though we may only have televisions with 27 and 32 inch diagonals  Given the query log Q the TAGGER outputs a set of attributetoken pairs AT Q  It would appear that our job is now done  since we can compute the popularity of attributes and values as before  simply by interchanging tokens with values  and attribute value pairs with attribute token pairs  However  this is not the case due to ambiguity in the token interpretation  The domains of attributes in a table often overlap  and as a result a token t can map to more than one attribute  For example  in the query    30 inch lcd TV     the token    30 inch    is potentially mapped to the diagonal  width  height  and depth attributes  since all these attributes are numeric with the same unit  similarly in the query    camera with 3x zoom        3x    can be either optical zoom or digital zoom  Such confusion is not limited to just numeric attributes  For example in the query    gold watch     the token    gold    can specify either the material  or the color attribute  similarly for    leather dress shoes    the token    leather    can map to either the top shoe material or the shoe sole  There are many such examples that appear in web queries  We computed the number of attributes of the same table that a token matches on average  over all queries  As we can see from Figure 2  approximately half the tokens are confused over at least two or more attributes  So ambiguity is a real problem that needs to be addressed  Estimating attribute popularity in the presence of ambiguity becomes problematic  Our data no longer looks like Figure 1 a   but instead like Figure 1 b   Computing the attribute popularity over attribute token pairs using Equation 3 will lead to misleading results  TV height will incorrectly be deemed equally important to TV diagonal  and camera digital zoom the same as optical zoom  In order to obtain correct estimates for the true popularity of an attribute  we need to disambiguate the ambiguous tokens  Let t denote a token in a query q  We say that token t is ambiguous for a table T  if there are at least two attribute token pairs  Ai t   involving t   We use At  to denote the set of candidate attributes to which the token t is mapped  Our goal is to disambiguate between the attributes in At   We do this by estimating the probability of an attribute given the ambiguous token  We thus have the following problem de   nition  PROBLEM 3  ATTRIBUTE DISAMBIGUATION   Given a table T  a query log Q over T  and an ambiguous token t  2 LQ that maps to the set of attributes At    T A  compute the disambiguation probability distribution  denoted P Ajt    over the set of candidate attributes A 2 At   Given P Ajt   we now have a measure of likelihood for the mapping of token t to attribute A  We can use this probability to perform either hard or soft disambiguation  In hard disambiguation  we map token t to the most likely attribute At    arg maxA2At  P Ajt    In hard disambiguation our data will take the form of Figure 1 a   and we can apply directly Equation 3 to estimate the attribute popularity  In soft disambiguation  we modify Equation 3 such that each occurrence of attribute A with the ambiguous token t is weighted by the probability P Ajt    We thus have F  Q Ai    X q2Q X  Ai t 2AT q wqP Aijt   5  We have P Ajt    1 if token t maps unambiguously to attribute A  3  ATTRIBUTE DISAMBIGUATION In this section we describe four different approaches for attribute disambiguation  The    rst three rely on the principle that the best solution is the one that better explains the attribute token pairs we observe  If we assume that the attributes in the table are generating queries and tokens  we identify the attribute that is most likely to have generated the tokens we are observing  The three different algorithms differ in the granularity at which they try to explain the data  becoming progressively more complex  The last approach relies on user feedback from a click log to disambiguate between different interpretations of a token  3 1 Token level Disambiguation The    rst algorithm we consider treats each token independently and tries to    nd the attribute that is most likely to have generated this speci   c token  Let t denote an ambiguous token and let At  the set of candidate attributes for t   We want to estimate P Ajt   for all A 2 At   Using Bayes rule we have P Ajt     P t jA P A  P Ai2At  P t jAi P Ai  We will estimate the right hand side using the data in T  Assuming that all attributes are equally likely in the table  i e   P A  is the same for all attributes in the table   then P Ajt   is proportional to PT  t jA   the probability that the token t is generated from the distribution of A in the table  We have that PT  t jAi    jT A t  j jAj where T A t   denotes the set of entries in the table where the attribute A takes the value t   and jAj is the number of table entries with some value for attribute A  Numeric attributes are similarly described using histograms to deal with the continuity of numbers  Therefore  according to this algorithm  the most likely attribute is the one that is most likely to have generated token t   From an information theoretic point of view  this is the attribute who   s value distribution can encode token t with the fewest number of bits  i e   the one that reduces the uncertainty the most for t  3 2 Cluster level Disambiguation The token level disambiguation approach considers the probability of each token independently mapping to a candidate attribute  A natural extension to this model is to consider the ambiguity among a cluster of tokens confused with the same set of attributes  In this cluster based approach  we aggregate over all tokens in the querylog to    nd clusters of ambiguous attribute token pairs  We then resolve which attributes in the cluster are better candidates to model the distribution of confused tokens over the full cluster  Consider the bipartite graph formed by the set of attribute token pairs in AT Q  e g   see Figure 3  a    Each time a token is mapped to a set of attributes  it supports the ambiguity of these attributes  For example  if the token    8x    is mapped to both digital zoom and optical zoom  it supports the ambiguity of these two attributes  If the token    2x    is mapped to digital zoom  optical zoom  and model it supports the ambiguity of all three of these attributes  By aggregating over all tokens in the query log  we can    nd the groups of attributes most commonly confused  In particular  if we consider the set of all tokens that form a bi clique with a set of attributes  then these attributes are all pairwise ambiguous over the same set of tokens  and the support for the ambiguity of this cluster is proportional to the number of tokens in the bi clique  It is these strongly supported bi cliques that we aim to    nd  To    nd clusters of ambiguous attributes  we    rst construct an attribute ambiguity graph per table  For a table T  consider a graph G    V  E  w  where there exists a vertex va 2 V for every attribute a 2 T  A  We create an edge e   hvA1   vA2 i between two vertices vA1 and vA2 if some token in the query log is mapped to both A1 and A2 over all interpretations of queries  The weight function w e  assigns a weight to the edge e equal to the number of tokens confused between  the attributes denoted by  vA1 and vA2   We then enumerate all cliques in the attribute ambiguity graph  with the support being the minimum edge weight in the clique  For example  in Figure 3  b   the support of the 2 clique containing digital zoom and optical zoom is ten  while the support of the 3  clique containing digital zoom  optical zoom  and model is two  After identifying the clusters of ambiguous attributes  we want to compute how likely each attribute is to model the set of confused tokens  We do this by computing the KL divergence  KullbackLeibler divergence  between the value distribution of the attribute and the set of confused tokens  Let C    AC  LC  denote an attribute token cluster  We compute the disambiguation probability of an attribute A 2 AC given the cluster C as follows  P AjC    KL AjjC  P Ai2C KL AijjC  where the KL divergence between two distributions P and Q is de   ned as follows  KL PjjQ    X x p x  log p x  q x  For every ambiguous attribute token pair  A t   2 LC  we set the disambiguation probability P Ajt     P AjC   The intuition in this kind of disambiguation is that the attribute which forms a better model for the query token distribution is more likely to be the correct attribute users had in mind when formulating these queries  For example  if a collection of tokens around measurements are confused among television height and diagonal  we would expect to    nd the distribution of values in the diagonal attribute to be a better model for the set of confused query tokens  As an example  consider Figure 3  There are eight tokens with the same connectivity of the token labeled    8x     two tokens with the same connectivity as    2x     and    ve tokens with the same connectivity Figure 3   a  a bipartite token attribute graph and  b  the resulting attribute ambiguity graph  as    Pix     The ambiguous attribute clusters correspond to the 2  clique  Digital Zoom  Optical Zoom  with support 10  the 2 clique  Product Name  Model  with support    ve  and lastly the 3 clique  Digital Zoom  Optical Zoom  Model  with a support of two  The 3 clique will be used to disambiguate among tokens confused with all three attributes  while the sub clique will be used to disambiguate between tokens confused among only those two attributes  We may also    nd the remaining two 2 cliques  Digital Zoom  Model  and  Optical Zoom  Model  however in our implementation we are not concerned with these as there are no tokens that confuse Model with one of the other attributes and not the third  i e   all tokens confused with Model and Optical Zoom are also confused with Digital Zoom  and all tokens confused with Model and Digital Zoom are also confused with Optical Zoom   3 3 Query log level Disambiguation We now present an approach that considers the tokens in the full query log  and estimates the probability P Ajt   for ambiguous tokens t   such that the likelihood of the full observed query log is maximized  Let Q be a query log over table T  Let LQ denote the set of all tokens generated by the TAGGER that appear in the attribute token pairs AT Q  Each token t is assigned a weight wt   X q2Q X  Ai t 2AT q wq  the total weight of queries in which token t appears  For simplicity we can consider wt to be the frequency of the token t in AT Q  We assume that the tokens in LQ were generated according to a generative model  which generates tokens as follows  an attribute Ai is selected with probability P Ai   and then a token t is drawn from the data distribution PT  tjAi   Thus token t is generated with a probability given by the following  P t    X Ai2T A PT  tjAi P Ai  The probability of observing the full set of tokens LQ is then P LQ    Y t2Q P t  wt The generative model we have described has   parameters  i   P Ai   one for each attribute Ai in the table T  Parameter  i is the probability that attribute Ai is activated for the generation of a given token  and we have that P  i 1  i   1  Let   denote the vector of parameters  1           We perform Maximum LikelihoodEstimation of    and we look for the value of   that maximizes the probability that we observe the tokens in LQ  Using log likelihood notation we want to minimize the following  L LQ     log P LQ     X t2Q wt log P t  We use an iterative EM algorithm for    nding the Maximum Likelihood Estimate for    The algorithm initializes the values of the parameters  i to an arbitrary distribution  in our implementation uniform  Then it alternates between the following two steps  In the E step given an estimation for the parameters    for each attribute token pair  Ai  tj   in LQ we compute the posterior probability of the attribute Ai given token tj P Aijtj     P P tj jAi  i A 2AT  A P tj jA      6  In the M step  we can now estimate new values for the parameters in   as follows   i   X  Ai t 2AT Q P Aijt P t    wt W X  Ai t 2AT Q P Aijt   7  where W   P t2LQ wt is the total weight of all tokens  We repeat these two steps until the algorithm converges  Given that the optimization function is convex we know that it will reach a global maximum  After the algorithm has converged  for an ambiguous token t we can now compute P Ajt   using Equation 6  The Maximum Likelihood Estimation of the probabilities P Ai     nds a set of values that best explains the full query log we observe  Intuitively what this means is that when observing a token t  even though attribute A1 may have the highest probability P tjA1   the token t may be attributed to another attribute A2 since this is overall more likely to occur in the log  For example  consider a query log with queries over the televisions table  and let Linch denote the set of all tokens of the form number followed by the unit    inch     If the overall distribution of these tokens agrees better with the attribute diagonal than height then for a token like    30 inch    in Linch the attribute diagonal will have higher probability P Ajt  even though the value is more likely in the height attribute  This agrees well with our intuition that    30 inch    is more likely to refer to the diagonal rather than the height  since it is typical for users to query for diagonal when querying for televisions  The iterative algorithm captures nicely this intuition  3 4 Clicks based Disambiguation The ambiguity problem is essentially an intent issue  Given an ambiguous token  and multiple interpretations of the token in the data table we have no indication which interpretation was intended by the user  In an online environment  a strong indication of intent is the user click behavior  A query click log QC   f q1  e1        qn  en g over a table T is a set of query entity pairs  where q is a query posed in a commercial search engine  and e is an entity in table T that was shown to the user as a result of the query and the user chose to click on it to obtain further information about it  An entity e is an entry in table T  We represent it as a set of attribute value pairs e   f Ai  Ai vj  g for every attribute Ai in table T  Given a query click pair  q  e   we make the assumption that the user who posed the query intended to access the attributes of clicked entity e  Since we now have an indication for the intent of the user  the disambiguation problem becomes signi   cantly easier to tackle  Given an ambiguous token t that maps to attributes At    fAi1        Aik g we disambiguate by selecting from entity e the attribute A   2 At  that has value A    v that best matches token t   The notion of a match depends on the type of token that we consider  In the case of categorical tokens we require that A    v   t   In such cases we have that P A   jt     1  since we assume complete certainty in our mapping  If there is more than one attribute that match token t then we assume that t maps to all of them  and we assign equal probability to all  It is worth noting that  although possible  this event is not very probable  In our experiments  we did not observe any case where multiple categorical attributes match on the same token for the same clicked entity  In the case of numerical tokens  we map token t  to the attribute A   that has the closest value  i e   it minimizes the difference jt  A    vj  we compute the disambiguation probability as P A   jt     exp jt  A    vj  P A2At  exp jt  A   vj  favoring the attributes with smallest distance  Clicks have been used extensively in web search for eliciting the preferences of users  14  19   It is well known that they contain a very strong signal for the intent of the user  but they also come with different caveats  such as presentation bias  users click only on what they are shown   ranking bias  users tend to click on the top positions   and noise  We discuss some of these issues in the experimental evaluation  see Section 6   4  ATTRIBUTE VALUE SELECTION Equally important to    nding important attributes  is the problem of    nding important values to populate the facets  We apply the same philosophy for values as we do for attributes  popular values in a query log will be strong facet values that are important to users  In the following section  we discuss two types of facet value selection  category dependent facet value selection  and dynamic context dependent facet value selection  4 1 Table Dependent Value Selection To compute the value score  we consider Equation 4 and modify it such that each occurrence of attribute A and value v with the ambiguous token t  is weighted by the probability P A vjt    We then have the popularity of a value as the following  F  Q Ai vj     X q2Q X  Ai t  2AT q wqP Ai vj jt    8  In the case that value vj does not map to the token t   the probability will be zero  Also  for all attributes Ai of the same table we have P Ai vj jt    P Aijt  since the token to value mapping is part of the attribute based disambiguation  Hence each of the attribute disambiguation methods from Section 3 are directly applicable  For numeric attributes  we can compute weights for buckets of values via the usage of a histogram  This method allows for each table attribute to display a set of values based on user popularity  For example  for the golf clubs table and brand attribute  the top 3 facet values would be    Nike        Ping     and    TaylorMade     4 2 Context Dependent Value Selection We described how to use query log popularity to    nd good facet values given the table and an attribute  However  we still have the problem that certain combination of values across attributes will return empty results  For example  consider a faceted search engine which includes brand and golf club type among its facets for the golf clubs table  There exist some specialized brands that only produce very few types of golf clubs  i e     Odyssey    or    Scotty Cameron    only produce    putters     So now when a user selects    Odyssey     then golf club type values such as    drivers    or    wedges    are not good facet       choices since selecting them would produce empty results  Showing    putters    would be a much better choice  Also consider the    ip side of this example that exposes another interesting problem  When the    rst user action is to select    putters    as golf club type  then popular choices such as    Nike    or    TaylorMade    for brand will produce valid results  albeit not necessarily the most desirable ones  Golf a   cionados may prefer    Odyssey    or    Scotty Cameron    putters  Although it is not that dramatic to put the generally popular    Nike    on top  it is arguably a better user experience to be able to show    Odyssey    given the    putters    selection  The example output from our implementation can be seen in Figure 4  In summary  it is preferable for a facet system to take into consideration the existing user context  in the form of pre selected attribute value pairs from a facet selection or a user query  via  20  24    and dynamically adjust the shown facet values  One way to achieve this task would be to consider the data and pre compute the correlation between all possible value combinations across attributes  This information is then stored for online processing and use the context to    lter the possible choices  However  such computation is very expensive for the scale of data sets available in a web search engine  Even after this expensive computation  supporting the retrieval of such information for real time search is a non negligible investment that will have an overall impact in resource usage and result response time  Besides the computational cost  it is also not clear whether data correlation frequency corresponds well with the user intent  To limit the computational cost while still trying to best satisfy the user  we turn again to query logs  We compute co occurrence for value pairs across different attributes that appear frequently together in the same query  We then validate the co occurring values against the database to ensure a non zero entity count in the results  By going to the query logs we signi   cantly reduce the space of possible data value pairs to consider when compared to all attributes and all values of a table  As a result  we effectively address the issue of empty results without signi   cantly affecting the response time  resource usage and off line computation cost  More importantly  we further optimize the user experience by dynamically placing the most relevant facet values at the top of the list for a given selection  To compute the co occurrence  we de   ne the conditional score of a value for a given attribute by the probability that the value represents the attribute in a query that co occurs with a given attribute value pair  Let Ac vc denote conditional value vc bound to attribute Ac  A v denote value v bound to attribute A  and t be a token  We want to estimate P A vjt  A  c vc   the probability of value v bound to attribute A given the observed token and conditional attribute value binding  Since tokens are tagged independent of one another  this simpli   es to P A vjt   which we can compute using Equation 8  The difference is in which queries are used from the query log when computing the popularity  Given a query log Q and attribute value pair AV    Ac  Ac vc   let QAV denote the subset of queries fq j q 2 Q    Ac  Ac vc  2 AVqg  We then estimate the popularity of a value over all queries satisfying the given attribute value condition  F  QAV  A v    X q2QAV X  A v 2AVq wqP A vjt   The last consideration for context dependent facet values is the case where there are multiple selection conditions speci   ed in the query  This may occur either as multiple recognized attribute values in a keyword query  or as multiple facet selections  Supporting n way conditionals with exact information is impractical  The datadriven approach suffers the combinatorial explosion of an already large data set  and the query log driven approach suffers from the Input Table  Golf Clubs Query     golf putters    Facet Values Nike Odyssey Ping Ping TaylorMade Scotty Cameron Figure 4  Example top 3 facet values for golf club brand  sparseness of queries  After    ltering by multiple selection conditions  the number of queries from which to compute popularity frequencies becomes too small  For this scenario  we adopt a simple approach of intersecting value lists and aggregating popularity counts by addition  This will    nd values relevant to multiple selections  and score them proportionally to how relevant they are to their respective conditional selections  Handling an empty intersection is more of a user interface design decision  One could eliminate the facet entirely  or default to the category dependent values  5  FACETS AND DATA STATISTICS The query logs can reveal which attributes in the table are popular among the users  However  just because an attribute is popular does not necessarily mean that it is good to be used as a facet  In this section we discuss how to use some signals from the data to discover which attributes make better facets  Information Content  Recall that the goal of facets is to enable the users to quickly zoom into a smaller subset of products that are of interest to them  Therefore  in order for an attribute A to be a good candidate for a facet  it should have high information content  that is  the value selection v for the attribute A should give information about the tuples of interest to the user  This property is naturally captured in the entropy of the attribute A  For an attribute A de   ned over the domain A V  the entropy of A is de   ned as H A     X v2V PA v  log PA v  where PA v  is the fraction of entries in the table where attribute A takes value v  over the number of entries in the table where the attribute A takes any value  i e   is non null   Attributes with low entropy do not make good facets  For such attributes the distribution of values is highly skewed  and the knowledge of the value gives very little information for the subset of entities of interest to the user  This is clear in the extreme example when the entropy is zero  meaning that all entities take the same value  In this case  the attribute is useless for navigation  since it gives no information about which entities the user is interested in  For example  the attribute color in the televisions table conveys no useful information as a facet if all televisions are    black     An alternative interpretation of H A  is that it captures the decrease in the uncertainty for the preference of the user for the entities  rows  in table T  When no facet attribute has been selected  any entity e 2 E is equally likely to be of interest to the user  therefore  if N is the number of entities in the table  each entity has probability P e    1 N  The uncertainty is captured in the entropy of the random variable E  which in this case is maximum  H E    log N  Now assume that the user is allowed to use attribute A to zoom in on a smaller subset of entries  The uncertainty for E decreased given the knowledge of the value in A is measured as H E   H EjA   where H EjA  is the conditional entropy of E given A  It is well known that H EjA    H E  A   H A   Since each entity is distinct from the rest  and assuming that A takes a value for all entities  we have that H E  A    H E    log N  Therefore  H EjA    H E   H A   Thus the entropy H A  captures how much our uncertainty for the entities that the user is interested in will    decrease when we have the knowledge for attribute A  If H A    0  then the knowledge of A gives no extra information for E and thus it is redundant  It follows naturally  that attributes with low entropy should not be used as facets  Sparsity  Real world data is often noisy with missing or incomplete information  As a result there are often cases where some attributes are only sparsely populated  If such an attribute is selected for faceted navigation  then selecting any value will immediately discard most of the entities since they do not have a values for the sparse attribute  This could be ok if this corresponded to a rare feature  in which case missing information corresponds to negative information for the existence of the feature  However  it is often the case that sparsity is due to noise in the data collection  in which case  the entities that are eliminated are valid for the selected facet  yet will never be visible to users  Furthermore  noisy attributes are often likely to contain incorrect values  confusing the tagging process of the queries and the corresponding probabilities  Therefore  we exclude from the candidate attributes the ones that are very sparse  The sparsity of attribute A is de   ned as R A    jT A j jTj where T A  is the set of entries in which the attribute has a non null value  and jTj is the total number of entries in the table  Similarly to entropy  sparse attributes are usually not good facet candidates  6  EXPERIMENTAL EVALUATION We abstract the structured data as tables  each containing entities of the same type  For our experiments we have 1164 such tables that we crawled using the MSN shopping XML API 2   The tables correspond to products used to answer shopping queries and are similar to the data used by sites like Amazon or Google Product Search  We consider each category of products to be a table of entities of the same type  The available range covers entities from electronics like digital cameras or televisions to golf clubs and soft goods like shoes and shirts  In total  there were around 10 million structured distinct product entities occupying approximately 300GB on disk when stored in our database  Besides the structured data  we also use a web query log and a vertical click log  The web query log contains queries posed on a major search engine on a period of    ve months from July to November 2009  The web queries are approximately 31 million distinct queries all in the English language  As query weight we use the aggregated impressions count     each time a query is asked it increments its impression count  We limit ourselves to queries with at least 10 impressions to reduce noise in the queries  The total aggregate impression weight of the log is approximately 4 2 billion impressions  The average query length is 3 42 words and 22 04 characters  The click log is available via toolbar usage logs collected from users that have explicitly opted in to install the toolbar browser add on of a major search engine  It contains queries and clicks on Amazon products over a period of one year from March 2009 to February 2010  Since our structured data set is in the shopping domain  we were able to map the Amazon click log to our product entities using unique identi   er references like UPC and EIN codes  The format of the log is query  entity id and number of clicks as the query weight  The total number of distinct queries is in the order of 2 2 million with average length of 3 67 and 24 1 characters  It is worth noting that the click log is smaller although the period used 2 See http   shopping msn com xml v1 getresults aspx text digital cameras for for a table of digital cameras and http   shopping msn com xml v1 getspecs aspx   itemid 25236859 for an example of a camera entity with its attributes  is longer  This is attributed to two reasons  there are more queries on a search engine than on Amazon  and the toolbar  as an add on  means that it can only capture a fraction of the click activity since it is opt in only and many users do not install such add ons  However both logs provide very valuable information  For our experiments  we implemented our work as part of  23  and exploit the query annotator described in  24  to map queries to tables in our collection and provide the token to attribute value mapping  We run the annotator on our web query log and kept only the interpretation to tables that are above the threshold of     1  Since a query can map to more than one table  we took all the plausible tables and normalized their probabilities to fractions summing to 1  Then we used each fraction to map the query to the table while appropriately adjusting the query weight  Thus we produced a subset of the query log for each table  with each query having a weight that is the corresponding fraction of its impressions multiplied by the normalized table probability  We tested the various techniques we proposed in the paper and we present the results below  The naming scheme is as follows  DataProb is the probabilistic disambiguation from Section 3 1  ClusterProb the cluster based approach from Section 3 2  Iterative is the approach described in Section 3 3  and Clicks utilizes the click log as described in Section 3 4  In addition to these techniques  we use a simpler one  shown as NoProb  that computes the score using for each query mapped to a table T  all the plausible token attribute pairs from the table T  while assigning to all of them equally the query weight multiplied by the normalized table weight produced by the annotator  NoProb is not blind counting because it does consider the query weight and annotation probability  from  24   that we used in assigning the queries to tables  Thus we consider NoProb as a state of art baseline  The main difference with our proposed disambiguation techniques is that it does not use an explicit probabilistic disambiguation on the possible token attribute mappings within each table  6 1 Relevance Judgments To measure the effectiveness of our results we conducted an extensive study using Amazon Mechanical Turk  Assessing the entire dataset of all possible tables and all possible attributes would have been prohibitively expensive  Instead we chose a diverse subset of our tables covering a variety of different areas representing as much as possible the entire data spectrum  The tables used in our evaluation were televisions  microwave ovens  cell phones  golf clubs  mp3 players  shoes  laptop computers  printers  watches  video games  engagement rings  digital cameras  skin cleansers  and camera bags and cases  We ran all our approaches on the full set of tables and produced a set of facet attributes for each table and each algorithm  Then we took the results for the evaluation tables and created a single pool of attributes for judging by merging the results of all algorithms  We posted Mechanical Turk HITs  Human Intelligence Tasks  to obtain user judged relevance for the quality of attributes for search  Users were asked to judge how important a given attribute was for searching within a given category  with the example of faceted search explained  on a three point scale  The scale items were labeled    highly important        somewhat important     and    not important     Producing high quality judgments in a crowd sourced environment like Mechanical Turk is a challenging problem on its own  We employed a number of methods to ensure quality judgments  First  we created HITs of ten judgments from our result set and we added an extra honey pot judgment used for spam detection  The additional honey pot judgment was drawn from a pool of judgments we performed manually  and for which we felt there was a clear    highlyjudgements mturk 1 2 3 manual judgments 1 6 15 3 2 6 34 4 3 2 17 50 Figure 5  Confusion matrix between mturk and manual judgments  important    or    not important    answer  The deviation of workers from our gold standard tasks was often a clear indication of spam  However  to ensure fair judgment  we manually inspected the results of any candidate spammer for consistency  The average completion time of a worker was also used as an aid in detecting unreliable workers  Lastly  each task was repeated by nine unique workers  This allowed us to    nd majority decisions for roughly two thirds of all attribute judgments  For the remaining third without a majority  we took the average score  The result is a single valuation of the importance of each attribute  Using these graded judgments we computed the normalized discounted cumulative gain  NDCG  for the ranked output of each approach  Given a list of results L  DCG was computed using linear gain and a logarithmic rank discount as follows  DCG L    Xk i 1 rel Li  log2 i   1  Where rel Li  denotes the judged relevance of result Li  Relevance scores where assigned from zero to two for the three judgment levels  Let I denote the ideal result ordering  the list of all possible results ordered by judged relevance   then NDCG is de   ned as follows  NDCG L    DCG L  DCG I  We report on various values of k  the number of attributes returned  If a system returned less than k attributes it was penalized with a relevance score of 0 for each missing value  While we were con   dent in our quality assurance techniques  we further validated the collection for signi   cant outliers  by manually judging    ve categories for which we have personal knowledge of the domain through shopping experience  The table in Figure 5 shows the confusion matrix for the judgments  Across the top are the Mechanical Turk judgments  and on the left are the manual judgments  Entry  i  j  is the number of times the manual judgment was i  and the Mechanical Turk was j  We round averaged Mechanical Turk valuations to the nearest integer for this computation  We can see from this that the Mechanical Turk workers often favor the    safe    middle valuation  The effect of averaging judgements that do not reach a majority may also push valuations to the middle  The manual judgments tend to distribute more judgments to the    highly important    and    not important    valuations  Overall  we see exact agreement on 66  of judgments  with opposing 1 vs 3 valuations on only 4   The Mechanical Turk valuations are therefore somewhat more coarse grained than our careful manual evaluations  but are still similar in overall trend  We have also run all of our experiments over the manual judgments and found the results to be equivalent in terms of trend and rank order of the systems  We omit these graphs for brevity of presentation  6 2 Attribute Pre selection with Data Filters We start with an experiment that measures the effect of data statistics on the produced facets such as selectivity and entropy as described in Section 5  The results are summarized in Figure 6  Figure 6  Attribute pre selection using data    lters  The    rst step was to take all attributes for each table 3 and perform a run on our two simpler algorithms  shown as NoProb Full and DataProb Full in Figure 6  Surprisingly  DataProb Full does not improve much over NoProb Full  In fact it actually performs worse at higher values of k  Upon further investigation  this is understandable due to how certain attributes affect the computed probabilities  For example  there is an attribute called video input type for digital cameras where the value is almost always    digital camera     The probability P tjA  for the token    digital camera    was very high  affecting the disambiguation process whenever it was recognized in a query  As a result the DataProb Full got skewed in counting the proper attribute importance incorrectly  There were other such attributes with similar characteristics that produced a skewed behavior  We observed that such attributes had in common certain data statistics such as low value entropy and low attribute sparsity  We did a second run where we removed attributes that appear in less than 10   R A    0 1  of the entities and have entropy less than 0 1  H A    0 1   Using this data set we run again the same two algorithms  shown as NoProb and DataProb in Figure 6  The pre selection step reduced noise in the data signi   cantly and also removed attributes that have little information content  low entropy  and are not appropriate for facets  As a result the same algorithm with the pre selected data set perform much better  Now the actual data probabilities used for intra table attribute disambiguation are more meaningful and DataProb performs better than NoProb  Entropy and sparsity can be seen as continuous discount values  We saw in practice that using them as parameterized    lters is easier and produces good results  The speci   c values we used capture the problems with our particular data set and might not be optimal for all data sets  But the point we wish to make is that one has to consider such data    lters to pre select good attribute candidates in the mining algorithms  The full attribute set triggers bad results even on the more complex algorithms  It is not shown here for presentation simplicity  The remainder of the experimental section uses the same pre selected set of attributes for all our techniques  6 3 Explicit Disambiguation Using the above mentioned preselected attribute set we ran all of our disambiguation algorithms  The results are shown in Figure 7  As we discussed in Section 6 2  DataProb has clear advantages over NoProb  However DataProb has the problem of treating attributes independently  For example  in the query    30 inch television     DataProb will incorrectly determine the height attribute to be most probable over diagonal screen size  The confusion happens because televisions in that range generally have diagonals of 32 or 27 inches  whereas large 50 inch televisions tend to have a height around 30 3 We exclude metadata and boolean attributes with values like    yes no        true false   Figure 7  Token attribute disambiguation comparison inches  Since there are many 50 inch televisions in the data  height is assigned a higher probability for the token    30 inch     While ClusterProb appeared to be an intuitive solution to this issue  we see in practice that it performs no better than DataProb  ClusterProb works well for categorical attributes  but it actually does even worse on numerical ones for a couple of reasons  First  since user queries do not always re   ect the exact values of numeric attributes it tries to map the continuous domain into discrete buckets  That discretization process makes the issue of the query token    30 inch    matching the diagonal data values 27 or 32 inch even worse than DataProb  Because the actual data values vary from the query tokens  the values of the user intended attribute do not form a good model for the query tokens  Furthermore  KL divergence is only well de   ned for distributions that have non zero probability over the entire value domain  meaning even depth with actual small numeric values is assigned a small probability for large values like 50 inches  thus further enhancing the confusion of numeric attributes  Iterative tries to detect correlations between data and queries and it exploits the overall bias of users towards the correct attributes  Intuitively  the reason it disambiguates the best is due to how users enter queries  Although the system can be confused with multiple interpretations  users actually know which attributes they are looking for when they enter their values  When examined across the full query log  the user behavior tends to match the intended attribute  For example  although token    30 inch    can be consider closer to a TV height than diagonal screen size for many modern televisions  users query much more frequently for diagonal screen size including other values like    50inch    or    55inch     This creates a bias for inch tokens towards the attribute diagonal  which Iterative applies to the    30 inch    token  This causes diagonal size to be preferred over height even for that particular value  One surprise in the disambiguation methods is that Clicks do more or less similarly to DataProb  At    rst  one would think that clicks offer the perfect disambiguation method  A user click on a particular entity id allows us to select only the attributes that closely match that entity id disambiguating attributes almost perfectly  However  clicks have problems in the way they were produced that make them less than optimal for learning good facets  First  a user can only click on results shown to them  so clicks incorporate the engine bias and do not represent the true popularity of how many times all possible queries are asked  Hence some attributes are underrepresented in a click log due to the engine bias of what results were shown  Second  there are far fewer queries with clicked results than generally asked  meaning many categories have very few queries  This means the mined facets do not have much support and are somewhat erratic  Finally  the queries that tend to trigger clicks are very speci   c queries looking for one or few products  e g     canon Figure 8  Comparing with Amazon and Bing  eos 50d digital camera     More general category based queries tend to have few clicks and commonly followed by a re   nement since they return a large result set that users cannot easily consider  e g     12 megapixel digital camera     This creates a skew of the importance learned towards attributes that act as unique keys  However these attributes are not well suited for facets  If a user already knows the unique key they are looking for  like camera model   they can simply select it from the results and do not need a facet for it  Instead such attributes are better suited for entity snippets  that is summarized views that help users differentiate amongst entities  6 4 Commercial Faceted Search Commercial web engines are already supporting faceted search  Although we do not know the technical details of industrial implementations  we felt the best way to test the real world effectiveness of our solution is by comparing against state of the industry engines  Since our data is shopping based  we considered Amazon as the world   s most popular shopping engine  Furthermore  since our data comes from the public MSN Shopping API  we considered Bing Shopping  redirects from MSN Shopping  as a shopping engine that is using the same data set as our experiments  Both Amazon and Bing Shopping show facets  To test them  we crawled the attributes shown on Amazon and Bing for our test categories and submitted them in the same pool of attributes that we labeled with mturk judges  To remove any bias  we dropped any af   liation information on the web site  Furthermore  we did not consider the handful of generic shopping attributes shown on both Amazon and Bing  like price or shipping options  Although very valuable facets there is no need for an automated technique to discover them  as such attributes can easily be added to all categories  Instead we limited our comparison to only category speci   c attributes from each solution  The results are summarized in Figure 8  Our best solution performs better than Amazon and signi   cantly better than Bing Shopping  For many categories Amazon shows very few attributes and Bing shows even less  As a result  both Amazon and Bing drop dramatically as the size k increases in our experiments  It is important to note that Amazon does show up to 12 category dependent facets in some cases  e g   watches  so screen real estate is not a limiting factor  For small values of k  k   3  we still do better but the difference  particularly with Amazon  is smaller  We are not familiar with the details of their facet selection approach and so their techniques cannot be contrasted to ours  However  since they are a popular site with lots of query traf   c and domain knowledge  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sdwp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#data_on_web"/>
        <doc>Schema As You Go  On Probabilistic Tagging and Querying of Wide ### Tables Meiyu Lu     Divyakant Agrawal        School of Computing National University of Singapore  lumeiyu  dbt  atung  comp nus edu sg Bing Tian Dai     Anthony K H  Tung        Department of Computer Science University of California  Santa Barbara agrawal cs ucsb edu ABSTRACT The emergence of Web 2 0 has resulted in a huge amount of heterogeneous data that are contributed by a large number of users  engendering new challenges for data management and query processing  Given that the data are uni   ed from various sources and accessed by numerous users  providing users with a uni   ed mediated schema as data integration is insu   cient  On one hand  a deterministic mediated schema restricts users    freedom to express queries in their preferred vocabulary  on the other hand  it is not realistic for users to remember the numerous attribute names that arise from integrating various data sources  As such  a user oriented data management and query interface is required  In this paper  we propose an out of the box approach that separates users    actions from database operations  This separating layer deals with the challenges from a semantic perspective  It interprets the semantics of each data value through tags that are provided by users  and then inserts the value into the database together with these tags  When querying the database  this layer also serves as a platform for retrieving data by interpreting the semantics of the queried tags from the users  Experiments are conducted to illustrate both the e   ectiveness and e   ciency of our approach  Categories and Subject Descriptors H 2 8  Database Management   Database applications General Terms Management  Performance Keywords Probabilistic Tagging  Wide Table  Top k Query Processing ### 1  INTRODUCTION The rapid growth of Web 2 0 technologies and social networks has provided us with new opportunities for sharing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  data and collaborating with others  Within a community  users may like to contribute their own data  share the data with friends  and meanwhile explore the shared data at the community scale  E   ectively and e   ciently managing and exploring these data at a web scale level is an interesting and important problem  In this paper  we focus on the management of shared structured tables  e g  Google fusion table  16   which is an important type of data in Web 2 0  Such data have some inherent properties that make their management challenging  Heterogeneity  Data provided by di   erent users may be widely heterogeneous  containing di   erent attribute names that describe semantically similar values or same attribute name with di   erent semantics  For example  in Figure 1 a   user A chooses attribute make to describe the car manufacturer in his her private data  while user B uses manufacturer to represent the same meaning  Similarly  the semantics of car in  b  is model but in  c  it contains both make and model information  Numerosity  As all users can publish their data into the community as shown in Figure 1 a    b  and  c   the number of distinct attributes will grow drastically as more users participate  This results in a wide table  7  as Figure 1 d  which is essentially a structured table with a large number of attributes and many empty cells  Given such a wide table  most existing data integration approaches  23  create a global mediated schema that usually contains a huge number of attributes  Such huge mediated schema is di   cult to query as users cannot remember all the attributes  Therefore  a more    exible and user oriented query interface is required  This interface should allow users to choose di   erent attribute names to represent the same semantics in di   erent queries  and have the power to express their con   dence over the attribute semantic heterogeneity in posed queries  Personalized Attributes  Since attributes are contributed by users  there could be many personalized attributes  which cannot be captured by existing ontology or taxonomy  such as DBpedia 1 and WordNet 2   Examples of such personalized attributes include myear in Figure 1 c   which represents the production year of a car  and eadd  the abbreviation for email address  Due to this  existing techniques that rely on ontology for schema mapping  26  30  will not be suitable any more  A better system should be able to infer more popular 1 DBpedia  http   dbpedia org About 2WordNet  http   wordnet princeton educar myear BMW 525i 2005 Toyota Camry 2009 make Ford BMW model car Fiesta 318Ci manufacturer myear                   Ford Honda Focus 1 6A Accord                         BMW 525i Toyota Camry       2005 2009       User A User B User C Q1 Q4 Q2 User E User F User G User H Q3  a   b   c  make model Ford Fiesta BMW 318Ci manufacturer car Ford Focus 1 6A Honda Accord TID T1 T2 T4 T5 T6 T3  d  Figure 1  The Wide Table Tag1 Similarity make 0 95 make 0 5 manufacturer 0 52 model 0 65 Tag2 manufacturer car car car Figure 2  Tag Similarity attributes for such personalized ones  so that these data can be explored by the other users asides from its owner  Due to the inherent semantic heterogeneity of the attributes in user contributed data  we will no longer refer to them as    attribute     Instead  we borrow the term tag from the multimedia domain  1  2   and view these attribute names as just a way to annotate the values that are provided by users 3   In this paper  we describe a query system which enables users to e   ectively query a large quantity of user contributed data  Our system provides the following functionalities      Users can contribute and share data by inserting them into a single wide table together with their tags  as shown in Figure 1 d       Our system is able to automatically discover the semantic relationships between users    tags  which is handled by our tag inference approach in Section 3  Figure 2 illustrates the semantic similarities between each pair of tags produced by our approach for the example data in Figure 1 4       Users can use their own preferred tags to query the database  and the system then dynamically determines the semantics of these tags at query time  Combining the data from Figure 1 d  with the tag similarities in Figure 2  we can easily obtain the virtual probabilistic tagged data  as shown in Figure 3  where each value is associated with a list of htag  probabilityi pairs  The probability indicates the semantic association strength between the value and tag pair  The generation of such semantic association probabilities is called probabilistic tagging  and its formal de   nition will be given in Section 2  We next present an example to illustrate several characteristics of our query interface via a set of queries  3 In this case  users might even choose to annotate the same value with multiple tags  4 As our similarity measure is symmetric  we only show unidirectional tag similarity in Figure 2  Example 1  Figure 1 shows a scenario where four users  A  B  C and D contribute and share their datasets  The wide table is used to store the shared data  User E  F  G and H then submit four di   erent queries which are Q1  Q2  Q3 and Q4  respectively  The queries are shown in Table 1 in an extended SQL format  The delimiter         in the query is for users to explicitly specify their con   dence over tag semantics  Its formal de   nition is presented in Section 2  Here we can interpret make  0 9 as a set of tags in Figure 2 which are similar to make with con   dence no less than 90   The table name CAR refers to the probabilistic tagging table in Figure 3  Values in the result tuples have the same order as the tags in the SELECT clause  From the query result in Table 1  we observe that i  different users can choose di   erent tags to express the same semantics  such as Q1 and Q2  ii  explicit con   dence speci   cation gives users control over the search scope  For instance  with lower con   dence Q3 retrieves two more tuples than Q1  iii  schema for users    query is dynamically determined at query time  which is user oriented rather than prede   ned  Take Q4 as an example  where a user would like to retrieve values that are associated with car and model from CAR  From the data perspective  for T3 and T4  tag car in the query should be aligned with car in the source table  But from the users    perspective  intuitively the best schema alignment for T3 and T4 should be  car  model   This is what our system retrieves given Q4  To further see the bene   t of our approach  Figure 4 shows the result of Q4 over tuples T3 and T4 using our approach and the one proposed by Dong and Halevy in  12  25  5   Both records returned by our approach are correct and ranked in the right order  However  the top two ranked tuples retrieved by probabilistic integration are not correct although their score is much higher than the correct ones  The result di   ers because we are dynamically determining the semantics for user posed tags at query time  The reasons for such di   erences will be explained in Section 2 2  Besides the functionalities discussed above  our work also makes the following contributions      We proposed the concept of probabilistic tagging to represent the associations between values and tags      We presented an e   ective distance function to measure the semantic distance between a pair of tags  Based 5 Please refer to Appendix A for the detailed score computation for probabilistic integration Table 1  Example Queries QID Query Result Q1 SELECT make 0 9 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda  Q2 SELECT manufacturer 0 85 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda  Q3 SELECT make 0 5 FROM CAR  T1 Ford  T2 BMW  T3 Ford  T4 Honda   T5 BMW 525i  T6 Toyota Camry  Q4 SELECT car  model FROM CAR  T1 Ford Fiesta  T2 BMW 318Ci   T3 Ford Focus 1 6A  T4 Honda  Accord  TID Value Probabilistic Tagging T1 Ford  make 1 0   manufacturer 0 95   car 0 5  T1 Fiesta  model 1 0   car 0 65  T2 BMW  make 1 0   manufacturer 0 95   car 0 5  T2 318Ci  model 1 0   car 0 65  T3 Ford  manufacturer 1 0   make 0 95   car 0 52  T3 Focus 1 6A  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T4 Honda  manufacturer 1 0   make 0 95   car 0 52  T4 Accord  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T5 BMW  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T5 2005  myear 1 0  T6 Toyota  car 1 0   model 0 65   manufacturer 0 52   make 0 5  T6 2009  myear 1 0  Figure 3  Probabilistic Tagging TID car T4 T3 Ford Honda model Focus 1 6A Accord SELECT car  model FROM CAR Q4 rank 1 2 TID car T4 T3 Focus 1 6A Accord model Focus 1 6A Accord rank 1 2 T4 T3 Ford Honda Focus 1 6A Accord 3 4 Data Integration Result Our Result Query QID Figure 4  Query Result Comparison on this  we are able to automatically infer semantically similar tags for each value      We proposed an e   cient dynamic instantiation approach to associate the queried tags and data values during query processing      A complete and extensive experimental study is conducted to validate our approach  2  PROBLEM DEFINITION 2 1 Probabilistic Tagging As discussed earlier  one of our main objectives is to provide a    exible interface  which allows users to query the underlying database with any tag they like  The very    rst challenge is to discover which tags are similar in semantics  However  this is insu   cient since some tags are more similar to each other in semantics while others are not  Taking T1 in Figure 3 as an example  it is intuitive that the semantics of    Ford    is better re   ected by manufacturer than car since the semantics of manufacturer is quite speci   c  while car is a relatively more general term  Therefore  the second issue is that the semantic association strength between a value and tag should be well quanti   ed  as shown in Figure 3  In this paper  for a given value v in the wide table  we interpret its semantic association with a tag t as the likelihood belief that v is tagged by t  Definition 1  Probabilistic Tagging   Let T be a set of tags  for a value v in Wide Table and a tag t     T   probabilistic tagging refers to the process of associating a probability   v t for v and t  such that value v is tagged by tag t with probability   v t  The probability   v t is called the associated probability or association probability  If the semantic association between value v and tag t is stronger  then their associated probability   v t will be higher and vice versa  As shown by T1 in Figure 3  for the value    Ford     its associated probability with its original tag make is 1 0 while its associated probability with inferred tags manufacturer and car are 0 95 and 0 5  showing di   erence in the strength of semantic association  We note here that unlike previous works  12  25  17  in data integration  the probabilities for all tags being associated with a value v do not sum up to 1  This is because there is no assumption of mutual exclusion between tags at this stage of data processing  We avoid doing so to prevent a label bias problem in which a value v will have much lower associated probability with each tag if it can be semantically described by many di   erent tags  For example  a value associated with 5 or more tags will have much lower probability than value with 2 or less tags  Instead  we choose to introduce the assumption of mutual exclusion during query time where the semantic meaning of the tags are clearer  2 2 Query Semantics After discovering and quantifying the tag semantic relationships during probabilistic tagging  the resulting probabilistic table is shown as in Figure 3  In this table  each value is associated with a htag  probabilityi pair list  Query over such probabilistic table is much more challenging than with a prede   ned schema  We will now look into the query relevant issues over a probabilistically tagged table  2 2 1 Extended SQL Query In this paper we adopted a SQL like query syntax to support our tag based queries  Speci   cally  we extended SQL query to allow users to express their semantic con   dence for each tag using the delimiter          We show some examples in Table 1  Below is a more complex extended SQL query  SELECT car  year 0 9  price 0 3 FROM CAR WHERE make 0 5      Ford    AND price     10 000 AND price     20 000When a user provides a tag with high con   dence  it means that he is very sure about the semantics of the tag  and does not want our system to do much tag inference  In contrast  if the user is unsure of the semantics and he likes to use more tags that are similar to the queried tag  then he can provide a lower con   dence threshold  Definition 2  Tag Expansion   Given a tag tq and its semantic con   dence    in query q  tq    refers to the expanded tag set Te    tei  i   1          n   where for each tag tei     Te  Semsim tq  tei           We call Te as the expanded tag set for tq  In the above de   nition  Semsim tq  tei   is the semantic similarity between tag tq and tei   which is formally de   ned by Equation 6 in Section 3  When processing query q  tq will be expanded to a tag set Te  a set of tags that are semantically similar to tq based on a threshold     All values that are originally associated with any of the tags in Te are valid for this tag expansion condition  and should be taken into consideration when answering the query  If no semantic con   dence is explicitly speci   ed for tq  we will expand it to all our inferred tags  Take the queries in Table 1 as example  make 0 5 will be expanded to all the tags that are similar to make in semantics inferred by our system  that is  manufacturer car   but make 0 9 will only be expanded to  manufacturer   2 2 2 Query Answering Before explaining the semantics of query answering  we    rst look at the intuitions behind our query answering techniques  Intuitively  syntactically equivalent tags in user posed queries should be instantiated with the same value  no matter how many places it appears in the query  For example  although price occurs three times in the example query in Section 2 2 1  once in the SELECT clause and twice in the WHERE clause  it is obvious that all the occurrences of price refer to exactly the same semantics  i e  price of a car  Thus  in the resulting record  all these three instances of price should be instantiated with the same value  Based on this  we proposed the following Consolidation Rule for our query answering  Rule 1  Consolidation Rule   Let set Tq    ti i   1          n  be the set of tags in query q  for two tags ti  tj     Tq  if ti   tj   then ti and tj should be instantiated with the same value from the underlying record  Consider the query in Section 2 2 1  although we know tag car and make are similar in semantics from Figure 2  in this query they actually represent di   erent attributes because the user speci   es them as di   erent  In other words  car and make are mutually exclusive from each other in this query  and should be instantiated with di   erent values in the resultant record  We proposed our Mutual Exclusion Rule as below  Rule 2  Mutual Exclusion Rule   Let Tq    ti i   1          n  be the set of tags in query q  for two tags ti  tj     Tq  if ti  6 tj   then ti and tj should be instantiated with di   erent values from the underlying record  We have discussed the impact of our mutual exclusion rule using Q4 earlier  in Example 1   There for T3  we argued that car in the query should be aligned with manufacturer in the source table  Here we look at the query in Section 2 2 1  where a user would like to retrieve the value associated with car from a record whose value    Ford    is associated with make  for simplicity  we skipped the other constraints   Now for T3  it is more reasonable for car in the query to be aligned with car in the source table  Comparing these two queries  we can see that with our mutual exclusion rule  the same tag  car  can be aligned with di   erent semantics in di   erent queries even for the same record  T3   As explained earlier  applying the mutual exclusion rule during query answering time allows us to avoid the label bias problem  Moreover  our mutual exclusion rule cannot be trivially introduced in  12  25  17  since they enforced the fact that two attributes tags are either semantically equivalent  associated with the same value  or not in each possible world  As such  the problem illustrated by Figure 4 in Example 1 is inherent for  12  25  17   For a given tuple record  we aim to    nd its possible best alignment to the queried tags  As the semantics of queried tags are determined dynamically on the    y  our query answering is referred to as Dynamic Instantiation  Definition 3  Dynamic Instantiation   Let set Tq    ti i   1          n  be the set of tags in query q  and Vr    vj  j   1          m  be the set of values from a record r  we refer to a mapping f   Tq     Vr as the dynamic instantiation of Vr with respect to Tq  Based on f  the instantiated score Score r  for r with respect to q is de   ned as the multiplication of associated probabilities for edges in f  i e  Score r    Qn i 1   f ti  ti   In addition  all the following conditions must be satis   ed in f  1  f satis   es all the value constraints in the WHERE clause  2  f satis   es both Consolidation Rule and Mutual Exclusion Rule  3  The associated probability for each edge in f  i e    f ti  ti   must satisfy its corresponding tag expansion  4  Among the mappings in which all the above three conditions hold  dynamic instantiation refers to f whose score is maximized  i e  f   arg maxf Score r   The    rst condition states that if tag t has value constraints in the WHERE clause  then its mapped value f t  should satisfy all the value constraints over t in query q  The second condition is for us to better capture the users    intention as discussed earlier  The third condition is used to meet the tag semantic speci   cations  Finally  the last condition is to    nd the best possible instantiation between tags set Tq and values set Vr  i e  the one with maximum instantiation score Score r   Our instantiated score is de   ned as the multiplication of associated probabilities in the instantiation f  that is because in our case the associations between tag and value pairs are independent  and based on our mutual exclusion rule  all the edges falling outside of f are invalid  Due to the semantic uncertainty of tags  answering a query q can result in a large number of records with score larger than 0  In view of this  we mainly focus on top k query processing to    nd the best k answers based on our query answering semantics  Definition 4  Top k Query Answering   Given a query q  a user speci   ed positive integer k  and a set ofrecords R    ri i   1           R    the problem of top k query answering is to retrieve a record set RS  such that RS     R   RS  is maximized with the condition  RS      k     r     RS and    r         RS  which is the complementary set of RS  Score r      Score r        From the above de   nition  it is possible that less than k results are retrieved  We however feel that it is a worthy cost to pay for freeing users from a    xed mediated schema and providing relaxation over the tag speci   cations  3  TAG INFERENCE We next describe our tag inference process which is essential for discovering the semantic relationships between tags  and computing the associated probabilities between tag and value pairs  We use T    ti i   1          n  to denote all the tags in our database  and V    vj  j   1          m  to denote all the values  For a tag t     T   we use Vt     V to represent the set of values that fall under the tag t in the wide table  E   ectively discovering the semantic similarities between user contributed tags however is challenging  Although several approaches have been proposed in the literature  23  26  30   none of them is suitable for our case  First  string similarity over tag names could not convey all cases  26  23   With such approaches  model and model year will be treated as similar but make and manufacturer will be treated as dissimilar  Second  ontology and taxonomy based methods  26  30  are not applicable  as there are many personalized tags like myear and eadd  which cannot be captured by any existing ontology and taxonomy  Finally  similarity function based on values overlap  23  works only for categorical values  not for string and numerical values  For example  given two set of emails that are contributed by different users using two di   erent tags  it is unlikely that there is much overlap between the two set of emails  Our approach to discovering the semantic similarities in this paper can generally be described in two steps  First  soft clustering  28  is applied to the values in the wide table to separate them probabilistically into K clusters  i e  each value can belong to each of the K clusters with different probabilities  Since each tag is associated with a set of values with di   erent probabilities  we can thus indirectly compute the probability of these tags being associated with the K clusters  In the second step  we measure the distance between tags by comparing the probability distribution of their associated membership to each of the K clusters using KL divergence  8  9   KL divergence is a function for measuring the relative entropy between two distributions  Speci   cally  if the probabilistic cluster membership distributions for two tags are p and q respectively  their relative entropy is KL pkq    P i 1  K pi log pi qi   where pi is the probability of the    rst tag being associated to cluster i and qi is the probability of the second tag being associated to cluster i  3 1 Tag Semantic Distance Discovery To perform the soft clustering  our approach    rst extracts a set of features F from the underlying data V   Depending on the type of values in t  di   erent approaches are adopted to generate the features for t  If values in t are string values  we extract a set of frequent q grams called sigrams as its features  If values in t are numerical values  bins of the distribution histogram over Vt are extracted as its features  After extracting the features F for all values V   soft clustering  28  8  is performed over F  In soft clustering  the membership of a value in a cluster is probabilistic  Suppose there are K clusters C    c1          cK   for a value v  its membership probabilities in each cluster form a vector     v    hp1          pKi  where PK i 1 pi   1  and pi is the probability that v belongs to cluster ci  We call     v  the cluster probability distribution for value v over C  Based on  8   our soft clustering based tag inference is robust when K 100  We set K to be 200 in this paper  Since a tag t can be associated with di   erent values which have di   erent cluster probability distributions on the K clusters  we can indirectly compute a cluster probability distribution for a tag t  denoted as     t   by taking the average cluster probability distributions of all its associated values Vt      t    X vt   Vt 1  Vt         vt  To measure the semantic relationship between two tags  t1 and t2  one important factor that needs to be considered    rst is the semantic scope of each of these tags  Intuitively  if the semantics of a tag t is more general  the values associated with it will be more diverse  For instance  both make and model information of a car can be tagged by car  while only the make information can be tagged by make  Thus  the semantic scope Scope t  for tag t can be captured by the average KL divergence between     vt  and     t   Scope t    X vt   Vt 1  Vt     KL     vt       t    1  Another important factor we need to consider is the distance between tag t1 and the values of tag t2  If t1 is similar to t2  then     vt1   should be close to     t2   For clarity  we de   ne the distance between the value set Vt1 of tag t1 and another tag t2 as DV  Vt1   t2   DV  Vt1   t2    X vt1    Vt1 1  Vt1      KL     vt1        t2    2  Based on the above discussion  if two tags t1 and t2 are similar in semantics  they should i  have similar semantic scopes  and ii  close to the value set of each other  Now we consider one extreme case  the distance between two semantically equivalent tags t1 and t1  We can easily    nd that DV  Vt1   t1    Scope t1   From this we can see that if tag t1 is more similar to t2  the ratio of DV  Vt1   t2  over Scope t1  will be closer to 1  and vise versa  Thus  the dissimilarity between two tags t1 and t2 is de   ned by such ratios  Dis t1  t2    DV  Vt1   t2  Scope t1    DV  Vt2   t1  Scope t2   3  However  it is costly to compute such tag dissimilarity for each tag pairs based on Equation 3 since we need to enumerate all values in Vt1 and compute their KL divergence to t2 to obtain DV  Vt1   t2  and vice versa  Assuming there are n tags in T   and each tag t being associated with m values  the complexity for computing all tag pair dissimilarity using Equation 3 is O C 2 n    m   nm   Fortunately  we are able to use the following proposition to reduce the computational complexity Proposition 1  Bregman Information Equality   Let X be a random variable that takes values in X    xi  n i 1     R d following the probability measure          xi   n i 1  given a Bregman divergence    6 and another point y  E   d   X   y     E   d   X          d       y  where      E   X    Proof  E   d   X   y     Pn i 1    xi d   xi  y    Pn i 1    xi     xi         y      hxi     y       y i    Pn i 1    xi     xi                 hxi                 i  hxi               i             y    hxi           y       y i    Pn i 1    xi     xi                 hxi                 i    Pn i 1    xi                y      h       y       y i    E   d   X          d       y  As Pn i 1    xi  xi           0  and both Pn i 1    xi hxi                  i   Pn i 1    xi hxi              y i   0  that is how the second last equation comes from  Intuitively  our proposition simply states that the expected KL divergence between a random variable X following the probability distribution    and a point y is equivalent to the sum of the KL divergence between X and its means    plus the KL divergence between    and y  By this proposition and  3   we can show that the distance between the set of values associated with tag t1  i e  Vt1   and the tag t2 can in fact be computed simply by using Scope t1  and the KLdivergence between     t1  and     t2   DV  Vt1   t2    X vt1    Vt1 1  Vt1      KL    vt1       t2     X vt1    Vt1 1  Vt1      KL    vt1       t1    KL    t1 k   t2     Scope t1    KL    t1 k   t2    4  By applying Equation 4 over Equation 3  we can now de     ne our semantic distance between t1 and t2 as  D t1  t2    KL     t1 k    t2   Scope t1    KL     t2 k    t1   Scope t2   5  D t1  t2  is symmetric  however  it does not satisfy triangle inequality  This coincides with the intuition that two tags cannot be inferred as similar via a third tag  For example  we cannot say model and make are similar although both are semantically similar to car  Note that unlike Equation 3  the computation of the new distance function only involves the semantic scope  and pairwise KL divergence between tags  The computational complexity for all tag pair distances is thus reduced from O C 2 n    m   nm  to O C 2 n   nm   3 2 Tag Inference With the tag distance de   ned above  now we give our semantic similarity between two tags by taking the power of the negative distance  i e  SemSim t1  t2    e        D t1 t2   6  6 Note that KL divergence is a special instance of Bregman divergence  32  and thus the derivation here immediately applies to KL divergence  Table 2  TagTable for make TID LID Value Probability 1 1 Ford 1 0 2 1 BMW 1 0 3 1 Ford 0 95 4 1 Honda 0 95 5 1 BMW 525i 0 5 6 1 Toyota Camry 0 5 where    is a parameter which controls the power of inference  When    is set to be 0  every value will be associated with every tag  when    is set to be in   nity  there will be no inference among tags  In the experiments  we set    at 0 5  An example tag similarity table is shown in Figure 2  Suppose value v is originally associated with tag t in user contributed data  then the associated probability of v with respect to other tags ti     T is de   ned as follows    v ti     v t    Semsim t  ti    Semsim t  ti   7  associated probability   v t   1 as v is originally associated with t  After applying the above tag inference equation  each value in the database will be associated with a set of semantically similar tags with proper probabilities  One example probabilistic tagged data is shown in Figure 3  4  TOP k QUERY PROCESSING Next we will look at our top k query processing approaches  4 1 Data Organization To facilitate top k query processing  we partition the probabilistic tagged table according to tags  For a speci   c tag t  all its associated values are stored into one relational table  and this table is named by the tag name  i e  t  We call such a table TagTable  Table 2 shows the TagTable for make  Each TagTable has four attributes  TID  tuple id   LID  location o   set in the source tuple   Value  data value  and Probability  associated probability   Note that the majority of    is approximately 0 and only values with associated probability above a cuto    threshold are stored  On the other hand  value v may be associated with multiple tags  so there is a copy of v in each of its associated TagTable  For each TagTable  a B    tree index is built over the attributes hV alue  P robabilityi  which will be used for the sorted list retrieval in top k query processing  4 2 Dynamic Instantiation Given a query q and record r  let Tq be the set of tags in q  and Vr be the set of values from r  We can build a weighted bipartite graph G    U  V  E  as follows  Let U   Vr  V   Tq  and for each vr     Vr  each tq     Tq  if vr satis   es the value constraints over tq and   vr tq meets the tag expansion condition  we add an edge ehvr  tqi to E  with weight ln   vr tq   For simplicity  we denote this bipartite graph as G    Vr  Tq  ln   Vr Tq    After taking logarithm over the associated probabilities   Vr Tq   it is easy for us to apply Hungarian algorithm  18  on G    Vr  Tq  ln   Vr Tq   to    nd the best one to one matching between Vr and Tq  where the total sum of cost weight is maximized  We denote the maximum score found by Hungarian algorithm for record r as ScoreH r   Based on De     nition 3  we can see that the mapping found by Hungarianprius price mileage make   model Toyota    10 000   Prius    20 000 year 2006 16204 12198 18500 toyota 1 0 0 9 0 9 0 6 0 7 0 9 0 8 0 3 0 3 Figure 5  Dynamic Instantiation Illustration algorithm is exactly the dynamic instantiation  The    rst and third statements in De   nition 3 are met by our edge insertion constraints discussed above  one to one mapping guarantees the second statement  and the last statement is ful   lled as the solution found by Hungarian algorithm is optimal  From the above discussion  it can be easily proven that Score r    e ScoreH r  Take the following extended SQL query as an example  The values associated with tags year  price and mileage are to be retrieved  with constraints issued over tags make  model and price  SELECT year 0 9  price 0 3  mileage FROM CAR WHERE make 0 8      Toyota    AND model      Prius    AND price     10 000 AND price     20 000 The corresponding bipartite graph is shown in Figure 5  For clarity  edge weights in the    gure are original associated probabilities without logarithm  As illustrated by Figure 5  if mileage is instantiated with 16204 and price is instantiated with 18500 as the darker lines indicate  it gives the best instantiation as the multiplication of the associated probabilities is maximized  4 3 Top k Query Answering Next we will discuss the retrieval of top k answers  For ease of illustration  we refer to tags in the SELECT clause as QTags  and tags in the WHERE clause as VTags  Tag set in query q is denoted as Tq  4 3 1 Sorted List Retrieval For each queried tag tq     Tq  we aim to retrieve a list of valid values from its TagTable  sorted over associated probability from high to low  For a tag that appears in both VTags and QTags  we merge its constraints from SELECT and WHERE clauses  and retrieve one sorted list  We    rst study the situation where a user speci   ed semantic con   dence    is issued over tq  i e  tq     Based on De   nition 2  tq will be expanded to a tag set Te    tei  Semsim tq  tei            Next we will look at the associated probabilities with tq for valid and invalid values  respectively  Valid case  Given a valid value v  suppose it is originally associated with tag tei     Te  The associated probability between v and tq is   v tq   Semsim tq  tei           This means that for each valid value  its associated probability in TagTable tq is not less than     Invalid case  Given an invalid value v       we know that its original tag falls outside of Te  Assume its original tag is tv      From Definition 2  we know that Semsim tq  tv            Therefore  the associated probability between v     and tq  i e    v     tq   Semsim tq  tv            In other words  for each invalid value  its associated probability in TagTable tq is less than     In conclusion  associated probability with valid value is not less than     while with invalid value is  Hence  the sorted list for tq    can be retrieved by the following SQL statement issued over the TagTable for tq  SELECT   FROM tq WHERE Probability        ORDER BY Probability DESC  We then look at the complex situation where there is value constraint over tq besides the tag expansion  i e  tq    op vq  Here op is a comparison operator and vq is a speci   c value  Its sorted list can be resolved by adding one WHERE condition    Value op vq    into the above SQL statement  For the simple cases where there is no tag expansion or value constraint over tq  their sorted lists can be easily retrieved by removing the corresponding WHERE condition from the SQL statement  Bene   ting from the B    tree index built over attributes hV alue  P robabilityi for each TagTable  most sorted lists can be done by an index only scan  After retrieving the sorted list for each tag tq  Threshold Algorithm  TA   13  can be applied to fetch the top k results  using Hungarian algorithm to    nd the best instantiation  We proposed two TA based approaches for top k query processing  Eager and Lazy  4 3 2 Eager Top k Query Answering In eager query answering  for each tag tq in query q  its sorted list is retrieved and maintained  TA  13  is then performed over all the retrieved sorted lists  The main idea is shown in Algorithm 1  A priority queue RS is maintained to store the best k records that have been processed so far  All the scanned records are inserted into Sr to avoid accessing the same record more times  Each time  we scan the top unseen records from all the sorted lists  and pop the record r with the highest logarithmic probability  MP  from its sorted list  line 14   If there are more than one sorted lists whose top probability is equal to MP  our algorithm randomly chooses one from them  The popped record r is then retrieved and Hungarian algorithm is applied to get its score ScoreH r   line 15   Finally  priority queue RS is updated accordingly  line 16 19   and r is inserted into the scanned record set Sr  line 20   For all the unseen records  their scores are upper bounded by the summation of the top probabilities from each sorted list  i e  UBS  This is because our sorted list is ranked over probability  so the probability for each unseen record in SLtq is not greater than SLtq  top prob  Algorithm 1 can be terminated when the k th score in the result set RS is not less than the upper bound score UBS  line 23   or any sorted list is exhausted  line 21   Eager query processing searches through the sorted lists for all queried tags  including all QTags and VTags  However  this is not e   cient enough  Generally  the sorted lists for QTags are much longer than those for VTags  because VTags have value constraints in the WHERE clause  while most QTags do not  especially for those that only exist in QTags  Due to the low selectivity of QTags  their sorted lists usuallyAlgorithm 1  Eager top k Query Processing Input   A query q in extended SQL A positive integer k Output  Result set RS 1 Tq     tags in query q  2 RS             top k result set 3 Sr             scanned records 4 foreach tag tq     Tq do 5 SLtq     sorted list of tq  6 Let SLtq  top be the most top unseen item in SLtq   7 Let SLtq  top prob be the probability of item SLtq  top  8 while true do 9 UBS     0     upper bound score for unseen records 10 MP                maximum in all SLtq  top prob 11 foreach tag tq     Tq do 12 UBS     UBS   SLtq  top prob  13 MP     max MP  SLtq  top prob   14 Pop record r from SLt where SLt top prob   MP  15 Let score   ScoreH r      computed by Hungarian 16 if  RS    k or score   the k th score in RS then 17 if  RS    k then 18 pop one item from RS  19 insert  score  r  into RS  20 insert record r into Sr  21 if SLt has reached its end then 22 break     while loop is terminated 23 if  RS    k and UBS     the k th score in RS then 24 break     while loop is terminated 25 return RS  contain lots of invalid items  Based on this  we proposed another approach  namely lazy top k query answering  which only focuses on the sorted lists of VTags  4 3 3 Lazy Top k Query Answering Unlike the eager approach  our lazy approach only retrieves and maintains sorted lists for VTags in query q  For a record  any values that are not related to Vtags are only accessed when computing its Hungarian score  line 15   For queried tags falling outside of Vtags  we assume their associated probabilities to be 1  the largest associated probability  when computing the upper bound score  UBS   Compared to the eager approach  our lazy approach does not need to probe the long sorted lists which include many invalid records  making it more I O e   cient  In both eager and lazy approaches  the score for each unseen record is upper bounded by the summation of all the top item scores  Hence  the correctness of our algorithms can be guaranteed  5  EXPERIMENTAL STUDIES We now present the experiments to validate the performance of our system  We have two main goals in this section  1  to examine the e   ectiveness of our tag inference approach  2  to show that our top k query processing is capable of retrieving top k results with high precision and low cost  5 1 Experimental Setup We implement our system on top of MySQL  The tag inference and query answering algorithms are written in C    The system is set up on a PC with Intel R  Core TM 2 Duo CPU at speed 2 33GHz and 3 5GB memory  Through all the experiments  we randomly select 2  samples for soft clustering  1  Car Datasets  CAR Domain   The    rst datasets we deal with are datasets in the car domain  We insert 101 di   erent tables from di   erent sources into a wide table to simulate the scenario where di   erent users share their data  Among the 101 tables  one is obtained from Google Base  and has 24 thousand tuples and 15 columns  while the others are downloaded from Many Eyes 7   These tables have di   erent sizes  with number of tuples ranging from dozens to thousands  and the number of columns ranging from 5 to 31  In total  there are 94 854 rows  284 distinct tags and 850 215 distinct values after combining the data into one wide table  2  Directory Datasets  DIR Domain   We extract three directory datasets from three websites that provide directory services 8   The directory data sets consist of information about companies  such as company name  address  telephone  fax  email and so on  In total  there are 139 022 tuples  31 distinct tags and 495 908 distinct values  5 2 Tag Inference Validation We verify the e   ectiveness of our tag inference approach    rst on real datasets  and compare it with the matching results from Open II  We then further validate it over the semi real Google Base car datasets  5 2 1 Validation over Real Datasets To evaluate our tag inference approach  we    rst compare it with Open II  26  based on the golden standard  Open II is an open source data integration toolkit that has implemented several schema matching methods 9   In this experiment  for Open II  we choose all the matching methods it implemented to produce its matching result  These matchers include    Name Similarity Matcher        Documentation Matcher       Mapping Matcher        Exact Matcher       Quick Matcher    and    WordNet Matcher     The golden standard is identi   ed by manually scanning through all the columns in each domain  In car domain  totally there are 160 semantically equivalent tag pairs in the golden standard  Both the output format of Open II and our system are a list of tag pairs  each associated with a similarity score  For car domain  Figure 6 shows the precision for the top k similar tag pairs in Open II and our approach  with k varying from 20 to 200  The result clearly demonstrates that our approach achieves better precision than Open II when we are looking at the same number of top k similar tag pairs  The reason is that we de     ne the tag semantic similarity based on soft clustering and 7 http   manyeyes alphaworks ibm com manyeyes  8 They are thegreenbook com  streetdirectory com and yellowpages com sg respectively  9 Note that Open II is a schema level mapper while our approach is an instance based approach  23   As such  Open II takes only one second for all its attribute mappings while our approach takes on average 2 5 hours to perform high dimensional clustering  These operations are however preprocessing which do not a   ect the query answering e   ciency of our approach  0  0 2  0 4  0 6  0 8  1  20 40 60 80 100 120 140 160 180 200 Precision Top k similar tag pairs OpenII Ptagging Figure 6  Comparison with Open II KL divergence  rather than directly over attributes  Hence  we are able to discover similar tags that have di   erent syntaxes  such as make and manufacturer  while they cannot be found by Open II  At the same time  Open II makes many wrong predictions on tag pairs which are similar in names but di   erent in semantics  Examples include fuel type and body type  model and model year etc  Figure 6 also indicates that our precision decreases when k increases  This is because there are some semantically di   erent tags whose value distributions are so similar that our approach cannot tell them apart  such tags include price and mileage in car domain  telephone and fax in directory domain  For such tag pairs  it might not even be possible for human being to distinguish them if no extra information is provided  The result in directory domain is similar  due to limited space  we do not list it here  5 2 2 Validation over Semi real Datasets We further validate our tag inference by arti   cially introducing semantically similar tags  The large Google Base  which contains 15 di   erent columns and about 2 million records  is pre processed as follows 10   For each attribute  we create 9 di   erent attributes from the original one  with shared pre   x  For example  we create another 9 attributes make a  make b         make i from attribute make  These attribute names are then used to simulate the scenario where di   erent tags with same semantics come from heterogeneous data sources  In addition  with the observation that there are many small datasets but few large ones  we take this into account in our data generation  The 10 semantically similar tags are divided into four groups with di   erent probabilities  The    rst group contains 4 attributes  make  make a  make b and make c  each with probability 5   the second group contains 3 attributes  make d  make e and make f  each with probability 10   the third group has only two attributes  make g and make h  each having probability 15   the last group  with the sole attribute make i  is assigned with probability 20   We then assign every value to a tag according to this probability distribution and with an associated probability of 1  In the experiment  we consider each attribute as a tag  and run our soft clustering 3 times with di   erent initializations on a 2  sample     40 thousand data values   Figure 7 shows the pairwise similarity between tags  Each row and column represents a tag  The similarity between the row and column tag at a given pixel is related to the darkness 10We used the entire Google Base data in data generation  but only 10  samples in tag inference to avoid dominance  body type color condition doors drivetrain engine fuel type hull material make mileage model price transmission vehicle type year Figure 7  Tag Similarity Bitmap of the pixel  The darker the square is  more similar the tag pairs are  Note that the tags are arranged in the same order in both row and column orientations  and tags generated from the same original tags are grouped together  From Figure 7 we can see that the squares on the diagonal are much darker than the other squares  This coincides with the fact that tags generated from the same attribute are much more similar with each other than the other tags  However  the only outlier is the fourth last attribute price and the sixth last attribute mileage  Their diagonal intersections only give a moderate grey scale  This is because mileage and price share very similar value distributions  as explained earlier  5 3 Top k Query Processing Validation We evaluate the performance of our top k query processing over both car domain datasets and directory domain datasets  For car domain data  we vary the number of distinct tags in a query from 1 to 5  and denote them as T 1 to T 5  For each T i  i      1          5   we choose 5 di   erent queries with approximately equal number of tags in SELECT clause and WHERE clause  When selecting queries  we randomly vary the selectivity for predicates in the WHERE clause  The process is similar for directory domain data  except that we vary the distinct number of tags from 2 to 4  as directory domain has less number of tags  To get the precision and recall for query result  we build the golden standard as follows  We manually check the extended SQL query to determine its semantics  then translate it to a set of traditional SQL queries over each source data set  and    nally merge the results retrieved by SQL queries  For instance  if tags in  make  manufacturer  g make  are semantically equivalent to each other  then an extended SQL query containing make will be broadcasted to each data source that includes any equivalent tag of make  On average  a tagbased query can be translated to 4 to 6 SQL queries  Based on golden standard  we de   ne precision and recall as follows  let P be the set of answers retrieved by the query processor  and GS be the set of answers in golden standard  then P recision    P   GS   P     and Recall    P   GS   GS    5 3 1 Effectiveness Figure 8 a  shows the precision over car datasets  with top k  varying from 20  to 100   For each k   it also illustrates the precision for each T i  with i  i e   the number of distinct tags  ranging from 1 to 5  Note that our top k  query is a little di   erent from the traditional top k query  In top k query  the number of returned answers is k  How 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Precision Top k    of Tag 1   of Tag 2   of Tag 3   of Tag 4   of Tag 5  a  Precision in CAR domain 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Recall Top k    of Tag 1   of Tag 2   of Tag 3   of Tag 4   of Tag 5  b  Recall in CAR domain 0 0 2 0 4 0 6 0 8 1  20 40 60 80 100 Precision Top k  OpenII Ptagging  c  Precision Comparison 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Precision Top k    of Tag 2   of Tag 3   of Tag 4  d  Precision in DIR domain 0 0 2 0 4 0 6 0 8 1 0 20 40 60 80 100 Recall Top k    of Tag 2   of Tag 3   of Tag 4  e  Recall in DIR domain  0  0 2  0 4  0 6  0 8  1  20 40 60 80 100 Recall Top k  OpenII Ptagging  f  Recall Comparison Figure 8  Precision and Recall by Varying k  ever  in top k  query it is  GS      k   The reason we choose top k  query is that we would like to see how recall changes with di   erent number of distinct tags  As golden standard size for di   erent queries may di   er a lot  setting the same absolute k for all queries will make recall bias the performance towards queries with small answer set  Top k  query avoids this problem since no matter how much the golden standard di   ers  recall for top k  query is upper bounded by k   Figure 8 a  shows that our approach achieves high precision in all the cases  and is not sensitive to k   It also indicates that for T 1  precision increases slightly with larger k  while T 2 has a slight decrement  The reason is that our tag inference approach is unable to distinguish tag pairs with similar value distributions  e g  price and mileage   Such false positives are ranked higher in T 1 than in T 2  However  this problem can be eliminated by our dynamic instantiation when price and mileage are queried together  as our query processor always attempts to    nd the best instantiation  Figure 8 b  shows the corresponding recall for car domain datasets  We can see that when k  increases  there is a steady increase for recall  As mentioned before  recall in k  case is upper bounded by k   However  since we missed some true similar tag pairs in tag inference  we cannot grantee 100  recall  Such missed tags are those which are semantically similar but have heterogeneous values  For example  it is di   cult to associate year which has format    yyyy    with model year which has format    yy     This problem will become severer if there are more tags in query  because the possibility of including missed tags also increases  as shown in case T 5  However  for queries containing less distinct tags  our approach is still able to achieve good recall  Consider the    rst 4 cases T 1 to T 4  at the point where k  equals to 100   our approach can achieve 0 7 recall in average  Together with the high precision we provide  our proposed approach is quite e   ective  We also get precision and recall on the directory data  shown in Figure 8 d  and Figure 8 e  respectively  As there are less number of tags than the car dataset  we vary its distinct number of tags from 2 to 4  We observe similar behavior as over car datasets  Since the number of tags in the directory datasets is less  and most are well formatted  its recall is much better than the car datasets  Finally  we compare our work with Open II for the query processing  Comparisons over precision and recall are shown in Figure 8 c  and Figure 8 f   respectively  In particular  we only list the results for T 3 case over car datasets  The reason is that Open II makes many wrong inferences  as such  precision in most cases is really low  Here we choose its best case  i e  T 3  for comparison  It is indicated that our approach provides better precision and recall than Open II  Speci   cally  among the 5 queries in T 3  Open II produces 0  precision for two of them because they contain wrong matched tags  and 100  precision for the rest three  In average  Open II has 60  precision across all k  cases  With less correct records returned  Open II has lower recall than ours  5 3 2 Ef   ciency We validate the e   ciency of our top k query processing over T 3 on car datasets  For the directory data sets and the other queries  the overall trend is similar  We    rst compare the running time of our approaches with the translated SQL queries  as shown in Figure 9  Note that when k  is set to 100   our approaches retrieve almost the same number of tuples as the translated SQLs  As illustrated  the running time of the translated SQL queries is quite stable with respect to di   erent top k   because they always retrieve the golden standard  i e  all the true answers   no matter how k  changes  In contrast  the running time of our approaches  i e   lazy retrieval and eager retrieval  increases linearly when top k  increases  However  eager retrieval takes longer time than lazy retrieval by almost 1 5 times since eager retrieval maintains and probes the sorted lists for QTags  but most tuples in them are invalid  The running time that lazy retrieval takes is almost k  of translated SQLs  which clearly shows the e   ciency of our lazy retrieval approach  We next look at how the number of random and sequential 0  0 04  0 08  0 12  0 16  0 2  20 40 60 80 100 Running Time sec Top k  SQL Eager Retrieval Lazy Retrieval Figure 9  Running Time by Varying k  accesses vary as we vary k  Figure 10 a  shows the number of random accesses for eager and lazy retrieval when varying k from 20 to 120  The corresponding graphs for sequential I Os are in Figure 10 b   Note that in both    gures  we adopt top k rather than top k   as the number of tuples returned by top k  varies proportionally to query   s answer set  Consistent with earlier observation that eager retrieval takes almost 1 5 times longer than lazy retrieval  the same conclusion holds for both the number of sequential accesses and random accesses  6  RELATED WORK Query Relaxation In the recent decade  there are several e   orts to provide users a more friendly query interface that requires minimal or no user knowledge of the database schema  Such e   ort includes the extensive studies about keyword searches over RDBMS  19  31   However  keyword query con   nes users from issuing structural constraints  and it is not applicable for range queries  Moreover  it does not allow users to express semantic con   dence over tags  In  15  21   it was illustrated that tags on multimedia objects are prevalent over Web 2 0 application  1  2   to facilitate searching over multimedia objects  Our use of tags here serves the same purpose  Database Relaxation  There is also some e   ort to relax the strict schema de   ned in RDBMS  including researches on malleable schemas  11  33   Bigtable  6   Wide table  7   WebTable  5   and RDF tables  22   These approaches provide some    exibility for users to interact with the schema without the strict constraints of a RDBMS  However  most of them only provide a    exible interface to store data  but the columns or attributes in their system play the same role as in RDBMS  While  33  did look at issues involving query relaxation  they only do so for string attributes and have to generate many alternate relaxed queries in order to retrieve relevant data  Users are also not given the control over which attributes to relax  Probabilistic Database  There also exist a l</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>It was easy  when apples and blackberries were only fruits Surender ### Reddy Yerva  Zolt an Mikl os  and Karl Aberer EPFL IC LSIR Lausanne  Switzerland fsurenderreddy yerva  zoltan miklos  karl abererg epfl ch Abstract  Ambiguities in company names are omnipresent  This is not accidental  companies deliberately chose ambiguous brand names  as part of their marketing and branding strategy  This procedure leads to new challenges  when it comes to  nding information about the company on the Web  This paper is concerned with the task of classifying Twitter messages  whether they are related to a given company  for example  we classify a set of twitter messages containing a keyword apple  whether a message is related to the company Apple Inc  Our technique is essen  tially an SVM classi er  which uses a simple representation of relevant and irrelevant information in the form of keywords  grouped in speci c  pro les   We developed a simple technique to construct such classi ers for previously unseen companies  where no training set is available  by training the meta features of the classi er with the help of a general test set  Our techniques show high accuracy  gures over the WePS 3 dataset ### 1 Introduction Twitter 1 is a popular service where users can share short messages  a k a  tweets  on any subject  Twitter is currently one of the most popular sites of the Web  as of February 2010  Twitter users send 50 million messages per day 2   As users are sharing information on what matters to them  analyzing twitter messages can reveal important social phenomena  indeed there are number of recent works  for example in  11   exploring such information  Clearly  twitter messages are also a rich source for companies  to study the opinions about their products  To perform sentiment analysis or obtain reputation related information  one needs  rst to identify the messages which are related to a given company  This is a challenging task on its own as company or product names are often homonyms  This is not accidental  companies deliberately choose such names as part of their branding and marketing strategy  For example  the company Apple Inc  shares its name with the fruit apple  which again could have a number of  gurative meanings depending on the context  for example   knowledge   Biblical story of Adam  Eve and the serpent  or New York  the Big Apple   1 http   twitter com 2 http   www telegraph co uk technology twitter 7297541  Twitter users send 50 million tweets per day htmlIn this paper  we focus on how to relate tweets to a company  in the context of the WePS 3 challenge  where we are given a set of companies and for each com  pany a set of tweets  which might or might not be related to the company  i e  the tweets contain the company name  as a keyword   Constructing such a classi er is a challenging task  as tweet messages are very short  maximum 140 charac  ters   thus they contain very little information  and additionally  tweet messages use a speci c language  often with incorrect grammar and speci c abbreviations  which are hard to interpret by a computer  To overcome this problem  we con  structed pro les for each company  which contain more rich information  For each company  in fact  we constructed several pro les  some of them automati  cally  some of them manually  The pro les are essentially sets of keywords  which are related to the company in some way  We also created pro les  which explic  itly contains unrelated keywords  Our technique is essentially an SVM classi er  which uses this simple representation of relevant and irrelevant information in the  pro les   We developed a simple technique to construct such classi ers for previously unseen companies  where no training set is available  by training the meta features of the classi er with the help of a general test set  available in WePS 3  Our techniques show high accuracy  gures over the WePS 3 dataset  The rest of the paper is organized as follows  Section 2 gives a more precise problem de nition  Section 3 presents our techniques  while Section 4 gives more details on the classi cation techniques we used  Section 5 gives details on the experimental evaluation of our methods  Section 6 summarizes related work and  nally Section 7 concludes the paper  2 Problem Statement In this section we formulate the problem and our computational framework more formally  The task is concerned to classify a set of Twitter messages    fT1          Tng  whether they are related to a given company C  We assume that each message Ti 2  contains the company name as a sub string  We say that the message Ti is related to the company C  related Ti   C   if and only if the Twitter message refers to the company  It can be that a message refers both to the company and also to some other meaning of the company name  or to some other company with the same name   but whenever the message Ti refers to company C we try to classify as TRUE otherwise as FALSE  The task has some other inputs  such as the URL of the company url C   the language of the webpage  as well as the correct classi cation for a small number of messages  for some of the companies   3 Information representation The tweet messages and company names alone contain very little information to realize the classi cation task with good accuracy  To overcome this problem  we created pro les for the companies  several pro les for each company  These set of pro les can be seen as a model for the company  In this section  we discuss  how we represent tweet messages and companies and we also discuss how we obtained these pro les  In the the classi cation task we eventually compare a tweet against the pro les representing the company  see Section 4   3 1 Tweet Representation We represented a tweet as a bag of words  unigrams and bigrams   We do not access the tweet messages directly in our classi cation algorithm  but apply a preprocessing step  rst  which removes all the stop words  emoticons  and twitter speci c stop words  such as  for example  RT  username   We store a stemmed 3 version of keywords  unigrams and bigrams   i e  Ti   setfwrdjg  3 2 Company Representation We represent each company as a collection of pro les  formally Ek   fP k 1   P k 2           P k n g  Each pro le is a set of weighted keywords i e  P k i   fwrdj   wtjg  with wtj   0 for positive evidence and wtj   0 for negative evidence  For the tweets classi cation task  we eventually compare the tweet with the entity  i e  company  pro le  For better classi cation results  the entity pro le should have a good overlap with the tweets  Unfortunately  we do not know the tweet messages in advance  so we tried to create such pro les from alternative sources  independently of the tweet messages  The entity pro le should not be too general  because it would result many false positives in the classi cation and also not too narrow  because then we could miss potential relevant tweets  We generated most of our pro les automatically  i e  if one would like to construct a classi er for a previously unseen company  one can automatically generate the pro les  Further  small  manually constructed pro les could further improve the accuracy of the classi cation  as we explain in Section 5  In the following we give an overview of the pro les we used  and their con  struction  Homepage Pro le For each company name  the company homepage URL was provided in the WePS 3 data  To construct the homepage pro le  we crawled all the relevant links up to a depth of level d  2   starting from the given homepage URL  We extracted all the keywords present on the relevant pages  then we removed all the stopwords   nally we stored in the pro le the stemmed version of these keywords  From this construction pro  cess one would expect that homepage pro le should capture all the impor  tant keywords related to the company  However  since the construction is 3 Porter stemmer from python based natural language toolkit available at http   www nltk organ automated process  it was not always possible to capture good quality representation of the company  for various reasons  the company Webpages use java scripts   ash  some company pages contain irrelevant links  there are non standard homepages etc  Metadata Pro le HTML standards provides few meta tags 4   which enables a webpage to list set of keywords that one could associate with the webpage  We collect all such meta keywords in this pro le whenever they are present  If these meta keywords are present in the HTML code  they have high quality  the meta keywords are highly relevant for the company  On the negative side  only a fraction of webpages have this information available  Category Pro le The category  to which the company belongs  is a good source of relevant information of the company entity  The general terms associated with the category would be a rich representation of the entity  One usually fails to  nd this kind of keywords in the homepage pro le  We make use of wordnet  a network of words  to  nd all the terms linked to the category keywords  This kind of pro le helps us assign keywords like  software install  update  virus  version  hardware  program  bugs etc to a software company  GoogleSet CommonKnowledge Pro le GoogleSet is a good source of ob  taining  common knowledge  about the company  We make use of Google  Sets 5 to get words closely related to the company name  This helps us identify companies similar to the company under consideration  we get to know the products  competitor names etc  This kind of information is very useful  es  pecially for twitter streams  as many tweets compare companies with others  With this kind of pro le  we could for example associate Mozilla  Firefox  Internet Explorer  Safari keywords to Opera Browser entity  UserFeedback Positive Pro le The user himself enters the keywords which he feels are relevant to the company  that we store in the manually con  structed UserFeedback pro le  In case of companies where sample ground truth is available  we can infer the keywords from the tweets  in the training set  belonging to the company  UserFeedback Negative Pro le The knowledge of the common entities with which the current company entity could be confused  would be a rich source of information  using which one could classify tweets e ciently  The common knowledge that  apple  keyword related to  Apple Inc  company could be interpreted possibly as the fruit  or the New York city etc  This particular pro le helps us to collect all the keywords associated with other entities with similar keyword  An automated way of collecting this information would be very helpful  but it is di cult  For now we make use of few sources as an initial step to collect this information  The user himself provides us with this information  Second  the wiki disambiguation pages 6 contains this information  at least for some entities  Finally this information could be 4 http     www w3schools com html html meta asp 5 http   labs google com sets 6 http   en wikipedia org wiki Apple  disambiguation  page contains apple entitiesgathered in a dynamic way i e   using the keywords in all the tweets  that do not belong to the company  This information could also be obtained if we have training set for a particular company with tweets that do not belong to the company entity  Table 1 shows how an  Apple Inc  7 company entity is represented using di erent pro les  Table 1  Apple Inc Company Pro les Pro le Type Keywords WebPage iphone  ipod  mac  safari  ios  iphoto  iwork  leopard  forum  items  em  ployees  itunes  credit  portable  secure  unix  auditing  forums  mar  keters  browse  dominicana  music  recommend  preview  type  tell  no  tif  phone  purchase  manuals  updates   fa  8GB  16GB  32GB       HTML Metatag femptyg Category opera  code  brainchild  movie  telecom  cruncher  trade  cathode ray  paper  freight  keyboard  dbm  merchandise  disk  language  micropro  cessor  move  web  monitor  diskett  show   gure  instrument  board  lade  digit  good  shipment  food  cpu  moving picture   uid  con  sign  contraband  electronic  volume  peripherals  crt  resolve  yield  server  micro  magazine  dreck  byproduct  spiritualist  telecommunica  tions  manage  commodity   ick  vehicle  set  creation  procedure  con  sequence  second  design  result  mobile  home  processor  spin o   wan  der  analog  transmission  cargo  expert  record  database  tube  pay  load  state  estimate  intersect  internet  print  factory  contrast  out  come  machine  deliver  e ect  job  output  release  turnout  convert  river       GoogleSet itunes  intel  belkin  512mb  sony  hp  canon  powerpc  mac  apple  iphone  ati  microsoft  ibm       User Positive ipad  imac  iphone  ipod  itouch  itv  iad  itunes  keynote  safari  leop  ard  tiger  iwork  android  droid  phone  app  appstore  mac  macintosh User Negative fruit  tree  eat  bite  juice  pineapple  strawberry  drink 4 Classi cation Task In machine learning literature  the learning tasks could be broadly classi ed as supervised and unsupervised learning  The problem scenario for the WePS 3 task  classi cation of tweets with respect to a company entity can be seen as a problem where one needs a machine learning technique between supervised and unsupervised learning  since we have no training set for the actual classi cation task  but a test training set is provided for a separate set of companies  Here we brie y discuss the di erent classes of machine learning techniques  and outline our classi cation method  7 http   www apple comSupervised Learning for Classi cation Task Supervised learning is a ma  chine learning technique for deducing a function from training data  The training data consist of pairs of input objects  typically vectors   and desired outputs  The output of the function can predict a class label of the input object  called classi cation   The task of the supervised learner is to predict the value of the function for any valid input object after having seen a number of training exam  ples  i e  pairs of input and target output   To achieve this  the learner has to generalize from the presented data to unseen situations in a  reasonable  way  An example of supervised learning in our current setting is  given a training set of tweets for a particular company XYZ company   with example of tweets belonging to and not belonging to the company  one learns a classi er for this particular company XYZ company   Using this classi er the new unseen tweets related to this company XYZ company  can be classi ed as belonging or not belonging to that company  Unsupervised Learning In machine learning  unsupervised learning is a class of problems in which one seeks to determine how the data are organized  Many methods employed here are based on data mining methods used to preprocess data  It is distinguished from supervised learning in that the learner is given only unlabeled examples  In broad sense  the task of classifying tweets of an unknown company  without seeing any relevant examples can fall into this category  Generic Learning For the current scenario  WePS 3   challenge 2   we are pro  vided with training sets corresponding to few companies  C T R    Finally we have to classify test sets corresponding to new companies C T est    with C T R T C T est   0  This particular scenario can be seen as in between supervised and unsuper  vised learning  It is unsupervised as we are not given any labeled tweets cor  responding to the test set  At the same time it is also related to supervised learning as we have access to few training sets  with labeled tweets correspond  ing to the companies  This kind of generic learning needs the classi er to identify the generic features from the general training set  based on which one can make accurate classi cation of tweets corresponding to the unseen companies  The classi ers based on the features of the tweet decides if it belongs to a company or not  In the following section 4 1  we discuss the features which our classi ers take as input  After the features are introduced  we propose di erent ways of developing a generic classi er in section 4 2 4 1 Features Extraction We de ne a feature extraction function  which compares a tweet Ti to the com  pany entity representation Ek and outputs a vector of features  F n Ti   Ek    f metaf eatures z      G1          Gm   F1          Fn    z   tweetspecif ic   heuristics z      U1          Uzg  Here the Gi are generic meta features  which are entirely based on the quality of the entity pro les and do not depend on Tweet message Ti   One could use di erent ways of quantifying the quality of the pro les    Boolean  In this work we make use of boolean metrics to represent if a pro le is empty or has su cient keywords    Other possibility is that a human can inspect the pro les and assign a metric of x 2  0 1  based on the perceived quality  One could think of exploring an automated way of assigning this number  The Fi features are tweet speci c features  i e  they quantify how close a tweet overlaps with the entity pro les  We use a comparison function to compare the tweet message Ti   which is a bag of words  with j th pro le P k j   which is also a bag of weighted keywords  to get the F th j feature  In this work we make use of a simple comparison function  which compares two bags of words looking for exact overlap of keywords  and for all such keywords the sum of their weights quantify how close the tweet message is to the entity pro le  Formally with Ti   Setfw t 1   w t 2           w t k g and P k j   Setfw p 1   wt1  w p 2   wt2          w p m   wtmg  we compute the Fj feature using the simple comparison function as  Fj   CmpF n Ti   P k j     X q wtq  where q such that w p q 2 Setfw t 1   w t 2           w t kg   Setfw p 1   w p 2           w p mg  1  The above comparison function is simple and easy to realize  but it may miss out some semantically equivalent words  One could make use of cosine similarity  or semantic similarity based comparison functions  The Ui features encapsulate some user based rules  for example  presence of the company URL domain in the tweet URL list  is a big enough evidence to classify the tweet as belonging to the company  4 2 Generic Classi er The classi er is a function which takes the feature vector as input and classi es the tweet as fT RUE  F ALSEg  with TRUE label if the tweet is related to the company and as FALSE otherwise  We are provided with training data corre  sponding to a set of companies  C T R    Based on the training data we have the task of training a generic classi er  which should be used to classify the tweets corresponding to a new set of companies  C T est    We present here two possible ways of designing this generic classi er  Ensemble of Naive Bayes Classi ers  We adapt the Naive Bayes Classi er model for this task  For each company in the training set C T R    based on the company tweets we  nd the conditional distribution of values over features for two classes i e  a class of tweets which are related to the company and another class of tweets which are not related to the company  With these conditionalprobabilities  shown in equations 2 3  and by applying Bayes theorem  we can classify an unseen tweet whether it is related to the company or not  Let us denote the probability distribution of features of the tweets that are related to a given company with P f1  f2          fn j C    2  and the probability distribution of features of the tweets that are not related to the company with P f1  f2          fn j C    3  Then  for an unseen tweet t  using the features extraction function we com  pute the features values  f1  f2          fn   The posterior probabilities of whether the tweet is related to the company or not  are calculated as in equations  4  5   P C j t    P C    P t j C  P t    P C    P f1  f2          fn j C  P f1  f2          fn   4  P C j t    P C    P t j C  P t    P C    P f1  f2          fn j C  P f1  f2          fn   5  Depending on whether P C j t  is greater than P C j t  or not  the naive Bayes classi er decides whether the tweet t is related to the given company or not  respectively  Corresponding to each company ci 2 C T R   we train a naive Bayes clas  si er 12   15   NBCi   for which the input features are tweet speci c features F1          Fn and heuristics based features U1          Uz  as discussed in the section 4 1  Along with training a naive Bayes classi er  we also assign an accuracy measure for this classi er and keep a note of meta features G1          Gm of this classi er  The generic classi er makes use of ensemble function which either chooses the best classi er or combines the decision of classi ers from this set  to classify an unseen tweet corresponding to a new company i e  ci 2 C T est   The ensemble function would make use of the meta features and accuracy measures to pick up the right classi er or the right combination of classi ers  We refer to  9   21  for details about the design of such ensemble functions  SVM Classi er  Alternatively one could train a single classi er based on all the features  meta features  tweet speci c features and heuristics features  This single classi er can be seen as using an ensemble function implicitly in either picking an apt classi er or aptly combing the classi er decisions  In the current work  we train an SVM Classi er  10   16  as a generic classi er  which makes use of all features  meta features  tweet speci c features and heuristics based features  in its classi cation task 5 Experiments and Evaluation Our experimental setup was the following  We are given a general training set  which consists tweets related to about 50 companies  we denote this set as C T R    For each company c 2 C T R we are provided around 400 tweets with their cor  responding ground truth  i e  if the tweet is related to the company or not  For each company  we are provided with the following meta information  URL  Language  Category  We have trained a generic classi er based on this train  ing set  The test set for this task consisted tweets of around 50 new compa  nies  We denote this set of companies as C T est   There was no overlap with the training set  C T R T C T est   0  For each company c 2 C T est there are about 400 Tweets  which are to be classi ed  We classi ed them with our trained generic classi er  as explained in Section 4  The WePS 3 dataset is available at http   nlp uned es weps weps 3 data  The task is of classifying the tweets into two classes  one class which repre  sents the tweets related to the company  positive class  and second class repre  sents tweets that are not related to the company  negative class   For evaluation of the task  the tweets can be grouped into four categories  true positives  T P   true negatives  T N   false positives  F P  and false negatives  F N   The true positives are the tweets that belong to positive class and in fact belong to the company and the other tweets which are wrongly put in this class are false posi  tives  Similarly for the negative class we have true negatives which are correctly put into this class and the wrong ones of this class are false negatives  We use the following metrics to study the performance of our classi cation process  Accuracy   T P T N T P F P T N F N P recsion       T P T P F P   Recall     T P T P F N   F  Measure     2 P recsion    Recall   P recsion  Recall  P recsion    T N T N F N   Recall    T N T N F P   F  Measure    2 P recsion   Recall  P recsion Recall In Table 2 we show the average values of the di erent performance metrics  along with the corresponding variances  Table 2  Performance of Classi er which makes use of all pro les Metric  Mean Value Variance Accuracy 0 83 0 02 Precision  positive class  0 71 0 07 Recall  positive class  0 74 0 13 F Measure  positive class  0 63 0 1 Precision  negative class  0 84 0 07 Recall  negative class  0 52 0 17 F Measure  negative class  0 56 0 15 The results show high accuracy  gures for our classi er  The precision and recall values corresponding to positive class can be further increased by re ning         the pro les corresponding to positive evidence  for example by using more sources to accumulate more relevant keywords and by using e cient quality metrics for rejecting irrelevant keywords  In spite of using very few sources for populating the negative pro le of a company  we are still able to have high precision and decent recall values for </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>Eliminating the Redundancy in Blocking based Entity Resolution Methods ### George Papadakis       Ekaterini Ioannou    Claudia Nieder  e    Themis Palpanas z   and Wolfgang Nejdl     National Technical University of Athens  Greece gpapadis mail ntua gr   Technical University of Crete  Greece ioannou softnet tuc gr   L3S Research Center  Germany  surname  L3S de z University of Trento  Italy themis disi unitn eu ABSTRACT Entity resolution is the task of identifying entities that refer to the same real world object  It has important applications in the context of digital libraries  such as citation matching and author disambiguation  Blocking is an established methodology for e ciently addressing this problem  it clusters similar entities together  and compares solely entities inside each cluster  In order to e ectively deal with the current large  noisy and heterogeneous data collections  novel blocking methods that rely on redundancy have been introduced  they associate each entity with multiple blocks in order to increase recall  thus increasing the computational cost  as well  In this paper  we introduce novel techniques that remove the super   uous comparisons from any redundancy based blocking method  They improve the time e ciency of the latter without any impact on the end result  We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity  For applications with space limitations  we also present an alternative  lightweight solution that operates at the abstract level of blocks in order to discard a signi   cant part of the redundant comparisons  We evaluate our techniques on two large  real world data sets and verify the signi   cant improvements they convey when integrated into existing blocking methods  Categories and Subject Descriptors H 3 3  Information Search and Retrieval   Information    ltering General Terms Algorithms  Experimentation  Performance Keywords Data Cleaning  Entity Resolution  Redundancy based Blocking ### 1  INTRODUCTION Nowadays  the growing availability of semi structured and structured data in the Web of Data opens new opportunities for digital libraries  These data collections can clearly pro   t from a variety of digital library principles and technologies  such as the systematic Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  JCDL   11  June 13   17  2011  Ottawa  Ontario  Canada  Copyright 2011 ACM 978 1 4503 0744 4 11 06     10 00  and uniform description of data by metadata  metadata harvesting services and technologies for federated search  Furthermore  they can be exploited to create new types of services by combining them with traditional types of library content  The integration of related data in meaningful ways relies on the detection of data records  from di erent collections  that refer to the same object  e g   author  The process of identifying  among a set of entities  those referring to the same real world object is called Entity Resolution  ER   There are two main applications of this process in digital libraries  citation matching for identifying references that describe the same publication  and author disambiguation for identifying author pro     les that pertain to the same person  9  10  25  27   The latter consists of detecting   within a collection of bibliographical records   the correct coupling between author names and persons  by resolving the mixed citation problem  same name   di erent persons  and the split citation problem  same person   di erent names   14   At its core  ER constitutes a quadratic problem  as each entity has to be compared with all others  To enhance its e ciency  blocking methods are typically employed  6  11  15   they extract from every entity pro   le  or record  a Blocking Key Value  BKV  that encapsulates its most distinguishing information and de   ne blocks on the equality  or similarity  of BKVs  Thus  each block corresponds to a speci   c instance of the BKV and contains all entities associated with that value  However  in order to select the most reliable and distinguishing attributes of the given entity pro   les  traditional blocking methods rely on a prede   ned entity schema  This renders them inapplicable for the Web of Data  due to the special characteristics of the latter  it involves individual collections that are highly heterogeneous  stemming from a rich diversity of sources  which evolve autonomously  following an unprecedented growth rate  especially the user generated data of the Social Web  and the data created by sensors   More speci   cally  the following challenges are present in these settings  Loose schema binding  The schemata describing entities may range from locally de   ned attributes to pure tag style annotations  and data often have no strict binding to the employed schemata  Noisy  missing  and inconsistent values  They are introduced in the data due to extraction errors  sources of low quality  and use of alternative descriptions  As a result  entity pro   les may contain de   cient  or even false information  Extreme levels of heterogeneity  This is caused by the fact that data stem from a variety of distributed  self organized  collaborative sources  Actually  heterogeneity pertains not only to schemata describing the same entity types  but also to pro   les describing the same entity  For instance  GoogleBase 1 encompasses 100  000 distinct schemata corresponding to 10  000 entity types  16   High growth rates in terms of volume and fast evolution  This is caused partly due to automatic generation and partly due to the high involvement of users  they typically add new content  and update incorrect  outdated  or simply irrelevant information  These inherent characteristics of heterogeneous information spaces break the fundamental assumptions of traditional blocking techniques  Novel blocking schemes  that do not require a prede     ned schema  have been introduced to e ectively deal with these challenges  They all rely on redundancy  associating each entity with multiple blocks  3  20  21  28   In this way  they minimize the likelihood that two duplicate entities have no block in common  and achieve high levels of e ectiveness  i e   detected duplicate entities   This comes  though  at the cost of time e ciency  the resulting blocks are overlapping  and the same pairs of entities may be compared multiple times  Therefore  the main challenge for improving the e ciency of redundancy based blocking methods is to eliminate the super   uous comparisons they entail  without a ecting their accuracy  In this paper  we address the above problem through an abstraction of the redundancy based blocking techniques  blocks are associated with an index indicating their position in the processing list and entities are associated with a list of the indices of the blocks that contain it  Thus  we can identify whether a pair of entities contained in the current block has already been compared in another block simply by comparing their least common block index with the index of the current block  In this way  we achieve the optimal solution to the problem  since we e ciently propagate all executed comparisons  without explicitly storing them  The above approach has low computational cost  but results in quadratic space complexity  In order to remedy this drawback  we introduce a method that approximates the optimal solution by gracefully trading space for computational cost  It comprises of a series of block manipulation techniques  which discard those blocks that exclusively entail super   uous comparisons  i e   they are entirely contained in another block   and merge pairs of highly overlapping blocks  giving birth to blocks that entail less comparisons  These functionalities are facilitated by mapping the blocks to a Cartesian space and contrasting their spatial representations  without the need to examine their contents analytically  In summary  the contributions of this paper are the following   1  We formulate the problem of purging redundant comparisons from a blocking technique and explain how its solution can be facilitated through the abstraction of blocks  i e   enumerating and mapping them to the Cartesian space    2  We describe Comparisons Propagation  an optimal solution to this problem  which e ciently propagates all executed comparisons based on the enumeration of blocks  This method is suitable for applications that can a ord high space complexity   3  We further propose a solution that partially discards redundant comparisons  trading space requirements for time complexity  It consists of a series of methods that remove blocks involving exclusively redundant comparisons and merge highly overlapping ones   4  Finally  we thoroughly evaluate our methods on two large  realworld data sets  demonstrating the great bene   ts they convey to the e ciency of existing blocking methods  The rest of the paper is structured as follows  Section 2 summarizes previous work and Section 3 de   nes the basic notions of our algorithms  Section 4 introduces our approach to determining the processing order of blocks and mapping them to the Cartesian 1 See http   www google com base  Space  and Section 5 presents our approach to propagating comparisons and manipulating blocks  Experimental evaluation is presented in Section 6  while Section 7 concludes the paper  2  RELATED WORK A variety of methods for solving the ER problem has been presented in literature  They range from string similarity metrics  2   to similarity methods using transformations  19  26  and relationships between data  5  12   A comprehensive overview of the existing work in this domain can be found in  4  6  13   The approximate methods of data blocking typically associate each record  i e   entity  with a Blocking Key Value  BKV   They de   ne blocks on the equality  or similarity  of BKVs and compare solely the entities that are contained in the same block  6   For instance  the Sorted Neighborhood approach  11  orders records according to their BKV and slides a window of    xed size over them  comparing the records it contains  The StringMap method  15  maps the BKV of each record to a multi dimensional Euclidean space  and employs suitable data structures for e ciently identifying pairs of similar records  The q grams blocking approach  8  builds overlapping clusters of records that share at least one qgram  i e   sub string of length q  of their BKV  Canopy clustering  17  employs a cheap string similarity metric for building high dimensional overlapping blocks  whereas the Su x Arrays approach  3  considers the su xes of the BKV instead   28  explores another aspect of these blocking approaches  arguing that more duplicates can be detected and more pair wise comparisons can be saved through the iterative distribution of identi   ed matches to subsequently  re  processed blocks  The performance of blocking methods typically depends on the    ne tuning of a wealth of application  and data speci   c parameters  3  29   To automate the parameter setting procedure  several methods that model it as a machine learning problem have been proposed in the literature  For instance   18  de   nes it as learning disjunctive sets of conjunctions that consist of an attribute  used for blocking  and a method  used for comparing the corresponding values   Similarly   1  considers disjunctions of blocking predicates  i e   conjunctions of attributes and methods  along with predicates combined in disjunctive normal form  DNF   On the other hand   29  introduces a method for adaptively and dynamically setting the size of the sliding window of the Sorted Neighborhood approach  Attribute agnostic blocking methods were recently introduced to make blocking applicable to voluminous  heterogeneous data collections  such as the Web of Data  These methods do not need a prede   ned schema for grouping entities into blocks  as they completely disregard attribute names  In this way  they are able to handle thousands of attribute names without requiring the    ne tuning of numerous parameters  Instead  they tokenize  on all special characters  the attribute values of each entity pro   le  and create an individual block for each token  that is  every block corresponds to a speci   c token and contains all entities having this token in their pro   le  21   Blocks of such low level granularity guarantee high e ectiveness due to the high redundancy they convey  each entity is associated with multiple blocks  which are  thus  overlapping  Hence  the likelihood of a missed match  i e   a pair of duplicates that has no block in common  is low  This redundancy based approach is a common practice among blocking techniques for noisy  but homogeneous data  as well  3  17  20  28   To the best of our knowledge  this is the    rst work on formally de   ning and dealing with the problem of eliminating redundant comparisons of blocking methods for ER 3  PROBLEM DEFINITION To formally describe the problem we are tackling in this paper  we adopt the de   nitions introduced in  21  for modeling entity pro     les and entity collections  As such  an entity pro   le p is a tuple hid  Api  where Ap is a set of attributes ai   and id 2 ID is a global identi   er for the pro   le  Each attribute ai 2 Ap is a tuple hni   vii  consisting of an attribute name ni and an attribute value vi   Each attribute value can also be an identi   er  which allows for modeling relationships between entities  An entity collection E is a tuple hAE  VE  IDE  PEi  where AE is the set of attribute names appearing in it  VE is the set of values used in it  IDE   ID is the set of global identi   ers contained in it  and PE  IDE is the set of entity pro   les that it comprises  We de   ne a blocking scheme as follows  Definition 1  A blocking scheme bs for an entity collection E is de   ned by a transformation function f t   E 7  T and a set of constraint functions f i cond   T   T 7  ftrue  f alseg  The transformation function ft derives the appropriate blocking representation from the complete entity pro   le  or parts of it   The constraint function f i cond is a transitive and symmetric function that encapsulates the condition that has to be satis   ed by two entities  if they are to be placed in the same block bi   Apparently  any blocking method can de   ne and use its own blocking scheme that follows the above de   nition  For example  the schemes described in Section 2 consist of a transformation function that extracts the BKV from an entity pro   le and a set of constraint functions that de   ne blocks on the equality  or similarity  of the BKVs  Once a blocking scheme is applied on an entity collection  a set of blocks is derived  whose instances are formally de   ned as follows  Definition 2  Given an entity collection E and a blocking scheme bs for E  a block bi 2 B is the maximal subset  with a minimum cardinality of 2  that is de   ned by the transformation function f t and one of the constraint functions f i cond of bs  bi   E   8p1  p2 2 E   f i cond  f t p1   f t p2     true   p1  p2 2 bi   The ER process on top of a blocking method consists of iterating over its set of blocks B in order to compare the entities contained in each one of them  We use mi j to denote a match between two pro   les pi and pj that have been identi   ed as matching pi   pj  i e   describing the same real world object   The output  therefore  of a blocking method is a set of matches  which we denote as M  To address the aforementioned characteristics of heterogeneous information spaces  redundancy bearing blocking methods have been recently introduced  3  20  21  28   They associate each entity with multiple  overlapping blocks  This practice minimizes the likelihood that two duplicate entities have no block in common  thus resulting in higher e ectiveness for the ER process  Ef     ciency  on the other hand  is signi   cantly downgraded  due to the redundant comparisons between pairs of entities that appear in many blocks  Apparently  the higher the redundancy conveyed by a blocking method  the lower the e ciency of the ER process  In this paper  we focus on developing methods that enhance the e ciency of redundancy based blocking methods  without a ecting their e ectiveness  To this end  our techniques aim at eliminating the super   uous comparisons of redundancy bearing blocking methods in order to save considerable computational e ort  In this way  they can operate on top of any blocking method  without altering its e ectiveness  producing an output that is equivalent to the original one  The following de   nition introduces the concept of semantically equivalent blocking sets  Definition 3  A blocking set B 0 is semantically equivalent to blocking set B  if the set of matches resulting from blocking set B 0 are equal to the set of matches resulting from blocking set B  i e   MB0 MB   Based on the above de   nition  we now formally state the problem we are addressing in this paper  Problem 1  Given a set of blocks B that are derived from a redundancy bearing blocking technique     nd the semantically equivalent blocking set B 0 that involves no redundant pair wise comparisons  4  BLOCK SCHEDULING AND MAPPING As stated above  our goal is to propose generic methods for enhancing the e ciency of any redundancy bearing blocking technique  such as the ones discussed in Section 2   Therefore  the methods we describe make no assumptions on the mechanism or functionality of the underlying blocking method  Instead  they treat blocks at an abstract level  considering solely the identi   ers of the entities they contain  i e   each block is represented as a set of entity ids   We distinguish between two types of blocks according to the lineage of their entities  The    rst type of blocks is called unilateral  since it contains entities of the same lineage  i e   stemming from the same entity collection  This type of blocks arises when integrating one dirty collection  i e   a collection that contains duplicate entities  either with a clean  duplicate free collection  i e   DirtyClean   or with another dirty collection  i e   Dirty Dirty   Both cases are equivalent with resolving a single  dirty entity collection  where each entity pro   le could match to any other  24   More formally  the blocks of this kind are de   ned as follows  Definition 4  A unilateral block is a block containing entity ids from a single entity collection E  thus being of the form bi   fid1  id2          idng  where idi 2 ID  The second type of blocks is called bilateral  and arises when integrating two individually clean entity collections  E1 and E2  that are overlapping  Clean Clean   24   The goal is  therefore  to identify matches only between E1 and E2  thus requiring that each block contains entities from both input collections  More formally  this kind of blocks is de   ned as follows  Definition 5  A bilateral block is a block containing entity ids from two entity collections  E1 and E2  It follows the form bi  j   ffidi 1  idi 2         idi ng fidj 1  idj 2         idj mgg  where idi k 2 ID1 and idj l 2 ID2  The subsets bi   fidi 1  idi 2         idi ng and bj   fidj 1  idj 2          idj mg are called the inner blocks of bi  j   4 1 Block Scheduling Specifying the processing order of blocks is important for the e ectiveness of ER techniques  This order forms the basis for block enumeration  which associates each block with an integer that represents its position in the processing list  This practice    nds application in various techniques  such as the propagation of comparisons  see Section 5 1   The processing order is also an integral part of lossy e ciency techniques  like Block Pruning in  21   these are methods that sacri   ce  to some extent  the e ectiveness of a blocking method in order to enhance its e ciency  They do so by discarding comparisons according to some criteria  even if they involve non redundant comparisons among matching entities  Scheduling techniques typically associate each element of the given set of blocks B with a numerical value and sort B in ascending or descending order of this value  Their computational cost a  0 1 2 3 4 5 b1 b b2 3  b  0 1 2 3 4 5 1 2 3 4 b1 1 b2 2 b3 3 Figure 1  Illustration of block mapping  is O jBj   log jBj   which scales even for large sets of blocks  In each case  the most suitable approach for determining the processing order of blocks depends heavily on the application at hand  For the needs of the methods we introduce in Section 5  we de   ne a di erent scheduling method for each kind of block  In particular  unilateral blocks are ordered in ascending order of cardinality  the more entities a block bi contains  the higher its position in the list  Bilateral blocks  on the other hand  are ordered in ascending order of their utility  21   ubi  j   1 max jbi j jb j j    where jbi j and jbj j are the cardinalities of the inner blocks of the bilateral block bi  j   Bilateral blocks of equal utility are ordered in ascending order of the cardinality of their smallest inner block  4 2 Block Mapping We now introduce our approach for Block Mapping  The gist of this technique is that it allows us to e ciently check whether two blocks have overlapping content  i e   they share some entities   without exhaustively comparing them  The mapping is performed by transforming blocks into the Cartesian space  for unilateral blocks this corresponds to Cartesian coordinates in one dimension  i e   lines   and for bilateral blocks to coordinates in two dimensions  i e   rectangles   Thus  Block Mapping is performed by assigning each entity to a point on the corresponding axis  Example 1  Figure 1 a  illustrates the mapping of the unilateral blocks b1   fid2  id3  id4g  b2   fid0  id1  id4g  and b3   fid0  id1  id3  id4g on the X axis  Their entities are assigned to coordinates as follows  C  hid0  3i  hid1  4i  hid2  0i  hid3  1i  hid4  2i   Figure 1 b  illustrates the mapping of the bilateral blocks b1 1   ffid1 0  id1 2g  fid2 0  id2 1gg  b2 2   ffid1 0  id1 3g  fid2 1  id2 3gg and b3 3   ffid1 0  id1 3  id1 4g  fid2 1  id2 2  id2 4gg to the XY axes  where id1 i 2 E1 and id2 i 2 E2  The entities of E1 are transformed to points on the X axis as follows  CX  hid1 0  3i hid1 1  4i  hid1 2 1i  hid1 3  5i  hid1 4  2i   whereas the entities of E2 are mapped to points on the Y axis as follows  CY  hid2 0  3i hid2 1  1i hid2 2 0i  hid2 3  2i  hid2 4  4i   The di erence between the size b sp i of the spatial representation of a block bi and its actual size b as i is called spatial deviation spi of block bi   More formally  it is de   ned as follows  spi   b sp i  b as i   Algorithm 1  Mapping Blocks to the Cartesian Space  Input  B fbig a set of unilateral blocks Output  C fhidi   jig a mapping of entity ids to coordinates 1 B 0   blockScheduling B   2 C       3 lastIndex   0  4 foreach bi 2 B 0 do 5 E   sortInAscendingOrderOfFrequency bi  entities     6 foreach e 2 E do 7 if   C containsKey e id   then 8 C   C    he id  lastIndexi   9 lastIndex    10 return C  where b sp i is the length  area  of the spatial representation of a unilateral  bilateral  block bi   and b as i is the actual length  area  of the unilateral  bilateral  block bi   In the case of unilateral blocks  we have b as i   jbi j  1  while for a bilateral block bi  j it is equal to b as i  j    jbi j  1     jbj j  1   For example  b2 2 has an actual area of   2 1   2 1   1  whereas its spatial representation has an area of   5 3   2 1   2  that is  sp2 2   1  The value of spi is always positive  but in the ideal case it should be equal to 0  This requirement can be easily satis   ed for nonoverlapping blocks  by associating the entities of each block with contiguous coordinates  In the case of overlapping blocks  though  the spatial transformation leads to a positive deviation  since it cannot be done independently for each block  assigning a coordinate to an entity idi in the context of a block bi can be contiguous with the rest of entities in bi   but not necessarily with the other entities that share blocks with idi   Example 2  Consider the entities in Figure 1 a   The way the depicted blocks are mapped is the optimal one  since the entities of every block are contiguous  Imagine  though  that we place an additional entity to each block  id5 to b1  id6 to b2 and id7 to b3  In this case  there is no way of mapping the new blocks to the X axis  so that the entities of each block are contiguous  The above discussion gives rise to the following optimization problem  Problem 2  Given a set of blocks B  transform its elements to the Cartesian space  so that their aggregated spatial deviation P bi2B  b sp i  b as i   is minimized  In our methods  we require that emphasis is placed on minimizing the spatial deviation of large blocks  we elaborate on the reasons in Section 5 2 2   That is  the larger a block is  the lower its spatial deviation should be  More formally  this optimization problem can be de   ned as the minimization of the following quantity  X bi2B bi  size      b sp i  b as i     1  where bi  size   is the size of block bi   For a unilateral block  it is equal to its cardinality  i e   number of entities it contains   while for a bilateral block bi  j it is equal to the sum of cardinalities of its inner blocks  bi  j  size     jbi j   jbj j  We solve this optimization problem using a scalable method  applicable to voluminous data collections  Algorithm 1 outlines this method for the case of unilateral blocks  for bilateral ones  the algorithm is applied twice  independently for each axis  considering in each iteration solely the corresponding entity collection  In essence  the algorithm assigns coordinates from the interval  0  jEj  1  to the entity pro   les of the given collection E        After Block Scheduling  it starts assigning the entities of the last  usually largest  block to contiguous coordinates  thus minimizing the spatial deviation of this block  To ensure the minimal spatial deviation for the rest of the blocks  as well  the pro   les are ordered and mapped in ascending order of their frequency  i e   the number of blocks associated with each entity   the least frequent of the not yet mapped entities takes the    rst available coordinate  the second least frequent takes the next coordinate etc  Two entities that share many blocks are more likely to be contiguous in this way  The algorithm is then repeated for the remaining blocks  traversing their ordered list from bottom to top  The algorithm has a linear space complexity  O jBj   jEj   and time complexity of O jBj   log jBj   jE   log jEj   due to the sorting of blocks and entities  Example 3  The result of applying this algorithm is illustrated in Figure 1 a   The pro   les of b1 are mapped to the X axis as follows  id2 has frequency 0 and goes to the    rst available coordinate  i e   0   id3 with frequency 1 goes to next available coordinate  i e   1    and     nally  id4 with frequency 2 goes to point 2  5  APPROACH In this section  we present the methods we developed for reducing the redundancy of blocking methods  Problem 1   based on the Block Scheduling and Mapping techniques we introduced above  The optimal solution to this problem  i e   the one that discards all redundant comparisons  is presented in Section 5 1  Its e ectiveness  though  comes at the cost of high space complexity  caused by the data structure it employs  As an alternative  we present in Section 5 2 an approximate solution  that removes the high space requirements  5 1 Comparisons Propagation Block Scheduling determines the processing order of blocks  and enables their enumeration  that is  each block is assigned to an index indicating its position in the processing list  Based on this enumeration  the propagation of comparisons is made feasible through a common data structure  namely a hash table  in particular  its keys are the ids of the entities of a given collection E  and its values are lists of the indices of the blocks that contain the corresponding entities  The elements of these lists are sorted in ascending order  from the lowest block index to the highest  This data structure can be used in the context of a blocking method in the following way  to compare a pair of entities  the Least Common Block Index Condition should be satis   ed  That is  the lowest common block index of these entities should be equal to the index of the current block  indicating in this way that this is the    rst block in the processing list that contains both of them  Otherwise  if the former index is lower than the latter  the entities have already been compared in another block  and the comparison is redundant  In this way  each pair of entities is compared just once  and Comparisons Propagation provides the optimal solution to Problem 1  Theorem 1  Optimality of Comparisons Propagation   Given a set of blocks B  Comparisons Propagation produces the semantically equivalent set of blocks B 0 that entails no redundant pair wise comparisons  Proof  Let us assume that the set of blocks produced by Comparisons Propagation entails redundant comparisons  This means that there is a blocking set B 00 that is semantically equal to B and involves no redundant comparisons  Hence  there must be at least one pair of entities that is compared twice in B 0 and just once in B 00   The Least Common Block Index Condition is  therefore  satis   ed in two blocks of B 0   which is a contradiction  Thus  B 00   B 0   Algorithm 2  Propagating Comparisons  Input  B a set of blocks Output  B 00 the semantically equivalent set of blocks with no redundant comparisons 1 B   blockScheduling B   2 B 0   blockEnumeration B   3 B 00       4 entityIndex   indexBlocksOnEntityIds B 0    5 foreach bi 2 B 0 do 6 E   bi  entities    7 for i   1 to E size do 8 BEi   entityIndex associatedBlocks E i    9 for j   i   1 to E size do 10 BEj   entityIndex associatedBlocks E j    11 if  bi  index   leastCommonBlockIndex BEi  BEj    then 12 bi   newBlock E i    E j    13 B 00   B 00 S bi   14 return B 00   Algorithm 2 outlines the way Comparisons Propagation operates on a set of blocks B in order to produce its semantically equivalent set of blocks B 0 that is free of redundant comparisons  In essence  B 0 consists of blocks with minimum cardinality  since each nonredundant comparison results in a new block that contains the corresponding pair of entities  This may result in a very large number of blocks  and storing them poses a serious challenge  Processing them on the    y  though  is an e cient alternative  Comparisons Propagation can be integrated in the execution of any blocking method  without a ecting its time complexity  The reason is that the computation of the least common block index is linear to the number of blocks associated with the corresponding pair of entities  due to the ordering of indices in the values of the hash table   Its space complexity  though  is equal to O jBj   jEj   in the worst case  i e   each entity is placed in all blocks   where jBj is the total number of blocks  and jEj is the cardinality of the given entity collection  for Clean Clean ER  this cardinality is equal to jE1j   jE2j   In practice  however  space complexity depends on the level of redundancy introduced by the underlying blocking method  In fact  it is equal to O jEj   BPE      where BPE   is an estimate of redundancy  denoting the average number of blocks per entity  5 2 Block Manipulation Block Manipulation consists of a series of techniques that operate on two levels     rst  they investigate the given set of blocks in order to discard those elements that contain purely redundant comparisons  In this way  they reduce not only the number of comparisons  but also the number of blocks that will be processed in the next level  Second  they aim at identifying pro   table block merges  that is  pairs of highly overlapping blocks  which  when combined  result in a block with fewer comparisons  The combined result of these two levels approximates the optimal solution of Comparisons Propagation at a lower space complexity  The individual strategies of Block Manipulation are analytically presented in the following paragraphs  in the order they should be executed  5 2 1 Block Cleaning Cleaning a set of blocks B is the process of purging the duplicate elements from it  These are blocks that contain exactly the same entities with another block  regardless of the constraint function de   ning each of them  i e   independently of the information that is associated with them   We call such blocks identical  and  depending on their lineage  we formally de   ne them as follows Algorithm 3  Mining a clean set of blocks  Input  B a clean set of highly similar blocks Output  B 00 the semantically equivalent  mined set of blocks 1 B 0   blockScheduling B   2 DominatedB       3 for i   1 to B 0  size do 4 for j   B 0  size to i   1 do 5 if  B 0  i  size     B 0  j  size     then 6 break  7 if  areaConditionHolds B 0  i   B 0  j   then 8 if  isDominated B 0  i   B 0  j   then 9 DominatedB   DominatedB   B 0  i   10 break  11 B 00   B 0   DominatedBlocks  12 return B 00   Definition 6  Given a set of unilateral blocks B  a block bi 2 B is unilaterally identical with another block bj 2 B  denoted by bi   bj   if both blocks contain the same entities  regardless of their constraint functions  f i cond and f j cond   bi   bj   bi   bj   bj   bi   Definition 7  Given a set of bilateral blocks B  a block bi  j 2 B is bilaterally identical with another block bk l 2 B  denoted by bi j   bk l   if their corresponding inner blocks are unilaterally identical  bi  j   bk l   bi   bk   bj   bl   In this context  the process of Block Cleaning can be formally de   ned as follows  Problem 3  Block Cleaning   Given a set of blocks B  reduce B to its semantically equivalent subset B 0   B that contains no identical blocks  We call B 0 a clean set of blocks  The solution to this problem can be easily implemented by associating each block with a hash signature  its value is equal to the sum of the coordinates assigned to its entities by Block Mapping  Identical blocks necessarily have the same signature  but not vice versa  signature equality can also lead to false positives  For this reason  the size as well as the elements of two blocks with the same signature are analytically compared to make sure that they are indeed identical  In practice  this functionality is e ciently offered by default by most programming languages  Both its time and space complexity are linear to the size of the input set of blocks  i e   O jBj    as it involves traversing its elements just once  5 2 2 Block Mining Given a clean set of blocks  the process of mining it consists of identifying the blocks that are subsets of at least one other block in the set  that is  blocks whose entities are all contained in some other block  independently of the corresponding constraint functions  This situation is called a relation of dominance  where the latter is the dominant block  and the former the dominated one  This is more formally de   ned as follows  Definition 8  Given a clean set of unilateral blocks B  a block bi 2 B is unilaterally dominated by another block bj 2 B  denoted by bi   bj   if bi is a proper subset of bj   regardless of the constraint functions f i cond and f j cond   bi   bj   jbi j   jbj j    idi 2 bi   idi   bj   Definition 9  Given a clean set of bilateral blocks B  a block bi  j 2 B is bilaterally dominated by another block bk l 2 B  denoted by bi j   bk l   if at least one inner block of bi  j is unilaterally dominated by the corresponding inner block of bk l and the other is either unilaterally identical or unilaterally dominated  bi  j   bk l    jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j   jbj j   jbl j   In this context  the problem of Block Mining can be formally de   ned as follows  Problem 4  Block Mining   Given a clean set of block B  reduce B to its semantically equivalent subset B 0   B that contains no dominated blocks  We call B 0 a mined set of blocks  Apparently  this constitutes another quadratic problem  since the elements of every block have to be compared with those of all others  However  the abstract representation of blocks we are proposing leads to a series of necessary conditions that have to be satis   ed by a pair of blocks  if one of them is dominated  The bene   t is that these conditions can be checked in a fast and easy way  without the need to analytically compare the elements of the blocks  The conditions are also complementary  with their conjunction forming a composite mining method that e ectively restricts the required number of comparisons  In the following  we describe them in more detail   i  Size Condition  SC   In a clean set of blocks B  there cannot be a relation of dominance among a pair of equally sized blocks  Instead  the dominant block has to be larger in size than the dominated one  Therefore  to check whether a block is dominated  we need to compare it solely with blocks of larger size   ii  Area Condition  AC   Block mapping adds an additional condition for a relation of dominance  the spatial representation of the dominated block has to be fully contained in the representation of the dominant one  that is  the line  area  of the former lies entirely inside the line  area  mapped to the latter  More formally  the AC for a unilateral block bi to be dominated by a block bj is expressed as follows  bi   bj    min bj  entityCoods    min bi  entityCoods   max bi  entityCoods    max bj  entityCoods    where bk  entityCoods is the set of coordinates assigned to the entities of block bk   Similarly  the AC for a bilateral block bi  j to be dominated by a block bk l takes the following form  bi  j   bk l    min bk  entityCoods    min bi  entityCoods   max bi  entityCoods    max bk  entityCoods     min bl  entityCoods    min bj  entityCoods   max bj  entityCoods    max bl  entityCoods    AC is illustrated in Figure 1 a   b1 is smaller than b3  but cannot be dominated by it  since the line of b1 is not entirely covered by the line of b3  On the other hand  b2 satis   es the AC with respect to b3 but not with b1  As mentioned in Section 4 2  the priority of Algorithm 1 is to ensure that large blocks end up with low spatial deviation  The reason is that AC is intended to be used in conjunction with SC  In this way  the latter condition saves comparisons among equally sized blocks  even if their spatial representations are intersecting  while the former saves unnecessary comparisons that involve large blocks  Without AC  each block is inevitably compared with all smaller blocks  even if they share no entities   iii  Entity Condition  EC   Another  straightforward way of considering the content of blocks before comparing them is to considermerely those pairs of blocks that have at least one entity in common  This can be achieved by using the same data structure that is employed in Comparisons Propagation  a hash table that contains for each entity a list of the blocks that are associated with it  However  this solution has the same  quadratic  space complexity with Comparisons Propagation  thus violating the requirement for an alternative solution with minimal space requirements  Thus  we replace it with a near duplicates detection method  Locality Sensitive Hashing  LSH   7  constitutes an established method for hashing items of a high dimensional space in such a way that similar items  i e   near duplicates  are assigned to the same hash value with a high probability p1  In addition  dissimilar items are associated with the same hash values with a very low probability p2  In our case  blocks are the items that are represented in the high dimensional space of E  or E1 and E2  through Block Mapping  Thus  LSH can be employed to group highly similar blocks in buckets  so that it su ces it compare blocks contained in the same bucket  The details of the con   guration of LSH we employ are presented in Section 6  Algorithm 3 outlines the steps of our Block Mining algorithm  In short  it encompasses two nested loops  with SC and AC integrated in the inner one  Note that the input consists of the blocks contained in the same bucket of LSH  It is worth noting that the nested loop starts from the bottom of the ordered list of blocks and traverses it to the top  In this way  smaller blocks are    rst compared with the largest ones  The reason is that the larger the di erence in the size of the two blocks  the higher the likelihood that the larger block contains all the elements of the smaller one  thus  dominating it   SC is encapsulated in lines 5 and 6  while AC in line 7  Note that SC terminates the inner loop as soon as an equally sized block is encountered  because Block Scheduling ensures that the next blocks are of equal or smaller size  5 2 3 Block Merging An e ective way of discarding super   uous comparisons is to identify blocks that are highly overlapping  These are blocks that share so many entities that  if merged  they would result in fewer comparisons than the sum of the comparisons needed for each one  Merging such blocks eliminates their redundant comparisons  thus enhancing e ciency without any impact on e ectiveness  Indeed  pairs of entities that are common among the original blocks  i e   the source of redundancy  are considered only once in the new blocks  while the pairs of duplicate entities are maintained without any change  This situation is illustrated in the following example  Example 4  Consider the following unilateral blocks  b1   fid1  id2  id3  id4  id5g  b2   fid1  id2  id4  id5  id6g and b3   fid1  id3  id4  id5  id7  id8g  Individually  these blocks involve 35 comparisons  in total  However  b2 and b3 contain most of the comparisons in b1  they share four entities with b1   Merging b1 with b2 leads to the new block b 0 2   fid1  id2  id3  id4  id5  id6g  We now need 30 comparisons in total  between b 0 2 and b3   In addition  merging b 0 2 with b3 in a single block containing all 8 entities further reduces the total number of comparisons to 28  i e   20  less than the initial number of comparisons   More formally  the merge of two unilateral bilateral blocks is de   ned as follows  Definition 10  Given a set of unilateral blocks B  the unilateral merge of a block bi with a block bj is a new unilateral block bmi j that contains the union of the entities of bi and bj  bmi  j   bi   bj   Definition 11  Given a set of bilateral blocks B  the bilateral merge of a block bi  j with a block bk l is a new bilateral block Algorithm 4  Merging a mined set of blocks  Input  B a clean  mined set of blocks Output  B 00 the semantically equivalent  merged set of blocks 1 B 0   blockMapping B   2 FromMergesB       3 MergedB       4 OpenB       5 ProcessedB       6 for i   0 to spatialBlocks maxDimension do 7 NewOpenB   getBlocksStartingAtIndex i  B 0    8 NewOpenB   NewOpenB S FromMergesB  9 NewOpenB   NewOpenB n MergedB  10 NewOpenB   NewOpenB n OpenB  11 NewOpenB 0   blockScheduling NewOpenB   12 OpenB   blockScheduling OpenB S NewOpenB   13 FromMergesB       14 foreach bi 2 NewOpenB 0 do 15 mostS imilarBlock   getMostSimilarBlock bi   OpenB   16 if mostS imilarBlock   null then 17 newBlock   mergeBlocks bi   mostS imilarBlock   18 newBlock mapToCartesianSpace    FromMergesB add newBlock   19 MergedB add bi   20 MergedB add mostS imilarBlock   21 OpenB remove bi   22 OpenB remove mostS imilarBlock   23 OpenB addAll NewOpenB   24 EndingBlocks   getBlocksEndingAtIndex i  OpenB   25 OpenB removeAll EndingBlocks   26 ProcessedB addAll EndingBlocks   27 return ProcessedB  bmi k mj l   whose inner blocks constitute the unilateral merge of the corresponding inner blocks of bi  j and bk l  bmi k  mj l   fbi   bk   bj   blg  Typically  each block shares comparisons with many other blocks of the input set  However  these pairs of blocks di er in their degree of overlap  i e   the number of comparisons they share   In fact  the higher the Jaccard coe cient 2 of two blocks  the more comparisons their merge saves  Thus  to maximize the e ect of Block Merging  each block should be merged with the block that has the highest proportion of common entities with it  We call this block maximum Jaccard block  We can now formally de   ne the problem of Block Merging as follows  Problem 5  Block Merging   Given a mined set of blocks B  identify for each block bi its maximum Jaccard block bj   and merge these bi bj pairs  so as to produce a semantically equivalent set B 0 with a smaller number of redundant comparisons  We call B 0 a merged set of blocks  To address this problem  we introduce Algorithm 4  At its core lies the idea that each block should have overlapping spatial representations with its maximum Jaccard block  In other words  there is no point in estimating the Jaccard similarity between two blocks with disjoint spatial representations  Based on this principle  our algorithm works for unilateral blocks as follows  two main lists are maintained  the Open and the Processed ones  lines 4 and 5  respectively   The former contains the blocks that are available for comparison  whereas the latter encompasses the blocks that do not 2 The Jaccard similarity coe cient J A  B  of two sets A and B is equal to  J A  B    jA Bj jA Bj   a  0 1 2 3 4 5 b1 b b2 3 1 2 5 3 4 6  b  4 b3 3 5 5 4 6 1 2 3 b1 1 b2 2 2 3 4 0 1 2 3 4 5 1 Figure 2  Illustration of the block merge algorithm  need to be considered any more  Starting from the beginning of the X axis  the algorithm traverses the X axis by one point in every iteration  line 6   blocks  whose spatial representation starts from the current point  line 7   are compared with the blocks in the Open list  line 14   Then  at the completion of the iteration  they are added in it  together with the merged blocks  lines 23 and 8  respectively   Blocks  whose spatial representation ends at the current point  are placed in the Processed list  lines 24 26   The reason is that they do not overlap with any of the subsequently examined blocks  In the case of bilateral blocks  the only di erence in the execution of the algorithm is that it traverses both axes simultaneously  The following example illustrates the functionality of this algorithm  Example 5  Figure 2 a  is an illustration of the Block Merging algorithm for unilateral blocks that are mapped to the X axis  The main idea is that the grey area expands by one unit after each step  All blocks lying partly within it are in the OpenB list  while all blocks that lie entirely within it are placed in the ProcessedB list  At the highlighted Step 4  b2 and b3 lie in the former list  while b1 is placed in the latter  The execution of the algorithm is as follows  at Step 1  only b1 is in OpenB  while  at Step 2  b3 is also added and compared with b1  At Step 3  b1 is removed from OpenB  Then  b2 is placed in OpenB and compared with all other blocks  The remaining blocks  b2 and b3  are placed in ProcessedB at the end of Step 5  Figure 2 b  is an illustration of the block merge algorithm for bilateral blocks that are mapped to the XY space  Again  the grey area expands at each step by one unit  in both dimensions this time  All blocks lying partly within it are placed in the OpenB list  while all blocks that lie entirely inside its borders are moved to the ProcessedB list  In the depicted case  step 5   b1 1 lies in the ProcessedBlocks list  while b2 2 and b3 3 are in the OpenBlocks one  As mentioned in De   nition 5  the input to Block Merging is a mined set of blocks  that is  the input set contains neither identical nor dominated blocks  since they do not contribute non redundant comparisons  The computational cost of Algorithm 4 is thus significantly reduced  without a ecting its output  However  during the execution of Algorithm 4  blocks that belong to one of these categories can be produced  and should be discarded on the    y  Indeed  merges that lead to a block identical to another block of the input set are immediately removed  lines 9 10 Algorithm 4   Regarding new relations of dominance  merges can be involved in them only as dominant blocks  Otherwise  the original blocks that produce them would have already been dominated  More formally  bmi  j   bk    bi   bj    bk    bi   bk     bj   bk   Thus  only a block of the input set can be dominated by the merge of two other blocks  Apparently  it is not e cient to apply the Block Mining method after each iteration of Block Merging  Dominated blocks are  therefore  removed after the completion of the merging process  by comparing the original  non merged blocks with the set of merges  6  EVALUATION In this section we present a series of experiments that investigate the higher e ciency conveyed by our techniques  when incorporated into existing  redundancy bearing blocking methods  Our techniques were fully implemented in Java 1 6  and the experiments were performed on a server with Intel Xeon 3 0GHz  Metrics  Given that our techniques aim at eliminating redundant comparisons  we evaluate them on the basis of an established metric for measuring the e ciency of blocking methods  namely the Reduction Ratio  RR   1  3  18   It expresses the reduction in the number of pair wise comparisons required by a method with respect to the baseline one  Thus  it is de   ned as follows  RR   1  mc bc  where mc stands for the number of comparisons entailed by our technique  and bc expresses the number of comparisons entailed by the baseline  in our case  this is the original blocking method   RR takes values in the interval  0  1   for mc   bc   with higher values denoting higher e ciency  Recall  named Pair Completeness in the context of blocking  21   is not reported  since our techniques do not a ect the duplicates identi   ed by a blocking method   their goal is exclusively to detect and avoid the super   uous and repeated comparisons  Data Sets  To evaluate the impact of our techniques on existing blocking methods  we employ two real world data sets  one for the Clean Clean  and one for the Dirty Dirty and Dirty Clean cases of ER  They are DBPedia and the BTC09 data set  respectively  which are described in more detail in the following paragraphs  DBPedia Infobox Dataset  DBPedia   This data collection consists of two di erent versions of the DBPedia Infobox Data Set 3   They have been collected by extracting all name value pairs from the infoboxes of the articles in Wikipedia   s English version  at speci   c points in time  Although it may seem simple to resolve two versions of the same data set  this is not the case  More speci   cally  the older version  DBPedia1  is a snapshot of Wikipedia Infoboxes in October 2007  whereas the latest one  DBPedia2  dates from October 2009  During the two years that intervene between these two versions  Wikipedia Infoboxes were so heavily modi   ed that there is only a small overlap between their pro   les  even for duplicate entities  just 25  of all name value pairs are shared among the entities common in both versions  As matches  we consider those entities that have exactly the same URL in both versions  The attribute agnostic blocking method introduced in  21  was applied on this data set to produce the set of bilateral blocks we employ in our experiments  The blocks we consider are those resulting from the Block Purging step  21   involving 3 98 10 10 comparisons and PC   99 89   The technical characteristics of DBPedia are presented in Table 1  3 See http   wiki DBPedia org Datasets  DBPedia DBPedia1 Entities 1  190  734 DBPedia1 Name Value Pairs 17  453  516 DBPedia2 Entities 2  164  058 DBPedia2 Name Value Pairs 36  653  387 Bilateral Blocks 1  210  262 Comparisons 3 98   10 10 Av  Comparisons Per Block 32  893 Av  Blocks Per Entity 15 38 BTC09 Entities 1 82   10 8 Name Value Pairs 1 15   10 9 Unilateral Blocks 8 04   10 7 Comparisons 4 05   10 9 Av  Comparisons Per Block 50 37 Av  Blocks Per Entity 2 61 Table 1  Technical characteristics for both data sets  Billion Triple Challenge 2009  BTC09   This data set constitutes a publicly available 4   large snapshot of the Semantic Web  aggregating RDF statements from a variety of sources  In total  it comprises 182 million distinct entities  described by 1 15 billion triples  The ground truth set of duplicate entities is derived from the explicit as well as implicit equivalence relationships  i e   the owl sameAs statements and the Inverse Functional Properties  respectively   BTC09 was employed as a test bed for the attribute agnostic blocking method presented in  22   which is the source of our experimental set of unilateral blocks  well  Similar to DBPedia  the blocks we employ in our experiments are those stemming from the Block Purging method of  22   They entail 4 05   10 9 comparisons  exhibiting a PC very close to 90   A more detailed overview of this data set is presented in Table 1  6 1 Block Mapping In this section  we compare Algorithm 1 with the Random Mapping  RM  of blocks to the Cartesian space  As mentioned in Section 4 2  the higher the performance of a mapping method  the lower the sum of Formula 1 should be  Table 2 shows the outcome for both algorithms and for both data sets  In each case  we considered 100 iterations with RM in order to get a safe estimation of its performance  The sums in Table 2 is equal to the average value  It is remarkable that standard deviation is equal with  4 16 10 16 and  3 97   10 12 for DBPedia and BTC09  respectively  thus being 4 orders of magnitude lower than the mean value in both cases  This indicates that the performance of RM is relatively stable  We can see that Algorithm 1 substantially outperforms RM in both cases  for bilateral blocks  it improves RM by 30 34   whereas for unilateral blocks the enhancement is 25 28   The reason for the slightly lower improvement in the second case is twofold  First  redundancy is much higher in DBPedia than in BTC09  as documented in Table 1  in the former data set  an entity is associated with more than 15 blocks  in comparison with the less than 3 blocks for the latter  Thus  mapping an entity to a suboptimal  random  coordinate a ects the spatial deviation of more blocks in DBPedia than in BTC09  Second  suboptimal mappings have a larger impact in the two dimensional space than in the unidimensional one  Consider for example Figure 1  Assigning entity id4 to point 5 of the X axis instead of 4  increases the spatial deviation of b2 by 1  However  assigning entity id1 4 to point 5 of the Y axis instead of 4 increments the spatial deviation of b3 3 by 4  For these 4 See http   vmlion25 deri ie  Method DBPedia Sum BTC09 Sum Algorithm 1 0 81   10 20 1 33   10 16 Random Mapping 1 16   10 20 1 78   10 16 Table 2  Comparison of the Block Mapping technique with respect to the sum of Formula 1 for both data sets  Method DBPedia Comp  BTC09Comp  Cartesian Product 5 11   10 11 3 23   10 19 LSH   Algorithm 3 1 27   10 8 4 52   10 9 Table 3  Performance of the Block Mining algorithm  DBPedia BTC09 Method Comp  RR Comp  RR Input Set 3 98   10 10   4 01   10 9   Comp  Prop  2 59   10 9 93 49  3 08   10 9 23 19  Table 4  E ect of Comparisons Propagation on the e ciency of both data sets  reasons  the performance of RM is slightly closer to Algorithm 1 for unilateral blocks  6 2 Block Mining We now present the performance of our Block Mining algorithm  i e   LSH and Algorithm 3  with respect to the number of block comparisons it requires in order to examine the blocks inside each bucket  In general  to determine the structure of LSH  two parameters have to be speci   ed  L and k  the former denotes the total number of hash tables that will be employed  while the latter speci   es the number of hash functions that are merged to compose the hash signature for each entity  for a particular hash table  In our implementation  we followed  23  and used exclusive or for e   ciently merging the k hash functions into the composite signature  In more detail  L was set to 10  and k to 12  while the probabilities p1 and p2  see Section 5 2 2  were set to 0 9 and 0 1  respectively  Table 3 shows the performance of our method in comparison with the naive method of examining all possible pairs of blocks  The aim is not to examine the optimal con   guration of LSH  Rather  the main conclusion to be drawn from these numbers is that it is a scalable approach  whose performance depends on the level of redundancy of the underlying blocking method  the higher the redundancy  the more similar the blocks are between them  and the larger the corresponding buckets get  this results in more comparisons and lower e ciency  This explains why the performanc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#ser09p3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09_entity_resolution"/>
        <doc>Eliminating the Redundancy in Blocking based Entity Resolution Methods ### George Papadakis       Ekaterini Ioannou    Claudia Nieder  e    Themis Palpanas z   and Wolfgang Nejdl     National Technical University of Athens  Greece gpapadis mail ntua gr   Technical University of Crete  Greece ioannou softnet tuc gr   L3S Research Center  Germany  surname  L3S de z University of Trento  Italy themis disi unitn eu ABSTRACT Entity resolution is the task of identifying entities that refer to the same real world object  It has important applications in the context of digital libraries  such as citation matching and author disambiguation  Blocking is an established methodology for e ciently addressing this problem  it clusters similar entities together  and compares solely entities inside each cluster  In order to e ectively deal with the current large  noisy and heterogeneous data collections  novel blocking methods that rely on redundancy have been introduced  they associate each entity with multiple blocks in order to increase recall  thus increasing the computational cost  as well  In this paper  we introduce novel techniques that remove the super   uous comparisons from any redundancy based blocking method  They improve the time e ciency of the latter without any impact on the end result  We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity  For applications with space limitations  we also present an alternative  lightweight solution that operates at the abstract level of blocks in order to discard a signi   cant part of the redundant comparisons  We evaluate our techniques on two large  real world data sets and verify the signi   cant improvements they convey when integrated into existing blocking methods  Categories and Subject Descriptors H 3 3  Information Search and Retrieval   Information    ltering General Terms Algorithms  Experimentation  Performance Keywords Data Cleaning  Entity Resolution  Redundancy based Blocking 1  INTRODUCTION Nowadays  the growing availability of semi structured and structured data in the Web of Data opens new opportunities for digital libraries  These data collections can clearly pro   t from a variety of digital library principles and technologies  such as the systematic ### Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  JCDL   11  June 13   17  2011  Ottawa  Ontario  Canada  Copyright 2011 ACM 978 1 4503 0744 4 11 06     10 00  and uniform description of data by metadata  metadata harvesting services and technologies for federated search  Furthermore  they can be exploited to create new types of services by combining them with traditional types of library content  The integration of related data in meaningful ways relies on the detection of data records  from di erent collections  that refer to the same object  e g   author  The process of identifying  among a set of entities  those referring to the same real world object is called Entity Resolution  ER   There are two main applications of this process in digital libraries  citation matching for identifying references that describe the same publication  and author disambiguation for identifying author pro     les that pertain to the same person  9  10  25  27   The latter consists of detecting   within a collection of bibliographical records   the correct coupling between author names and persons  by resolving the mixed citation problem  same name   di erent persons  and the split citation problem  same person   di erent names   14   At its core  ER constitutes a quadratic problem  as each entity has to be compared with all others  To enhance its e ciency  blocking methods are typically employed  6  11  15   they extract from every entity pro   le  or record  a Blocking Key Value  BKV  that encapsulates its most distinguishing information and de   ne blocks on the equality  or similarity  of BKVs  Thus  each block corresponds to a speci   c instance of the BKV and contains all entities associated with that value  However  in order to select the most reliable and distinguishing attributes of the given entity pro   les  traditional blocking methods rely on a prede   ned entity schema  This renders them inapplicable for the Web of Data  due to the special characteristics of the latter  it involves individual collections that are highly heterogeneous  stemming from a rich diversity of sources  which evolve autonomously  following an unprecedented growth rate  especially the user generated data of the Social Web  and the data created by sensors   More speci   cally  the following challenges are present in these settings  Loose schema binding  The schemata describing entities may range from locally de   ned attributes to pure tag style annotations  and data often have no strict binding to the employed schemata  Noisy  missing  and inconsistent values  They are introduced in the data due to extraction errors  sources of low quality  and use of alternative descriptions  As a result  entity pro   les may contain de   cient  or even false information  Extreme levels of heterogeneity  This is caused by the fact that data stem from a variety of distributed  self organized  collaborative sources  Actually  heterogeneity pertains not only to schemata describing the same entity types  but also to pro   les describing the same entity  For instance  GoogleBase 1 encompasses 100  000 distinct schemata corresponding to 10  000 entity types  16   High growth rates in terms of volume and fast evolution  This is caused partly due to automatic generation and partly due to the high involvement of users  they typically add new content  and update incorrect  outdated  or simply irrelevant information  These inherent characteristics of heterogeneous information spaces break the fundamental assumptions of traditional blocking techniques  Novel blocking schemes  that do not require a prede     ned schema  have been introduced to e ectively deal with these challenges  They all rely on redundancy  associating each entity with multiple blocks  3  20  21  28   In this way  they minimize the likelihood that two duplicate entities have no block in common  and achieve high levels of e ectiveness  i e   detected duplicate entities   This comes  though  at the cost of time e ciency  the resulting blocks are overlapping  and the same pairs of entities may be compared multiple times  Therefore  the main challenge for improving the e ciency of redundancy based blocking methods is to eliminate the super   uous comparisons they entail  without a ecting their accuracy  In this paper  we address the above problem through an abstraction of the redundancy based blocking techniques  blocks are associated with an index indicating their position in the processing list and entities are associated with a list of the indices of the blocks that contain it  Thus  we can identify whether a pair of entities contained in the current block has already been compared in another block simply by comparing their least common block index with the index of the current block  In this way  we achieve the optimal solution to the problem  since we e ciently propagate all executed comparisons  without explicitly storing them  The above approach has low computational cost  but results in quadratic space complexity  In order to remedy this drawback  we introduce a method that approximates the optimal solution by gracefully trading space for computational cost  It comprises of a series of block manipulation techniques  which discard those blocks that exclusively entail super   uous comparisons  i e   they are entirely contained in another block   and merge pairs of highly overlapping blocks  giving birth to blocks that entail less comparisons  These functionalities are facilitated by mapping the blocks to a Cartesian space and contrasting their spatial representations  without the need to examine their contents analytically  In summary  the contributions of this paper are the following   1  We formulate the problem of purging redundant comparisons from a blocking technique and explain how its solution can be facilitated through the abstraction of blocks  i e   enumerating and mapping them to the Cartesian space    2  We describe Comparisons Propagation  an optimal solution to this problem  which e ciently propagates all executed comparisons based on the enumeration of blocks  This method is suitable for applications that can a ord high space complexity   3  We further propose a solution that partially discards redundant comparisons  trading space requirements for time complexity  It consists of a series of methods that remove blocks involving exclusively redundant comparisons and merge highly overlapping ones   4  Finally  we thoroughly evaluate our methods on two large  realworld data sets  demonstrating the great bene   ts they convey to the e ciency of existing blocking methods  The rest of the paper is structured as follows  Section 2 summarizes previous work and Section 3 de   nes the basic notions of our algorithms  Section 4 introduces our approach to determining the processing order of blocks and mapping them to the Cartesian 1 See http   www google com base  Space  and Section 5 presents our approach to propagating comparisons and manipulating blocks  Experimental evaluation is presented in Section 6  while Section 7 concludes the paper  2  RELATED WORK A variety of methods for solving the ER problem has been presented in literature  They range from string similarity metrics  2   to similarity methods using transformations  19  26  and relationships between data  5  12   A comprehensive overview of the existing work in this domain can be found in  4  6  13   The approximate methods of data blocking typically associate each record  i e   entity  with a Blocking Key Value  BKV   They de   ne blocks on the equality  or similarity  of BKVs and compare solely the entities that are contained in the same block  6   For instance  the Sorted Neighborhood approach  11  orders records according to their BKV and slides a window of    xed size over them  comparing the records it contains  The StringMap method  15  maps the BKV of each record to a multi dimensional Euclidean space  and employs suitable data structures for e ciently identifying pairs of similar records  The q grams blocking approach  8  builds overlapping clusters of records that share at least one qgram  i e   sub string of length q  of their BKV  Canopy clustering  17  employs a cheap string similarity metric for building high dimensional overlapping blocks  whereas the Su x Arrays approach  3  considers the su xes of the BKV instead   28  explores another aspect of these blocking approaches  arguing that more duplicates can be detected and more pair wise comparisons can be saved through the iterative distribution of identi   ed matches to subsequently  re  processed blocks  The performance of blocking methods typically depends on the    ne tuning of a wealth of application  and data speci   c parameters  3  29   To automate the parameter setting procedure  several methods that model it as a machine learning problem have been proposed in the literature  For instance   18  de   nes it as learning disjunctive sets of conjunctions that consist of an attribute  used for blocking  and a method  used for comparing the corresponding values   Similarly   1  considers disjunctions of blocking predicates  i e   conjunctions of attributes and methods  along with predicates combined in disjunctive normal form  DNF   On the other hand   29  introduces a method for adaptively and dynamically setting the size of the sliding window of the Sorted Neighborhood approach  Attribute agnostic blocking methods were recently introduced to make blocking applicable to voluminous  heterogeneous data collections  such as the Web of Data  These methods do not need a prede   ned schema for grouping entities into blocks  as they completely disregard attribute names  In this way  they are able to handle thousands of attribute names without requiring the    ne tuning of numerous parameters  Instead  they tokenize  on all special characters  the attribute values of each entity pro   le  and create an individual block for each token  that is  every block corresponds to a speci   c token and contains all entities having this token in their pro   le  21   Blocks of such low level granularity guarantee high e ectiveness due to the high redundancy they convey  each entity is associated with multiple blocks  which are  thus  overlapping  Hence  the likelihood of a missed match  i e   a pair of duplicates that has no block in common  is low  This redundancy based approach is a common practice among blocking techniques for noisy  but homogeneous data  as well  3  17  20  28   To the best of our knowledge  this is the    rst work on formally de   ning and dealing with the problem of eliminating redundant comparisons of blocking methods for ER 3  PROBLEM DEFINITION To formally describe the problem we are tackling in this paper  we adopt the de   nitions introduced in  21  for modeling entity pro     les and entity collections  As such  an entity pro   le p is a tuple hid  Api  where Ap is a set of attributes ai   and id 2 ID is a global identi   er for the pro   le  Each attribute ai 2 Ap is a tuple hni   vii  consisting of an attribute name ni and an attribute value vi   Each attribute value can also be an identi   er  which allows for modeling relationships between entities  An entity collection E is a tuple hAE  VE  IDE  PEi  where AE is the set of attribute names appearing in it  VE is the set of values used in it  IDE   ID is the set of global identi   ers contained in it  and PE  IDE is the set of entity pro   les that it comprises  We de   ne a blocking scheme as follows  Definition 1  A blocking scheme bs for an entity collection E is de   ned by a transformation function f t   E 7  T and a set of constraint functions f i cond   T   T 7  ftrue  f alseg  The transformation function ft derives the appropriate blocking representation from the complete entity pro   le  or parts of it   The constraint function f i cond is a transitive and symmetric function that encapsulates the condition that has to be satis   ed by two entities  if they are to be placed in the same block bi   Apparently  any blocking method can de   ne and use its own blocking scheme that follows the above de   nition  For example  the schemes described in Section 2 consist of a transformation function that extracts the BKV from an entity pro   le and a set of constraint functions that de   ne blocks on the equality  or similarity  of the BKVs  Once a blocking scheme is applied on an entity collection  a set of blocks is derived  whose instances are formally de   ned as follows  Definition 2  Given an entity collection E and a blocking scheme bs for E  a block bi 2 B is the maximal subset  with a minimum cardinality of 2  that is de   ned by the transformation function f t and one of the constraint functions f i cond of bs  bi   E   8p1  p2 2 E   f i cond  f t p1   f t p2     true   p1  p2 2 bi   The ER process on top of a blocking method consists of iterating over its set of blocks B in order to compare the entities contained in each one of them  We use mi j to denote a match between two pro   les pi and pj that have been identi   ed as matching pi   pj  i e   describing the same real world object   The output  therefore  of a blocking method is a set of matches  which we denote as M  To address the aforementioned characteristics of heterogeneous information spaces  redundancy bearing blocking methods have been recently introduced  3  20  21  28   They associate each entity with multiple  overlapping blocks  This practice minimizes the likelihood that two duplicate entities have no block in common  thus resulting in higher e ectiveness for the ER process  Ef     ciency  on the other hand  is signi   cantly downgraded  due to the redundant comparisons between pairs of entities that appear in many blocks  Apparently  the higher the redundancy conveyed by a blocking method  the lower the e ciency of the ER process  In this paper  we focus on developing methods that enhance the e ciency of redundancy based blocking methods  without a ecting their e ectiveness  To this end  our techniques aim at eliminating the super   uous comparisons of redundancy bearing blocking methods in order to save considerable computational e ort  In this way  they can operate on top of any blocking method  without altering its e ectiveness  producing an output that is equivalent to the original one  The following de   nition introduces the concept of semantically equivalent blocking sets  Definition 3  A blocking set B 0 is semantically equivalent to blocking set B  if the set of matches resulting from blocking set B 0 are equal to the set of matches resulting from blocking set B  i e   MB0 MB   Based on the above de   nition  we now formally state the problem we are addressing in this paper  Problem 1  Given a set of blocks B that are derived from a redundancy bearing blocking technique     nd the semantically equivalent blocking set B 0 that involves no redundant pair wise comparisons  4  BLOCK SCHEDULING AND MAPPING As stated above  our goal is to propose generic methods for enhancing the e ciency of any redundancy bearing blocking technique  such as the ones discussed in Section 2   Therefore  the methods we describe make no assumptions on the mechanism or functionality of the underlying blocking method  Instead  they treat blocks at an abstract level  considering solely the identi   ers of the entities they contain  i e   each block is represented as a set of entity ids   We distinguish between two types of blocks according to the lineage of their entities  The    rst type of blocks is called unilateral  since it contains entities of the same lineage  i e   stemming from the same entity collection  This type of blocks arises when integrating one dirty collection  i e   a collection that contains duplicate entities  either with a clean  duplicate free collection  i e   DirtyClean   or with another dirty collection  i e   Dirty Dirty   Both cases are equivalent with resolving a single  dirty entity collection  where each entity pro   le could match to any other  24   More formally  the blocks of this kind are de   ned as follows  Definition 4  A unilateral block is a block containing entity ids from a single entity collection E  thus being of the form bi   fid1  id2          idng  where idi 2 ID  The second type of blocks is called bilateral  and arises when integrating two individually clean entity collections  E1 and E2  that are overlapping  Clean Clean   24   The goal is  therefore  to identify matches only between E1 and E2  thus requiring that each block contains entities from both input collections  More formally  this kind of blocks is de   ned as follows  Definition 5  A bilateral block is a block containing entity ids from two entity collections  E1 and E2  It follows the form bi  j   ffidi 1  idi 2         idi ng fidj 1  idj 2         idj mgg  where idi k 2 ID1 and idj l 2 ID2  The subsets bi   fidi 1  idi 2         idi ng and bj   fidj 1  idj 2          idj mg are called the inner blocks of bi  j   4 1 Block Scheduling Specifying the processing order of blocks is important for the e ectiveness of ER techniques  This order forms the basis for block enumeration  which associates each block with an integer that represents its position in the processing list  This practice    nds application in various techniques  such as the propagation of comparisons  see Section 5 1   The processing order is also an integral part of lossy e ciency techniques  like Block Pruning in  21   these are methods that sacri   ce  to some extent  the e ectiveness of a blocking method in order to enhance its e ciency  They do so by discarding comparisons according to some criteria  even if they involve non redundant comparisons among matching entities  Scheduling techniques typically associate each element of the given set of blocks B with a numerical value and sort B in ascending or descending order of this value  Their computational cost a  0 1 2 3 4 5 b1 b b2 3  b  0 1 2 3 4 5 1 2 3 4 b1 1 b2 2 b3 3 Figure 1  Illustration of block mapping  is O jBj   log jBj   which scales even for large sets of blocks  In each case  the most suitable approach for determining the processing order of blocks depends heavily on the application at hand  For the needs of the methods we introduce in Section 5  we de   ne a di erent scheduling method for each kind of block  In particular  unilateral blocks are ordered in ascending order of cardinality  the more entities a block bi contains  the higher its position in the list  Bilateral blocks  on the other hand  are ordered in ascending order of their utility  21   ubi  j   1 max jbi j jb j j    where jbi j and jbj j are the cardinalities of the inner blocks of the bilateral block bi  j   Bilateral blocks of equal utility are ordered in ascending order of the cardinality of their smallest inner block  4 2 Block Mapping We now introduce our approach for Block Mapping  The gist of this technique is that it allows us to e ciently check whether two blocks have overlapping content  i e   they share some entities   without exhaustively comparing them  The mapping is performed by transforming blocks into the Cartesian space  for unilateral blocks this corresponds to Cartesian coordinates in one dimension  i e   lines   and for bilateral blocks to coordinates in two dimensions  i e   rectangles   Thus  Block Mapping is performed by assigning each entity to a point on the corresponding axis  Example 1  Figure 1 a  illustrates the mapping of the unilateral blocks b1   fid2  id3  id4g  b2   fid0  id1  id4g  and b3   fid0  id1  id3  id4g on the X axis  Their entities are assigned to coordinates as follows  C  hid0  3i  hid1  4i  hid2  0i  hid3  1i  hid4  2i   Figure 1 b  illustrates the mapping of the bilateral blocks b1 1   ffid1 0  id1 2g  fid2 0  id2 1gg  b2 2   ffid1 0  id1 3g  fid2 1  id2 3gg and b3 3   ffid1 0  id1 3  id1 4g  fid2 1  id2 2  id2 4gg to the XY axes  where id1 i 2 E1 and id2 i 2 E2  The entities of E1 are transformed to points on the X axis as follows  CX  hid1 0  3i hid1 1  4i  hid1 2 1i  hid1 3  5i  hid1 4  2i   whereas the entities of E2 are mapped to points on the Y axis as follows  CY  hid2 0  3i hid2 1  1i hid2 2 0i  hid2 3  2i  hid2 4  4i   The di erence between the size b sp i of the spatial representation of a block bi and its actual size b as i is called spatial deviation spi of block bi   More formally  it is de   ned as follows  spi   b sp i  b as i   Algorithm 1  Mapping Blocks to the Cartesian Space  Input  B fbig a set of unilateral blocks Output  C fhidi   jig a mapping of entity ids to coordinates 1 B 0   blockScheduling B   2 C       3 lastIndex   0  4 foreach bi 2 B 0 do 5 E   sortInAscendingOrderOfFrequency bi  entities     6 foreach e 2 E do 7 if   C containsKey e id   then 8 C   C    he id  lastIndexi   9 lastIndex    10 return C  where b sp i is the length  area  of the spatial representation of a unilateral  bilateral  block bi   and b as i is the actual length  area  of the unilateral  bilateral  block bi   In the case of unilateral blocks  we have b as i   jbi j  1  while for a bilateral block bi  j it is equal to b as i  j    jbi j  1     jbj j  1   For example  b2 2 has an actual area of   2 1   2 1   1  whereas its spatial representation has an area of   5 3   2 1   2  that is  sp2 2   1  The value of spi is always positive  but in the ideal case it should be equal to 0  This requirement can be easily satis   ed for nonoverlapping blocks  by associating the entities of each block with contiguous coordinates  In the case of overlapping blocks  though  the spatial transformation leads to a positive deviation  since it cannot be done independently for each block  assigning a coordinate to an entity idi in the context of a block bi can be contiguous with the rest of entities in bi   but not necessarily with the other entities that share blocks with idi   Example 2  Consider the entities in Figure 1 a   The way the depicted blocks are mapped is the optimal one  since the entities of every block are contiguous  Imagine  though  that we place an additional entity to each block  id5 to b1  id6 to b2 and id7 to b3  In this case  there is no way of mapping the new blocks to the X axis  so that the entities of each block are contiguous  The above discussion gives rise to the following optimization problem  Problem 2  Given a set of blocks B  transform its elements to the Cartesian space  so that their aggregated spatial deviation P bi2B  b sp i  b as i   is minimized  In our methods  we require that emphasis is placed on minimizing the spatial deviation of large blocks  we elaborate on the reasons in Section 5 2 2   That is  the larger a block is  the lower its spatial deviation should be  More formally  this optimization problem can be de   ned as the minimization of the following quantity  X bi2B bi  size      b sp i  b as i     1  where bi  size   is the size of block bi   For a unilateral block  it is equal to its cardinality  i e   number of entities it contains   while for a bilateral block bi  j it is equal to the sum of cardinalities of its inner blocks  bi  j  size     jbi j   jbj j  We solve this optimization problem using a scalable method  applicable to voluminous data collections  Algorithm 1 outlines this method for the case of unilateral blocks  for bilateral ones  the algorithm is applied twice  independently for each axis  considering in each iteration solely the corresponding entity collection  In essence  the algorithm assigns coordinates from the interval  0  jEj  1  to the entity pro   les of the given collection E        After Block Scheduling  it starts assigning the entities of the last  usually largest  block to contiguous coordinates  thus minimizing the spatial deviation of this block  To ensure the minimal spatial deviation for the rest of the blocks  as well  the pro   les are ordered and mapped in ascending order of their frequency  i e   the number of blocks associated with each entity   the least frequent of the not yet mapped entities takes the    rst available coordinate  the second least frequent takes the next coordinate etc  Two entities that share many blocks are more likely to be contiguous in this way  The algorithm is then repeated for the remaining blocks  traversing their ordered list from bottom to top  The algorithm has a linear space complexity  O jBj   jEj   and time complexity of O jBj   log jBj   jE   log jEj   due to the sorting of blocks and entities  Example 3  The result of applying this algorithm is illustrated in Figure 1 a   The pro   les of b1 are mapped to the X axis as follows  id2 has frequency 0 and goes to the    rst available coordinate  i e   0   id3 with frequency 1 goes to next available coordinate  i e   1    and     nally  id4 with frequency 2 goes to point 2  5  APPROACH In this section  we present the methods we developed for reducing the redundancy of blocking methods  Problem 1   based on the Block Scheduling and Mapping techniques we introduced above  The optimal solution to this problem  i e   the one that discards all redundant comparisons  is presented in Section 5 1  Its e ectiveness  though  comes at the cost of high space complexity  caused by the data structure it employs  As an alternative  we present in Section 5 2 an approximate solution  that removes the high space requirements  5 1 Comparisons Propagation Block Scheduling determines the processing order of blocks  and enables their enumeration  that is  each block is assigned to an index indicating its position in the processing list  Based on this enumeration  the propagation of comparisons is made feasible through a common data structure  namely a hash table  in particular  its keys are the ids of the entities of a given collection E  and its values are lists of the indices of the blocks that contain the corresponding entities  The elements of these lists are sorted in ascending order  from the lowest block index to the highest  This data structure can be used in the context of a blocking method in the following way  to compare a pair of entities  the Least Common Block Index Condition should be satis   ed  That is  the lowest common block index of these entities should be equal to the index of the current block  indicating in this way that this is the    rst block in the processing list that contains both of them  Otherwise  if the former index is lower than the latter  the entities have already been compared in another block  and the comparison is redundant  In this way  each pair of entities is compared just once  and Comparisons Propagation provides the optimal solution to Problem 1  Theorem 1  Optimality of Comparisons Propagation   Given a set of blocks B  Comparisons Propagation produces the semantically equivalent set of blocks B 0 that entails no redundant pair wise comparisons  Proof  Let us assume that the set of blocks produced by Comparisons Propagation entails redundant comparisons  This means that there is a blocking set B 00 that is semantically equal to B and involves no redundant comparisons  Hence  there must be at least one pair of entities that is compared twice in B 0 and just once in B 00   The Least Common Block Index Condition is  therefore  satis   ed in two blocks of B 0   which is a contradiction  Thus  B 00   B 0   Algorithm 2  Propagating Comparisons  Input  B a set of blocks Output  B 00 the semantically equivalent set of blocks with no redundant comparisons 1 B   blockScheduling B   2 B 0   blockEnumeration B   3 B 00       4 entityIndex   indexBlocksOnEntityIds B 0    5 foreach bi 2 B 0 do 6 E   bi  entities    7 for i   1 to E size do 8 BEi   entityIndex associatedBlocks E i    9 for j   i   1 to E size do 10 BEj   entityIndex associatedBlocks E j    11 if  bi  index   leastCommonBlockIndex BEi  BEj    then 12 bi   newBlock E i    E j    13 B 00   B 00 S bi   14 return B 00   Algorithm 2 outlines the way Comparisons Propagation operates on a set of blocks B in order to produce its semantically equivalent set of blocks B 0 that is free of redundant comparisons  In essence  B 0 consists of blocks with minimum cardinality  since each nonredundant comparison results in a new block that contains the corresponding pair of entities  This may result in a very large number of blocks  and storing them poses a serious challenge  Processing them on the    y  though  is an e cient alternative  Comparisons Propagation can be integrated in the execution of any blocking method  without a ecting its time complexity  The reason is that the computation of the least common block index is linear to the number of blocks associated with the corresponding pair of entities  due to the ordering of indices in the values of the hash table   Its space complexity  though  is equal to O jBj   jEj   in the worst case  i e   each entity is placed in all blocks   where jBj is the total number of blocks  and jEj is the cardinality of the given entity collection  for Clean Clean ER  this cardinality is equal to jE1j   jE2j   In practice  however  space complexity depends on the level of redundancy introduced by the underlying blocking method  In fact  it is equal to O jEj   BPE      where BPE   is an estimate of redundancy  denoting the average number of blocks per entity  5 2 Block Manipulation Block Manipulation consists of a series of techniques that operate on two levels     rst  they investigate the given set of blocks in order to discard those elements that contain purely redundant comparisons  In this way  they reduce not only the number of comparisons  but also the number of blocks that will be processed in the next level  Second  they aim at identifying pro   table block merges  that is  pairs of highly overlapping blocks  which  when combined  result in a block with fewer comparisons  The combined result of these two levels approximates the optimal solution of Comparisons Propagation at a lower space complexity  The individual strategies of Block Manipulation are analytically presented in the following paragraphs  in the order they should be executed  5 2 1 Block Cleaning Cleaning a set of blocks B is the process of purging the duplicate elements from it  These are blocks that contain exactly the same entities with another block  regardless of the constraint function de   ning each of them  i e   independently of the information that is associated with them   We call such blocks identical  and  depending on their lineage  we formally de   ne them as follows Algorithm 3  Mining a clean set of blocks  Input  B a clean set of highly similar blocks Output  B 00 the semantically equivalent  mined set of blocks 1 B 0   blockScheduling B   2 DominatedB       3 for i   1 to B 0  size do 4 for j   B 0  size to i   1 do 5 if  B 0  i  size     B 0  j  size     then 6 break  7 if  areaConditionHolds B 0  i   B 0  j   then 8 if  isDominated B 0  i   B 0  j   then 9 DominatedB   DominatedB   B 0  i   10 break  11 B 00   B 0   DominatedBlocks  12 return B 00   Definition 6  Given a set of unilateral blocks B  a block bi 2 B is unilaterally identical with another block bj 2 B  denoted by bi   bj   if both blocks contain the same entities  regardless of their constraint functions  f i cond and f j cond   bi   bj   bi   bj   bj   bi   Definition 7  Given a set of bilateral blocks B  a block bi  j 2 B is bilaterally identical with another block bk l 2 B  denoted by bi j   bk l   if their corresponding inner blocks are unilaterally identical  bi  j   bk l   bi   bk   bj   bl   In this context  the process of Block Cleaning can be formally de   ned as follows  Problem 3  Block Cleaning   Given a set of blocks B  reduce B to its semantically equivalent subset B 0   B that contains no identical blocks  We call B 0 a clean set of blocks  The solution to this problem can be easily implemented by associating each block with a hash signature  its value is equal to the sum of the coordinates assigned to its entities by Block Mapping  Identical blocks necessarily have the same signature  but not vice versa  signature equality can also lead to false positives  For this reason  the size as well as the elements of two blocks with the same signature are analytically compared to make sure that they are indeed identical  In practice  this functionality is e ciently offered by default by most programming languages  Both its time and space complexity are linear to the size of the input set of blocks  i e   O jBj    as it involves traversing its elements just once  5 2 2 Block Mining Given a clean set of blocks  the process of mining it consists of identifying the blocks that are subsets of at least one other block in the set  that is  blocks whose entities are all contained in some other block  independently of the corresponding constraint functions  This situation is called a relation of dominance  where the latter is the dominant block  and the former the dominated one  This is more formally de   ned as follows  Definition 8  Given a clean set of unilateral blocks B  a block bi 2 B is unilaterally dominated by another block bj 2 B  denoted by bi   bj   if bi is a proper subset of bj   regardless of the constraint functions f i cond and f j cond   bi   bj   jbi j   jbj j    idi 2 bi   idi   bj   Definition 9  Given a clean set of bilateral blocks B  a block bi  j 2 B is bilaterally dominated by another block bk l 2 B  denoted by bi j   bk l   if at least one inner block of bi  j is unilaterally dominated by the corresponding inner block of bk l and the other is either unilaterally identical or unilaterally dominated  bi  j   bk l    jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j jbj j   jbl j  W  jbi j   jbk j   jbj j   jbl j   In this context  the problem of Block Mining can be formally de   ned as follows  Problem 4  Block Mining   Given a clean set of block B  reduce B to its semantically equivalent subset B 0   B that contains no dominated blocks  We call B 0 a mined set of blocks  Apparently  this constitutes another quadratic problem  since the elements of every block have to be compared with those of all others  However  the abstract representation of blocks we are proposing leads to a series of necessary conditions that have to be satis   ed by a pair of blocks  if one of them is dominated  The bene   t is that these conditions can be checked in a fast and easy way  without the need to analytically compare the elements of the blocks  The conditions are also complementary  with their conjunction forming a composite mining method that e ectively restricts the required number of comparisons  In the following  we describe them in more detail   i  Size Condition  SC   In a clean set of blocks B  there cannot be a relation of dominance among a pair of equally sized blocks  Instead  the dominant block has to be larger in size than the dominated one  Therefore  to check whether a block is dominated  we need to compare it solely with blocks of larger size   ii  Area Condition  AC   Block mapping adds an additional condition for a relation of dominance  the spatial representation of the dominated block has to be fully contained in the representation of the dominant one  that is  the line  area  of the former lies entirely inside the line  area  mapped to the latter  More formally  the AC for a unilateral block bi to be dominated by a block bj is expressed as follows  bi   bj    min bj  entityCoods    min bi  entityCoods   max bi  entityCoods    max bj  entityCoods    where bk  entityCoods is the set of coordinates assigned to the entities of block bk   Similarly  the AC for a bilateral block bi  j to be dominated by a block bk l takes the following form  bi  j   bk l    min bk  entityCoods    min bi  entityCoods   max bi  entityCoods    max bk  entityCoods     min bl  entityCoods    min bj  entityCoods   max bj  entityCoods    max bl  entityCoods    AC is illustrated in Figure 1 a   b1 is smaller than b3  but cannot be dominated by it  since the line of b1 is not entirely covered by the line of b3  On the other hand  b2 satis   es the AC with respect to b3 but not with b1  As mentioned in Section 4 2  the priority of Algorithm 1 is to ensure that large blocks end up with low spatial deviation  The reason is that AC is intended to be used in conjunction with SC  In this way  the latter condition saves comparisons among equally sized blocks  even if their spatial representations are intersecting  while the former saves unnecessary comparisons that involve large blocks  Without AC  each block is inevitably compared with all smaller blocks  even if they share no entities   iii  Entity Condition  EC   Another  straightforward way of considering the content of blocks before comparing them is to considermerely those pairs of blocks that have at least one entity in common  This can be achieved by using the same data structure that is employed in Comparisons Propagation  a hash table that contains for each entity a list of the blocks that are associated with it  However  this solution has the same  quadratic  space complexity with Comparisons Propagation  thus violating the requirement for an alternative solution with minimal space requirements  Thus  we replace it with a near duplicates detection method  Locality Sensitive Hashing  LSH   7  constitutes an established method for hashing items of a high dimensional space in such a way that similar items  i e   near duplicates  are assigned to the same hash value with a high probability p1  In addition  dissimilar items are associated with the same hash values with a very low probability p2  In our case  blocks are the items that are represented in the high dimensional space of E  or E1 and E2  through Block Mapping  Thus  LSH can be employed to group highly similar blocks in buckets  so that it su ces it compare blocks contained in the same bucket  The details of the con   guration of LSH we employ are presented in Section 6  Algorithm 3 outlines the steps of our Block Mining algorithm  In short  it encompasses two nested loops  with SC and AC integrated in the inner one  Note that the input consists of the blocks contained in the same bucket of LSH  It is worth noting that the nested loop starts from the bottom of the ordered list of blocks and traverses it to the top  In this way  smaller blocks are    rst compared with the largest ones  The reason is that the larger the di erence in the size of the two blocks  the higher the likelihood that the larger block contains all the elements of the smaller one  thus  dominating it   SC is encapsulated in lines 5 and 6  while AC in line 7  Note that SC terminates the inner loop as soon as an equally sized block is encountered  because Block Scheduling ensures that the next blocks are of equal or smaller size  5 2 3 Block Merging An e ective way of discarding super   uous comparisons is to identify blocks that are highly overlapping  These are blocks that share so many entities that  if merged  they would result in fewer comparisons than the sum of the comparisons needed for each one  Merging such blocks eliminates their redundant comparisons  thus enhancing e ciency without any impact on e ectiveness  Indeed  pairs of entities that are common among the original blocks  i e   the source of redundancy  are considered only once in the new blocks  while the pairs of duplicate entities are maintained without any change  This situation is illustrated in the following example  Example 4  Consider the following unilateral blocks  b1   fid1  id2  id3  id4  id5g  b2   fid1  id2  id4  id5  id6g and b3   fid1  id3  id4  id5  id7  id8g  Individually  these blocks involve 35 comparisons  in total  However  b2 and b3 contain most of the comparisons in b1  they share four entities with b1   Merging b1 with b2 leads to the new block b 0 2   fid1  id2  id3  id4  id5  id6g  We now need 30 comparisons in total  between b 0 2 and b3   In addition  merging b 0 2 with b3 in a single block containing all 8 entities further reduces the total number of comparisons to 28  i e   20  less than the initial number of comparisons   More formally  the merge of two unilateral bilateral blocks is de   ned as follows  Definition 10  Given a set of unilateral blocks B  the unilateral merge of a block bi with a block bj is a new unilateral block bmi j that contains the union of the entities of bi and bj  bmi  j   bi   bj   Definition 11  Given a set of bilateral blocks B  the bilateral merge of a block bi  j with a block bk l is a new bilateral block Algorithm 4  Merging a mined set of blocks  Input  B a clean  mined set of blocks Output  B 00 the semantically equivalent  merged set of blocks 1 B 0   blockMapping B   2 FromMergesB       3 MergedB       4 OpenB       5 ProcessedB       6 for i   0 to spatialBlocks maxDimension do 7 NewOpenB   getBlocksStartingAtIndex i  B 0    8 NewOpenB   NewOpenB S FromMergesB  9 NewOpenB   NewOpenB n MergedB  10 NewOpenB   NewOpenB n OpenB  11 NewOpenB 0   blockScheduling NewOpenB   12 OpenB   blockScheduling OpenB S NewOpenB   13 FromMergesB       14 foreach bi 2 NewOpenB 0 do 15 mostS imilarBlock   getMostSimilarBlock bi   OpenB   16 if mostS imilarBlock   null then 17 newBlock   mergeBlocks bi   mostS imilarBlock   18 newBlock mapToCartesianSpace    FromMergesB add newBlock   19 MergedB add bi   20 MergedB add mostS imilarBlock   21 OpenB remove bi   22 OpenB remove mostS imilarBlock   23 OpenB addAll NewOpenB   24 EndingBlocks   getBlocksEndingAtIndex i  OpenB   25 OpenB removeAll EndingBlocks   26 ProcessedB addAll EndingBlocks   27 return ProcessedB  bmi k mj l   whose inner blocks constitute the unilateral merge of the corresponding inner blocks of bi  j and bk l  bmi k  mj l   fbi   bk   bj   blg  Typically  each block shares comparisons with many other blocks of the input set  However  these pairs of blocks di er in their degree of overlap  i e   the number of comparisons they share   In fact  the higher the Jaccard coe cient 2 of two blocks  the more comparisons their merge saves  Thus  to maximize the e ect of Block Merging  each block should be merged with the block that has the highest proportion of common entities with it  We call this block maximum Jaccard block  We can now formally de   ne the problem of Block Merging as follows  Problem 5  Block Merging   Given a mined set of blocks B  identify for each block bi its maximum Jaccard block bj   and merge these bi bj pairs  so as to produce a semantically equivalent set B 0 with a smaller number of redundant comparisons  We call B 0 a merged set of blocks  To address this problem  we introduce Algorithm 4  At its core lies the idea that each block should have overlapping spatial representations with its maximum Jaccard block  In other words  there is no point in estimating the Jaccard similarity between two blocks with disjoint spatial representations  Based on this principle  our algorithm works for unilateral blocks as follows  two main lists are maintained  the Open and the Processed ones  lines 4 and 5  respectively   The former contains the blocks that are available for comparison  whereas the latter encompasses the blocks that do not 2 The Jaccard similarity coe cient J A  B  of two sets A and B is equal to  J A  B    jA Bj jA Bj   a  0 1 2 3 4 5 b1 b b2 3 1 2 5 3 4 6  b  4 b3 3 5 5 4 6 1 2 3 b1 1 b2 2 2 3 4 0 1 2 3 4 5 1 Figure 2  Illustration of the block merge algorithm  need to be considered any more  Starting from the beginning of the X axis  the algorithm traverses the X axis by one point in every iteration  line 6   blocks  whose spatial representation starts from the current point  line 7   are compared with the blocks in the Open list  line 14   Then  at the completion of the iteration  they are added in it  together with the merged blocks  lines 23 and 8  respectively   Blocks  whose spatial representation ends at the current point  are placed in the Processed list  lines 24 26   The reason is that they do not overlap with any of the subsequently examined blocks  In the case of bilateral blocks  the only di erence in the execution of the algorithm is that it traverses both axes simultaneously  The following example illustrates the functionality of this algorithm  Example 5  Figure 2 a  is an illustration of the Block Merging algorithm for unilateral blocks that are mapped to the X axis  The main idea is that the grey area expands by one unit after each step  All blocks lying partly within it are in the OpenB list  while all blocks that lie entirely within it are placed in the ProcessedB list  At the highlighted Step 4  b2 and b3 lie in the former list  while b1 is placed in the latter  The execution of the algorithm is as follows  at Step 1  only b1 is in OpenB  while  at Step 2  b3 is also added and compared with b1  At Step 3  b1 is removed from OpenB  Then  b2 is placed in OpenB and compared with all other blocks  The remaining blocks  b2 and b3  are placed in ProcessedB at the end of Step 5  Figure 2 b  is an illustration of the block merge algorithm for bilateral blocks that are mapped to the XY space  Again  the grey area expands at each step by one unit  in both dimensions this time  All blocks lying partly within it are placed in the OpenB list  while all blocks that lie entirely inside its borders are moved to the ProcessedB list  In the depicted case  step 5   b1 1 lies in the ProcessedBlocks list  while b2 2 and b3 3 are in the OpenBlocks one  As mentioned in De   nition 5  the input to Block Merging is a mined set of blocks  that is  the input set contains neither identical nor dominated blocks  since they do not contribute non redundant comparisons  The computational cost of Algorithm 4 is thus significantly reduced  without a ecting its output  However  during the execution of Algorithm 4  blocks that belong to one of these categories can be produced  and should be discarded on the    y  Indeed  merges that lead to a block identical to another block of the input set are immediately removed  lines 9 10 Algorithm 4   Regarding new relations of dominance  merges can be involved in them only as dominant blocks  Otherwise  the original blocks that produce them would have already been dominated  More formally  bmi  j   bk    bi   bj    bk    bi   bk     bj   bk   Thus  only a block of the input set can be dominated by the merge of two other blocks  Apparently  it is not e cient to apply the Block Mining method after each iteration of Block Merging  Dominated blocks are  therefore  removed after the completion of the merging process  by comparing the original  non merged blocks with the set of merges  6  EVALUATION In this section we present a series of experiments that investigate the higher e ciency conveyed by our techniques  when incorporated into existing  redundancy bearing blocking methods  Our techniques were fully implemented in Java 1 6  and the experiments were performed on a server with Intel Xeon 3 0GHz  Metrics  Given that our techniques aim at eliminating redundant comparisons  we evaluate them on the basis of an established metric for measuring the e ciency of blocking methods  namely the Reduction Ratio  RR   1  3  18   It expresses the reduction in the number of pair wise comparisons required by a method with respect to the baseline one  Thus  it is de   ned as follows  RR   1  mc bc  where mc stands for the number of comparisons entailed by our technique  and bc expresses the number of comparisons entailed by the baseline  in our case  this is the original blocking method   RR takes values in the interval  0  1   for mc   bc   with higher values denoting higher e ciency  Recall  named Pair Completeness in the context of blocking  21   is not reported  since our techniques do not a ect the duplicates identi   ed by a blocking method   their goal is exclusively to detect and avoid the super   uous and repeated comparisons  Data Sets  To evaluate the impact of our techniques on existing blocking methods  we employ two real world data sets  one for the Clean Clean  and one for the Dirty Dirty and Dirty Clean cases of ER  They are DBPedia and the BTC09 data set  respectively  which are described in more detail in the following paragraphs  DBPedia Infobox Dataset  DBPedia   This data collection consists of two di erent versions of the DBPedia Infobox Data Set 3   They have been collected by extracting all name value pairs from the infoboxes of the articles in Wikipedia   s English version  at speci   c points in time  Although it may seem simple to resolve two versions of the same data set  this is not the case  More speci   cally  the older version  DBPedia1  is a snapshot of Wikipedia Infoboxes in October 2007  whereas the latest one  DBPedia2  dates from October 2009  During the two years that intervene between these two versions  Wikipedia Infoboxes were so heavily modi   ed that there is only a small overlap between their pro   les  even for duplicate entities  just 25  of all name value pairs are shared among the entities common in both versions  As matches  we consider those entities that have exactly the same URL in both versions  The attribute agnostic blocking method introduced in  21  was applied on this data set to produce the set of bilateral blocks we employ in our experiments  The blocks we consider are those resulting from the Block Purging step  21   involving 3 98 10 10 comparisons and PC   99 89   The technical characteristics of DBPedia are presented in Table 1  3 See http   wiki DBPedia org Datasets  DBPedia DBPedia1 Entities 1  190  734 DBPedia1 Name Value Pairs 17  453  516 DBPedia2 Entities 2  164  058 DBPedia2 Name Value Pairs 36  653  387 Bilateral Blocks 1  210  262 Comparisons 3 98   10 10 Av  Comparisons Per Block 32  893 Av  Blocks Per Entity 15 38 BTC09 Entities 1 82   10 8 Name Value Pairs 1 15   10 9 Unilateral Blocks 8 04   10 7 Comparisons 4 05   10 9 Av  Comparisons Per Block 50 37 Av  Blocks Per Entity 2 61 Table 1  Technical characteristics for both data sets  Billion Triple Challenge 2009  BTC09   This data set constitutes a publicly available 4   large snapshot of the Semantic Web  aggregating RDF statements from a variety of sources  In total  it comprises 182 million distinct entities  described by 1 15 billion triples  The ground truth set of duplicate entities is derived from the explicit as well as implicit equivalence relationships  i e   the owl sameAs statements and the Inverse Functional Properties  respectively   BTC09 was employed as a test bed for the attribute agnostic blocking method presented in  22   which is the source of our experimental set of unilateral blocks  well  Similar to DBPedia  the blocks we employ in our experiments are those stemming from the Block Purging method of  22   They entail 4 05   10 9 comparisons  exhibiting a PC very close to 90   A more detailed overview of this data set is presented in Table 1  6 1 Block Mapping In this section  we compare Algorithm 1 with the Random Mapping  RM  of blocks to the Cartesian space  As mentioned in Section 4 2  the higher the performance of a mapping method  the lower the sum of Formula 1 should be  Table 2 shows the outcome for both algorithms and for both data sets  In each case  we considered 100 iterations with RM in order to get a safe estimation of its performance  The sums in Table 2 is equal to the average value  It is remarkable that standard deviation is equal with  4 16 10 16 and  3 97   10 12 for DBPedia and BTC09  respectively  thus being 4 orders of magnitude lower than the mean value in both cases  This indicates that the performance of RM is relatively stable  We can see that Algorithm 1 substantially outperforms RM in both cases  for bilateral blocks  it improves RM by 30 34   whereas for unilateral blocks the enhancement is 25 28   The reason for the slightly lower improvement in the second case is twofold  First  redundancy is much higher in DBPedia than in BTC09  as documented in Table 1  in the former data set  an entity is associated with more than 15 blocks  in comparison with the less than 3 blocks for the latter  Thus  mapping an entity to a suboptimal  random  coordinate a ects the spatial deviation of more blocks in DBPedia than in BTC09  Second  suboptimal mappings have a larger impact in the two dimensional space than in the unidimensional one  Consider for example Figure 1  Assigning entity id4 to point 5 of the X axis instead of 4  increases the spatial deviation of b2 by 1  However  assigning entity id1 4 to point 5 of the Y axis instead of 4 increments the spatial deviation of b3 3 by 4  For these 4 See http   vmlion25 deri ie  Method DBPedia Sum BTC09 Sum Algorithm 1 0 81   10 20 1 33   10 16 Random Mapping 1 16   10 20 1 78   10 16 Table 2  Comparison of the Block Mapping technique with respect to the sum of Formula 1 for both data sets  Method DBPedia Comp  BTC09Comp  Cartesian Product 5 11   10 11 3 23   10 19 LSH   Algorithm 3 1 27   10 8 4 52   10 9 Table 3  Performance of the Block Mining algorithm  DBPedia BTC09 Method Comp  RR Comp  RR Input Set 3 98   10 10   4 01   10 9   Comp  Prop  2 59   10 9 93 49  3 08   10 9 23 19  Table 4  E ect of Comparisons Propagation on the e ciency of both data sets  reasons  the performance of RM is slightly closer to Algorithm 1 for unilateral blocks  6 2 Block Mining We now present the performance of our Block Mining algorithm  i e   LSH and Algorithm 3  with respect to the number of block comparisons it requires in order to examine the blocks inside each bucket  In general  to determine the structure of LSH  two parameters have to be speci   ed  L and k  the former denotes the total number of hash tables that will be employed  while the latter speci   es the number of hash functions that are merged to compose the hash signature for each entity  for a particular hash table  In our implementation  we followed  23  and used exclusive or for e   ciently merging the k hash functions into the composite signature  In more detail  L was set to 10  and k to 12  while the probabilities p1 and p2  see Section 5 2 2  were set to 0 9 and 0 1  respectively  Table 3 shows the performance of our method in comparison with the naive method of examining all possible pairs of blocks  The aim is not to examine the optimal con   guration of LSH  Rather  the main conclusion to be drawn from these numbers is that it is a scalable approach  whose performance depends on the level of redundancy of the underlying blocking method  the higher the redundancy  the more similar the blocks are between them  and the larger the corresponding buckets get  this results in more comparisons and lower e ciency  This explains why the performanc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>On k skip Shortest Paths ### Yufei Tao     Cheng Sheng     Jian Pei         Department of Computer Science and Engineering     School of Computing Science Chinese University of Hong Kong Simon Fraser University New Territories  Hong Kong Burnaby  BC Canada  taoyf  csheng  cse cuhk edu hk jpei cs sfu ca ABSTRACT Given two vertices s  t in a graph  let P be the shortest path  SP  from s to t  and P   a subset of the vertices in P  P   is a k skip shortest path from s to t  if it includes at least a vertex out of every k consecutive vertices in P  In general  P   succinctly describes P by sampling the vertices in P with a rate of at least 1 k  This makes P   a natural substitute in scenarios where reporting every single vertex of P is unnecessary or even undesired  This paper studies k skip SP computation in the context of spatial network databases  SNDB   Our technique has two properties crucial for real time query processing in SNDB  First  our solution is able to answer k skip queries signi   cantly faster than    nding the original SPs in their entirety  Second  the previous objective is achieved with a structure that occupies less space than storing the underlying road network  The proposed algorithms are the outcome of a careful theoretical analysis that reveals valuable insight into the characteristics of the k skip SP problem  Their ef   ciency has been con   rmed by extensive experiments with real data  Categories and Subject Descriptors H3 3  Information search and retrieval   Search process General Terms Algorithms Keywords k skip  shortest path  road network 1 ###  INTRODUCTION Finding shortest paths  SP  in a graph is a classic problem in computer science  Formally  let G    V   E  be a graph where V is a set of vertices and E a set of edges  Each edge carries a non negative weight  De   ne the length of a path P  represented as  P   to be the total weight of the edges in P  Given two vertices s  t     V   a SP query returns the path from s to t with the minimum length  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  The SP problem has received considerable attention from the research community in the past few years  see the recent work  4  24  27  and the references therein   due to its profound importance in Spatial Network Databases  SNDB   An SNDB manages geometric entities positioned in an underlying road network  and supports ef   cient data retrieval with predicates on network distances  and optionally also on spatial properties  a representative system is Google Maps   A standard modeling of a road network is a graph where each vertex corresponds to a junction and each edge represents a road segment  An edge   s weight is often set to the length of the corresponding road segment or the average time for a vehicle to pass through the segment  Traditionally  a SP query retrieves every vertex on the shortest path P  which is sometimes unnecessarily detailed in practice  Consider  for example  that a person leaves home for a retreat destination  Typically  the SP would    rst wind through her his neighborhood R1  continue onto a set of highways R2  and eventually enter the neighborhood R3 of the destination  The region  in which    nedetailed directions are imperative  is R3  In R1 and R2  it is often suf   cient to guide the user at a coarse level  in a manner similar to putting sign posts along the way  for example  by naming some streets to be passed  and the highways to be taken in succession  In fact  even the computation of turn by turn driving directions does not always demand all the vertices on P  This is because P may contain vertices at which no turning is needed  To illustrate  assume that P stays on the Main Street in an urban area for one kilometer  during which the street intersects another street every 100 meters  Each of those 10 intersections is a vertex in P  but only the    rst and last of them are enough to generate the instructions for turning into and away from the Main Street  respectively  The situation is similar if P involves a long highway  in which the vertices between the points where P enters and leaves the highway respectively can be ignored  In this paper  we are interested in computing a subset  say P     of the vertices in P  Instead of an arbitrary subset  however  we demand that P     be k skip shortest path  namely  it should contain at least one vertex out of every k consecutive vertices in P  Figure 1 shows an example with k   4  where P consists of all the  black and white  vertices while P   only the black ones  Note that there can be more than one black vertex in every 4 consecutive vertices in P  but it is impossible to have none  P   succinctly describes P by sampling its vertices with a rate of at least 1 k  Such a sample set can replace P in answering queries like  what are the highways  alternatively  neighborhoods or cities  that P needs to go through  P   is equally useful in discovering the gas stations  similarly  attractions or hotels  along P  becauses t Figure 1  The black vertices constitute a 4 skip shortest path it is often suf   cient to    nd the stations close to the vertices in P     as opposed to all the vertices of P  This is the idea of  18  in approaching the continuous nearest neighbor problem  Moreover  P   is also adequate for estimating various statistics about P  as  in the query     nd the percentage of dessert coverage in the route from Los Angeles to Las Vegas  Finally  P   is exactly what is needed to plot the original P in a digital map with a decreased resolution  For example  at Google Maps  only a subset of the vertices on a path need to be displayed according to the current zooming level  For traditional mapping purposes  P   has two notable advantages over P  First  it is  much  smaller in size  and hence  requires less bandwidth to transmit  This is a precious feature in mobile computing that will especially be appreciated by users charged by the amount of Internet usage  Second  using a clever algorithm  P   can be faster to compute because  intuitively  fewer vertices need to be retrieved than P  Such an ef   ciency gain provides valuable opportunities for in car navigation devices and routing websites such as Google Maps to support a great variety of on route services in shorter time  The concept of k skip SP comes with a zoom in operation  Given consecutive vertices u  v on P     the operation    nds the missing vertices on P from u to v  As there are at most k     1 missing vertices  for small k a zoom in incurs negligible time  because it only needs to compute a very short path  This adds a nice pay as you go feature in applying k skip SP  First  a driver can request the least zoom in   s to complete the part of the route outside her his knowledge  This allows her him to pay for the most useful information only  Second  an algorithm preparing turn by turn directions can zoom in only when necessary  i e   a turning may exist between two adjacent vertices P      thus saving the cost of locating the skipped vertices  Contributions  Somewhat surprisingly  the notion of k skip SP  or in general the idea of sampling  the vertices in  a SP  has not been mentioned previously to the best of our knowledge  let alone any algorithm dealing with the problem of k skip SP computation  We    ll the gap with a systematic study of this problem  In particular  our objectives are twofold  1  Resolve a k skip query signi   cantly faster than calculating the original SP in entirety  2  Achieve the previous purpose with a data structure that occupies less space than storing the input graph G    V   E  itself  This permits the structure to reside in memory even for the largest road network  as is crucial for real time query processing in SNDB  The    rst contribution of this paper is to formally establish the fact that  only a small subset V   of the vertices in G need to be considered to attack the k skip problem  In other words  the other vertices in V  V   are never needed to form any k skip SP  Referring to V   as a k skip cover  we show that there always exists a V   with size roughly proportional to  V   k  This theoretical    nding is of independent interest because it is not limited to road networks  but actually holds for general graphs  Our second contribution is a full set of algorithms required to process k skip SP queries within a stringent time limit  Speci     cally  these algorithms settle three sub problems   i     nd a small V   in time linear to the complexity of G   ii  construct from V   a space economical structure  and  iii  answer a k skip query by accessing only a small portion of the structure  We thoroughly evaluated these algorithms with extensive experiments on real data  including the massive road network of the US which has about 24 million vertices and 58 million edges  Our results prove that the proposed technique very well satisfy both requirements mentioned earlier in all settings  Roadmap  The next section reviews the literature of SP computation  Section 3 formally de   nes the target problem  and gives an overview of our solutions  Section 4 elaborates the theoretical results on k skip covers  and the algorithms for constructing the proposed structure  Section 5 clari   es how to answer a k skip query and perform a zoom in ef   ciently  Section 6 contains the experimental results  while Section 7 concludes the paper with directions for future work  2  RELATED WORK There is an extremely rich literature on the SP problem  In Section 2 1  we clarify the details of the reach algorithm  which is the state of the art for SP computation in road networks  Section 2 2 surveys the other recent progress in the database and theory communities  2 1 Dijkstra and reach To facilitate discussion  given two vertices u  v in the input graph G    V   E   we denote by S P u  v  the SP from u to v  The length of S P u  v   i e    S P u  v    is called the distance between u and v  If  u  v  is an edge in G  we represent its weight as   u  v   In case G is directed   u  v  is an edge from u to v  and  v u  means the opposite  To avoid unnecessary distraction  our examples use undirected graphs  but all the notations in our description are carefully written so that they are also correct for directed graphs  Dijkstra  Let us    rst review the Dijkstra   s algorithm  5   which is the foundation of the reach method described shortly  Dijkstra    nds S P s  t  by exploring the vertices in ascending order of their distances to the source s  At any time  each vertex u     V has a status chosen from  unseen  labeled  scanned   Furthermore  u is associated with a label l u  equal to the distance of the SP from s to u found so far  i e   an even shorter path may be discovered later   On this path  the vertex immediately preceding u is kept in pred u   referred to as the predecessor of u  At the beginning of Dijkstra  all vertices u     V have status unseen  l u        and pred u         The only exception is s  whose status is labeled  with l s    0 and pred u         At all times  the vertices of status labeled are managed by a min heap Q  using their labels as the sorting key  In each iteration  the algorithm  i  de heaps the vertex u     Q with the minimum l u    ii  relaxes all edges  u  v  such that the status of v is not scanned  and  iii  changes the status of u to scanned  Speci   cally  relaxation of  u  v  involves the following steps  relax  u  v  1  if the status of v is unseen then 2  l v      l u      u  v  3  the status of v     labeled 4  else if l v    l u      u  v  thena b c d e f g 4 5 5 5 1 3 3  4 5 s t 1 1  a  The input graph a b c d e f g s t 1 0 6 5 6 0 1 0 0  b  Global reaches of the vertices Figure 2  Illustration of reach 5  l v      l u      u  v  6  pred v      u The present l u  is guaranteed to be  S P s  u    The algorithm terminates as soon as t  the destination  is selected by an iteration  To illustrate  assume that we want to compute S P s  t  in Figure 2a  In the    rst iteration  Dijkstra scans s and relaxes  s  a   after which Q    a  and l a    1  The next iteration de heaps a and relaxes  a  b   a  c   Note that  a  s  is not relaxed because the status of s has become scanned  Now Q contains b  c with labels 5  6  respectively  The algorithm then scans b and relaxes  b  d   which labels d with 10  This is followed by de heaping c  and relaxing  c  d   c  e   Note that the relaxation of  c  d  decreases l d  to 9  The rest of the algorithm proceeds in the same manner  By tracing the execution  one can verify that  at termination  all the vertices except t have already been scanned  Dijkstra can be implemented in O m  n log n  time  3   where n  m are the number of vertices and edges  respectively  i e   n    V    m    E    In a road network  each vertex has an O 1  degree  so the time complexity is essentially O n log n   Bi directional  Dijkstra starts a graph traversal from s and gradually approaches t  call this a forward search  Immediately by symmetry  the SP problem can also be solved by a backward search from t to s  if G is directed  the backward search implicitly reverses the direction of each edge   The bi directional algorithm  10  20  achieves better ef   ciency by performing both searches synchronously  the effect of which is essentially to explore the vertices u     V in ascending order of rs t u   where rs t u    min  S P s  u     S P u  t      1  In fact  if u is closer to s  than to t   then it will    rst be touched in the forward search  otherwise  the backward search will    nd it    rst  The forward backward search proceeds as in the standard Dijkstra   s algorithm  as if the other search did not exist   Let Qf  Qb  be the heap of the forward  backward  direction  Synchronization is carried out by advancing in each iteration the direction that has a smaller label at the top of the heap  Bi directional monitors the distance    of the shortest s to t path found so far     is set to     initially  and may be updated when the forward search  the backward case is symmetric  de heaps a vertex u whose status in the backward direction is labeled 1   Specifically  at this time  the algorithm computes        lf  u    lb u   1 This status cannot be scanned  otherwise  the algorithm would have terminated right after u was de heaped by the forward search  as will be clear shortly  s t Figure 3  Comparison of Dijkstra  bi directional  and reach where lf  u  and lb u  are the labels of u in the forward and backward search  respectively     is reduced to      if              since it implies the existence of a shorter s to t path that concatenates S P s  u    u  v   and S P v t   where v is the current predecessor of u in the backward search  The algorithm terminates when either direction de heaps a vertex already scanned in the other direction  Let us demonstrate bi directional on the graph in Figure 2a  After three iterations of each direction  which are the same as in Dijkstra  the forward  backward  search has scanned s  a  b  t  g  f   such that Qf  Qb  contains vertices c  d  e  d  with labels 6  10  respectively  Currently            Next  the forward search de heaps c     Qf and relaxes edges  c  d    c  e   after which lf  e     7  lf  d    9  Similarly  the backward search then de heaps e     Qb  As the status of e in the forward direction is labeled  the algorithm updates    to lf  e   lb e    7   6     13  before it relaxes  e  c   e  d   The forward search continues by de heaping e     Qf   which  however  has been scanned in the backward search  The algorithm therefore terminates  without deheaping d in either direction  Reach  Intuitively  if    is the length of S P s  t   Dijkstra searches a ball that centers at s  and has radius     shown as the dashed circle in Figure 3  Bi directional  on the other hand  explores two balls with radius    2 that center at s  t respectively  the two solid circles in Figure 3   The reach algorithm  which is the current state of the art  dramatically shrinks the search area to a small dumb bell shape  as illustrated by the shaded region in Figure 3  Let us start the explanation with the notion of local reach  Let u be a vertex on S P s  t   The local reach of u in S P s  t  equals rs t u  as calculated in Equation 1  Note that this notion relies on a particular SP  Any vertex v      S P s  t  has no local reach de   ned in S P s  t   Now we extend the de   nition to global reach  DE FI N I T I O N 1  GL O BA L  R E AC H   1 2       The global reach  denoted as r u   of a vertex u is the maximum local reach of u in all the shortest paths that pass u  Formally  r u     max s t u   S P  s t  rs t u    2  The reach r u  can be understood intuitively as follows  If u is on S P s  t   then either s or t must have distance at most r u  to u  In other words  in case neither s nor t is within distance r u  from u  u can be safely pruned in computing S P s  t   Consider  for instance  vertex c in Figure 2a  To decide its global reach r c      rst collect the set S of all the SPs that pass c  namely  S    S P s  t   S P s  g        SP a  e         We then calculate the local reach of c in each SP of S  For example  its local reach in S P a  e  is min  S P a  c     S P c  e      1  The    nal r c equals the maximum of all the local reaches  which can be veri   ed to be 6  as is its local reach in S P s  t    Figure 2b shows the global reaches of all the vertices  In computing S P s  t   the reach algorithm  10  12  proceeds in the same way as bi directional  but may prune a vertex in relaxing an edge  u  v   Without loss of generality  suppose that the relaxation takes place in the forward search  the backward case is symmetric   This implies that the forward search just scanned u  but has never scanned v  otherwise the edge would not have been relaxed   The pruning of reach takes place as follows  RU L E 1  Vertex v is pruned if both of the following hold  1  v has status labeled or unseen in the backward direction 2  r v    lf  u  where lf  u  is the label of u in the forward search  In general  reach can be understood as the execution of bidirectional on the vertices that survive pruning  In Figure 2  for example  reach    nds S P s  t  by scanning only s  a  c  e  g  t  As in bi directional  reach    rst scans s  t  in the forward  backward  direction  after which Qf  Qb  includes a  g  with label 1  Next  the forward search de heaps a from Qf   and relaxes  a  b   a  c   The relaxation of  a  b  prunes b by Rule 1 because r b     0   lf  a     1  The backward search then eliminates f in a similar fashion  The rest of the algorithm proceeds as in bi directional  Vertex d does not need to be scanned for the reason explained earlier for bi directional  The space consumption of reach is very economical because  except G itself  only one extra value needs to be stored for each vertex  However  it can be expensive to calculate the exact reach of every vertex  Fortunately  there are fast algorithms  10  12  to obtain approximate reaches that are guaranteed to upper bound their original values  Pruning remains the same except that r u  should be replaced by its upper bound  The outstanding ef   ciency of this algorithm on road networks has been theoretically justi   ed  1   2 2 More results on SP computation The A   algorithm  13  is a classic SP solution that captures Dijkstra as a special case  The effectiveness of A   relies on a method to calculate  typically in constant time  a lower bound of  S P u  v   for any two vertices u  v  Apparently  0 can be a trivial lower bound  but in that case A   degenerates into Dijkstra  In general  the tighter the lower bounds are  the faster A   is than Dijkstra  Motivated by this  Agrawal and Jagadish  2  proposed to precompute the distances between each vertex and a special set of vertices called landmarks  In answering a SP query  those distances are deployed to derive lower bounds based on triangle inequality  This idea is also the basis of the ALT algorithm developed by Goldberg and Harrelson  9   which has lower query time  than  2   at the tradeoff of consuming more space  The experiments of  10  indicate that ALT is slower than the reach method described in Section 2 1  Note  however  that ALT and reach are compatible  in the sense that their heuristics can be applied at the same time to maximize ef   ciency  10   Sanders and Schultes  8  25  26  presented a highway hierarchy  HH  technique  whose rationale is to identify some edges that mimic the role of highways in reality  In computing a SP  the algorithm of HH modi   es Dijkstra so that the search can skip as many non highway edges as possible  and thus  terminate earlier  Based on the empirical evaluation of  10  25   HH has comparable performance with respect to reach  The separator technique is another popular approach  15  16  17  to preprocess a graph G for ef   cient SP retrieval  The idea is to divide the vertices of G into several components  and for each component  extract a set of boundary vertices such that the SP between two vertices in different components must go through a boundary vertex  Query ef   ciency bene   ts from the fact that  most computation of a SP can be restricted only to the boundary vertices  According to  25   however  separator based methods are not as ef   cient as HH on road networks  Another disadvantage is that these methods often have expensive space consumption  For example  the space of  15  is at the order of n 1 5  where n is the number of vertices   which is prohibitive for massive graphs  In  28   Wagner et al  described a geometric container technique  which associates each edge  u  v  in G with a geometric region  The region covers all the vertices t such that the SP from u to t goes through v  In running Dijkstra  such regions can be used to prune many vertices that do not appear in the SP  Lauther  19  integrated a similar idea with the bi directional algorithm discussed in Section 2 1  Samet et al   24  proposed to break the geometric regions into smaller disjoint pieces that can be indexed by a quadtree  Their solution lowers the cost of SP calculation  compared to  19  28    but entails O n 1 5   space  A common shortcoming of  19  24  28  is that their preprocessing essentially determines the SPs between all pairs of vertices  The all SP problem is notoriously dif   cult  Even on planar graphs  the fastest solution requires O n 2   time  7   which is practically intractable for large n  Recently  Sankaranarayanan et al   27  proposed a path oracle that is constructed from well separated pair decomposition  and can be used to accelerate SP retrieval  Such an oracle consumes O s 2 n  space in 2 d space  where s is shown to be around 12 for practical data  Xiao et al   30  showed that SP queries can be accelerated by exploiting symmetry in the data graph  Their approach  however  is limited to the case where all edges have a unit weight  Wei  29  developed an alternative solution by resorting to tree decomposition  23   Rice and Tsotras  22  studied the shortest path problem with label restrictions  Some other work considers how to estimate shortest path distances  e g    11  21   We emphasize that all the works aforementioned calculate traditional  complete  SPs  The concept of k skip SP has not appeared previously  and is formalized in the next section for the    rst time in the literature  3  K SKIP SHORTEST PATHS For simplicity  we assume that the data graph G    V   E  is undirected  and will discuss directed graphs only when the extension is not straightforward  The following de   nition formalizes kskip SP  DE FI N I T I O N 2  k S K I P  S H O RT E S T  PAT H   Let S P s  t  be the shortest path from source s to destination t  Label the vertices in S P s  t  in the order they appear  v1  v2       vl  i e   v1   s  vl   t   where l is the total number of vertices in the path  If l     k  let P   be an ordered subset of  v1       vl   where the ordering respects that in S P s  t   P   is a k skip shortest path from s to t if P        vi       vi k   1          3  for every 1     i     l     k   1 a b c d e f a b c d e f     3 3 3 3 3 3 2  a  Black vertices  b  3 skip graph constitute a 3 skip cover Figure 4  Preprocessing for 3 skip SP computation Even for    xed s and t  there can be multiple P   satisfying the above condition  In other words  k skip SPs are not unique  although all of them have to be subsets of S P s  t   Our objective is to preprocess G into a space economical structure such that  given any s  t at run time  we can    nd a k skip SP between any s and t ef   ciently  The    rst step of our preprocessing is to eliminate the redundant vertices in G  The intuition is that  since most vertices in a SP need not be reported under the k skip de   nition  certain vertices would end up being never returned  For example  consider the graph G in Figure 4a where all edges have a unit weight  Observe that  for k   3  any SP with 3 vertices must contain at least one black vertex  In other words  it suf   ces to form k skip SPs using just the black vertices  whereas the other  white  vertices can be discarded  We refer to the set of black vertices in the above example as a 3 skip cover  The next de   nition generalizes the notion to kskip cover  which contains a subset of the vertices that need to be considered for k skip SPs  DE FI N I T I O N 3  k S K I P  C OV E R   Let V   be a subset of the vertices in G  V   is a k skip cover if it has the property that  for arbitrary s  t     V such that S P s  t  has at least k vertices  V       S P s  t  is a k skip SP from s to t  after ordering the vertices of V       S P s  t  appropriately  As will be clear in Section 5  the ef   ciency of our k skip SP algorithm crucially depends on the fact that  it does not need to process the vertices of V   V     Hence  the minimization of  V     is essential  but it turns out to be rather challenging on an arbitrarily complex G  In fact  it is non trivial even if one simply wants to know whether a small V   always exists  These issues will be resolved in the next section  We want to capture the adjacency of the vertices in V   as far as k skip SPs are concerned  For example  the black vertex a in Figure 4a can be consecutive only to e and f in a 3 skip SP  observe that e and f have blocked all the possible ways that can lead a to any other black vertex   We represent this by generating two    super edges    that link a to e  f respectively  After this  the original edges  of G  incident on a can be ignored in computing any k skip SP beginning from a  Let us formalize the rationales behind the preceding paragraph in the next two de   nitions  DE FI N I T I O N 4  k S K I P  N E I G H B O R   Let u  v be two vertices in a k skip cover V     We say that v is a k skip neighbor of u if S P u  v   namely  the shortest path from u to v in G  does not pass any other vertex in V     It is easy to see that the SP from u to any of its k skip neighbor v can have at most k edges  In an undirected G  the relation implied by the above de   nition is symmetric  i e   u is a k skip neighbor of v if and only if v is a k skip neighbor of u  This is not necessarily true for directed graphs  In any case  let Nk u  be the set of all k skip neighbors of u  For instance  in Figure 4a  N3 a     e  f   DE FI N I T I O N 5  k S K I P  G R A P H   A k skip graph of G is a graph G      V     E      where     V   is a k skip cover of G      for every vertex u     V     E   has an edge  u  v  for each k skip neighbor v of u  namely  v     Nk u   The weight of  u  v      E   is  S P u  v    We call each edge  u  v      E   a super edge  Figure 4b demonstrates the 3 skip graph G   of the graph G in Figure 4a  The weight of  a  e  in G   equals 3  which is the distance of a and e in G  As shown later     nding a k skip SP on the original graph G can be reduced to computing a  traditional  SP on a k skip graph G     In the next section  we will delve into the properties of G     and give a method to construct it ef   ciently  Then  the reduction and the accompanied algorithms will be elaborated in Section 5  4  K SKIP GRAPH The effectiveness of our pre computed structure  namely a kskip graph G      V     E      relies on the size of the k skip cover V     No performance gain would be possible if a small V   could not be guaranteed  Fortunately  we will show that such a good V   always exists  Section 4 1   Sections 4 2 and 4 3 then elaborate the algorithms for building G     4 1 Size of k skip cover This subsection will establish  TH E O R E M 1  Let G    V   E  be a graph with n    V   vertices  For any 1     k     n  G has a k skip cover V   of size O  n k log n k    Note that the above result applies to all graphs  that is  regardless of whether they are directed  planar  etc  Our proof leverages the theory of Vapnik Chervonenkis  VC  dimension  which quanti   es how decomposable a search problem is  Speci   cally  let D be a dataset and Q be a set of queries that can be issued on D  Given a query q     Q  de   ne q D  to be the result of q on D  A shatterable set S     D is such that  for any S       S  there is always a query q     Q with q D      S   S     In other words  any subset S       S needs to have the property that  a query q     Q retrieves all the items of S     and nothing from S   S    the result of q  however  may also include items from D   S   The VC dimension of  D  Q  is the size of the largest shatterable subset of D  Note that the VC dimension is not de   ned on a dataset  but instead  on a pair of a dataset and a query set  Now let us analyze the VC dimension of the SP problem  Here  we have an input graph G    V   E   The D mentioned earlier corresponds to V   A SP query qs t is uniquely de   ned by a source vertex s and a destination vertex t  Hence  the result qs t V   of qs t is S P s  t   Let Q be the union of all the possible SP queries  namely  Q     s t   V qs t  thus   Q    n 2    Next  we give a crucial lemma LE M M A 1  For any graph G  the VC dimension of  V   Q  is 2  PRO O F  Assume for contradiction that the VC dimension d of  V   Q  is at least 3  note that d must be an integer   Hence  there is a shatterable set S with size d  which belongs to the SP returned by a query q     Q  Let u1  u2       ud be the vertices of S ordered by their appearance on the SP  Hence  u2 is on the SP from u1 to ud  In this case  however  S      u1  ud  cannot be in any SP that does not contain u2  contradicting the fact that S is shatterable  It remains to verify that the VC dimension can be 2  We omit the details as this is trivial  The rest of the proof  for Theorem 1  concerns   net  Let D  Q be as de   ned earlier in our introduction to VC dimension  A set S     D is an   net of  D  Q  if S     q D         for any q satisfying  q D         D   In other words  if q retrieves no less than   D  items  at least one of these items must appear in S  The lemma below draws the correspondence between   net and k skip cover  LE M M A 2  A   k n   net V   of  V   Q  is a k skip cover of G  PRO O F  Let Q   be the set of queries q     Q such that the SP q V   returned by q has exactly k vertices  By the de   nition of  k n  net  for each q       Q     V       q    V           Now consider a query q     Q   Q   whose result q V   has more than k vertices  Clearly  any sub path of q V   including k vertices is the result q    v  of some q       Q     from which V   contains at least a vertex  Therefore  V   is a k skip cover  The   net theorem  14  dictates that any  D  Q  with VC dimension d has an   net of size O  d   log 1      This  combined with Lemmas 1 and 2  gives Theorem 1  Remark 1  Effectively  the proof of the   net theorem  14  shows that a random subset of D with size O  d   log 1     is an   net with high probability  Hence  we can    nd a k skip cover V   by simply taking O  n k log n k   vertices from V randomly  Remark 2  There is a trivial lower bound of n k on the size of a kskip cover  Hence  the upper bound in Theorem 1 is asymptotically tight  up to only a logarithmic factor  4 2 Computing a k skip cover As mentioned in the previous subsection  a k skip cover V   can be obtained by taking O  n k log n k   random vertices from V   It is well known that randomized techniques generally work much better in practice than predicted by theory  Therefore  the V   of a real graph would be much smaller  rendering a sample set of size O  n k log n k   most probably unnecessarily large  Of course  we could arti   cially reduce the number of samples  but a good sample size appears to be dif   cult to decide  A large size gives only marginal improvement  whereas a small size has the risk that the sample set may no longer be a k skip cover  We propose an adaptive sampling  AS  algorithm to resolve the above issue  Before explaining the algorithm  we need a few more de   nitions  Let the    hop neighbor set of a vertex u     V   denoted as V   u   be the set of all vertices v     V that can be reached from u by crossing at most    edges  Each v     V   u  is called a    hop neighbor of u  For example  assume an input graph G as shown in Figure 5a  where all edges  except those explicitly labeled  have weight 1  The 2 hop neighbor set V2 u  of u contains all the vertices in the shaded diamond  Note that V2 u  is decided without taking the edge weights into account  Denote by G   u  the graph induced by V   u   which is referred to as the    hop vicinity of u  That is  G   u  includes all and only the edges in G that are between the vertices of V   u   For instance  in Figure 5a  the edges of G2 u  are those that fall entirely in the diamond  By computing the SPs inside G   u  from u to all the other vertices in V   u   that is  each SP uses only the edges of G   u    one obtains a    hop shortest path tree T   u  rooted at u  For every vertex v in G   u   the u to v path in T   u is the SP from u to v inside G   u   Figure 5b demonstrates the 2 hop SP tree T2 u  of u  It is worth noting the difference between a SP inside the tree and a SP in the whole graph  For example  the u to c path in T2 u  has length 5  Although this is the shortest when only the edges of G2 u  are considered  there is an even shorter path u     f     e     d     c which uses two edges  e  d   d  c  outside the 2 vicinity of u  We now arrive at the most crucial concept underlying the AS algorithm  Let V   be a subset of the vertices in G  We say that a    hop SP tree T   u  is covered by V     if every u to v path in T   u  having at least    edges goes through at least one vertex in V     where v is a vertex in T   u   In Figure 5a  for example  let V   be the set of black vertices  Then  the T2 u  in Figure 5b is not covered because the path u to b has 2 edges but passes no black vertex  The next lemma gives an important property  LE M M A 3  V   is a k skip cover if it covers the Tk   1 u  of all u     V   PRO O F  Assume for contradiction that a V   ful   lling the ifcondition of the lemma is not a k skip cover  It follows that there exist two vertices u  v     V such that S P u  v  has k vertices none of which is in V     Since S P u  v  has k   1 edges  by the construction of Tk   1 u   we know  i  all the vertices on S P u  v  are in Tk   1 u   and  ii  the u to v path in Tk   1 u  is exactly S P u  v   This means  however  that Tk   1 u  has not been covered by V     thus reaching a contradiction  We are ready to clarify the AS algorithm  adaptive sampling 1  V         2  randomly permutate the vertices of V 3  for each vertex u     V 4  if Tk   1 u  is not covered by V   then 5  add u to V   The correctness of the algorithm follows from Lemma 3  and the fact that  if Tk   1 u  has not been covered by V   yet  including u itself to V   immediately makes Tk   1 u  covered  For instance  as shown earlier  the T2 u  in Figure 5b is not covered  but it will be  once u is added to V     Heuristic  We can further reduce the size of V   by    rst sorting the vertices of V in descending order of their degrees  and then  randomly permute the vertices with an identical degree  This increases the chance of sampling a vertex with a higher degree  which is bene   cial because such a vertex tends to lie on more SPs  and therefore  have stronger covering power  Time complexity  We close the subsection by analyzing the execution time of the algorithm  assuming that each vertex has an O 1  degree  which is true in road networks  The randomization at Line 1 can be implemented in O n  time  6  where n    V    the sorting in the high degree favoring heuristic can be done in O n 4 5 2 2 2 2 2 2 2 2 2 u a b e c d f 2     u a b e f c 2  a  2 hop vicinity of u and its  b  2 hop SP tree of u Figure 5  Deciding whether to sample u into a 3 skip cover time when there are only a constant number of possible degrees   The    hop vicinity of a node u can be found by a standard breath    rst traversal  BFT  initiated at u  which terminates in O      u   time where      u  is the number of    hop neighbors of u  i e        u     V   u    Then  the    hop SP tree T   u  can be extracted from G   u  using the Dijkstra   s algorithm in O      u  log      u   time  Checking whether T   u  is covered by V   demands a single traversal of T   u  in O      u   time  Hence  the total cost of the algorithm is O n    k   1 log     k   1   where     k   1 is the average number of  k     1  hop neighbors of the vertices in V   The value of     k   1 depends on the structure of the road network  but not the number n of the vertices  For example  consider two simple road networks that are a 100    100 and 1000    1000 grid  respectively  Although the second grid has 100 times more vertices  the two grids have the same     k   1      k 2    Hence  when k is far smaller than n  O n    k   1 log     k   1  grows linearly with n  4 3 Computing a k skip graph Recall that the goal of our preprocessing is to produce a k skip graph G      V     E      V   is simply a k skip cover  whose derivation has been clari   ed in Section 4 2  Next we complete the puzzle by explaining the derivation of E     Our discussion concentrates on the subproblem that  given a vertex u     V     how to calculate the set Nk u  of its k skip neighbors  De   nition 4   Once this is done  it is trivial to obtain E   according to De   nition 5  because we only need to add to E   a super edge from each u to every vertex v     Nk u   We will call each vertex of V   a sample from now on  due to the sampling nature of the AS algorithm  A naive approach to acquire Nk u  is to    rst    nd the SP from u to every other sample v     V     and then add v to Nk u  if the path passes no other sample  However  since  V         n k  see Remark 2 of Section 4 1   doing so for all u     V   would incur     n 2  k  time  which is prohibitive for large n  We circumvent the performance pitfall by aiming at a superset Mk u  of Nk u   As will be clear shortly  despite that Mk u  may result in a G   with more edges  it has the advantage of being computable in signi   cantly less time  thus allowing our technique to support gigantic graphs  Mk u  can be conveniently de   ned by recycling the notations of the previous subsection  Let us    rst have the k hop SP tree Tk u  of u  as opposed to the Tk   1 u  in the AS algorithm   Then  Mk u  includes all the samples v    u in Tk u  such that the uto v path in Tk u  does not pass any other sample  For illustration  Figure 6a shows a 3 skip cover V    the black vertices  on the data graph of Figure 5a  To determine M3 u   we    rst extract the 3  hop SP tree T3 u  as in Figure 6b  Then  it becomes clear that u a b e c d f g h 5 5 2 2 2 2 2 2 2 2 2 2 u a b e c d f g h  a  3 hop vicinity of u  b  3 hop SP tree of u Figure 6  Deciding M3 u  M3 u     b  f  g  h   Note that a  for example  is not in M3 u  due to the presence of b that blocks the path from u to a  The lemma below establishes the relationship of Mk u  and Nk u   LE M M A 4  Nk u      Mk u   PRO O F  By De   nition 4  the fact v     Nk u  implies that  i  S P u  v  does not pass any other sample  and  ii  S P u  v  has no more than k edges  Property  ii  indicates that all the vertices of S P u  v  must be in the k hop vicinity of u  and hence  are present in Tk u   It follows that the u to v path in Tk u  is exactly S P u  v   Property  i  further shows that the path cannot contain any other sample  Therefore  by the construction of Mk u   we know v     Mk u   We call Mk u  a super neighbor set of u  After acquiring it  we create a super edge  u  v  in E   from u to each vertex v     Mk u   and set its weight to the length of the u to v path in Tk u   In other words  the super edge represents a path in the original graph  The    nal E   is complete after carrying out the above procedure for all the vertices u     V   Time complexity  After Tk u  is ready  Mk u  can be easily obtained by a single traversal of Tk u  in O   k u   time  where   k u  is the number of the k hop neighbors of u  i e   the number of vertices in Tk u    A straightforward adaptation of the analysis in Section 4 2 shows that the cost of processing all u     V   is O n    k log     k   where     k is the average number of khop neighbors size of the vertices in V   For k far lower than n  O n    k log     k  is linear in n  due to the reasons explained at the end of Section 4 2  5  QUERY ALGORITHM Given vertices s  t in the data graph G  next we will explain how to obtain a k skip SP from s to t using the k skip graph G   precomputed  Section 5 1    rst gives an overview of our algorithm  A part of the algorithm needs to retrieve a traditional SP from G     for which Section 5 2 presents an improved version of the reach method  Finally  Section 5 3 will discuss how to perform a zoomin operation ef   ciently  5 1 High level description Recall that the vertex set of G   is a k skip cover V     The task of k skip SP calculation is simple when both s and t are samples  namely  they belong to V     In this case  we only need to    nd thes t Mk s  Mk t  G  Figure 7  In place sampling of s and t SP from s to t in G     that is  traveling only on the super edges  By the de   nition of G     this SP is guaranteed to be a k skip SP in the original graph G  Let us focus on the situation where neither s nor t is a sample  Our solution is to sample them into G   right away  so that the case can be converted to the previous scenario where s and t are samples  The inclusion of s  t as samples is temporary  after query processing  they will be removed from G     whose size therefore does not change  Incorporation of s  t in G   involves two steps  First  s and t are inserted in V     Second  some super edges are created to re     ect the appearance of s  t  in the same way the existing superedges are computed  That is  given s  similarly for t   we    rst obtain the super neighbor set Mk s  of s  and then add to E   a super edge from s to each u     Mk s   all exactly as described in Section 4 3  This process is illustrated in Figure 7  The analysis of Section 4 3 shows that  the above strategy runs in O   s k  log   s k    t k  log   t k   time  Remember that   s k   the number of k hop neighbors of s  is low when k is small  similarly for   t k    Hence  sampling in place s and t incurs insigni     cant overhead  Let G   s t be the resulting G   with the new super edges  and S P    s  t  be the SP from s to t on G     The rest of our algorithm returns directly S P    s  t   whose correctness is shown in the lemma below  LE M M A 5  S P    s  t  is a k skip SP of S P s  t   PRO O F  Let u be the    rst sample  counting also t  when we walk from s along S P s  t   By the de   nition of k skip SP  S P s  u  has at most k edges  all of which appear in the k hop SP tree of s  Hence  u     Mk s   otherwise  there would be another sample on S P s  u   contradicting the choice of u   which means  s  u  is a super edge in G   s t   Let v be the last sample  counting also s  on S P s  t  before we arrive at t  A similar argument shows that  v t  is also a superedge  The correctness of the lemma then follows from the fact that every super edge has a weight equal to the length of the path it represents  Remark on directed graphs  Let S be the set of super edges on t newly computed for G   s t   In the above  we computed S by    rst    nding the super neighbor set Mk t  of t  and then inserting a super edge  u  t  for each u     Mk t   If G is directed  the derivation of S is slightly different  in the sense that we need to    rst reverse the directions of the edges in G  before proceeding as described earlier  Intuitively  Mk t  should contain the samples that can reach t    directly     without passing another sample   Reversing directions allows us to apply the same algorithm in Section 4 2 to extract Mk t   originally designed to    nd samples reachable from t directly  The reversing incurs no additional execution time  beu v a b c d e 3 2 4 10 5 4 2 Figure 8  Super reach calculation cause a direction change can take place only when the relevant edge is touched by the algorithm  5 2 Reach   Now that we have converted k skip SP computation to    nding S P    s  t  on G    in case s  t are not samples  simply treat G   s t as G      many SP algorithms such as Dijkstra  bi directional  and reach  can be plugged in to obtain S P    s  t   However  as those methods are not designed for our context  they may be improved by taking into account the characteristics of G     Next  we achieve the purpose for reach  As discussed in Section 2 1  reach prunes a vertex v in relaxing an edge  u  v   if the global reach r v  of v is small  compared to the distance that the algorithm has traveled in the forward backward search  see Rule 1   On a k skip graph  using only r v  for pruning may miss plenty of pruning opportunities  The reason is that  a super edge  u  v  implicitly captures a path in the original graph  and hence  can be pruned as long as any vertex on that path has a low global reach  Motivated by this  we formulate a new notion  DE FI N I T I O N 6   SU P E R  R E AC H   Let  u  v  be a super edge in G     and P be the path in G that  u  v  represents  The super reach of  u  v   denoted as sr u  v   equals the minimum h w  of all the vertices w     P  where h w    r w      min  P1    P2    4  where r w  is the global reach of w  and P1  P2  is the path on P from u to w  w to v   To illustrate  assume that the path P captured by a super edge  u  v  is as shown in Figure 8  The number above each vertex is its global reach  e g   r u    3   and suppose for simplicity all the edges have weight 1  To decide  for example  h b   we    rst observe  P1    2 and  P2    4  and then calculate by Equation 4 h b    4     min 2  4    2  After S    h u   h a        h e   h v   is ready  the super reach sr u  v  can be determined as the minimum of S  i e   h a    1  Apparently  sr u  v  can be computed in time linear to the number of vertices in P  i e   at most k  Also  preserving all the super reaches entails small space  as only one extra value per super edge is stored  Next  we propose a new rule to enhance the pruning power of reach  RU L E 2  When the forward search is about to relax a super edge  u  v   prune the edge if sr u  v    lf  u   where lf  u  is the label of u  A similar rule applies to the backward search  More precisely  pruning the super edge  u  v  means that  i  the relaxation is not performed  and  ii  v is not en heaped at this time  it is possible for v to get en heaped due to another later relaxation though   Also note that the pruning happens regardless of the status of v in the other direction  unlike Rule 1 which requires v to have status labeled or unseen in the opposite search   This turns out to be a valuable property that permits the development of a crucial heuristic for maximizing ef   ciency  as discussed shortly  Our algorithm  named reach     for SP computation over G   is identical to bi directional  see Section 2 1   except that Rule 2 ischecked prior to every edge relaxation in an attempt to avoid the relaxation  The following theorem establishes the correctness of the algorithm  TH E O R E M 2  Reach      nds a SP on G   correctly  PRO O F  If the forward or backward search applies Rule 2 to prune a super edge on S P    s  t   we say that a blow occurs  The lemma is obvious if no blow ever happens  so the subsequent discussion considers that there was at least one blow  Our argument proceeds in two steps  First  we will show that there can be only one blow during the execution of reach     This implies that every super edge in S P    s  t  must be eventually relaxed in at least one direction  since two blows are needed to eliminate a super edge from both directions  Equipped with these facts  we will prove the lemma in the second step  Step 1  Without loss of generality  assume that the    rst blow occurred in the forward search  and eliminated super edge  u  v      S P    s  t   It is easy to see that  when the blow happened   i  the forward direction had scanned all the vertices in S P      s  u   and  ii  sr u  v    lf  u   by Rule 2   where lf  u  equals  S P    s  u      S P s  u    Thus  sr u  v     S P s  u     5  Let P be the path in G that  u  v  represents  and w be the vertex in P that minimizes h w   see Equation 4   i e   sr u  v    r w      min  S P u  w     S P w  v     Hence  r w       S P u  w       sr u  v   6  r w       S P w  v       sr u  v    7  Inequalities 5 and 6 lead to r w     S P s  u      S P u  w      S P s  w    By de   nition  r w  is at least min  S P s  w     S P w  t     So we know r w       S P w  t   which  together with Inequality 7  gives  sr u  v       S P w  t         S P w  v      S P v t     8  Inequalities 5 and 8 indicate  S P s  u      S P v t    Due to  i  the way bi directional synchronizes the two directions and  ii  the choice of  u  v   the backward search must have scanned all the vertices on S P v t  before the blow happened  If there was a second blow  either the forward search needed to de heap a vertex in S P v t   or the reverse search needed to deheap a vertex in S P s  u   But both events would have terminated the algorithm immediately  because bi direction ends when the forward backward search de heaps a vertex already scanned in the other direction  Therefore  no second blow could have occurred  Step 2  The analysis of Step 1 shows that  at the moment the blow took place  the status of v was scanned in the backward search  This means that  u had a status of labeled in the backward direction  Consequently  when the forward search de heaped u  right before the blow   as in bi directional  reach   updated     which records the length of the SP found so far  to lf  u    lb u     S P    s  u      S P    u  t      S P    s  t    In other words  reach   found the correct SP successfully  After the forward  or backward  search de heaps a vertex u  our current reach   attempts to prune each out going super edge at u with Rule 2  Hence  the rule has to be applied numerous times if many super edges out of u can be eliminated  This can harm the ef   ciency because each vertex in G   may have a large degree  unlike G  where each vertex   s degree is bounded   the result of which is that we may end up applying the rule a huge number of times during the entire algorithm  The next heuristic allows us to signi   cantly reduce the cost  while still eliminating as many super edges as before  with no increase in the space assumption at all  The idea is to store the outgoing super edges of each vertex u in G   in descending order of their super reaches  After de heaping u in the algorithm  we attempt to prune those edges in the sorted order  The bene   t is that once an edge has been eliminated by Rule 2  we can assert that all the remaining edges can be pruned as well  because all of them must have lower super reaches  than the one pruned   and therefore  will satisfy Rule 2 for sure  The above heuristic is made possible by the fact that  to prune an edge  u  v   Rule 2 does not require checking the status of v  in the search opposite to the one where the pruning happens   If this was not the case  the heuristic would virtually promise no performance gain as checking the status of v takes nearly the same amount of time as applying Rule 2 on  u  v   Remark  It is worth pointing out that  the mechanism behind reach     namely the integration of bi directional and the no statuschecking pruning strategy of Rule 2  actually extends the algorithmic paradigm for SP computation as illustrated in Section 2 1  In retrospect  reach   is an immediate bene   t of this extension  5 3 Zoom in As mentioned in Section 1  the concept of k skip SP is naturally accompanied by a zoom in operator  Given consecutive vertices u  v on a k skip S P    s  t   the operator    nds all the vertices between u and v on the full S P s  t   A naive way to zoom in is to run Dijkstra to extract the SP from u to v  A faster solution applies bi directional  In fact  one can do even better using reach  However  simply executing reach afresh to compute S P u  v  is not likely to outperform bi directional much  This is because the pruning of reach  i e   Rule 1 in Section 2 1  is effective only if the vertex u de heaped in the  for example  forward search has a large label lf  u   This requires the forward search to have come a long way from the source  a situation that will not happen between u and v  because they are at most k vertices apart on their SP  There is a simple remedy to signi   cantly boost the ef   ciency  The main idea is to pretend as if we were running reach to compute S P s  t   as opposed to S P u  v    and that the algorithm had just come to u and v in the forward and backward searches  respectively  We    continue    the forward direction by setting lf  u    min  S P s  u     S P v t     and making u the only vertex in the heap Qf  which implies giving u the status labeled   Note that both  S P s  u   and  S P v t   are available in the k skip S P    s  t  already calculated  Similarly  the backward direction is also continued by setting lb v    lf  u  and creating a heap Qb with only v inside  We then start a normal iteration and proceed as in reach  6  EXPERIMENTS In this section  we empirically evaluate the performance of the proposed solutions  Our experimentation used four spatial networks 2 whose speci   cations are summarized in Table 1  Specifically  NY  BAY  CA NV  and USA contain the road networks in New York city  San Francisco Bay area  California and Nevada combined  and the entire US  respectively  The weight of an edge 2 All datasets can be downloaded from http   www dis uniroma1 it    challenge9 download shtml dataset num  of vertices num  of edges NY 264 346 733 846 BAY 321 270 800 172 CA NV 1 890 815 4 657 742 USA 23 947 347 58 333 344 Table 1  Dataset speci   cations k dataset 4 6 8 10 12 14 16 NY 51  38  31  26  22  20  18  BAY 46  33  26  22  18  16  14  CA NV 46  33  21  19  16  16  15  USA 45  32  25  21  18  16  15   a  Vertex ratio dataset 4 6 8 10 12 14 16 NY 72  65  61  58  56  53  52  BAY 66  57  52  48  45  43  42  CA NV 63  54  49  45  43  41  40  USA 61  51  47  44  41  39  38   b  Edge ratio Table 2  Sizes of k skip graphs equals the travel time on the corresponding road segment  All of our results were obtained on a computer equipped with an Intel Core 2 DUO 3 0Ghz CPU and 2 Giga bytes memory  running Fedora Linux 13  Size of the pre computed structure  Our technique has the feature of demanding only a structure with sub linear size  i e   a kskip graph G      V     E     occupies less space than the underlying road network G    V   E   The    rst set of experiments demonstrates this by proving that G   has fewer vertices and edges than G  Equivalently  if we de   ne the vertex ratio to be  V      V   and the edge ratio to be  E      E   the goal is to show that both ratios are below 1 by a comfortable margin  Moreover  remember that V   is a k skip cover  De   nition 3   Hence  the vertex ratio also re   ects the effectiveness of the AS algorithm in Section 4 2  Table 2a presents the vertex ratios of each dataset as k varies from 4 to 16  Interestingly  we noticed that the ratio roughly equals 2 k in all cases  that is  AS    nds a k skip cover with size about 2 V   k  In the same style  Table 2b shows the corresponding edge ratios  which are also much lower than 1  and decrease with the increase of k  A general observation is that  both ratios tend to be smaller  i e   greater size reduction  when the underlying network is sparser  NY is the densest among all the datasets   Que</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>GBLENDER  Visual Subgraph Query Formulation Meets Query Processing ### Changjiu Jin    Sourav S Bhowmick       Xiaokui Xiao    Byron Choi     Shuigeng Zhou        School of Computer Engineering  Nanyang Technological University  Singapore    Singapore MIT Alliance  Nanyang Technological University  Singapore     Department of Computer Science  Hong Kong Baptist University  Hong Kong     Fudan University  China cjjinjassouravjxkxiao ntu edu sg  choi hkbu edu hk  sgzhou fudan edu cn ABSTRACT Due to the complexity of graph query languages  the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of graph data management tools to wider community  We present a novel HCI  human computer interaction  aware graph query processing paradigm  where instead of processing a query graph after its construction  it interleaves visual query construction and processing to improve system response time  We demonstrate a system called GBLENDER that exploits GUI latency to prune false results and prefetch candidate data graphs by employing a novel action aware indexing scheme and a data structure called spindle shaped graphs  SPIG   We demonstrate various innovative features of GBLENDER and its promising performance in evaluating subgraph containment and similarity queries  Categories and Subject Descriptors H 2 4  Database Management   Systems   Query processing General Terms Algorithms  Experimentation  Performance Keywords Graph Databases  Graph Indexing  Visual Query Formulation  Frequent Subgraphs  Infrequent Subgraphs  Prefetching 1 ### INTRODUCTION Querying graph databases has emerged as an important research problem due to explosive growth of graph structured data in recent years  A wide variety of graph queries in many applications involve the core substructure search problem  also called subgraph containment query   In this problem  given a graph database D and a query graph q  the aim is to    nd all data graphs in D in which q is a subgraph  Note that q is a subgraph of a data graph g 2 D if there exist a subgraph isomorphism from q to g  A common problem for this type of query is that in many occasions there may not exists any g 2 D that matches the query  In this case  it is often useful to    nd out data graphs that    nearly  contain the query graph  which is called the substructure similarity search problem  5   also called subgraph similarity query   Copyright is held by the author owner s   SIGMOD   11  June 12   16  2011  Athens  Greece  ACM 978 1 4503 0661 4 11 06  A number of graph query languages  e g   SPARQL  have been proposed that can be used to formulate subgraph queries  Unfortunately  in many real life domains it is unrealistic to assume that users are pro   cient in expressing graph queries using these languages  The traditional approach to address this query formulation challenge is to build a user friendly visual framework on top of a state of the art graph query processing technique  e g    5    In this traditional visual query processing paradigm  although the    nal query that a user intends to pose is revealed gradually in a step bystep manner during query construction  it is not exploited by the query processor prior to clicking of the Run icon to execute the query  That is  query processing is initiated only after the user has    nished drawing the query  This often results in slower system response time  SRT  1 as the query processor remains idle during the entire query formulation process  2  3   In this demonstration  we present GBLENDER  Graph blender   2  3    a novel HCI aware visual subgraph querying system that challenges the aforementioned traditional paradigm of visual querying by blending the two orthogonal areas of visual graph query formulation and query processing  The key bene   ts of this novel query evaluation paradigm are two fold  First  it ensures that the query processor does not remain idle during visual query formulation  Second  it signi   cantly improves the SRT  2  3   In traditional paradigm  SRT is identical to the time taken to evaluate the entire query  In contrast  in this new paradigm SRT is the time taken to process a part of the query that is yet to be evaluated  if any   At each visual query formulation step taken by the user  GBLENDER employs a novel action aware indexing scheme and a data structure called SPIG  spindle shaped graphs  to ef   ciently compute candidate data graphs that contain  approximately if necessary  the current query fragment by exploiting the GUI latency  It also supports modi   cations to a query gracefully as a user may change her mind or commit mistakes during query construction  In this demonstration  we shall demonstrate various interactive and innovative features of GBLENDER that are necessary to realize the proposed visual query processing paradigm  2  SYSTEM OVERVIEW Figure 2 shows the system architecture of GBLENDER and mainly consists of the following modules  The reader may refer to  2  3  for details related to these modules  The GUI module  Figure 1 a  depicts the screenshot of the visual interface of GBLENDER  A user begins formulating a query by 1 Duration between the time a user presses the Run icon to the time when the user gets the query results 1 5 6 4 3 2 8 7 Panel 2 Panel 1 Panel 3 Panel 4  a  Visual interface   b  The Interaction Viewer module   c  The Modi   cation Handler module  Figure 1  The GBLENDER system  The identi   ers on the edges represent the sequence of visual steps for query formulation   Matching Query GBLENDER GUI Actions Database Graph Query Fragment Verifier Candidates Verificationfree  candidates User Frequent Fragment Extractor Constructor Index Results Visualizer Action Aware Indices Results Generator SPIG  Interaction  Viewer Modification  Handler Figure 2  Architecture of GBLENDER  choosing a database as the query target and creating a new query canvas using Panel 1  The left panel  Panel 2  displays the unique labels of nodes that appear in the dataset in lexicographic order  In the query formulation process  the user chooses labels from Panel 2 for creating the nodes in the query graph  Panel 3 depicts the area for formulating graph queries  A user drags a node that is part of the query from Panel 2 and drops it in Panel 3  Next  she adds another node in the same way  Then  she creates an edge between the added nodes by left and right clicking on them  Additional nodes and edges are added to the query graph by repeating these steps  Finally  the user can execute the query by clicking on the Run icon in Panel 1  Panel 4 displays the query results  The Frequent Fragment Extractor module  This module mines the frequent fragments from the graph database D using an existing frequent graph mining technique  the current version uses gSpan  6    Informally  we use the term fragment  resp  query fragment  to refer to a small subgraph existing in graph databases  resp  query graphs   Given a fragment g which is a subgraph of G  denoted as g   G  and G 2 D  we refer to G as the fragment support graph  FSG  of g  Since each data graph in D is denoted by an unique identi   er  fsgIds g  denotes the set of identi   ers of FSGs of g  A fragment g is frequent in D if its support is no less than   jDj where 0        1 is the minimum support threshold  Otherwise  g is an infrequent fragment  The Action Aware Index Constructor module  The action aware frequent index  A 2 F  is a graph structured index having a memoryresident and a disk resident components  We refer to them as memory based frequent index  MF index  and disk based frequent index  DF index   respectively  Speci   cally  small sized frequent fragments  frequently utilized  are stored in MF index whereas larger frequent fragments  less frequently utilized  reside in DF index  The DF index is an array of fragment clusters  A fragment cluster is a directed graph C    VC  EC  where each node v 2 VC is a frequent fragment f where the size of f  denoted as jfj  is greater than the fragment size threshold     i e   jfj        There is an edge  v       v  2 EC iff f     is a proper subgraph of f  denoted as f       f  and jfj </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sgdqp3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#graph_data_and_querying"/>
        <doc>BitPath     Label Order Constrained Reachability in Large Graphs Technical Report ### Medha Atre 1   Vineet Chaoji 2   Mohammed J  Zaki 1   and James A  Hendler 1 1 Dept  of Computer Science  Rensselaer Polytechnic Institute  Troy NY  USA 2 Yahoo  Labs  Bangalore  India ABSTRACT With ever expanding size of the web  sizes of graphs with semantic relationships are growing very rapidly  The semantic relationships are typically presented as edge labeled graphs where nodes are the entities and the edge label signi   es the relationships between the two entities  RDF graphs are an example of such graphs  Pure reachability queries on these graphs do not give any information about the semantic association between two remote entities  On the other hand  the typical size of semantic graphs present on the web has made general regular path query processing infeasible  In this paper we focus on the following graph path query problem     given a source node s  a destination node d and a sequence of ordered edge labels    label seq     does there exist any path between the two nodes which has the edges labels in    label seq     in the given order  on the path  We solve this problem by a combination of graph indexing  and a query processing algorithm based on a divide and conquer procedure and greedy pruning of the search space  We have evaluated our technique on graphs with more than 22 million edges and 6 million nodes     much larger compared to the datasets used in published work on path queries  We compare our approach with optimized DFS  optimized focusedDFS  and bidirectional BFS methods  1 ###  INTRODUCTION More and more data is represented as graphs these days resulting from applications that span social networks  friendof a friend network 1    life sciences  e g  UniProt  3    security and intelligence networks  and even Government data  data gov project 2    For instance  in case of the UniProt protein sequence and annotation data in RDF format  the edge labels may provide important information about the interaction between two proteins or a possible relationship between proteins and genes  Similarly  in social network graphs  the edge labels de   ne a relationship between two persons or entities  e g   a university and a person  Within security and intelligence networks  these semantically rich graphs are analyzed to identify connections between two seemingly unrelated entities  Also with the advent of Semantic Web  Resource Description Framework  RDF  is becoming a de facto standard 1 http   www foaf project org  2 http   www data gov semantic Copyright  c is held by the authors  Dept  of Computer Science  Rensselaer Polytechnic Institute  Troy NY February 28  2011   to present semantic relationships between entities  RDF graphs are edge labeled directed graphs  Due to the prevalence of graph data  path queries have become an important area of research  The approaches taken to evaluate path queries on XML data  e g   XPath  cannot be applied to general graphs  since XML is assumed to be tree structured data for all practical purposes  General regular path queries on any graph ask for all pairs of nodes which have at least one path between them that satis   es the given regular path expression  This problem has been proved to be NP hard  18   On the other hand  pure reachability queries on graphs do not give any information about the semantic relationship between two remote nodes  Hence researchers have started to work on a hybrid problem     constrained reachability  14   The problem of constrained reachability asks if a destination node y is reachable from x through a path which satis   es the constraints in the query  The de   nition of constraints depends on the targeted problem  e g   number of hops  type of edge labels  or a regular path expression  With the growing size of graph datasets  there is also more variety in the semantic relationships between nodes  e g   an RDF representation of DBLP and IMDB graph contains 145 and 222 distinct edge labels over 13 million and 5 million total edges  respectively   This poses two main challenges      1  without knowing the exact structure or schema of the graph  it can be di   cult to pose a precise query between two nodes  It can take several iterations of various combination of edge labels to arrive at the one which fetches the correct answer  and  2  the amount of preprocessing required for answering a path query can limit the scale of the graphs that can be handled by an algorithm  Hence a method which achieves a trade o    between  a  the expressiveness of path query   b  indexing  preprocessing  time  and  c  query execution time  is more desirable  In this work  we address these issues by proposing a light weight method to process label order constrained reachability  LOCR  queries  Given a source node x  a destination node y and an ordered sequence of edge labels    a    b    c    d      an LOCR query represented by Q x  y   a  b  c  d   asks if there exists a path between x and y  such that edge labels a  b  c and d appear somewhere on the path in the given order  The query returns a binary response     Yes or No  The         and          in the label order allow any other labels  0 or more  to appear in between on the path  These symbols are used in accordance with the Perl regular expression  regex  syntax  Note that as per Perl regex     a     means 0 or more    a   s which alters the meaning of our query  hence we use    a        which enforces at least one    a    followed by any other labels allowed by           If there exists a path between x and y with follow ing edge labels    d d a b e f g c h d i j     the answer to the query is YES  On this path given labels a  b  c  d appear in the given order but also have other labels interleaved among them  But if the only path between x and y has edge labels    d a e f b b c     then answer to the query is NO  because a  b  c  d labels do not appear in the given order in the query  Such queries can be utilized in social networks or intelligence services graphs  wherein we are not only interested in the presence of a path between the two entities but also the interconnections along the path  For instance  let us assume we are interested in    nding the trace to a fraudulent organization XYZ through the connections of a political    gure ABC  Speci   cally  we are interested in the query expression      friendOf    hasMembers    worksFor    funds        In a di   erent setting  consider social network graphs such as Facebook and LinkedIn  LinkedIn contains a variety of relationships among people and between people and organizations groups  e g   classmates  colleagues  ex colleagues  worked at  working at  group memberships and so on  Without complete knowledge of the interconnection between various relationships  we may be interested in    nding out if there exists a path between person DEF and an organization in San Francisco satisfying the following path      classmates    workingAt    locatedIn        We believe that LOCR queries achieve a faster response to    exible path queries over large graphs  The problem of LOCR queries addressed in this paper is di   erent from the problem of LCR queries handled by Jin et al in  14   The di   erences are pointed out in Section 2  Our main contributions in this work are  1  Introduction of label order constrained reachability problem in the context of edge labeled graphs  2  A light weight graph indexing scheme based on compressed bit vectors     BitPath     which stores all the successor and predecessor edge sets of nodes in the graph  Successor edges are edges that can be reached from a given node through a depth    rst or breadth     rst traversal from the given node  Similarly  predecessor edges are edges that appear on any path leading to the given node  The proposed indexing scheme requires just two depth    rst passes on the graph  3  A divide and conquer query algorithm using greedypruning strategy for e   cient pruning of the search space using the aforementioned index  The algorithm recursively divides the given query into sub queries to arrive at the answer  4  Experiments on large graphs     more than 22 million edges and 253 distinct edge labels  To the best of our knowledge these graphs are largest amongst the published literature on path queries  2  RELATED WORK Early work by Mendelzon and Wood  18  shows that the general regular path query problem on graphs is NP hard  In general the indexing schemes on graphs for evaluation of path queries can be grouped into the following three categories   1  P indexes  path indexes which typically index every unique path    2  D indexes  16   node index     used for determining in constant time ancestor descendant relationship    3  T indexes  for path pattern such as twig in case of XML   23  5  6  17  9   D indexes and T indexes can only be used in the context of XML graphs as it is non trivial to decide ancestor descendant relationship in constant time on a directed graph which does not assume tree structure  21   Some approaches build P indexes with equivalence classes of nodes based on the incoming or outgoing paths to and from the nodes  19  4  15  8  13   Some other approaches that suggest building P indexes are Index Fabric  11  and APEX index  10   Most of the work for building P indexes has been in the context of XML graphs  except early proposals of P indexes  e g   1 index  2 index by Milo and Suciu  19    Among these approaches Bitmapped Path Index  BPI   12  and BitCube  22  use bitvector based indexes  The BitPath scheme proposed in this paper also uses compressed bit vectors for indexes  While BPI and BitCube use the bitmapped indexes to index paths in the XML graphs  BitPath uses compressed bit vectors to only index the unique edges in the graph  While a vast number of techniques have been proposed for path indexing in XML graphs  those approaches cannot be used for general edge labeled graphs since XML is widely viewed as tree structured data  On the other hand  number of paths in an RDF graph with 10 15 million unique edges over 5 6 million nodes can be of the order of 10 25 or more  It is computationally infeasible to index such a large number of paths due to space and runtime constraints  Recent work by Jin et al   14   proposes a novel approach for evaluating label constrained reachability  LCR  queries  Given a source node x and destination node y and a set of edge labels S  an LCR query checks the existence of at least one path between x and y such that each edge label on that path is in S  No labels other than those in S can appear on that path  Their solution uses either building complete transitive closure or by utilizing approximate maximal spanning tree on the given graph  Approximate maximal spanning tree is found by recursively constructing di   erent approximate spanning trees  The construction of an approximate maximal spanning tree is to reduce the index space  The LCR query processing algorithm utilizes indexes built on top of the approximate maximal spanning tree  interval labeling  and kd trees  range search tree   The computational complexity of their index building procedure using approximate maximal spanning tree construction is O n V   E     P     P   2    n n0  E     V  log V      and using the generalized transitive closure M is O  V   2 2 2  P      where P is the set of unique edge labels in the graph  and n and n0 are the sample sizes for approximate spanning tree  For example  with 253 labels  as for some of the graphs in our study  their complexity would be  253 126   or 2 2   253 which is prohibitive  For LCR queries  the set of unique edge labels appearing on the satisfying path is a subset of the labels speci   ed in the query  The satisfying path can have these edge labels in any order  On the other hand  LOCR queries handled by BitPath allow any other edge labels to appear on the path as long as the given order of edge labels in the query is satis   ed  Our algorithm tries to keep the index construction light weight by a  not indexing all paths in the graph  b  utilizes compressed bit vectors for index representation thereby optimizing the index size  and c  using a divideand conquer approach for recursively splitting the query into sub queries  For splitting the query into two sub queries it utilizes a greedy pruning heuristic based on the BitPath indexes  3  BITPATH INDEXES Let G    V  E  L  f  be a directed edge labeled graph  where V is the set of nodes  E is the set of edges  L is the   the thirteenth floor  the matrix  movie  1999   the matrix reloaded 1 2 3 4 5 6 Edge List EID  the thirteenth floor    releasedIn     1999   the thirteenth floor    similar to        the matrix  the matrix                 similar to        the matrix reloaded  the matrix                 releasedIn       1999   the matrix                 rdf type          movie  the thirteenth floor    rdf type          movie 1 2 3 4 5 6  the thirteenth floor  1999  111111  the matrix  the matrix reloaded  movie 000000 Node N   SUCC   E N   PRED   E 000000 000000 001110 000000 010011 011000 010000 110100 100100 011000 Edge Label EL   ID  releasedIn  similar to rdf type 000011  similar to  similar to  releasedIn  releasedIn rdf type rdf type Figure 1  BitPath indexes set of unique edge labels  and f is a edge labeling function f   E     L  for each edge  vi  vj    Before building the BitPath indexes  we transform the given graph into a directed acyclic graph by collapsing the strongly connected components  SCCs   Since the LOCR queries specify constraints on the edge labels on the path  it is imperative to capture the edges  and their labels  that get collapsed in a SCC  Following steps outline the process  1  Identify SCCs in the graph using Tarjan   s algorithm3   2  Let z be a new node representing the SCC    C     For each edge e in C that gets removed as a result of collapsing the SCC  add a self edge  z  z  with label f e  in the graph  The purpose of adding self edges with same edge labels is to keep track of the edge labels that appear in a given SCC  These edges help in determining paths going through an SCC without having to traverse the entire SCC subgraph at query time  A label order constrained reachability  LOCR  query requires the knowledge of relative order of edge labels occurring in a given path  Basic methods to answer these queries can run a DFS  depth    rst search  or BFS  breadth    rstsearch  traversal on the subgraph below the source node while examining each path for label ordering  These approaches are acceptable on small graphs and when the query has a valid answer  But as shown in our evaluation  when these properties are not satis   ed  the query performance suffers with such baseline approaches  Another way to answer these type of queries can be by storing each unique path in the graph separately indexed by its source and destination node  But as pointed out in Section 2  it is computationally infeasible to index paths of the order of 10 25 or more due to time and space constraints  In view of the above mentioned challenges  we solve the LOCR problem by creating 4 types of indices on the graph  and designing a query answering algorithm  based on a combination of greedy pruning and a divide and conquer heuristic  The 4 types of indices are as follows  1  EID  edge to ID   For each edge e     E is mapped to a unique integer ID  For instance  for the graph shown in Figure 1  edge   the matrix  movie  with label rdf type is mapped to ID 5  3 http   en wikipedia org wiki Tarjan   s strongly connected  components algorithm 2  N SUCC E  node   s successor edges   For each node  we index IDs of all the successor edges  i e   edges that will get visited if we traverse the subgraph under the given node  The self edges added to the graph as a result of collapsing an SCC can be handled trivially by examining the head and tail of the given edge  Following the example in Figure 1  node  the thirteenth    oor will have edge IDs 1  2  3  4  5  6 in its successor list  3  N PRED E  node   s predecessor edges   Similarly  for each node we index the predecessor edges  i e   edges that will get visited if we make a backward traversal on the entire subgraph above the given node  In Figure 1  node  movie will have edge IDs 2  5  6 in its predecessor list  4  EL ID  edge label to edge ID   For each unique edge label l     L  we index IDs of all the edges in E which have edge label l  In Figure 1  edge label rdf type will have IDs 5 and 6 in its list  In practice  we use bit vectors of length  E   total number of edges in the graph   for building N SUCC E  N PREDE  and EL ID indexes  Each bit position in the bit vector corresponds to the unique ID assigned to an edge as per the EID index  For the node  the thirteenth    oor in Figure 1  its N SUCC E bit vector index will be    111111     for the node  movie its N PRED E index will be    010011     and the ELID index of edge label rdf type will be    000011     We apply run length encoding on N SUCC E and N PRED E indices of each node depending on the compression ratio  Typically  run length encoding delivers high compression ratio if there are large gaps in the bitvector  A bit vector with large gaps is one where a lot of 0s or 1s appear together  For instance  a bit vector    111111000001111111    has large gaps as opposed    11001010101010    which has smaller gaps  The structure of the graph and the ordering of edges impacts the gaps in these indices  Typically the unique edge labels in the graph are much fewer compared to the number of nodes  Hence in an EL ID index of an edge label  there are many more interleaved 0s and 1s as compared to an N SUCC E or N PRED E index  Since the EL ID index typically has many small gaps  we do not apply run length encoding on the EL ID index  Note that at the time of querying we do not uncompress any compressed indices  All the algorithms are implemented to perform bitwise operations on both the gap compressed indices as well as non compressed indices  3 1 BitPath Index Creation AlgorithmWe create EID  N SUCC E  and EL ID indexes in one DFS pass over the DAG generated after collapsing the SCCs as outlined in Section 3  N PRED E index is created by making one backward DFS pass on the graph and using the EID mapping generated in the    rst pass  In the    rst pass  we    nd all the nodes with 0 in degree  the root nodes   Starting with the    rst root node  we make a DFS pass over the entire subgraph below it  and do the same for other roots  Starting with ID 1  every new edge encountered in the DFS pass is given a new ID sequentially  For instance  in Figure 1  starting at root node  the thirteenth    oor  we visit edge   the thirteenth    oor    1999     with label  releasedIn    rst and assign ID    1    to it  Next when we visit edge   the thirteenth    oor     the matrix     with label  similar to it is given ID    2     Since this is a DFS traversal  we continue traversing the subgraph below node  the matrix  and sequentially assign ID    3    to next edge visited  While assigning IDs to edges  we simultaneously maintain N SUCC E list for each node encountered  Every time we visit a new node  it is pushed on a DFS node stack  This stack keeps track of all the nodes that were visited on a path from root node to the current node  While exploring an unvisited node  we add all the outgoing edges of that node in the N SUCC E list of each node in the DFS stack  The node is popped out of the stack when it is marked as    visited     i e   when the entire subgraph below the node has been traversed  If a    visited    node is encountered through a di   erent path  instead of exploring it again  we simply add all the edge IDs in its N SUCC E list to the N SUCC E list of all the nodes in the DFS node stack  Once a node is marked    visited     we build a bit vector of N SUCC E index  Each bit position marked 1 in this bitvector corresponds to the EIDs of the node   s successor edges  Since IDs to the edges were assigned as they are visited  while constructing the EID index   this scheme generates N SUCC E bit vectors with large gaps for most nodes  We make use of this fact to apply run length encoding on NSUCC E bitvectors  Note that this was a heuristic observed for many real life graphs  and it is possible to generate a pathological graph where run length encoding does not fetch the desired bene   t  Since for a typical RDF or any edge labeled graph   L  is much smaller than  V    EL ID index is typically smaller than EID and N SUCC E indexes  and even N PRED E discussed below   At the end of the    rst DFS pass  we get EID  NSUCC E  and EL ID indexes  In the second pass  we start from the leaf nodes     nodes with 0 out degree     and make a backward DFS traversal on the graph  N PRED E index for each node is built in the same way as the N SUCC E index  The only di   erence is that in the second pass  we utilize the EID index that was populated in the    rst pass  Hence every time we encounter an edge  we simply look up its ID in the EID index and use it to construct the N PRED E index  When a node is marked    backward visited    in this pass  we generate its bitvector N PRED E index in the same manner as N SUCC E index  While building N PRED E indexes  sometimes we encounter bit vectors with a lot of small gaps  Hence we use following heuristic     if the gap compressed size of the bit vector is larger than half of the size of the corresponding uncompressed version of the bit vector  then store the bitvector in uncompressed form  At the end of the second pass  all the indexes are written to the disk  In the next section  we describe the LOCR query processing algorithm using these 4 indexes  4  BITPATH QUERY ALGORITHM In an LOCR query  x  y   a  b  c    l       we want to    nd if there exists any path between a source x and destination y  such that labels  a  b  c    l  appear on that path in the given order  other edge labels can appear on this path as well  ref  Section 1   The           denotes that there can be any number of labels speci   ed in the order constraint  The evaluation of an LOCR query can be broken down into the following steps  1  Is y is reachable from x  2  If the earlier condition is satis   ed  we want to    nd if there exists any path between x and y  such that all of the labels a  b  c     l appear somewhere on that path in that order  3  Suppose we know that there exist some path where label b appears somewhere on the path  We want to    nd an edge  m  n  with label b on that path  4  Next  we want to    nd if there exists a path between x and m  such that label a appears somewhere on it  5  If we    nd that such a path exists between x and m  we want to    nd if there exists a path between n and y  such that labels c     l appear somewhere on it in the given order  As can be seen by the steps outlined above  we recursively divided the original query into smaller sub queries for evaluation  We make use of the 4 indexes     N SUCC E  NPRED E  EL ID  and EID     to evaluate the sub queries at every step  The N SUCC E index of a node gives us all the edges that can be reached from that node and N PRED E index of a node gives us the edges that can eventually lead to that node  So if the intersection of N SUCC E index of node x and N PRED E index of node y is non empty  i e   if they have at least one edge common between them  node y is reachable from x  This answers the    rst point above  If  N SUCC E x      N P RED E y      EL ID b   6      i e   the intersection of successor index of x  predecessor index of y and EL ID index of label b is non empty  there is at least one path from x to y  where edge label b appears somewhere on the path  This solves our second step  N SUCC E x   N PRED E y  and EL ID b  are bit vectors and their intersections requires two bitwise AND operations  but this operation costs O  E       O  V    for sparse graphs  Let the result of this bitwise AND operation be INTSECT  Position of a 1 bit in INTSECT bit vector gives EID of an edge with label b  A reverse look up in the EID index gives us an edge  say  m  n   with label b  This satis   es the third step above  If  N SUCC E x    N P RED E m    EL ID a   6      it means that there exists at least one path between x and m where edge label a appears somewhere on the path  This addresses the forth step  If we put the earlier result and this result together  it tells us that there exists at least one path between x and y  such that edge label a appears somewhere before label b  Recursively  we solve our    fth step to    nd if there exists any path between n and y such that edge labels c   l appears somewhere on it  In the example above  we chose to split the edge label order    a b c   l    on label b    rst for the ease of understanding  But for further optimization  we can choose the split point depending on the selectivity of the intersection of N SUCC x    a  b  c  d  e    y Greedy   pruning Split on    c    Split on    b    x    a    s p    e    y Split on    d    Greedy   pruning x    a  b    k l    d  e    y Greedy   pruning Figure 2  Divide and conquer with Greedy pruning E x   N PRED E y   and EL ID for each label in the query 4   We will always choose to split on the edge label with high selectivity  This is the greedy pruning step  The divide and conquer strategy along with greedy pruning is depicted in Figure 2 with a di   erent example of label order  a  b  c  d  e  between nodes x and y  A    Yes    answer at each sub query node is reported to its parent query node and the query stops its evaluation when a    Yes    answer reaches the root of this query tree or when all the candidate edges are exhausted  Figure 2 shows that at the very beginning of the query  edge label c has highest selectivity among all the labels  hence the label order was split on c  Here INTSECT    N SUCC E x      N P RED E y      EL ID c    Edge  k  l  with label c was picked    rst from INTSECT and the query is split as      1  does there exist a path between x and k such that edge labels  a  b  appear somewhere on the path in the given order  and  2  does there exist a path between l and y such that edge labels  d  e  appear somewhere on the path in the given order  If the answer to any of the two sub queries is    No     we pick the second edge  say  s  t   with label c from INTSECT and continue the evaluation until we    nd a    Yes    answer or we exhaust all edges in INTSECT  The greedy pruning strategy     used to choose the    splitpoint        is outlined below  Algorithm 1 greedy pruning x  y  label seq  1  min label   0 2  min edges       3  for each l in label seq do 4  intsect   N SUCC E x      N P RED E y      EL ID l  5  if  intsect     min edges  then 6  min edges   intsect 7  min label   l 8  return pair min edges  min label  Algorithm 1 outlines the greedy pruning strategy  label seq is the order of edge labels in the query  If at any given subquery node in the divide and conquer tree  see Figure 2  there are more than one edge labels in the given label seq  the greedy pruning strategy takes intersection of  N SUCC E x      N P RED E y      EL ID l   for each label l  lines 3   7 in Algorithm 1    The edge label  min label  which generates the smallest intersection set  min edges  are returned by the greedy pruning method  Line 8 in Algorithm 1   and are subsequently used to partition the initial query into two sub queries  For a typical real life graph like RDF  there are few distinct edge labels  L is very small  as compared to the total number of edges  For instance  the UniProt RDF graph of 22 million edges has only 91 distinct edge labels  Moreover  the distribution of these edge labels is not uniform  a large number of edges have few distinct edge labels  In the UniProt dataset  the edge label    rdf type    appears on 4 Selectivity is inversely proportional to the number of edges in the intersection     high selectivity means fewer edges and low selectivity means more edges     5 million edges  about 10 edge labels appear on 1 million edges each and about 20 edge labels occupy    100 000  200 000 edges each  We exploit this skewed label distribution to e   ectively prune the potentially large search space of edges  Although the skewed edge distribution holds true for most real life graphs  it is possible to synthetically build graphs where there is one root node  one sink node and a set of edge labels that follow uniform distribution  For such a graph  if we are given the root and sink nodes and a list of edge labels in a query  the greedy selection will not be able to achieve any pruning because every edge will be in the N SUCC E index of source node and N PRED E index of the sink node  Now  Algorithm 2 describes the divide andconquer strategy  Algorithm 2 divide and conquer x  y  label seq  1  res   FAIL 2  if topo order y    topo order x    label seq size   then 3  return FAIL 4  5  pair min edges  min label    greedy pruning x  y  label seq  6  7  if min edges       then 8  return FAIL 9  if label seq size      1 then 10  return SUCCESS 11  12  lseq1   get seq label seq begin    min label 1   13  lseq2   get seq min label 1  label seq end     14  15  for each eid in min edges do 16  edge e   eid to edge eid     for edge  k l   k is the tail  and l is the head of the edge 17  res   divide and conquer x  e tail  lseq1  18  if res    SUCCESS then 19  res   divide and conquer e head  y  lseq2  20  if res    SUCCESS then 21  break 22  return res For the sake of illustration  let us assume that the LOCR query checks if the label order  a  b  c  d  e  is satis   ed between source node x and destination node y  To evaluate this query  Algorithm 2  is called on x and y nodes with label seq containing all the edge labels a  b  c  d  e   Although this example considers a sequence for length    ve  the algorithm is invariant to the length of the query  Also  an edge label can be repeated any number of times in the sequence  e g    a  b  a  c  a   or  a  a  a  a  b  b   The label seq can as well be empty  in which case it simply translates to a reachability query  If di   erence between the topological order of x and y is lesser than the length of label seq  it means that there is no path from x to y of length  label seq  or more  Divideand conquer uses this simple heuristic to preclude exploring paths shorter than  label seq   Line 2 in Algorithm 2    Using greedy pruning  line 5 in Algorithm 2   we    rst get the minimal set of EIDs  min edges  such that they are common between the successor edges of x  predecessor edges of y  and also contain a label min label     label seq  If the minimum set of edges for any label l in label seq is empty  it clearly implies that nodes x and y do not have any path with label l between them  In this case  we stop exploring the paths further and return  line 8 in Algorithm 2    Otherwise  we split the label seq into 2 parts  such that  if the min label is c and the original label seq is  a  b  c  d  e   we split it into  a  b  and  d  e   lines 12  13 in Algorithm 2    For each edge e in min edges  f e    c  Let e be an edge over nodes  k  l  such that e tail   k  e head   l  line 16   Now the original query is divided into 2 parts      1  is there any path from node x to e tail such that it satis   es a label order  a b     2  is there any path from node e head to y such that it satis   es a label order  d  e   If the label seq is split over a or e  one of the sub queries will have an empty label seq  A sub query with empty label seq is just a reachability query  Such sub query is skipped as the reachability of the nodes is previously evaluated in the greedy pruning step  With respect to the runtime complexity  divide and conquer algorithm   s in memory program stack size is at most the size of original label sequence of the query  If we assume a uniform distribution of edge labels in the graph  in the worst case the divide and conquer algorithm can get called as many as  2     E   L   times  Lines 15   21 in Algorithm 2   for a given call to the divide and conquer method  Hence the worst case complexity of the entire runtime of the algorithm is O   E   L    label seq        since the BitPath algorithm will be called at most  E   L  times for each label in the sequence  This is true for a query on a graph with one super root and one super sink node  where the root and sink nodes are the target nodes in the query  But as for most real life graphs  the edge labels follow a non uniform distribution and rarely there is a single root and sink node in the graph  The complexity of greedy pruning is O  label seq     E    But since we use bit vectors for storing N SUCC E  N PRED E  and ELID indexes  for all practical purposes greedy pruning does not imply traversing the entire graph  label seq  times  4 1 Handling Nodes and Paths in SCC Since every node in an SCC is reachable to every other node in the same SCC  it is di   cult to decide the start and end of a path going through SCC and the order of the edgelabels on that path     which is required in LOCR query processing  Hence presently we process the paths going through SCCs by simply checking the self edges introduced while merging the SCCs  ref  Section 3   Let an LOCR query be  x  y   a  b  c  d  e    such that x and y are part of the same SCC in the original graph  They are represented by node z in the graph obtained by merging SCCs  For such a query  we simply check if there exist 5 self edges  z  z  with labels a  b  c  d  and e  As another example  a query with label seq  a  a  a  b  c  can be satis   ed by traversing the edge  z  z  labeled    a    thrice and checking other self edges  z  z  for labels    b    and    c     If either x or y are part of di   erent SCCs  say t and u respectively  we evaluate the original LOCR query as  t  u   a  b  c  d  e    We use this same technique for BitPath as well as the baseline methods used for performance evaluation  hence for all practical purposes we have sampled queries for experimental evaluation on the directed acyclic graph  with self edges  obtained after merging the SCCs  5  EVALUATION BitPath indexing and query processing algorithm is developed in C and compiled using g    v4 4  with  O3 optimization    ag  We used an OpenSUSE 11 2 machine with Intel Xenon X5650 2 67GHz processor  48 GB RAM with 64 bit Linux kernel 2 6 31 5 for our experiments  Although we used a desktop with 48 GB memory  the BitPath index size for the datasets is much smaller than that  refer Section 5 5   5 1 Competitive Methods As outlined in Section 2  path indexing approaches suggested in the context of XML XPath query processing cannot be used to evaluate LOCR queries on general graphs  RDF graphs can be represented in XML format 5   hence we explored the options of evaluating LOCR queries on XML representation of an RDF graph  This requires translating the given LOCR query into equivalent XML path query  A faithful translation of an LOCR query into equivalent query on the XML graph of RDF does not represent a path query  It often has to be processed using iterative join of two or more tree patterns  twig  queries  Native XQuery speci   cations do not support recursive joins of tree pattern queries  where the number of recursions are not known at the query time  An example of this is given in Appendix A  Also the BitPath method of indexing and processing LOCR queries can be applied to any other edge labeled directed graph  But any edge labeled directed graph     which does not satisfy RDF constraints     cannot be represented as an XML graph  Hence for our evaluation we used optimized versions of DFS and BFS as our baseline methods for comparative performance  1  Optimized DFS  DFS   Given an LOCR query between nodes x  source  and y  destination   we check the out degree of x and in degree of y  If the out degree of x is lesser than in degree of y  we start the DFS walk from x and continue until y is reached and the given path satis   es LOCR label seq  If y   s in degree is lesser  we start a reverseDFS walk from y with reversed order of labels in the query and continue up to x  This method is referred to as    DFS    in the rest of the text  2  Optimized Focused DFS  F DFS   This method is same as optimized DFS  but additionally at every node we check the reachability of the destination node y   or reachability of node x if we perform reverse DFS  We check reachability by using the intersection of N SUCC E and N PREDE BitPath indices  If y is not reachable from the given node n  N SUCC E n      N P RED E y        This method is further enhanced as follows  Maintain a reachability array  The very    rst time node n is explored  update reachability n  to note if y is reachable from n  If node n is visited again through a di   erent path  next time just look up reachability n  to decide if the paths below n should be explored or discontinued  3  Optimized Bidirectional BFS  B BFS   In bidirectional BFS  we traverse down the subgraph below x and above y one step at a time  maintaining forward and reverse BFS queues  Each node enqueued in the BFS queue has its own label seqn associated with it  which tells which labels in the original sequence have been seen on a path to node n  Similarly we maintain a reverse label seqn for nodes in the reverse BFS queue  At every iteration we perform an intersection of nodes in the forward and reverse BFS queues  If there is a common node in two BFS queues  we join their label seq to check the satis   ability of the query  We further optimize bidirectional BFS as follows  if a node in the BFS queue is reached through another    better    path 6 before being taken out of the BFS queue  that node   s label seqn is updated with the label seq seen on the    better    path  In our experience  the optimized bidirectional BFS performed better than the naive bidirectional BFS by an order of magnitude  For simplicity the optimizedbidirectional BFS method is referred to as    B BFS    in the rest of the text  5 2 Datasets and Queries We used 2 real RDF datasets  RDFized DBLP dataset by LSDIS lab     SwetoDBLP  2  and a smaller subset of UniProt 5 http   www w3 org TR rdf syntax grammar  6 A path is    better    if it has seen more labels in the LOCR label order Table 1  Datasets Characteristics  Edges  Nodes  Edge labels Max indeg Max outdeg Avg in outdeg Largest depth SCCs R Mat 14 951 226 4 085 180 253 19 13 053 3 65 12 0 UniProt 22 589 927 6 634 185 91 800 127 1046 3 35 10 423 SwetoDBLP 13 378 152 5 458 220 145 907 731 9245 2 44 66 146  0  0 001  0 002  0 003  0 004  0 005  0 006  0 007  0 008  0 009  2 4 6 8 10 12 Runtime  sec  Query length RMat BitPath   ve   0  1  2  3  4  5  6  7  8  2 4 6 8 10 Runtime  sec  Query length RMat BBFS   ve   0  0 0005  0 001  0 0015  0 002  0 0025  0 003  0 0035  0 004  2 4 6 8 10 Runtime  sec  Query length RMat FDFS   ve   0  0 0005  0 001  0 0015  0 002  0 0025  0 003  0 0035  0 004  2 4 6 8 10 Runtime  sec  Query length RMat DFS   ve   0  5e 05  0 0001  0 00015  0 0002  2 4 6 8 10 Runtime  sec  Query length RMat BitPath   ve   0  20  40  60  80  100  120  140  2 4 6 8 10 Runtime  sec  Query length RMat BBFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  2 4 6 8 10 Runtime  sec  Query length RMat FDFS   ve   0  0 005  0 01  0 015  0 02  0 025  0 03  0 035  2 4 6 8 10 Runtime  sec  Query length RMat DFS   ve   0  0 0001  0 0002  0 0003  0 0004  0 0005  0 0006  0 0007  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP BitPath   ve   0  0 5  1  1 5  2  2 5  3  3 5  4  4 5  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP BBFS   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP FDFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  0 07  0 08  1 2 3 4 5 6 7 8 9 Runtime  sec  Query length UniP DFS   ve   0  5e 05  0 0001  0 00015  0 0002  0 00025  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP BitPath   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP BBFS   ve   0  0 2  0 4  0 6  0 8  1  1 2  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP FDFS   ve   0  0 01  0 02  0 03  0 04  0 05  0 06  0 07  1 2 3 4 5 6 7 8 9 10 Runtime  sec  Query length UniP DFS   ve   0  0 02  0 04  0 06  0 08  0 1  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BitPath   ve   0  0 5  1  1 5  2  2 5  3  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BBFS   ve   0  0 002  0 004  0 006  0 008  0 01  0 012  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BitPath   ve   0  2  4  6  8  10  0 5 10 15 20 25 30 35 40 45 Runtime  sec  Query length DBLP BBFS   ve  Figure 3  Mean runtime with standard deviation for varying query size  Row 1  Positive queries for R Mat  Row 2  Negative queries for R Mat  Ro
</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management"/>
        <doc>Simple Data Storage and Manipulation For Scientists Charles Noneman ### Leonard McMillan Abstract When choosing a data management system  scientists are forced to select from using either spreadsheets or a relational database  However  neither of these systems is both  exible and powerful enough to ful ll all of a scientist s needs  We have developed a system that combines the simplicity of table creation and modi cation of a classical spreadsheet with the querying power of a full relational database  This system allows users to simply and e ciently input  query  and transform their data ###  1 Introduction Scientists are faced with a di cult trade o  regard  ing data management  This trade o  is between the two most common approaches to data manage  ment  spreadsheets and databases  Spreadsheets are very  exible  but lack the querying capabilities of a database  A database has powerful querying mecha  nisms and consistency guarantees  but is bound by a rigid schema  A spreadsheet is a group of cells organized on a grid  Each cell can contain data  typically in the form of strings  numbers  dates  and formulas  Cells are addressed by their column number  in practice represent by a letter  and their row number  Scien  tists  for the most part  are familiar with spreadsheets and are comfortable using them to store data and to do basic analysis  Typically no data management system is in place at the start of a project  so members of a group will each create ad hoc spreadsheets to store the data for their portion of the experiment  This leads to many di culties and complications as the project continues and as the team tries to analyse their data  One risk of using spreadsheets is the possibility of data loss or corruption  If one of the  les is accidentally deleted or incorrect data is written  there is no way to re  cover the information  Another problem is access to the spreadsheets  Teams often email sheets between members  or put the sheets on shared drives  The time and coordination required for this form of com  munication  combined with the lack of atomic trans  actions  means that data is only shared at the end of the experiment and that the integrity of the data is likely compromised  A better system would al  low researchers to query and reorganise data as it is generated so preliminary analysis can begin imme  diately and inconsistencies and missing data can be found and resolved before they are impossible to cor  rect  Another problem with the many spreadsheets approach is that someone must merge and factor the sheets by hand  This is a time consuming and error prone process  In addition to errors generated by lost columns or copying mistakes  the sheets themselves contain implicit information  such as batch numbers or the dates of experiments  which is destroyed in the combining process  Even if all of these issues are ad  dressed by a data management process and discipline on the part of the users  such as the process described in  6   the lack of proper joining and querying mech  anisms in spreadsheet systems means that the data will need to be imported into a database eventually  A relational database is collection of relations  commonly called tables  A relation has a  xed set of attributes and domain descriptions called a schema  A relation also has a set of tuples which are used to store data and all share the same attributes  At  tributes are commonly called columns and tuples are commonly called rows  When designing a relational database  one strives to represent the data in a nor  mal form  Normal form for relational data is a group of schemas where no data is replicated  This is im  1portant for e ciency and crucial for maintaining data consistency  Most relational databases systems are accessed via a query language  of which Structured Query Language  or SQL  is the most common  Like the spreadsheet approach to data storage  an approach based on a relational database system also does not  t the needs of scientists  Databases are more di cult to use than spreadsheets  Users lack the training in database theory to generate schemas in normal form and to write queries in SQL  This results in a need to hire someone to handle the database s design  This person will  rst have to de  sign the database schemas and then provide an in  terface for the researchers to use  This is the normal approach to database design and is called schema   rst 7   since the schema is completely or mostly de  signed before any data is entered  Schema  rst design assumes that the nature of the data is known a priori  but this is not often the case for real world projects  In the event that the researchers decide to run a new experiment  or simply want to add a new measure  ment to an existing experiment  the database schema no longer meets their needs  Users will need to re  purpose an existing attribute to  t their new needs or they will be forced to contact the database admin  istrators to perform the appropriate change in the schema  Additionally  if the researchers want to look at the data in a new way  they must have the adminis  trators write a new query before they can begin their new analysis  In practice there will be enough of these changes to the nature of the project that database ad  ministrators must be retained for the entirety of the project  Also  these changes take time  so users may revert to spreadsheets for everyday use  bringing all of the problems associated with spreadsheets into the system  Actively maintaining the schema in this way is both ine cient and expensive  Since the schema often cannot be determined be  fore data collection has started and updating the schema is challenging and fails to keep up with users  needs  it may be tempting to construct a database after the data is collected  This approach is called schema later  3   By using spreadsheets during data collection  researchers have the  exibility they want  and by then loading this data into a database  they will get the querying power of a relational database  The failing of this approach is the enormous task of converting the data into a form suitable for a database  There are many issues with data stored in a spread  sheet and many of them are di cult to resolve  Spreadsheet users will often put data of incorrect type into columns  For example   NA  into a column of real numbers  Dealing with these situations must cause either an over general typing strategy  such as making everything strings  or result in lost data  Users will include decorative information into a sheet such as a title or empty rows and columns  These must be removed before an import can occur  Users will include statistics in the bottom rows of a table  such as sums and averages  which must be removed  Since users lack a joining mechanism  they will create separate columns for a repeated measurement  such as  Weight 4 8 2010  and  Weight 4 22 2010   in  stead of putting these into a new table  This type of data must be transformed by hand or by writing code to parse the sheet  convert the data  and save the resulting sheets  Many of the design issues in spreadsheets come from one to many and many to  many relationships  such as the repeated measure  ment issue  These relationships are di cult to create in a spreadsheet and  in the many to many case  are di cult for users to understand since a table must represent a relationship and not just an entity  The process of converting from spreadsheet to database can easily become more work than actively maintain  ing the schema of a database  There are some partial solutions to these problems  Online spreadsheets facilitate sharing of information between users and provide access to old versions of the spreadsheet  but combining and querying sheets remains a challenge  Visual databases ease the cre  ation of queries 1   but still require an understanding databases and do little to aid in database design  In particular a user who has no database training is un  likely to store data in a normal form  even with visual tools  Additionally  schema rigidity is not solved by visual tools  Database usability is an active area of research 3   In this paper  we provide a description of a system  called S3  that is as simple to use as a spreadsheet  but also has the full power of a relational database  2This system allows each researcher to produce ta  bles and run queries that are easy to create  use  and change  By giving researchers the power man  age their own data  they can easily keep the schema and queries up to date and applicable to their cur  rent needs  Importantly  the system is always usable as a relational database with a changing schema  in  cluding the full expressive power of SQL  This form of technique is called schema during 7   The system is accessed using a web interface  Fig  ures 1 and 2  Having the database online ensures that users are always able to access their and other s information  Additionally  data can be entered di  rectly into the database  which avoids a typical time consuming and error prone approach of writing data by hand  entering it into a spreadsheet  merging the spreadsheet with other spreadsheets  transforming the data  and loading the  nal sheet into a database  Despite the  exibility in S3  users are encouraged and helped to convert their data into a more database appropriate form  For example  if the type of a col  umn is set to an integer  any cells that are not in  tegers are highlighted in red  By having an e cient and easy to use querying mechanism  users are more open to storing data in separate tables  unlike their tendency to want to force data into one large sum  mary sheet  By including statistics for columns and allowing aggregate functions in queries  users will not need to clutter the sheet by  lling cells with that type of summary information  The system is designed with a cell centric ap  proach  which allows for much of the  exibility seen in spreadsheets  This approach also enables the sys  tem to maintain cell history  which allows for users to recover historical information and  nd when it was entered  Since all of the data is stored in a relational database  the full power of the existing joining and querying system is always available  S3 provides a  gentle slope  approach to database design  It enables the transitions from data col  lection  to organization  to reorganization and to the transparent creation of a full  edged relational database  2 Related Work Researchers have been developing user friendly querying mechanisms for nearly as long as there have been databases  Query by Example  12  was one of the  rst  It involved users  lling in example values into columns to serve as placeholders for the values in the tuples that the database would return  Visual Query Engines  of which  1  is an example  show a digram of the schema of the relations and draw lines between the relations to represent foreign keys  These systems still require the user to create the query in SQL  or may support simple queries using a point  and click interface  Liu et al   4  describe an iter  ative querying mechanism which is displayed like a spreadsheet  Combinations of spreadsheets and databases have been proposed before  Tyszkiewicz  9  describes a method for translating SQL statements into equa  tions in a spreadsheet  Query by Excel  11  translates spreadsheets into a database  The data from cells is stored similarly to the way that S3 stores cells and the functions are translated using an extended SQL syntax described in  10   While previous work has focused on well designed databases in normal form  S3 is designed to address data input  querying  and manipulation on schema that may be poorly designed  Although S3 includes an easy to use querying interface and combines the concepts of spreadsheets and databases  these are only pieces of the whole  It provides a dynamic  yet fully functional  schema during the entirety of the data collection process  Joins  by default  are done using a method that allows scientists to easily access all relevant entities without having to consider the order of the joins  This means that data is simple to query and that it is easy to  nd where data is missing or incomplete  Finally  database refactoring is simple and is often done transparently for the user  3 Model First we will describe S3 from the user s perspec  tive  This will demonstrate the capabilities of the system and further clarify the problems being ad  3Figure 1  The hub page  Here  users can access and manage tables  enumerations  and reports  Figure 2  A typical table containing actual data  Users edit data as they would in a spreadsheet  Attribute information is edited in the sidebar  4dressed  The tasks performed by users are  adding data  querying that data  and manipulating the data to be closer to normal form  3 1 Adding Data A relational database requires the creation of a schema before data can be entered into a table  A schema is a list of the names of all of the columns and the data type number  text  date  etc  allowed in each column  Additionally column names must be unique within the table  Although these require  ments are good practice  in S3 this information is not required to create a table  Users may simply put data into the table and specify column name and type in  formation later  The system is designed to handle du  plicate or unnamed columns  Columns are assumed to store text by default  but this can be changed at any time  In a relational database  a column s type can only be changed if all of the data can immediately be converted to the new type  S3 always allows the change to happen and highlights problematic  elds in red  Additionally  extra information can be attached to a column  This includes units and a full text de  scription of the column s contents and intended use  A typical table is shown in Figure 2  In addition to standard types  S3 supports enumer  ation and  le uploads cells  An enumeration is a list of valid values for a column  For example  red  or  ange  and yellow could be listed as valid colors  Enu  merations are bene cial since they enforce a standard nomenclature  Files can be placed in cells just like data  this allows for images  video  or any  le type to be easily associated with applicable data  Users can then search for a sample and then simply click on the  le cell to open the  le  Sometimes  les are not merely stored in cells  but are a source of derived  elds  Data needs to be ex  tracted from these  often machine generated   les  These  les are handled by uploading them into cells like normal  les  A user with knowledge of the format writes a  lter to process one of these  les and asso  ciates the  lter with the column  S3 will run the  lter on all existing  les  new  les as they are uploaded  and new versions of  les  Each  le will result in data added to the current row or rows of data added to another table  as speci ed by the  lter  Since data often exists outside of S3  especially when converting from an existing project  importing directly from spreadsheets is supported by S3  Sim  ple data can be copied and pasted into tables  Ex  isting workbooks can be uploaded and split into any number of tables  As with typing into cells  these methods allow data of incorrect type to be entered into columns and highlights them in red  so no data is lost in the importing process  3 2 Querying Data The query interface is designed with simplicity in mind  Users  rst select which tables to include in the query  Then  for each table  users select which column contains data that is equivalent across the tables  A suggestion is given for a compatible col  umn for each table based on heuristics  such as col  umn name  matching data types  and joins in other queries  Users can then limit the selected columns to a subset of the tables  columns  Additionally users may add restrictions based on the data  For example limiting the data to mice born after a certain date  These queries can be saved as reports  Tables and re  ports can be exported in Common Separated Value  CSV  format  so they can be trivially imported into other applications  While the report generation interface is designed to handle common report cases  If more advanced queries are required  users can write arbitrary SQL queries based on the tables  All tables and reports result in actual views in the DBMS  3 3 Manipulating Data Most experiments involve a repeating measure of the same entity under di erent conditions  For exam  ple  a mouse s weight will be measured repeatedly at di erent points in its life  In a spreadsheet  a sci  entist will often create a column for each measure  ment  These columns might be   Weight 10 4 10    Weight 10 11 10    Weight 10 18 10  or simply   Weight 1    Weight 2    Weight 3   This is a bad data model  for a few reasons  Most crucially  im  portant information is stored in the column name  5where it cannot be accessed in a query  Additionally  the schema has to change whenever a new measure  ment is added  In the relational data storage model  the schema would look like   Mouse ID    Date    Weight  and there would be a row for every time any mouse was measured  The many columns method is commonly used in spreadsheets since users have never seen the correct way to store this data and even if they had  the lack of a sophisticated query mecha  nism would make data in normal form di cult to ac  cess  Additionally  users  nd a view with one line per mouse much easier to read  understand  and perform input on than one line per mouse measurement  To solve this problem  a combination of heuristics and user selection is used to identify these repeated measurement columns  Once the system identi es them  users have the option of viewing this data in three ways  The  rst is in the repeated columns that they entered  The second is a summary view that compresses the columns into a single column using an aggregate function such as mean  median  or sum  The  nal view has the data converted into the many  rows form of a database  This is achieved by taking every cell in the repeated columns and converting it into a row  This row contains a cell with the data part of the original column name  for example  10 4 10   a cell for the value in the original cell value  and    nally an exact copy of the data in the non repeated columns  Giving users access to this data in multiple forms means that they can interact with their data in the way that is easiest for the task at hand  4 System Design S3 is implemented in a relational DBMS  which gives S3 and its users access to the querying power of SQL  Despite storing information in a database  the data  model is more closely related to a spreadsheet  By representing the data in manner similar to that of a spreadsheet  S3 is able to avoid the limitations of a database  S3 makes cells the focus of the data model  instead of tuples like in a database  This allows for the data in cells to manipulated in ways that are not possible with a tuple centric model  4 1 Schema The basis of S3 is a representation of a  virtual  table  By representing a table instead of creating an actual database table  the system gains signi cant  exibility  This  exibility is apparent when considering the most important of the actual tables  the Cells table  Table 1  Table 1  Cells Table Cells value attribute id agglomeration id created replaced Each value in a virtual table has an entry in the Cells table  Each Cell has a reference to the Agglomerations table and the Attributes ta  ble  The Agglomerations table represents the vir  tual rows  The Attributes table represents the vir  tual columns and stores the name of the column  the data type that the system will use in views  and op  tionally units or any user notes about the column  Table 2  These are used to place the cell within a virtual table  An advantage of this approach to storing data is that NULL values take no space  They are stored implicitly by not having a Cell with a matching agglomeration id and attribute id  In a tradi  tional database  NULLs are stored by setting a speci c bit  or possibly even a byte  in the row header  Table 2  Attributes and Agglomerations Tables Attributes id name type id units notes Agglomerations id 6By storing the user s input as a string in the value  eld  the user s original input  and thus intention  can always be recovered  This method allows for the typing of a column to change without any risk of permanent information loss  Additionally it allows users to input data that does not match the column type  and then correct this mismatch at a later time  The enumeration and  le types are handled slightly di erently  For the  le type  when the user uploads a  le  it is stored and a record is added to the Files table which contains a unique id and some informa  tion about the  le  The id number is stored in the value  eld of the appropriate cell  For the enumer  ation type  the source attribute id is stored in the referring attribute s record  The Cells themselves have the agglomeration id of the wanted cell stored in value  like a foreign key  Unlike a traditional schema  entries in the Cells table are not updated when a user changes a value  A new entry is placed in the table  with created set to the current time  and the replaced  eld of the old Cell is changed from NULL to the current time  This maintains the history of the virtual table and allows for users to see the changes to the table over time  A few auxiliary tables exist to manage the vir  tual tables  columns  and rows  The Tables ta  ble contains the name and creation date of a table  The Table Columns table is a listing of all of the Attributes in a given Table and Table Rows is a list of all of the rows for any Table  This data model creates  exibility in not only the typing of the Cells  but in the location of the Attributes and Agglomerations  Attributes and Agglomerations can not only be moved within a ta  ble  but can be trivially moved to new tables  This provides a mechanism for schema refactoring  which is needed keep the schema applicable to current needs and to move the schema towards normal form  4 2 Views and Querying Both the virtual tables and the reports are mani  fest as views in the DBMS  The view for a Table is created by  rst creating a view for each Attribute  This is created by taking the Cells table and se  lecting only those cells where replaced is NULL  to get the current versions of the cells and the cells with the requisite attribute id  Only the agglomeration id and the value  cast as the appro  priate type  are projected  For the attribute with id equal to n  this can be stated in relational algebra as Equation 1   An   agglomeration id  value     replaced NULL  attribute id n  Cells     1  The Table view is then created by taking the Table Rows table  selecting the correct table id  t  and performing left joins against against each of the attribute views on agglomeration id  This is given by the expression in Equation 2    agglomeration id    table id t  Table Rows   n2NAn  2  To preform a query  users select which tables they want to join and an equivalent column for each of these tables  Optionally they can specify additional conditions  Users choose if they want the intersection of the values in the equivalent columns or the union  For an intersection an inner join is done between the views of the selected Tables  For a union  an SQL statement is constructed that  rst takes the union of the equivalent columns and then does a left join with each of the selected tables  With C as the set of views of compatible attributes and T as the set of tables  this yields Equation 3      A2C   value  A    V 2T V  3  This produces the report that users expect  In par  ticular every identi er is included no matter which Table it was in and each identi er appears only once  If instead of using the union method a chain of left joins was used  identi ers that did not appear in the  rst Table in the join would not appear in the result  If a full outer join is used  identi ers that are the same  but are in Tables that are not directly joined  will produce two rows if the identi er does not exist in any one of the intermediary tables  7Queries can be saved using a table called Reports  which stores the name of the Re  port  Report Wheres  Report Columns  and Report Tables store the selection criteria  cho  sen columns  and joined tables respectively  Report Tables additional</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sidmp2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#innovative_data_management"/>
        <doc>Managing Structured Collections of Community ### Data Wolfgang Gatterbauer University of Washington gatter cs washington edu Dan Suciu University of Washington suciu cs washington edu ABSTRACT Data management is becoming increasingly social  We observe a new form of information in such collaborative scenarios  where users contribute and reuse information  which resides neither in the base data nor in the schema information  This    superimposed structure    derives partly from interaction within the community  and partly from the recombination of existing data  We argue that this triad of data  schema  and higher order structure requires new data abstractions that     at the same time     must e   ciently scale to very large community databases  In addition  data generated by the community exposes four characteristics that make scalability especially di   cult   i  inconsistency  as different users or applications have or require partially overlapping and contradicting views   ii  non monotonicity  as new information may be able to revoke previous information already built upon   iii  uncertainty  as both user intent and rankings are generally uncertain  and  iv  provenance  as content contributors want to track their data  and    content re users    evaluate their trust  We show promising scalable solutions to two of these problems  and illustrate the general data management challenges with a seemingly simple example from community e learning     ce learning ###      1  A VISION  MASSIVE COMMUNITY E LEARNING WITH PAIRSPACE We will argue that management of collections of community data requires a new abstraction that does not    t well in the common dichotomy of data and schema information  We illustrate this idea with the vision of a massive online question answer learning community of users  grouped around a hypothetical tool we refer to as Pai rSpace  We prefer to keep the overall setup simple  This is a concrete community data management scenario that illustrates the main issues in this paper  while at the same time  seems to have a simple relational implementation  Note that the underlying challenges naturally extend to more complex and general community content management scenarios  5 th Biennial Conference on Innovative Data Systems Research  CIDR    11  January 9 12  2011  Asilomar  California  USA  This article is published under a Creative Commons Attribution License  http   creativecommons org licenses by 3 0    which permits distribution and reproduction in any medium as well allowing derivative works  provided that you attribute the original work to the author s  and CIDR 2011  1 day  3 days  1 week  1 month  6 months  correct  incorrect  Figure 1  Spaced repetition with    ashcard learning  Repetition intervals increase for subsequent boxes  Pai rSpace is a huge shared repository of learning nuggets organized into question answer  Q A  pairs that combines  a     ashcard learning with  b  spaced repetition and  c  a community built around it  Flashcards are sets of cards with a question on one side and an answer on the other  These cards are used as a learning drill to aid memorization of learning material through what is called    active recall 1      given a question  one produces the answer  Furthermore  those Q A pairs are usually grouped into collections of a similar nature  i e  meaningful learning units  Examples are the vocabularies of one lesson in a high school book  or the standardized questions to pass the US driving license in the State of Washington  Almost any cognitive subject can be translated into such a Q A format 2   Spaced repetition is a learning technique with increasing intervals of time between subsequent reviews of learned material  Items to memorize are entered into Pai rSpace as Q A pairs  virtual    ashcards   When a pair is due to be reviewed  the question is displayed  the user attempts to answer the question  and     after seeing the answer     decides whether he answered it correctly or not  If he succeeds  then the pair gets sent to the next box  if he fails it gets sent back to the    rst box  Each subsequent box has a longer period of time before pairs are revisited  Fig  1  3   Imagine a daily 1 In active recall  pieces of information are actively retrieved from memory as opposed to passive review  See  10  for a recent discussion  2 Further examples are  general cultural facts  such as world countries and their capitals   competition results for sports fans  e g   Who won the 2010 World Cup       lm facts for movie bu   s  e g   Who played William of Baskerville in    The name of the rose    of 1986    often asked terms during GRE and their synonyms  important paragraphs or cases in law  details on the periodic table in chemistry  multiplication tables in mathematics  names of bones and their location in the human body for medical students  basic formulae in any science  or lists of common abbreviations in computer science  e g   What does MVD stand for    3 The idea of spaced repetition traces back to the early 1930s  but only became later widely know as Pimsleur   s graduated interval recall  15   or    Leitner system     or    Ebbinghaus Forgetting Curve     While not widely popular in the USA     ashcard learning is hugely popular in Europe with several hundred  mostly o   ine    ashcard programsmorning routine in which a user repeats the learning nuggets that are due that day as suggested by the system  The third aspect is that those collections of pairs can be shared  re used  and even re combined  We envision one centralized and massive repository of Q A pairs for all disciplines  languages and kinds of human knowledge  This is the one central place  with obvious positive externalities  where learners go for repetitive learning needs to    nd relevant collections of information nuggets  to upload or combine pairs into new collections  and to train regularly  Major value of the stored information lies not just in the individual Q A pairs  e g   the translation of English    go    into Spanish    ir    can be easily found in any free online dictionary   but in the collection of these information nuggets into meaningful units of information whose mastery together allow the learner to acquire a certain skill level  e g   passing the knowledge based driving test   And this value is important to leverage when helping users    nd the right pairs  collections  or even other users with similar interests  Example 1  PairSpace Scenario   Alice is learning Spanish  She uplods Q A pairs of her    rst lesson  Bob is learning Spanish too and discovers Alice   s Spanish 1 lesson in Pai rSpace  His girlfriend is Mexican and has taught him to use andar instead of ir for go  He changes his Q A pair  go  ir  to  go  andar   Next assume Charlie is searching for basic Spanish lessons  What should the system return to Charlie  and how should it present this result  2  CHALLENGES FOR MANAGING COLLECTIONS OF COMMUNITY DATA The simple scenario of Example 1 already poses several challenges of how to search  return and present the results      What to return  Should the system return the collection Spanish 1 of either Alice or Bob  Should it present them as a derivation of each other with Bob   s collection as the most recent  or Alice   s as the original  Should it return just the intersection of Alice   s and Bob   s collections as new derived collection  Should it present and mark the tuples  go  ir  and  go  andar  as possibly con   icting pairs  Stated more abstractly  given two or more collections as input  how to inform and return to the user the structural variation in collections  How can the system learn to suggest new derived collections that    t the purpose of the user      How to bundle and present the results to the user  Can we take advantage of new    return structures    imposed by collections instead of returning individual pairs in an alltoo familiar list based fashion  cf  discussion in  3    If we have several partially overlapping and often complementary or contradicting collections  should we return collections or tuples by majority or by diversity  cf   20    Should we cluster these collection into meta collections in the search results  i e  go one level further in the abstraction      How to search  How does the user specify the information she is looking for  i e  what is the appropriate search paradigm for query formulation  What is the appropriate found on the Web  For example  Phase 6 is a German company entirely built around an o   ine    ashcard learning software that is used at 3 000 German schools and has supposedly been sold more than 500 000 times  source  http   www phase 6 com   The fact that there are hundreds of software tools available supports the thesis that one unique  and online Pai rSpace would a valuable tool to users  but nobody has yet    gured out the perfect solution to bring this to massive dimensions  cp  Friendster and LinkedIn before Facebook   User uses Collection contains Pai r  a  cname Alice go ir Bob uname Spanish 1 Spanish 1 User Collection Pai r Charlie go andar         Q A    b  Figure 2  An attempt at a relational encoding of Pai rSpace   a  an ER model  and  b  an instance  query language that     though possibly hidden from the user     allows to express the user   s search needs      What to include in ranking  What are those explicit or implicit features or associations that can be leveraged to learn and return relevant information to the user  Obvious candidates are the following   a  Semantic or syntactic similarity  How can one address synonymy and polysemy  For example  the expression bank can represent    river bank    in English     bench    in German  and a       nancial institution    in both languages   b  Structure  What is the generally appropriate way to think about the relative importance between pairs or items and collection of items   c  Trust or reputation  What is the appropriate abstraction of trust between users in this scenario  How can di   erent levels of trust be combined  i e  should they be de   ned either in a vote based  democratic  weight based  manner  or rather a rule based  strict  preference based  manner  For example  Charlie may specify to trust his school teacher strictly more than any other people  In such preference or rule based scenarios  the actual value of the weights does not matter  but rather the partial order between preference relations  18    d  Provenance  What kinds of provenance are suggested by this scenario  such as    social provenance     3   or derivative provenance  Should identical pairs speci   ed by di   erent users be linked to each other  How can one de   ne provenance on collections of items  How to incorporate all those forms of provenance into an appropriate ranking function  How to support querying those combined forms of provenance  e g  to support explanatory queries over this repository  One fundamental problem is already the question of how to best store  manipulate  and update all involved information over time  Figure 2a shows a simple ER model of the relation between users  collections and pairs in our scenario  Figure 2b is a simpli   ed depiction of the scenario of Example 1  If the collection of Alice contains 100 tuples  then storing Bob   s incremental variation would take   2 100 th of the space of Alice   s original lesson  For the sake of discussion  let   s call replicating all pairs as the explicit representation  alternatively eager or materialized   and some other representation that stores only the di   erence as implicit  alternatively lazy or virtual   But space is not the major issue here  even if the explicit representation is stored in a compressed form  valuable information about the relative derivation or evolution of content is lost  Note  that during querying  value lies not so much in the individual pairs or collections  but rather in the knowledge of how close two or more collections relate to each other  In turn  storing only the implicitinformation may decrease the access to the actual data considerably  Hence  there is this inherent trade o    between having the data explicit  or the relative derivations explicit  How to update those derivations if content evolves and users add  update  delete or transfer pairs between collections  Summing up the three main challenges that this seemingly simple scenario of managing three kinds of entities  items  collections  users  in a community scenario raises are   1  What is the right abstraction for the logical and physical representations of this partly redundant  partly overlapping information  grouped into vastly overlapping bundles   2  What is the right abstraction of a data manipulation and query language that allows one to reason in terms of collections rather than items   3  How to evaluate relative importance over the triple concepts of  items  collections  users  in a sound and principled way  In addition  how to reason about  i  inconsistency   ii  non monotonicity   iii  uncertainty  and  iv  provenance at the level of collections  3  WHY EXISTING MODELS AND APPROACHES DON   T SUFFICE Here we brie   y summarize related work that focuses on these challenges but fails short in solving them entirely  The overall area falls into what is classi   ed as sharing systems in  5  where users together build shared structured knowledge bases or a consistent data synthesis  In our scenario  the users do not share the goal of structured knowledge creation  but rather want to    nd individually    tting collections of Q A pairs that    t their respective learning needs  Our scenario is clearly di   erent from data integration or data fusion where the goal is to create one uni   ed view on the data  12   Instead  we want to e   ciently manage     nd  and compose meaningful collections bundles structures of base data that evolve over time  Our challenges are also reminiscent to those of dataspaces  11   where the focus is on incremental     pay as you go     integration  The value of the system increases over time with the number of matches between the data  In our scenario  collections of items have di   erent meanings to di   erent users at di   erent times and need to be managed from day one  The scenario is also related to the problem of con   ict resolution in community databases with the goal of automatically assigning each user in the system a unique value to each key  18  9   However  in our scenario  content import is    pull    instead of    push     i e  users actively search for content  In the scenario of searching over Yahoo  answers  1  2   the goal is to order the set of question answer pairs according to their relevance to the query  In our scenario  the goal not just to rank just pairs  i e  user generated content  but rather  or alternatively  collections of pairs  which may exist or may be re combined   or to suggest relevant users  Our notion of collections is also very reminiscent of superimposed information  14   i e  data that is placed over existing information to help organize and reuse items in these sources  One main di   erence to superimposed information management is the community aspect  we have di   erent alternative and overlapping collections of base information  and value lies not just in the groupings  but also the di   erence between alternative groupings  The concept of    nding and managing associations on top of base data is also related to inductive databases  16  and pattern base management systems  4   Both try to manage rules built upon base data as separate information inside an enhanced DBMS  The di   erence is that the collections that we are interested in do not have in general an intensional semantics  That means they cannot by themselves be expressed in a short implicit form  e g   by a query  cf   17    Rather  we are interested in the incremental and evolving di   erences between collections of base data  And we want to leverage this information about    data interference    during the ranking process  Our scenario is also reminiscent of revision control systems  RCS   such as SVN  which manage incremental changes to documents  programs  and other information  and optionally include compression  But while RCSs can store di   erences e   ciently  they do not expose general query facilities to sea</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sie09p1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sie09p1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#sig09information_extraction"/>
        <doc>Workload Balanced Processing of Top K Join Queries on Cluster Architectures ### Renwei Yu  Mithila Nagendra  Parth Nagarkar  K  Selc  uk Candan  Jong Wook Kim CIDSE  Arizona State University Tempe  AZ 85287  USA  renwei yu mnagendr pnagarka candan jong  asu edu Abstract    The observation that a signi   cant class of data processing and analysis applications can be expressed in terms of a small set of primitives that are easy to parallelize has resulted in increasing popularity of batch oriented  highly parallelizable cluster frameworks  These frameworks  however  are known to have shortcomings for certain application domains  For example  in many data analysis applications  the utility of a given data element to the particular analysis task depends on the way the data is collected  e g  its precision  or interpreted  However  since existing batch data processing frameworks do not consider variations in data utility  they are not able to focus on the best results  Even if the user is interested in obtaining a relatively small subset of the best result instances  these systems often need to enumerate entire result sets  even if these sets contain lowutility results  In this paper  we introduce and describe uSplit  a data partitioning strategy for processing top k join queries in batch oriented cluster environments  In particular  we describe how uSplit adaptively samples data from    upstream    operators to help allocate resources in a work balanced and wasted work avoiding manner for top k join processing  Experimental results show that the proposed sampling  data partitioning  and join processing strategies enable uSplit to return top k results with high con   dence and low overhead  up to     9   faster than alternative schemes on 10 servers ###   I    IN T RO D U C T I O N In many applications  the utility of the elements in the database to a particular task varies from data instance to data instance and users are interested not in all the data  but the ones that are best suited for the given task  Locating such high utility data is known as top k query processing  Applications that involve varying data and feature utilities include decision support and text and media analysis  in the case of text  the popular TF IDF keyword score is an example   In this paper  we investigate whether applications that require top k processing over large volumes of data can bene   t from recent developments in highly parallel  cluster based data processing  The observation that     while not all  30      a signi   cant class of data processing applications can be expressed in terms of a small set of primitives that are in many cases easy to parallelize  has led to popularity of batch oriented  highlyparallelizable cluster frameworks  such as MapReduce  18    3   Dynamo  19   Scope  10   PNUTS  17   HadoopDB  4    These systems have been successfully applied in data processing  mining  and information retrieval domains  27    32   Given an atomic task  these rely on the simple semantic properties of the task to partition the work onto many machines  R1 score  X                          R2 score  X  0 90 0 85 0 80 0 80 0 70 0 75 X2 X5 X6 X3 X5 X2 0 825 0 8             sorted access sorted access 0 70 0 75 0 60 0 74 0 50 0 74 0 40 0 70 X6 X4 X1 X3 X2 X6 X1 X4 0 72 0 40 0 625 X3 0 70 X4           random access           Fig  1  Typical top 3 join processing  a sorted access phase provide initial candidate results  a random access phase contributes additional candidates tat may have been missed during sorted access  the monotonicity of the combination function  average in this example  makes it possible to stop without having to consider all the data Example 1 1  MapReduce   MapReduce is a functional list processing language  18  where a data processing work   ow is constructed using map and reduce primitives  The userprovided map primitive takes a set of input key value pairs and produces a set of intermediate key value pairs  In essence  the map function extracts a set of features from the input data  The intermediate values from the map primitive are often supplied to a reduce function  The reduce function takes these intermediate key value pairs  i e   features  and produces its output by applying an aggregate function on all input values with the same key  The user provided map and reduce primitives can be combined into more complex work   ows  These systems achieve high degrees of scalability by carefully allocating resources to available processing elements and leveraging any opportunities to parallelize basic processing tasks  Signi   cant savings in execution times are obtained by independently parallelizing each step of the work   ow and executing them  except for some very recent efforts  such as  16   in a batched manner over a cluster of servers  It is important to note that such batch oriented processing is not limited to systems based on MapReduce  but also to most parallel DBMSs  such as Vertica  2   which rely on row  or column based data partitioning to support parallelism  In this paper  we note that existing batch data processing frameworks  whether based on MapReduce or traditional row or column partitioning in parallel databases  have dif   culties in domains where data utility is variable and present a novel data partitioning strategy called uSplit  which support workload balanced processing of top k join queries on server clusters 2 high utility T Top k results a Source  t Enumerated  but  pruned candidates T k Data Source 1 Dat high low utility low Data Source 1 high utility utility Fig  2  Relying on sorted  and random access operations  top k algorithms focus their processing to the high utility corner of the utility space  here  the curve represents the threshold    k  de   ned by the lowest of the scores of the best k results  consequently  and results below this threshold will be pruned  small circles under the threshold correspond to enumerated but pruned results due to random accesses  A  Related Work on Data with Non Uniform Utilities When data utility is non uniform  users are often not interested in obtaining all possible results to a query  but only the k best results  While batch based systems promise large scale parallelism  since they do not consider variations in data utility  they are unable to scale most effectively by focusing on the data that is likely to produce the best results  To avoid waste  data processing systems need to employ data structures and algorithms that can prune unpromising data objects from consideration without having to evaluate them  This is often referred to as ranked or top k query processing  5    8    11    20    21    24   Most existing ranked query processing algorithms  including Fagin   s algorithm  FA   20    21   threshold algorithm  TA   22    NRA   22   and others  such as  12    28    23    26   assume that one  or both  of the following data access strategies is available   a  streaming pipelined access to the sorted data to identify a set of candidates  and  b  index based random access to verify if these are good matches or not  Given monotonic 1 queries on the data  these help identify good candidates and prune non promising ones quickly  Figure 1   Figure 2 graphically represents the portion of the utility space covered during the top k join operation  Here  the curve represents the threshold    k  de   ned by the lowest of the scores of the best k results to be returned to the user  the shape of the curve depends on the combination function  Consequently  all the join work to produce the results above this curve is useful  Note that  as shown in Figure 2  top k join algorithms may also identify candidate results below this threshold and these need to be pruned during post processing  In systems which partition data and process them in batches  however  top k query processing cannot be ef   ciently supported  A critical dif   culty with parallelizing the top k algorithms  such as FA  TA  and NRA  is that the process is inherently sequential  as shown in Figure 1  in the    rst sorted access phase one pointer per data source is used for scanning each data stream in decreasing order of utility  This prevents ef   cient parallel implementations of these algorithms  Consider for example a naive parallelization scheme where the 1 An object that is as good as another one in all individual features is also better than the other object when these features are considered simultaneously  ty 1 1 high utili 2 2 1 2 w utility 1 Data low utility high utility low 1 1 Data 1 1 2 1 2 2 high utility T k Global top k results a Source  t Data Source 1 Da high low utility low Enumerated  but   locally  pruned  candidates Data Source 1 high utility low utility Fig  3  Random partitioning of the data will waste resources for producing large number of candidates that will be eventually pruned  points with different shapes correspond to enumerated but pruned results in different servers  ility LH HH high ut ow utility low utility high utility LL HL ol 2 3 2 high utility Top  k results 4 ata Source 1 T k Estimated boundary Data Source 1 Da low utility lowlow Data Source 1 high utility high utility Fig  4  The proposed algorithm estimates the cut off   k to prevent redundant work and partitions the useful join work above the threshold to available servers to speed up the join processing  This enables utility aware resource allocation for top k join processing join space is randomly split among U servers such that each server produces its own k results to be combined into a    nal candidate set from which the top k results will be selected  Since  top k join algorithms have O k 1 m J 1    1 m   complexity with high probability for m way joins  where J is the number of all potential join results  21   in this approach  each server would need to perform O k 1 m   J U  1    1 m   work to produce its local top k results  When for example m   2  the amount of per server work would be O    1 U     kJ   In other words  given U servers  the per server work is    1 U of the total 2 way top k join operation  not close to 1 U that would be the case if there were no wasted work 2  Figure 3   B  Our Contributions  Utility Sensitive Data Partitioning for Parallel Top k Join Processing In this paper  we describe a data partitioning strategy called uSplit  to support utility sensitive top k data processing  uSplit targets the needs of applications where  a  utilities of the data elements are non uniform  and  b  signi   cant gains in performance can be obtained by focusing on data that will produce highly ranked results  Figure 4   More speci   cally  our goal is to  a  estimate the threshold   k  the lowest score of the top k results  precisely to prevent wasted work and  b  develop a data and work partitioning strategy that can partition and assign the useful join work above this threshold to the available servers  Figure 4   2 Note that this naive partitioning strategy could provide close to    1 U gain when m is large  however  top k algorithms are often avoided for large m  because this would result in a total of O J  work  I e   a full join would need to be done anyhow  this is known as the curse of dimensionality  1  Data and Query Models  We assume a very simple key value data model   key value   Without loss of generality  each key value pair is extended with a utility score  u  between 0 and 1   key value u   In this paper  we focus on topk join queries over such data  For example  in a video recommendation application  the query top pairs   top k join male actors by movieID  female actors by movieID  avg m actScore f actScore   100 on two tables male actors m actorID  movieID  m actScore  and female actors f actorID  movieID  f actScore   would locate    highest scoring male female actor pairs who have played together in at least one movie    by equi joining the two tables on movieID and identifying the top 100 results on avg m actScore f actScore   More generally  we consider two way  top k join operations of the form R1   C R2  where C is the join condition  Each data element  t  in R1 and R2 has an associated utility score between 0 and 1  i e   t u      0  1   There is also a monotonic utility combination function        associated with the join operation  Given two elements  t1     R1 and t2     R2  possi   ble combination functions include Euclidean    Euc t1  t2     t1 u  2    t2 u  2   Average    avg t1  t2    t1 u t2 u 2   Product    prod t1  t2    t1 u    t2 u  Min    min t1  t2    min t1 u  t2 u   and Max    max t1  t2    max t1 u  t2 u  functions  6    20   Given a target score  these functions de   ne different utility boundaries on the utility space of results  2  Utility Sensitive Data Partitioning  In this paper  we develop utility based partitioning and resource allocation schemes based on two major criteria   a  wasted work criterion  this takes into account how much of the system resources are allocated to produce results that are not in top k and  b  partition work balance criterion  any imbalance in the work assignment means that certain system resources will stay under utilized  while others are over utilized  uSplit repartitions input data sets on the    y  during their assignment to the servers for join processing  in terms of data utility  To support this  uSplit collects relevant statistics to estimate the utility score    k  of the k th output result  and avoids allocating processing resources to those data combinations that will produce results with utilities less than   k  Note that  since the results may not be uniformly distributed in the utility space  both the shape of the utility boundary and statistics about the result distribution as a function of the input data utilities are needed to estimate the   k lower bound  Intuitively  this is similar to the    ltering strategy that has been successfully applied for top k query processing in relational databases  14    7    8    9   In the context of batched data processing  however  existing    ltering techniques cannot be directly applied due to the lack of index structures that provide ef   cient    ltering  and the lack of prior statistics about the intermediary data produced and consumed within a work   ow  3  Con   dence Target Parameter  Since we are relying on statistics to estimate the   k lower bound  we also take a      p11 p12 p1U      p21 p22 p2U      r11 r12 r1n      r21 r22 r2m selectivities R1  partitioned in utility  R2  partitioned in utility  R1  partitioned in value  R2  partitioned in value   c  Estimate join work distribution as a function of  d  Repartition those input data  which may result  in top   k results  in a balanced manner data utilities  b  Estimate result utility lowerbound   k based on      a  Estimate join selectivity as a function of data utilities  e  At each server  perform the corresponding join  f  Combine the results Fig  5  Top k join processing with uSplit con   dence target as a query processing parameter from the user  For example      95  con   dence target would mean that the user allows for up to 5  chance that the system may return less than k top results  In other words  high con   dence target indicates that the user wants the system return closer to k  top  results with high probability  whereas a low target indicates that the user might be satis   ed with less than k matches in most cases  Otherwise  the query model is deterministic in that the system will not return any results that are not in topk  As we will see in Section III  this con   dence target helps the system to choose tighter or looser   k thresholds based on the available statistics  Tighter thresholds means less wasted work  but may result in some misses  4  Summary  Figure 5 visualizes the steps of the uSplit top k join processing   a  uSplit    rst estimates the join selectivity as a function of the input data utilities and then  b  computes the lower bound    k  on the utilities of the top k results based on these selectivities  Next   c  uSplit estimates the amount of  useful  work needed for computing the topk results  and based on this estimate   d  it repartitions the input data in a work balanced manner  before assigning these partitions onto the available servers for processing   e  Each server creates the necessary data structures and performs the assigned join task  Finally   f  the results from the individual servers are combined to select the best k results  I I   RU N T I M E STAT I S T I C S CO L L E C T I O N Note that we cannot assume independence of join selectivities from the score distribution  Therefore  we need statistics to identify the join selectivities as a function of the input utilities  this enables that  given the con   dence limit  the cutoff can be estimated as tightly as possible  Since we cannot assume that statistics about the intermediary  transient data within the work   ow are available in advance  uSplit    rst collects any statistics needed to estimate the join selectivities as a function of the data utility 3   uSplit performs this by sampling intermediary data along the utility scores  3 Note that this is especially important when the join attributes and the utilities are not completely independent 0DS RU 5HGXFH  QSXW  DWD 2XWSXW  DWD 2XWSXW 6DPSOHV                     6WDWLVWLFV 6DPSOLQJ 6WUDWHJ  LQSXW GDWD WR WRS   MRLQ SURFHVVLQJ PHWDGDWD WR KHOS DOORFDWH UHVRXUFHV IRU WRS   MRLQ SURFHVVLQJ Fig  6  Piggy backed sampling  Given a sampling strategy  each  upstream  operator produces samples to support down stream operations R1 R2 top  k join out1 Samples1 out2 Samples2 S11 N d 11 out11 out12 out13 out14 out21 out22 out23 out24 S12 N d 12 S13 N d 13 S14 Node14 S21 N d 21 S22 Node22 S23 Node23 S24 Node11 Node12 Node13 Node14 Node21 Node22 Node23 Node24 Node24 in1 in2 Fig  7  Piggy backed sampling at the upstream operators Since the input data to the top k join operation itself may be produced using a parallel executing operator  we have three alternative runtime sampling options      Piggy backed sampling  For intermediary data generated within a work   ow  in order to eliminate unnecessary passes over the data  one option is to piggy back the sampling work on the    upstream    operators  each pararellel instance of the    upstream    operator produces a set of samples  using reservoir sampling technique  35   as it produces its output data  as shown in Figures 6 and 7      Partial output sampling  In this alternative  each upstream machine produces samples on its local output and later these individual samples are combined to obtain the full set of samples  Figure 8   Note that in this method  a full scan of the data is required  but each upstream machine can sample its data in parallel with the others      Combined output sampling  In this option  after the outputs of the upstream operators are merged  one server node samples the full combined data set  Figure 9   Note that  also in this method  a full scan of the data is required  Note that piggy backed sampling is the most ef   cient alternative as it avoids an extra scan of the data and allows each upstream machine to sample its own output data in parallel to the others  Partial output sampling is faster than combinedoutput sampling  since it also allows the data sampling process to be run in parallel in upstream operators  The combinedoutput sampling option requires a full  sequential scan of the combined data  however  this option may be necessary to avoid bias in sampling when the data partitioning attributes for the upstream operator and the data utilities relevant for the downstream top k operator are not independent  R1 R2 top  k join Samples1 S11 out1 S12 S13 S14 Samples2 S21 out2 S22 S23 S24 Node11 out11 out12 out13 out14 Node12 Node13 Node14 Node21 out21 out22 out23 out24 Node11 Node12 Node13 Node14 Node22 Node23 Node24 in1 Node21 Node22 Node23 Node24 in2 Fig  8  Partial output sampling at the upstream operators R1 R2 top  k join out1 Samples1 out2 Samples2 Node11 out11 out12 out13 out14 Node12 Node13 Node14 Node21 out21 out22 out23 out24 Node11 Node12 Node13 Node14 Node22 Node23 Node24 in1 Node21 Node22 Node23 Node24 in2 Fig  9  Combined output sampling at the upstream operators It is important to note that obtaining samples from joins is extremely dif   cult  In  15   Chaudhuri et al  showed the need for non oblivious and non uniform sampling strategies   15  also showed that there are lower bounds on the sizes of the samples even when these samples are generated in a nonoblivious manner  Here  we propose to address this dif   culty by budgeting the samples for top k processing intelligently where they matter the most  A  Allocation of the Data Sampling Budget Let us reconsider the ranked join operation  R   R1   C R2  where C is the join condition   R1    N  and  R2    M  Each data element  t  in R1 or R2 has an associated utility score  t u      0  1   The    rst task of uSplit is to sample R1 and R2 to estimate the join selectivities as a function of data utilities  Since the overall goal of the sampling process is to support top k joins  it is most important that the selectivities are estimated with higher precision towards the higher end of the utility spectrum  Therefore  instead of sampling R1 and R2 uniformly across the utility space to obtain histograms that are equally precise in all regions  as is commonly done in traditional databases  13    29    31    uSplit targets higher precision for higher utilities  Thus  uSplit segments the utility range and allocates samples to different ranges  in particular  as shown in Figure 10  the higher the utility of the segment  the larger the number of samples allocated  Let 0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1 denote the xand y boundaries 4   which are used for segmenting the two dimensions of the utility space  i e   xi    xi   1  xi   and yj    yj   1  yj  are the utility segments corresponding to the two dimensions of the utility space 5   For 1     i     n and 4We discuss how to set segment boundaries in Section II C  5 The segments x1    0  x1  and y1    0  y1   which are closed in both sides  are minor exceptions  Uniform  Sample Distribution 0 0  0 2 0 2  0 4 0 4  0 6 0 6  0 8 0 8  1 0 Utility Ranges Range  Adaptive Sample Distribution 0 0  0 4 0 4  0 84 0 84  0 96 0 96  0 99 0 99  1 0 Utility Ranges  a  Uniform sampling  b  Adaptive sampling  c  Join result samples produced with adaptive sampling Fig  10  Utility range adaptive sampling of input data ensures that more join result samples are collected at the high utility range of the utility space 1     j     m  let R1 i and R2 j   be the subsets of R1 and R2 in the corresponding utility ranges  and let r1 i    R1 i   and r2 j    R2 j   denote the number of data elements in each segment  In addition  let s1 i and s2 j   where   1   i   n s1 i   s1 and   1   j   m s2 j   s2  denote the number of samples uSplit allocates to the utility segments xi and yj  Here  s1   N is the overall sampling budget for R1  and s2   M is the sampling budget for R2  Given this sampling budget  uSplit estimates the join selectivities across the utility space by  a     rst sampling the input data sets R1 and R2  for each segment xi  1     i     n  and yj  1     j     m   for s1 i and s2 j data elements  respectively  and then  b  joining all resulting pairs of sample sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    where  S1 i     s1 i and  S2 j     s2 j   This provides a count  Ci j    S1 i   C S2 j    of results for each pair of utility segments xi and yj   1     i     n and 1     j     m  Ci j is then used for estimating the number of results in R   R1   C R2   R1 i   C R2 j       Ci j    r1 i    r2 j s1 i    s2 j   In other words  the join selectivity for a given pair  xi and yj  of segments is     Ci j s1 i  s2 j   B  Impact of the Sample Size on the Quality of Join Selectivity Estimates When s1 i   r1 i and s2 j   r2 j   the total cost of the sample based selectivity estimation process will be much less than the cost of the full join of the data  Despite these savings  however  it is important that the samples are allocated to the segments in such a way that the sampling errors are minimized at the higher end of the utility range  To achieve this in a systematic way  uSplit quanti   es the error margins associated with the selectivity estimates and identi   es a sampling strategy that minimizes these margins  Let us consider a pair  xi and yj  of utility segments and the corresponding sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    of samples  When a sample vl     S2 j joins with uh     S1 i   we denote this as matchh l   1  Similarly  a mismatch between vl     S2 j and uh     S1 i is denoted as matchh l   0  Let us assume that uh     S1 i matches with counth many samples out of the s2 j samples in S2 j   The mean likelihood    h  of a match to uh     S1 i by data in S2 j can be computed as   h   1 s2 j   1   l   s2 j matchh l   counth s2 j   uSplit similarly computes the standard deviation  stdh  of the matches to uh as std 2 h   1 s2 j   1   l   s2 j  matchh l       h  2   Lemma 2 1  Selectivity of a Single Entry   Given the above de   nitions of mean likelihood of match    h  and the standard deviation  stdh  if we want to be 100     1          con   dent of the rate of match we are computing for uh  then  assuming that the likelihood of matches are normally distributed 6   samples are randomly selected  and the number of samples are much smaller than the amount of data in the utility segment     i e   s2 j   r2 j   7 we can associate the following con   dence interval for the true likelihood of match         h     h     t s2 j   1     2     s2 j   stdh            h       h   t s2 j    1     2     s2 j   stdh    Here  the value t s2 j   1     2 is the t distribution  with  s2 j     1  degrees of freedom and corresponding to 100     1         percent con   dence  34   This con   dence interval implies that the margin of error  Eh  in the expected rate of matches for uh is Eh   2    t s2 j    1     2     s2 j    stdh  Since the t values tend to be relatively constant for degrees of freedom greater than 5  in general  for a given con   dence target     the error margin is inversely proportional to     s2 j   Theorem 2 1  Join Selectivity Estimate   Given tuples t1     R1 i and t2     R2 j   the likelihood of the combined tuple 6 It is empirically known that the con   dence intervals for    based on normality is highly reliable  in the sense of providing good coverage  even when the real distribution of the data is much different from normal  34   7When  S2 j    R2 j     s2 j r2 j     0 05  the con   dence interval can be corrected by reducing the margin of error with a    nite population correction factor  f     r2 j   s2 j r2 j   1  34   t1  t2  being in R1 i   C R2 j is   i j   1 s1 i   1   h   s1 i   h   1 s1 i   1   h   s1 i counth s2 j   Ci j s1 i    s2 j   The error margin  Ei j   corresponding to this rate of match at 100     1         percent con   dence is Ei j   2    t s1 i   1     2     s1 i    stdi j     Proof Sketch  Let us consider a pair  xi and yj  of utility segments and the corresponding sets  S1 i    u1  u2          us1 i   and S2 j    v1  v2          vs2 j    of samples  When a sample vl     S2 j joins with uh     S1 i   we denote this as matchh l   1  Similarly  a mismatch between vl     S2 j and uh     S1 i is denoted as matchh l   0  Let us assume that uh     S1 i matches with counth many samples out of the s2 j samples in S2 j  Now  let us consider a set of samples  S1 i    u1  u2          uh          us1 i    from the segment xi  and the corresponding rate    h  of matches with the tuples in yj   we can compute the average rate of matches of tuples in Si 1 with the tuples in yj as   i j   1 s1 i   1   h   s1 i   h  Similarly  the corresponding standard deviation  stdi j   is computed as std 2 i j   1 s1 i   1   h   s1 i    h       1 i   2   Thus  once again  if the likelihood of matches are normally distributed 8   samples are randomly selected  and s1 i   r1 i   then  targeting a statistical con   dence rate of 100   1         we can associate the con   dence interval     i j     t s1 i   1     2     s1 i    stdi j     i j   t s1 i   1     2     s1 i    stdi j   to the true rate of matches of tuples in xi with the tuples in yj  Here  the value t s1 i   1     2 is the t distribution  with  s1 i   1  degrees of freedom and corresponding to 100   1       percent con   dence  34   Note that  this true rate of matches  of tuples in xi with the tuples in yj   is nothing but the selectivity of the join between the tuples in partitions xi and yj  Thus  the theorem follows    Theorem 2 2  Sample Allocation   Given parameters   1    2   1 0 allocation of the sample budget such that    i j s1 i      2 1    s1  i   1  and s2 j      2 2    s2 j   1   subject to the overall sampling budget constraints 1   i   n s1 i   s1 and   1   j   m s2 j   s2  ensures that 8 See Footnote 6  the margins of errors decrease exponentially with increasing segment index  i e   with rates      i 1 and       j 2 for partitions xi and yj  respectively     Proof Sketch  As Theorem 2 1 implies  for a given statistical con   dence target 100     1           higher sampling rates s1 i and s2 j will lead to lower margins of errors  More speci   cally  since the t values tend to be relatively constant for degrees of freedom greater than 5  for a given con   dence target     this error margin is inversely proportional to     s1 i   From this  it follows that if we allocate more samples at the higher end of the utility spectrum  i e      i j s1 i   s1  i   1  and s2 j   s2 j   1 such that the ratio of the samples for consecutive segments are proportional with   1 and   2  respectively  the margins of errors decrease exponentially with increasing segment index    C  Selecting the Utility Partition Boundaries The sample allocation scheme described above considers the order of the segments  but is agnostic to where the segment boundaries are actually located in the utility space  This  however  can pose two dif   culties   a  Firstly  since our goal is to compute selectivities more precisely at the upper end of the utility spectrum  setting the boundaries of the segments  xi and yj   tighter for those segments that are nearer to 1 0 should help reduce errors at the high utility spectrum   b  Secondly  the error margin computation in Theorem 2 2 assumes that   h and stdh properly model the distribution of the likelihood of matches  This assumption will often hold for suf   ciently large segments  but will be more likely to be violated if the segment contains a small number of elements  Consequently  for extremely biased data distributions  such as the Zip   an  34   where there are only few high utility elements  naively selected segment boundaries can result in high utility segments with too few elements to support sampling with predictable error margins  Therefore  segment boundaries need to be selected carefully  To ensure tighter segments closer to 1 0  we adjust the segment boundaries  0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1  using two segment scaling factors    1    2   1 0  such that    2   i   n   1      xi     xi   1    2   j   m   2      yj     yj   1  subject to the constraints    1   i   n    n   i 1   xn   1  and   1   j   m   m   j 2   ym   1  where   xi   xi     xi   1 and   yj   yj     yj   1  For data sets where the utilities are highly biased  such as normal or Zip   an  where there are insuf   ciently many high utility data  sampling irregularities need to be prevented  We refer to this as the de biasing of the utilities  In general  scores in a biased distribution can be de biased 9 by considering highlevel data distribution parameters  9 De biasing is used only when determining the partition boundaries  during query processing and ranking  the actual utilities are used Theorem 2 3  De biasing   Let   D denote the utility distribution of a given data set  D  Given a utility score  u  let us replace it with u     such that u     C DF   D  x      a   a     D     a u     u    D    where C DF is the cumulative distribution function of   D  For any two ranges   v        v      and  w        w       in the transformed utility space  if  v         v            w         w         then the number of data entries within these two ranges are the same    Proof Sketch  Given  u        u      in the transformed utility space  it is easy to see that u         u           a   a   D   u    a s   u     D    for some u    and u   Similarly  v       v           a   a   D   v    a s   v     D    for some v    and v   Thus  if  u         u           v         v        then it follows that   a   a     D   u      a s     u        a   a     D   v      a s     v       Note that in a Zip   an data set with N  elements  if the number of elements above a given utility score  u  is proportional to 1 u   then N      c u elements have utilities less than or equal to u  for some c  Thus  the de biasing theorem implies that rescaling the utility scores of a Zip   an distribution as u     N     u     c N     u   de biases the data  Similarly  in data with normal utility distributions  rescaling the utility scores as u     C DF N ormal avg  stdev   x    1 2   1 2 Erf   u     avg stdev     2     where avg is the mean  stdev is the standard deviation  and Erf is the Gauss error function  34   de biases the data  In uSplit  extreme bias in the data utilities is detected and high level distribution parameters are estimated by a partial pre sampling step in the upstream operator  carried out before the overall data sampling strategy is decided  For example  whether a data set is normal is validated through statistical tests  such as the Kurtosis test which measures the peakedness or    atness of a distribution relative to the normal distribution  applied on a small initial sample  D  Discussion In practice  the sampling budget needs to be selected in such a way that the cost of statistics collection  scan   sample join  is below a maximum target sampling cost  This can be achieved using standard scan and join cost models  There are two concerns in selecting segment and sampling scaling factors     and     respectively   Firstly  as Figures 15 and 17 in the Experiments section  Section V  show  these cannot be too close to 1 0  since uniform sampling introduces higher errors   Similarly  values that are much larger than 1 0 can negatively impact performance by requiring too many samples in too small regions of the utility space  Experiments have shown good results for        1 15 and        2 0  Finally  again experiments reported in Section V show debiasing ensures that the con   dence targets on the estimation of   k can be matched even if the underlying distributions are signi   cantly skewed  Therefore  despite its minor additional cost  the pre sampling step should not be skipped  I I I   CO M P U T I N G  T H E LOW E R  B O U N D    k In the previous section  we discussed how uSplit segments the utility space and samples the input data sets in a way that enables the join selectivities to be estimated with small margins of error  closer to the high utility portion of the utility space  In this section  we discuss how to compute the utility score    k  associated with the k th best result of the join operation based on these selectivities  In the literature  this problem has been considered under different assumptions  14    15    25   In the most related work  25   authors have proposed a method to estimate the depth of a top k join query based on sample based estimators  however  none of these provide guarantees as to the error rate  while our goal is to match user provided con   dence targets  Let us consider two data sets  R1 i and R2 j  where  R1 i     r1 i and  R2 j     r2 j    Let us also consider a set  P  of utility segment pairs  or utility cells  and ask the question     What is the probability that the cells in P will  collectively  return exactly k results      This requires modeling the probability distribution of the sums of independent  but not identical  random variables  which themselves are distributed in a binomial fashion  While this is not straightforward  a reasonable approximation can be obtained by approximating each pi j  k  with a Poisson distribution  poisson     k   Theorem 3 1  Probability of Having k Results   The probability  pP  k  with which the set P of utility cells returns exactly k tuples is pP  k    poisson   P   k    e     P    P   k k    where   P      xi yj     P   i j     r1 i    r2 j      Proof Sketch  Let us consider two data sets  R1 i and R2 j  where  R1 i     r1 i and  R2 j     r2 j     and a pair of utility segments  xi and yj  from each data set  In the previous section  we computed the likelihood of a match between a given t1     R1 i and t2     R2 j   and represented this likelihood as   i j   Here  we note that  for each pair t1     R1 i and t2     R2 j   the corresponding likelihood of match can be modeled as a Bernoulli trial with a success probability of   i j   and a failure probability of  1       i j    34   Based on this  uSplit describes the probability  pi j  k   that the pair of segments  xi and yj  will result in exactly k results relying on a binomial model  pi j  k      r1 i    r2 j k         k i j     1       i j   r1 i  r2 j   k  Now  let us also consider a set  P  of utility segment pairs  or utility cells  and ask the question     What is the probability that the cells in P will  collectively  return exactly k results      This requires modeling the probability distribution of the sums of independent  but not identical  random variables  which themselves are distributed in a binomial fashion  While this is not straightforward  a reasonable approximation can be obtained by approximating each pi j  k  with a Poisson distribution  poisson     k   A commonly used statistical rule of thumb  34  is that it is possible to approximate a given binomial distribution  binom a  b  p      a b   p b  1   p  a   b   with a Poisson distribution  poisson     k    e           b b    where      ap  as long as a     20 and p     0 05  Therefore  if r1 i    r2 j     20  which is often the case   and the join selectivity    i j   is relatively small  we can approximate pi j  k  using Poisson distributions 10   Since the sum of independent Poisson processes with g parameters     1            g  itself is a Poisson process with parameter      1   i   g   i  34   the theorem follows    Given this  the probability  p     P  k   with which    P will return at least k tuples    can be computed as p     P  k      k   h pP  h     1               0   h k pP  h           Let p  be the maximum probability of error the user can accommodate in the estimation of   k  in other words  the con   dence lowerbound for the estimation of   k is 1   p    Let p     P     k  denote the probability with which the set P of utility cells will collectively return at least k tuples above the utility threshold     Given these  if C    x1        xn       y1        ym  denotes the set of all cells in the utility space  then we are looking for a utility score   k  where   k   arg max    p     C     k      1     p   Since  given     we are interested in only those results with utility scores above     we normalize the Poisson distribution parameter    C     in a way that takes into account the likelihood of the results having scores above     Given the above de   nition of p     C     k   the algorithm for computing   k   arg max   p     C     k  is shown in Figure 11  Note that  when the while loop of Step 5 of the algorithm ends  the    nal value of       is within      of the true value of   k  with con   dence 1     p    and thus  for suf   ciently small values of             can be used as an approximation of   k  Also  note that the Poisson distribution parameter    C     is computed assuming that the results are uniformly distributed in each utility cell   xi   yj       C  The utility space partitioning strategy presented in Section II which results in smaller cells nearer to the high utility region of the utility space  helps ensure that the assumption is more likely to hold at more critical  high utility  parts of the utility space  10 If the join selectivity is higher than 0 05  the Poisson approximation of the binomial distribution may not be appropriate  In that case  we compute the expected number of results    i j     r1 i    r2 j    that the pair  xi and yj   will return and we deduct this amount from the target  k  Inputs      The set  C    x1        xn      y1        ym   of utility cells      The error upper bound  p   1  given the segment boundaries  0   x0   x1              xn   1   xn   1 and 0   y0   y1              ym   1   ym   1  compute the utility score    i j   corresponding to each segment boundary intersection   xi  yj    2  compute the con   dence  p    Pi j    i j  k   for each segment boundary intersection   xi  yj    where Pi j     C is the set of utility cells to the north east of the segment boundary intersection   xi  yj    3     nd the largest   i j   such that p    Pi j    i j  k      1     p   and let           i j   4  let     be the next larger utility score of a segment boundary intersection  at this point we have             k         5  while                       do a  let                  2 b  if p    Pi j      k      1     p   then let            else let          6    k         7  return   k Fig  11  Algorithm for estimating   k IV  BA L A N C E D WO R K AL L O C AT I O N Once the target utility lowerbound    k  is computed within an acceptable error bound  the next step in the process is to partition the  suf   ciently high utility  portions of the input data  and to assign the resulting work quanta onto the available servers for join processing  In the simplest random split strategy  the data elements can be assigned to the available servers randomly  e g   hash partitioning   This approach is obviously very easy to implement  and could  in fact  ensure that the amount of input data assigned to the servers is balanced  On the other hand  in general  there is no guarantee that the useful workload  i e   workload for candidate input pairs with utility score greater than or equal to   k  will be balanced across different servers in the system  In contrast  a boundary aware  utility driven split strategy would rely on the utility statistics  which were already collected for obtaining the utility boundary   k     see Section II  to estimate the distribution of the workload in the utility space above   k  and partition the data along the utility dimensions in such a way that the useful workload  consisting of candidate input pairs  with utility score greater than or equal to   k  is balanced  and non useful workload is avoided as much as possible  As described in Section II  uSplit segments the two dimensions of the utility space  corresponding to data sources R1 and R2   for helping with the allocation of the sampling budget for estimating the join selectivities  For work partitioning  uSplit considers a second set of utility boundaries  0     0     1                u   1     u   1 and 0     0     1               v   1     v   1  which are uniformly spaced  i e        1     i     u and    1     j     v    i       i   1       xn and    j       j   1       ym  where   xn and   ym denote the sizes of the smallest segments for which statistics are available  see Section II C   During the statistics collection phase  see Section II and Figure 6   for each data partition    i      i   1    i   or   j      j   1    j    uSplit collects additional statistics including the numbers    1 i and   2 j   of data elements in these partitions 1000000 2000000 Count of Male Actors Score Distribution for Male Actors Count of Male Actors 0 1000000 2000000 Score Range Fig  12  Score distribution for male actors  the distribution for the female actors is similar uSplit selects one of the two sources  R1 or R2  as the pivot to drive the partitioning process  Without loss of generality  let us assume that the dimension corresponding to the utility partitions    i      i   1    i    is selected as the pivot  The work above the utility boundary  de   ned by    k and the combination function      for each pivot partition    i      i   1    i    is estimated using the selectivities that have been computed in Section II  Let W     W   i   denote the total work above the utility threshold for all partitions  If U servers are available for processing  we expect that each server will be assigned roughly W U units of work  Thus  uSplit aggregates the utility partitions into U roughly equi work  contiguous groups  or slices   each with     W U workload  The data corresponding to each slice are then allocated for one of the servers for join processing  V  EX P E R I M E N T S In this section  we evaluate the effectiveness of uSplit in  a  predicting join selectivities   b  estimating the utility threshold   k  and  c  end to end query processing time  For evaluation purposes  we used data sets with different characteristics and sizes   a  We have created synthetic data sets with up to 500M data elements  14GB  per source  We considered uniform  normal  mean   0 5  var   0 15   and Zip   an utility distributions   b  We have also run experiments using the Internet Movie Database  IMDB  data set  1   we focused on male and female actors  1M entries each   and used the average rating of the movies a given actor played in as the corresponding utility  Figure 12 shows that these ratings show a normal distribution  On the synthetic data set  we considered an equijoin  with     10 matches per data entry   For the actors data  we considered the following top k join query        nd the K highest scoring male female actor pairs who have played in at least one movie together     In order to observe the impact of de biasing  we de biased the data sets with Zip   an distributions  which has extreme bias   but left the actors and normal data sets intact  We used average  product  and Euclidean combinations  The experiments were executed over the Amazon EC2 platform with up to 20 RedHat CentOS 5 2 machines  each machine had 7 5 GB memory  4 EC2 64 bit Compute Units 1 E  04 2 E  04 3 E  04 Error Rate Error Rate   Predicted Selectivity   True Selectivity   with Adaptive Sampling              1 0 92 0 8 0 63 0 37 0 E 00 1 E  04 2 E  04 3 E  04 0 11 0 29 0 45 0 57 0 68 0 76 0 83 0 89 0 94 0 98 Error Rate Input  Data Utilities Error Rate   Predicted Selectivity   True Selectivity   with Adaptive Sampling              Fig  13  With adaptive sampling  errors in selectivity estimation are lower at high utility ranges     the true selectivity is     0 001  Uniform 10K data set  20 segments  segment scaling factor   1 1  sampling scaling factor   1 05  sampling budget   1000  Number of Elements above a Threshold Range Real Count Adaptive Seg  Uniform Seg      0 95 877 891 953     0 9 3393 3440 3704     0 8 22356 22402 23301     0 7 90965 92059 91696 Fig  14  Especially for high utility thresholds  adaptive segmentation provides highly precise predictions of the number of results  1M data set  segment scaling factor   1 1  num  of segments   23  0 1 1 10   Real   Uniform  Real   log scale  Adaptive vs  Uniform Sampling  as a function of the Sampling Scale Factor  Uniform Zipfian 0 01 0 1 1 10 1 05 1 1 1 15 2 3 4  Adaptive  Real   Uniform  Real   log scale  Sampling Scale Factor Adaptive vs  Uniform Sampling  as a function of the Sampling Scale Factor  Uniform Zipfian Normal Actors Fig  15  Effect of sampling scale factor  1M data  Euclidean combination  segment scaling factor   1 1  20 segments  sampling budget   1  of data   2 virtual cores with 2 EC2 compute units each   with high I O performance option  Once the data is sampled and partitioned  the actual joins on the servers were done using Vertica  2  DBMS  Vertica Analytic Database 3 5 10 0   Unless speci   ed otherwise  the results are averages of three runs  A  Effectiveness of the Adaptive Sampling Impact of Non Uniform Sampling  We    rst provide a highlevel overview of the impact of the adaptive approach to sampling  presented in Section II  As Figure 14 and Figure 13 show  the proposed strategies ensure that the selectivity estimation errors are very close to zero for the high utility region  which is the only region that matters for top k joins   instead of being uniform across the utility space  Figure 15 quanti   es the bene   ts of using non uniform sampling rates by varying the sampling scale parameter      between 1 05 and 4 0 and comparing the errors in the estimation of the number of results above a given threshold       1 33  to the errors resulting when uniform sampling is used  As the    gure shows  when        2 adaptive sampling leads to errors that are only 10  to 50  of the errors that occur with0 1 1 10 form  Real    Real   log scale  Error in the Estimation of number of Results  Uniform Sampling  Uniform Zipfian Normal Actors 0 01 0 1 1 10 0 10  0 50  1   Uniform  Real    Real   log scale  Sampling Budget  percentage of Data Size  Error in the Estimation of number of Results  Uniform Sampling  Uniform Zipfian Normal Actors  a  Uniform sampling 0 1 1 10 ptive  Real   Real   log scale  Error in the Estimation of number of Results  Adaptive Sampling  Uniform Zipfian Normal Actors 0 01 0 1 1 10 0 10  0 50  1   Adaptive  Real   Real   log scale  Sampling Budget  percentage of Data Size  Error in the Estimation of number of Results  Adaptive Sampling  Uniform Zipfian Normal Actors  b  Adaptive sampling Fig  16  Effect of the sampling budget  1M data  Euclidean comb     k   1 33  segment scaling factor   1 1  sampling scaling factor   1 05  20 segments  uniform sampling  Note also that using too large a    does not help as it concentrates too large a number of samples to a too small a portion of the utility space  resulting in degradations elsewhere  Impact of the Sampling Budget  Figure 16 veri   es that  as expected  a higher sampling budget helps reduce the amount of errors  but a relatively small number of samples are suf   cient for accurate estimates  Moreover adaptive sampling provides signi   cantly better result estimates  Impact of Using Non Uniform Utility Segments during Sampling  Figure 17 a  shows the bene   ts of using nonuniform segment boundaries during sampling  in the    gure  the segment size scale parameter      is varied between 1 05 and 2 0 and the corresponding errors in the estimation of the number of results above      1 33 are compared against the errors that occur when using uniform partition boundaries  i e        1 0   When the segment size scale parameter is 1 15  non uniform segmentation results in sampling errors that are only 10  to 40  of the errors that occur when using uniform boundaries  Further increasing the segment scaling factor however is not useful  while the very high utility region gets    nely segmented  the granularity drops quickly for values slightly lower than 1 0  resulting in degradations in accuracy  Figure 17 b  shows that using a larger number of segments does not imply better sampling  this is because  as discussed in Section II C  when the number of data elements in a segment is too small  this may negatively effect the estimation of error margins  This is con   rmed by Figure 17 b  which shows that the problem is more pronounced when the utilities are biased  Impact of the De biasing  Figure 18 tracks the success rate of obtaining top k results using different target con   dence 0 1 1 10 e  Real   Uniform  Real   log scale  Adaptive vs  Uniform Sampling  as a function of the Utility Segment Scaling factor  Uniform Zipfian 0 01 0 1 1 10 1 05 1 1 1 15 1 5 2  Adaptive  Real   Uniform  Real   log scale  Utility Segment Scaling Factor Adaptive vs  Uniform Sampling  as a function of the Utility Segment Scaling factor  Uniform Zipfian Normal Actors  a  Segment scaling factor 1 10 ve Real   Uniform    Real      log scale  Adaptive vs  Uniform Sampling  as a function of the number of utility segments  Uniform Zi fi  DE BIASED  0 1 1 10 20 50 100  Adaptive Real   Uniform    Real      log scale  Number of Segments Adaptive vs  Uniform Sampling  as a function of the number of utility segments  Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actor  NOT DE  BIASED   b  Number of segments Fig  17  Effects of  a  segment size scaling factor and  b  the number of segments  1M data  Euclidean combination  sampling scaling factor   1 05  20 segments  sampling budget   1   60  70  80  90  100  Percent Success Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  30  40  50  60  70  80  90  100  30  50  70  90  95  99  99 99  Percent Success Target Confidence Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actors  NOT DE  BIASED   a  k   100 60  70  80  90  100  ercent Success Success Rate vs  Confidence Target Uniform 30  40  50  60  70  80  90  100  30  50  70  90  95  99  99 99  Percent Success Target Confidence Success Rate vs  Confidence Target Uniform Zipfian  DE  BIASED  Normal  NOT DE  BIASED  Actors NOT DE  BIASED   b  k   1000 Fig  18  Effect of de biasing  10K data  average  sampling scaling factor   1 05  50 segments  segment scaling factor   1 1  sampling budget   10   rates  1     p   over 100 runs  As the    gure shows  for the uniform and the de biased Zip   an data sets  despite the initial extreme bias of the Zip   an distribution  the algorithm is able to provide success rates larger than or almost equal to the target con   dence  For non de biased data sets  Normal and Actors   on the other hand  the success rates are lower  especially for large ks and high target con   dences  This is because  when the utilities are skewed  this increases the chances of having sampling errors  The problem is compounded for large k10 100 1000 10000 btained no  of Results Target Number of Results vs  Obtained Number of Results  Confidence Target   99   Euclidean Comb   Uniform Zipfian Normal Actors 1 10 100 1000 10000 100 1000 10000 Obtained no  of Results Target no  of Results  k  Target Number of Results vs  Obtained Number of Results  Confidence Target   99   Euclidean Comb   Uniform Zipfian Normal Actors  a  Data distribution 10 100 1000 10000 tained no  of Results Target Number of Results vs  Obtained Number of Results  Confidence  target  99   Zipfian data  Average Product E lid 1 10 100 1000 10000 100 1000 10000 Obtained no  of Results Target no   of Results k  Target Number of Results vs  Obtained Number of Results  Confidence  target  99   Zipfian data  Average Product Euclidean  b  Combination function Fig  19  Targeted vs  obtained results  1M data  sampling scaling factor   1 05  20 segments  segment scaling factor   1 1  sampling budget   1   values  which lead to relatively lower   k values  thus moving into the regions of the utility space where sampling errors are relatively larger  This highlights the importance of debiasing the input data sets through an initial high level utility distribution analysis  Target Number of Results vs  Obtained Number of Results  Figure 19 shows the relationship between the target number of results  k  and the obtained number of results  As shown in this    gure  uSplit is highly accurate in matching the target number of results and this is true for different data distributions  Figure 19 a   and combination functions  Figure 19 b    B  Impact of uSplit on Processing Times After the above experiments in which we have explored the impact of uSplit based sampling on the success rate in top k result enumeration  in this section  we consider the execution time and scalability  In these experiments  we use two data sources  each with 500M entries  Each entry joins with     10 entries in the other source  The score merge function is average and the data utilities are uniformly distributed  Experiments are run on EC2  In implementing uSplit partitioning  we used the combined output sampling strategy  the costliest    i e   worst case    of the sampling strategies  which requires a full scan over the data  see Section II   Cost of top k Retrieval w o uSplit  As Figure 20 a  shows  the full join of these two sources using the Vertica  2  column DBMS  Vertica Analytic Database 3 5 10 0  on 10 EC2 machines takes 20 2 hours  Even using the    limit k    option with an order by clause  the execution time drops only to 358mins for k   100M and to 56mins for k   1000  Cost of Retrieval using uSplit based Thresholding  As described in Section II  uSplit piggy backs the data sampling process on the up stream operator  Nevertheless  523 389 358 56 1255 200 400 600 800 1000 1200 M i n u t e s Time with Vertica   limit K   option  10 servers  523 389 358 56 1255 0 200 400 600 800 1000 1200 900M 400M 100M 1000 FULL M i n u t e s Target   of Results Time with Vertica   limit K   option  10 servers  data loading q</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Hybrid In Database Inference for Declarative Information Extraction ### Daisy Zhe Wang University of California  Berkeley Michael J  Franklin University of California  Berkeley Minos Garofalakis Technical University of Crete Joseph M  Hellerstein University of California  Berkeley Michael L  Wick University of Massachusetts  Amherst ABSTRACT In the database community  work on information extraction  IE  has centered on two themes  how to effectively manage IE tasks  and how to manage the uncertainties that arise in the IE process in a scalable manner  Recent work has proposed a probabilistic database  PDB  based declarative IE system that supports a leading statistical IE model  and an associated inference algorithm to answer top k style queries over the probabilistic IE outcome  Still  the broader problem of effectively supporting general probabilistic inference inside a PDB based declarative IE system remains open  In this paper  we explore the in database implementations of a wide variety of inference algorithms suited to IE  including two Markov chain Monte Carlo algorithms  the Viterbi and the sum product algorithms  We describe the rules for choosing appropriate inference algorithms based on the model  the query and the text  considering the trade off between accuracy and runtime  Based on these rules  we describe a hybrid approach to optimize the execution of a single probabilistic IE query to employ different inference algorithms appropriate for different records  We show that our techniques can achieve up to 10 fold speedups compared to the non hybrid solutions proposed in the literature  Categories and Subject Descriptors H 2 4  Database Management   Systems   Textual databases  Query Processing  G 3  Mathematics of Computing   Probability and statistics   Probabilistic algorithms  including Monte Carlo  General Terms Algorithms  Performance  Management  Design Keywords Probabilistic Database  Probabilistic Graphical Models  Information Extraction  Conditional Random Fields  Viterbi  Markov chain Monte Carlo Algorithms  Query Optimization Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee ### SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  1 Introduction For most organizations  textual data is an important natural resource to fuel data analysis  Information extraction  IE  techniques parse raw text and extract structured objects that can be integrated into databases for querying  In the past few years  declarative information extraction systems  1  2  3  4  have been proposed to effectively manage IE tasks  The results of IE extraction are inherently uncertain  and queries over those results should take that uncertainty into account in a principled manner  Research in probabilistic databases  PDBs  has been exploring scalable tools to reason about these uncertainties in the context of structured query languages and query processing  5  6  7  8  9  10  11  12   Our recent work  4  13  has proposed a PDB system that natively supports a leading statistical IE model  conditional random    elds  CRFs    and an associated inference algorithm  Viterbi   It shows that the in database implementation of the inference algorithms enables   1  probabilistic relational queries that returns top k results over the probabilistic IE outcome   2  a tight integration between the relational and inference operators  which leads to signi   cant speed up by performing query driven inference  While this work is an important step towards building a probabilistic declarative IE system  the approach is limited by the capabilities of the Viterbi algorithm  which can only handle top k style queries over a limited class of CRF models  linear chain models  which do a poor job capturing features like repeated terms  Different inference algorithms are needed to deal with non linear CRF models  such as skip chain CRF models  complex IE queries that induce cyclic models over the linear chain CRFs  and marginal inference queries that produce richer probabilistic outputs than top k  The broader problem of effectively supporting general probabilistic inference inside a PDB based declarative IE system remains open  In this paper  we    rst explore the in database implementation of a number of inference algorithms suited to a broad variety of models and outputs  two variations of the general sampling based Markov chain Monte Carlo  MCMC  inference algorithm   Gibbs Sampling and MCMC Metropolis Hastings  MCMC MH    in addition to the Viterbi and the sum product algorithms  We compare the applicability of these four inference algorithms and study the data and the model parameters that affect the accuracy and runtime of those algorithms  Based on those parameters  we develop a set of rules for choosing an inference algorithm based on the characteristics of the model and the data  More importantly  we study the integration of relational query processing and statistical inference algorithms  and demonstrate that  for SQL queries over probabilistic extraction results  the proper choice of IE inference algorithm is not only model dependent  but also query  and text dependent  Such dependencies arise when re lational queries are applied to the CRF model  inducing additional variables  edges and cycles  and when the model is instantiated over different text  resulting in model instances with drastically different characteristics  To achieve good accuracy and runtime performance  it is imperative for a PDB system to use a hybrid approach to IE even within a single query  employing different algorithms for different records  In the context of our CRF based PDB system  we describe query processing steps and an algorithm to generate query plans that apply hybrid inference for probabilistic IE queries  Finally  we describe example queries and experiment results showing that such hybrid inference techniques can improve the runtime of the query processing by taking advantage of the appropriate inference methods for different combinations of query  text  and CRF model parameters  Our key contributions can be summarized as follows    We show the ef   cient implementation of two MCMC inference algorithms  in addition to the Viterbi and the sumproduct algorithms  and we identify a set of parameters and rules for choosing different inference algorithms over models and datasets with different characteristics    We describe query processing steps and an algorithm to generate query plans that employ hybrid inference over different text within the same query  where the selection of the inference algorithm is based on all three factors of data  model  and query    Last  we evaluate our approaches and algorithms using three real life datasets  DBLP  NYTimes  and Twitter  The results show that our hybrid inference techniques can achieve up to 10 fold speedups compared to the non hybrid solutions proposed in the literature  Based on our experience in implementing different inference algorithms  we also present four design guidelines for implementing statistical methods in the database in the Appendix  2 Related Work In the past few years  declarative information extraction systems  1  2  3  4  13  have been proposed to effectively manage information extraction  IE  tasks  The earlier efforts in declarative IE  1  2  3  lack a uni   ed framework supporting both a declarative interface as well as the state of the art probabilistic IE models  Ways to handle uncertainties in IE have been considered in  14  15   A probabilistic declarative IE system has been proposed in  4  13   but it only supports the Viterbi algorithm  which is unable to handle complex models that arise naturally from advanced features and relational operators  In the past decade  there has been a groundswell of work on Probabilistic Database Systems  PDBS   5  6  7  8  9  10  11  12   As shown in previous work  8  10  12   Graphical Modeling techniques can provide robust statistical models that capture complex correlation patterns among variables  while  at the same time  addressing some computational ef   ciency and scalability issues as well  In addition   8  showed that other approaches to represent and handle uncertainty in database  5  6   can be uni   ed under the framework of Graphical Models  which express uncertainties and dependencies through the use of random variables and joint probability distribution  However  there is no work addressing the problem of effectively supporting and optimizing different probabilistic inference algorithms in a single PDB  especially in the IE setting  3 Background This section covers our de   nition of a probabilistic database  the conditional random    elds  CRF  model and the different types of inference algorithms over CRF models in the context of information extraction  We also introduce a template for the types of IE queries studied in this paper  3 1 Probabilistic Database As we described in  10   a probabilistic database DB p consists of two key components   1  a collection of incomplete relations R with missing or uncertain data  and  2  a probability distribution F on all possible database instances  which we call possible worlds  and denote by pwd DB p    An incomplete relation R 2R is de   ned over a schema A d  A p comprising a  non empty  subset A d of deterministic attributes  that includes all candidate and foreign key attributes in R   and a subset A p of probabilistic attributes  Deterministic attributes have no uncertainty associated with any of their values  A probabilistic attribute A p may contains missing or uncertain values  The probabilistic distribution F of these missing or uncertain values is represented by a probabilistic graphical model  such as Baysian Networks or Markov Random Fields  Each possible database instance is a possible completion of the missing and uncertain data in R  3 2 Conditional Random Fields The linear chain CRF  16  17   similar to the hidden markov model  is a leading probabilistic model for solving IE tasks  In the context of IE  a CRF model encodes the probability distribution over a set of label random variables  RVs  Y  given the value of a set of token RVs X  We denote an assignment to X by x and to Y by y  In a linear chain CRF model  label yi is correlated only with label yi1 and token xi  Such correlations are represented by the feature functions ffk yi  yi1  xi g K k 1   EXAMPLE 1  Figure 1 a  shows an example CRF model over an address string x    2181 Shattuck North Berkeley CA USA     Observed  known  variables are shaded nodes in the graph  Hidden  unknown  variables are unshaded  Edges in the graph denote statistical correlations  The possible labels are Y   fapt num  streetnum  streetname  city  state  countryg  Two possible feature functions of this CRF are  f1 yi  yi1  xi     xi appears in a city list     yi   city  f2 yi  yi1  xi     xi is an integer     yi   apt num    yi1   streetname  A segmentation y   fy1       yT g is one possible way to tag each token in x of length T with one of the labels in Y   Figure 1 d  shows two possible segmentations of x and their probabilities  DEFINITION 3 1  Let ffk yi  yi1  xi g K k 1 be a set of realvalued feature functions  and     f kg 2 R K be a vector of real valued parameters  a CRF model de   nes the probability distribution of segmentations y given a speci   c token sequence x  p y j x    1 Z x  expf XT i 1 XK k 1  kfk yi  yi1  xi g   1  where Z x  is a standard normalization function that guarantees the probability distribution sums to 1 over all possible extractions    3 3 Relational Representation of Text and CRF We implement IE algorithms over the CRF model within a database using the relational representations of text and the CRF based distribution in the token table TOKENTBL and the factor table MR respectively         a  2181 Shattuck North Berkeley CA USA X tokens Y labels id docID pos token Label 1 1 0 2181 2 1 1 Shattuck 3 1 2 North 4 1 3 Berkeley 5 1 4 CA 6 1 5 USA  b  token prevLabel label score 2181  DIGIT  null street num 22 2181  DIGIT  null street name 5           Berkeley street street name 10 Berkeley street city 25           c   d  x 2181 Shattuck North Berkeley CA USA y1 street num street name city city state country  0 6  y2 street num street name street name city state country  0 1  Figure 1   a  Example CRF model   b  Example TOKENTBL table   c  Example MR table   d  Two possible segmentations y1  y2  Token Table  The token table TOKENTBL  as shown in Figure 1 b   is an incomplete relation R in DB p   which stores a set of documents or text strings D as a relation in a database  in a manner akin to the inverted    les commonly used in information retrieval  TOKENTBL  id  docID  pos  token  label p   TOKENTBL contains one probabilistic attribute   label p   and the main goal of IE is to perform inference on label p   As shown in the schema above  each tuple in TOKENTBL records a unique occurrence of a token  which is identi   ed by the text string ID  docID  and the position  pos  the token is taken from  The id    eld is simply a row identi   er for the token in TOKENTBL  Factor Table  The probability distribution F over all possible    worlds    of TOKENTBL can be computed from the MR  The MR is a materialization of the factor tables in the CRF model for all the tokens in the corpus D  The factor tables   yi  yi1 j xi   as shown in Figure 1 c   represent the correlation between xi  yi  and yi1  and are computed by the weighted sum of a set of feature functions in the CRF model    yi  yi1 j xi    PK k 1  kfk yi  yi1  xi   As in the following schema  each unique token string xi is associated with an array  which contains a set of scores ordered by fprevLabel  labelg  MR  token  score ARRAY    3 4 Inference Queries over a CRF Model There are two types of inference queries over the CRF model  17     Top k Inference  The top k inference computes the label sequence y  i e   extraction  with the top k highest probabilities given a token sequence x from a text string d  Constrained top k inference  18  is a special case  where the top  k extractions are computed  conditioned on a subset of the token labels  that are provided as evidence    Marginal Inference  Marginal inference computes a marginal probability p yt  yt 1       yt kjx  s  over a single label or a sub sequence of labels conditioned on the set of evidence s   fs1       sT g  where si is either NULL  i e   no evidence  or the evidence label for yi  Many inference algorithms are known that can answer the above inference queries over the CRF models  varying in their effectiveness for different CRF characteristics  e g   shape of the graph   In the next sections  three inference algorithms will be described  Viterbi  sum product  Markov chain Monte Carlo  MCMC  methods  3 5 Viterbi Algorithm Viterbi  a special case of the Max Product algorithm  19  20  can compute top k inference for linear chain CRF models  Viterbi is a dynamic programming algorithm that computes a two dimensional V matrix  where each cell V  i  y  stores a ranked list of partial label sequences  i e   paths  up to position i ending with label y and ordered by score  Based on Equation  1   the recurrence to compute the top 1 segmentation is as follows  V  i  y    8     maxy0  V  i  1  y 0     PK k 1  kfk y  y 0   xi    if i   0 0  if i   1   2  The top 1 extraction y   can be backtracked from the maximum entry in V  T  yT    where T is the length of the token sequence x  The complexity of the Viterbi algorithm is O T   jY j 2    where jY j is the number of possible labels  The constrained top k inference can be computed by a variant of the Viterbi algorithm which restricts the chosen labels y to conform with the evidence s  3 6 Sum Product Algorithm Sum product  i e   belief propagation  is a message passing algorithm for performing inference on graphical models  such as CRF  19   The simplest form of the algorithm is for tree shaped models  in which case the algorithm computes exact marginal distributions  The algorithm works by passing real valued functions called messages along the edges between the nodes  These contain the    in   uence    that one variable exerts on another  A message from a variable node yv to its    parent    variable node yu in a tree shaped model is computed by summing the product of the messages from all the    child    variables of yv in C yv  and the feature function f yu  yv  between yv and yu over variable yv   yv yu  yu    X yv f yu  yv  Y y  u2C yv   y  u yv  yv    3  Before starting  the algorithm    rst designates one node as the root  any non root node which is connected to only one other node is called a leaf  In the    rst step  messages are passed inwards  starting at the leaves  each node passes a message along the edge towards the root node  This continues until the root has obtained messages from all of its adjoining nodes  The marginal of the root note can be computed at the end of the    rst step  The second step involves passing the messages back out  starting at the root  messages are passed in the reverse direction  until all leaves have received their messages  Like Viterbi  the complexity of the sum product algorithm is also O T   jY j 2    Variants of the sum product algorithm for cyclic models require either an intractable junction tree step  or a variational approximation such as loopy belief propagation  BP   In this paper  we do not study these variants further as they are either intractable  junction tree   or can fail to converge  loopy BP  on models with longdistance dependencies such as those we discussed in this paper  3 7 MCMC Inference Algorithms Markov chain Monte Carlo  MCMC  methods are a class of randomized algorithms for estimating intractable probability distributions over large state spaces by constructing a Markov chain sampling process that converges to the desired distribution  Relative to other sampling methods  the main bene   ts of MCMC methods are that they  1  replace a dif   cult sampling procedure from a highdimensional target distribution   w  that we wish to sample with an easy sampling procedure from a low dimensional local distribution q  jw   and  2  sidestep the  P hard computational problem of computing a normalization factor  We call q  jw  a    pro       GIBBS  N  1 w0   INIT    w   w0     initialize 2 for idx   1       N do 3 i   idx n     propose variable to sample next 4 w0     wi j wi     generate sample 5 return next w0    return a new sample 6 w   w0      update current world 7 endfor Figure 2  Pseudo code for Gibbs sampling algorithm over a model with n variables  posal distribution     which   conditioned on a previous state w    probabilistically produces a new world w 0 with probability q w 0 jw   In essence  we use the proposal distribution to control a random walk among points in the target distribution  We review two MCMC methods we will adapt to our context in this paper  Gibbs sampling and Metropolis Hastings  MCMC MH   3 7 1 Gibbs Sampling Let w    w1  w2       wn  be a set of n random variables  distributed according to   w   The proposal distribution of a speci   c variable wi is its marginal distribution q  jw      wijwi  conditioned on wi  which are the current values of the rest of the variables  The Gibbs sampling algorithm  i e   Gibbs sampler     rst generates the initial world w0  for example  randomly  Next  samples are drawn for each variable wi 2 w in turn  from the distribution   wijwi   Figure 2 shows the pseudo code for the Gibbs sampler that returns N samples  In Line 4    means a new sample w 0 is drawn according to the proposal distribution   wijwi   3 7 2 Metropolis Hastings  MCMC MH  Like Gibbs  the MCMC MH algorithm    rst generates an initial world w0  e g   randomly   Next  samples are drawn from the proposal distribution w 0   q wijw   where a variable wi is randomly picked from all variables  and q wijw  is a uniform distribution over all possible values  Different proposal distribution q wijw  can be used  which results in different convergence rates  Lastly  each resulting sample is either accepted or rejected according to a Bernoulli distribution given by parameter      w 0   w    min 1    w0  q wjw0     w q w0 jw     4  The acceptance probability is determined by the product of two ratios  the model probability ratio   w 0     w   which captures the relative likelihood of the two worlds  and the proposal distribution ratio q wjw 0   q w 0 jw   which eliminates the bias introduced by the proposal distribution  3 8 Query Template Over the CRF based IE from text  the queries we consider are probabilistic queries  which inference over the probabilistic attribute label p in the TOKENTBL table  Each TOKENTBL is associated with a speci   c CRF model stored in the MR table  Such CRFbased IE is captured by a sub query with logic that produces the IE results from the base probabilistic TOKENTBL tables  The subquery consists of a relational part Qre over the probabilistic token tables TOKENTBL and the underlying CRF models  followed by an inference operator Qinf   A canonical    query template    captures the logic for the    SQL IE    sub query in Figure 3  It supports SPJ queries  aggregate conditions and two types of inference operators  Top k and Marginal  over the probabilistic TOKENTBL tables  The relational part of a    SQL IE    query Qre    rst speci   es  in the FROM clause  the TOKENTBL table s  over which the query and extraction is performed  SELECT Top k T1 docID  T1 pos exist      Marginal T1 docID  T1 pos exist       Top k T1 docID  T2 docID  T1 pos T2 pos exist       Marginal T1 docID T2 docID  T1 pos T2 pos exist    FROM TokenTbl1 T1   TokenTbl2 T2  WHERE T1 label      bar1     and T1 token      foo1      and T1 docID   X   and T1 pos   Y   and T1 label   T2 label   and T1 token   T2 token   and T1 docID   T2 docID  GROUP BY T1 docID   T2 docID  HAVING  aggregate condition  Figure 3  The    SQL IE    query template  The WHERE clause lists a number of possible selection as well as join conditions over the TOKENTBL tables  These conditions when involve label p are probabilistic conditions  and deterministic otherwise  For example  a probabilistic condition label    person    speci   es the entity types we are looking for is    person     while a deterministic condition token    Bill    speci   es the name of the entity we are looking for is    Bill     We can also specify a join condition T1 token T2 token and T1 label T2 label that two documents need to contain the same entity name with the same entity type  In the GROUP BY and the HAVING clause  we can specify conditions on an entire text    document     An example of such aggregate condition over a bibliography document can be that all title tokens are in front of all the author tokens  Following the Possible World Semantics  5   the execution of these relational operators involve modi   cation to the original graphical models as will be shown in Section 4 1  The inference part Qinf   of a    SQL IE    query  takes the docID  the pos  and the CRF model resulting from Qre as input  The inference operation is speci   ed in the SELECT clause  which can be either a Top k or a Marginal inference  The inference can be computed over different random variables in the CRF model   1  a sequence of tokens  e g   a document  speci   ed by docID  or  2  a token at a speci   c location speci   ed by docID and pos  or  3  the    existence     exist  of the result tuple  The    existence    of the result tuple becomes probabilistic with a selection or join over a probabilistic attribute  where exist variables are added to the model  10   For example  the inference Marginal T1 docID T1 pos   for each position  T1 docID T1 pos  computed from Qre  returns the distribution of the label variable at that position  The inference Marginal T1 docID exist  computes the marginal distribution of exist variable for each result tuple  We can also specify an inference following a join query  For example  the inference Top k T1 docID T2 docID   for each document pair  T1 docID T2 docID   returns the top k highest probability joint extractions that satisfy the join constraint  4 In Database MCMC Inference In this section  we    rst describe IE models that are cyclic  e g   the skip chain CRF model  and review the way that simple relational queries can often induce cyclic models   even over text that is itself modeled by simple linear chain CRFs  Such cyclic models call for an ef   cient general purpose inference algorithm such as an MCMC algorithm  Next  we describe our ef   cient in database implementation of the Gibbs sampler and MCMC MH  Finally  we discuss query driven sampling techniques that push the query constraints into the MCMC sampling process  4 1 Cycles from IE Models and Queries In many IE tasks  good accuracy can only be achieved using nonlinear CRF models like skip chain CRF models  which model the            IBM Corp  said that IBM for IBM  Figure 4  A skip chain CRF model that includes skip edges between non consecutive tokens with the same string  e g     IBM       a   b   c  Bill by Bill                                                                       assigned Clinton today CEO Gates talked about Bill Clinton met with Bill assigned by Bill Clinton today The Viterbi Algorithm Dave Forney Figure 5   a  and  b  are example CRF models after applying a join query over two different pairs of documents   c  is the resulting CRF model from a query with an aggregate condition  correlation not only between the labels of two consecutive tokens as in linear chain CRF  but also between those of non consecutive tokens  For example  a correlation can be modeled between the labels of two tokens in a sentence that have the same string  Such a skip chain CRF model can be seen in Figure 4  where the correlation between non consecutive labels  i e   skip chain edges  form cycles in the CRF model  In simple probabilistic databases with independent base tuples  the    safe plans     5  give rise to tree structured graphical models  8   where the exact inference is tractable  However  in a CRF based IE setting  an inverted    le representation of text in TOKENTBL inherently has cross tuple correlations  Thus  even queries with    safe plans    over the simple linear chain CRF model  result in cyclic models and intractable inference problems  For example  the following query computes the marginal inference Marginal T1 docID T2 docID exist   which returns pairs of docIDs and the probabilities of the existence  exist  of their join results  The join query is performed between each document pair on having the same token strings labeled as    person     The join query over the base TOKENTBL tables adds cross edges to the pair of linear chain CRF models underlying each document pair  Figure 5 a   b  shows two examples of the resulting CRF model after the join query over two different pairs of documents  As we can see  the CRF model in  a  is tree shaped  and the one in  b  is cyclic  Q1   Probabilistic Join Marginal  SELECT Marginal T1 docID T2 docID exist  FROM TokenTbl1 T1  TokenTbl2 T2 WHERE T1 label   T2 label and T1 token   T2 token and T1 label      person     Another example is a simple query to compute the top k extraction conditioned on an aggregate constraint over the label sequence of each document  e g   all    title    tokens are in front of    author    tokens   This query induces a cyclic model as shown in Figure 5 c   Q2   Aggregate Constraint Top k  SELECT Top k T1 docID  FROM TokenTbl1 T1 GROUP BY docID HAVING  aggregate constraint    true  1 CREATE FUNCTION Gibbs  int  RETURN VOID AS 2    3    compute the initial world  genInitWorld   4 insert into MHSamples 5 select setval    world id    1  as worldId  docId  pos  token  trunc random   num label 1  as label 6 from tokentbl  7    generate N sample proposals  genProposals   8 insert into Proposals with X as   select foo id  foo docID   tmp  bar doc len  as pos 9 from   select id    id 1    1 numDoc  1  as docID    id 1     1 numDoc   as tmp 10 from generate series 1  1  id  foo  doc id tbl bar 11 where foo doc id   bar doc id   12 select X id S docId S pos S token  null as label  null  integer   as prevWorld  null  integer   as factors 13 from X  tokentbl S 14 where X docID   S docID and X pos   S pos  15    fetch context  initial world and factor tables 16 update proposals S1 17 set prev world    select   from getInitialWorld S1 docId   18 from proposals S2 19 where S1 docId    S2 docId and S1 id   S2 id 1  20 update proposals S1 21 set factors    select   from getFactors S1 docId   22 from proposals S2 23 where S1 docId    S2 docId and S1 id   S2 id 1  24    generate samples  genSamples   25 insert into MHSamples 26 select worldId  docId  pos  token  label 27 from   28 select nextval    world id     worldId  docId  pos  token  label  getalpha agg  docId pos label prev world factors    getalpha io  over  order by id  alpha 29 from  select   from proposals order by id  foo  foo  30    31 LANGUAGE SQL  Figure 6  The SQL implementation of Gibbs sampler takes input N     the number of samples to generate  Next  we describe general inference algorithms for such cyclic models  4 2 SQL Implementation of MCMC Algorithms Both the Gibbs sampler and the MCMC MH algorithm are iterative algorithms  which contain three main steps  1  initialization  2  generating proposals  and 3  generating samples  They differ in their proposal and sample generation functions  We initially implemented the MCMC algorithms in the SQL procedure language provided by PostgreSQL   PL pgSQL   using iterations and three User De   ned Functions  UDF   s     GENINITWORLD   to compute the initialized world  line 1 for Gibbs in Figure 2     GENPROPOSAL   to generate one sample proposal  line 3 for Gibbs in Figure 2     GENSAMPLE   to compute the corresponding sample for a given proposal  line 4 for Gibbs in Figure 2   However  this implementation ran hundreds of times slower than the Scala Java implementation described in  12   This is mainly because calling UDF   s iteratively a million times in a PL pgSQL function is similar to running a SQL query a million times  A more ef   cient way is to    decorrelate     and run a single query over a million tuples  The database execution path is optimized for this approach  With this basic intuition  we re implemented the MCMC algorithms  where the iterative procedures are translated into set operations in SQL The ef   cient implementation of the Gibbs sampler is shown in Figure 6  which uses the feature of window functions introduced in PostgreSQL 8 4  MCMC MH can be implemented ef   ciently in a similar way with some simple adaptations  This implementation achieves similar  within a factor of 1 5  runtime compared to the Scala Java implementation of the MCMC algorithms  as shown in the results in Section 7 1  4 3 Query Driven MCMC Sampling Previous work  4  has developed query driven techniques to integrate probabilistic selection and join conditions into the Viterbi algorithm over the linear chain CRF model  However  the kind of constraint that Viterbi can handle is limited and speci   c to the Viterbi and potentially the sum product algorithm  In this section  we explore query driven techniques for the sampling based MCMC inference algorithms  Query driven sampling is needed to compute inference conditioned on the query constraints  Such query constraints can be highly selective  where most samples generated by the vanilla MCMC methods do not    qualify     i e   satisfy the constraints   Thus  we need to adapt the MCMC methods by pushing the query constraints into the sampling process  Note that our adapted  query driven MCMC methods still converge to the target distribution as long as the proposal function can reach every    quali   ed    world in a    nite number of steps  There are three types of query constraints   1  selection constraints   2  join constraints  and  3  aggregate constraints  Both  1  and  2  were studied for Viterbi in  4   The following query contains an example selection constraint  which is to    nd the top k highest likelihood extraction that contains a    person    entity    Bill     Q3   Selection Constraint Top k  SELECT Top k T1 docID  FROM TokenTbl1 T1 WHERE token      Bill    and label      person    An example of a join constraint can be found in Q1 in Section 4 1  and an example of an aggregate constraint can be found in Q2 in the same Section  The naive way to answer those conditional queries using MCMC methods is to     rst  generate a set of samples using Gibbs sampling or MCMC MH regardless of the query constraint  second     lter out the samples that do not satisfy the query constraint  last  compute the query over the remaining    quali   ed    samples  In contrast  our query driven sampling approach pushes the query constraints into the MCMC sampling process by restricting the worlds generated by GENINITWORLD    GENPROPOSALS   and GENSAMPLES   functions  so that all the samples generated satisfy the query constraint  One of the advantages of the MCMC algorithms is that the proposal and sample generation functions can naturally deal with the deterministic constraints  which might induce cliques with high tree width in the graphical model  Such cliques can easily    blow up    the complexity of known inference algorithms  19   We exploit this property of MCMC to develop query driven sampling techniques for different types of queries  The query driven GENINITWORLD   function generates an initial world that satis   es the constraint  The    rst    quali   ed    sample can either be speci   ed according to the query or generated from random samples  The query driven GENPROPOSAL   and GENSAMPLES   functions are called iteratively to generate new samples that satisfy the constraint  The next    quali   ed    jump  i e   new sample  can be generated by restricted jumps according to the query constraints or from random jumps  5 Choosing Inference Algorithms Different inference algorithms over the probabilistic graphical models have been developed in a diverse range of communities  e g   natural language processing  machine learning  etc   The characteristics of these inference algorithms  e g   applicability  accuracy  convergence rate  runtimes  over different model structures have since been studied to help modeling experts select an appropriate inference algorithm for a speci   c problem  19   In this section  we    rst compare the characteristics of the four inference algorithms we have developed over the CRF model  Next we introduce parameters that capture important properties of the model and data  Using these parameters  we then describe a set of rules to choose among different inference algorithms  5 1 Comparison between Inference Algorithms We have implemented four inference algorithms over the CRF model for IE applications   1  Viterbi   2  sum product  and two samplingbased MCMC methods   3  Gibbs Sampling and  4  MCMC MetropolisHastings  MCMC MH   In Table 1  we show the applicability of these algorithms to different inference tasks  e g  top k  or marginal  on models with different structures  e g   linear chain  tree shaped  cyclic   As we can see  Viterbi  Gibbs and MCMC MH can all compute top k queries over the linear chain CRF models  sum product  Gibbs and MCMC MH can all compute marginal queries over the linear chain and tree shaped models  while only MCMC algorithms can compute queries over cyclic models  Although there are heuristic adaptations of the sum product algorithm for cyclic models  past literature found MCMC methods to be more effective in handling complicated cyclic models with long distance dependencies and deterministic constraints  21  22   In terms of handling query constraints  Viterbi and sum product algorithms can only handle selection constraints  Gibbs sampling can handle selection constraints and aggregate constraints that do not break the distribution into disconnected regions  On the other hand  MCMC MH can handle arbitrary constraints in the    SQL IE    queries  5 2 Parameters Next  we introduce a list of parameters that affect the applicability  accuracy and runtime of the four inference algorithms that we have just described  1  Data Size  the size of the data is measured by the total number of tokens in information extraction  2  Structure of Grounded Models  the structural properties of the model instantiations over data   a  shape of the model  i e   linear chain  tree shaped  cyclic    b  maximum size of the clique   c  maximum length of the loops  e g   skip chain in linear CRF  3  Correlation Strength  the relative strength of transitional correlation between different label variables  4  Label Space  the number of possible labels  The data size affects the runtime for all the inference algorithms  The runtime of Viterbi and sum product algorithms is linear to the data size  The MCMC algorithms are iterative optimizations that can be stopped at any time  but the number of samples needed to converge depend linearly on the size of the data  The structure of the grounded model can be quanti   ed with three parameters  shape of the model  maximum size of the clique and the maximum length of the loops  The    rst parameter determines the applicability of the models  and is also the most important factorinference Top k Marginal Constraints algorithm Chain Tree Cyclic Chain Tree Cyclic Some Arbitrary Viterbi X X sum product X X X MCMC Gibbs X X X X X X X MCMC MH X X X X X X X Table 1  Applicability of different inference algorithms for different queries  e g   top k  marginal  over different model structures  e g   linear chain  tree shaped  cyclic   and in handling query constraints  in the accuracy and the runtime of the inference algorithms over the model  Although not studied in this paper  the maximum clique size and the length of the loops play an important role in the runtime of several known inference algorithms  including  for example  the junction tree and the loopy belief propagation algorithms   19   The correlation strength is the relative strength of the transition correlation between different label variables over the state correlation between tokens with their corresponding labels  The correlation strength does not in   uence the accuracy or the runtime of the Viterbi or the sum product algorithm  However  it is a significant factor in the accuracy and runtime of the MCMC methods  especially the Gibbs algorithm  Weaker correlation strengths result in faster convergence for the Gibbs sampler  At the extreme  zero transition correlation results in complete label independence  rendering consecutive Gibbs samples independently  which would converge very quickly  The size of the label space of the model is also an important factor of the runtime of all the inference algorithms  The runtime of the Viterbi and sum product algorithms is quadratic in the size of the label space  while the runtime of the Gibbs algorithm is linear in the label space because each sampling step requires enumerating all possible labels  5 3 Rules for Choosing Inference Algorithms Among the parameters described in the previous section  we focus on  1  the shape of the model   2  the correlation strength  and  3  the label space into consideration  because the rest are less in   uential in the four inference algorithms we study in this paper  The data size is important for optimizing the extraction order in the join over top k queries as described in  4   However  since the complexity of the inference algorithms we study in this paper are all linear in the size of the data  it is not an important factor for choosing inference algorithms  Based on analysis in the last section on the parameters  the following are the rules to choose an inference algorithm for different data and model characteristics  quanti   ed by the three parameters  and the query    For cyclic models    If cycles are induced by query constraints  choose querydriven MCMC MH over Gibbs Sampling    Otherwise  choose Gibbs Sampling over MCMC MH  As shown in our experiments in Sections 7 3 7 4  the Gibbs Sampler converges much faster than the random walk MCMCMH for computing both top k extractions and marginal distribution    For acyclic models    For models with small label space  choose Viterbi over MCMC methods for top k and sum product over MCMC methods for marginal queries    For models with strong correlations  choose Viterbi and sum product over MCMC methods    For models with both large label space and weak correlations  choose Gibbs Sampling over MCMC MH  Viterbi  and sum product  For a typical IE application  the label space is small  e g   10   and the correlation strength is fairly strong  For example  title tokens are usually followed by the author tokens in a bibliography string  Moreover  strong correlation exists with any multitoken entity names  e g   a person token is likely to be followed by another person token   Thus  the above rules translate in most cases in IE to  choose Viterbi and sum product over MCMC methods for acyclic models for top k and marginal queries respectively  choose Gibbs Sampling for cyclic models unless the cycles are induced by query constraints  in which case choose query driven MCMC MH  In this paper  we use heuristic rules to decide the threshold for a    small    label space and for a    strong    correlation for a data set  Developing a cost based optimizer to make such choices based on the data and model is one of our future directions  6 Hybrid Inference Typically  for a given model and dataset  a single inference algorithm is chosen based on the characteristics of the model  In this section  we    rst show that in the context of SQL queries over probabilistic IE results  the proper choice of IE inference algorithm is not only model dependent  but also query  and text dependent  Thus  to achieve good accuracy and runtime performance  it is imperative for a PDB system to use a hybrid approach to IE even within a single query  employing different inference algorithms for different records  We describe the query processing steps that employ hybrid inference for different documents within a single query  Then we describe an algorithm  which  given the input of a    SQL IE    query  generates a query plan that applies the hybrid inference  Finally  we show the query plans with hybrid inference generated from three example    SQL IE    queries to take advantage of the appropriate IE inference algorithms for different combinations of query  text and CRF models  6 1 Query Processing Steps In the context of SQL queries over probabilistic IE results  the proper choice of the IE inference algorithm is not only dependent on the model  but also dependent on the query and the text  First of all  the relational sub query Qre augments the original model with additional random variables  cross edges and factor tables  making the model structure more complex  as we explained in Section 4 1  The characteristics of the model may change after applying the query over the model  For example  a linear chain CRF model may become a cyclic CRF model  after the join query in Q1 or the query with aggregate constraint in Q2  Secondly  when the resulting CRF model is instantiated  i e   grounded  over a document  it could result in a grounded CRF model with drastically different model characteristics  For example  the CRF model  resulting from a join query over a linear chain CRF model  when instantiated over different documents  can result in either a cyclic or a tree shaped model  as we have shown in Figure 5 a  and  b   The applicability  accuracy and runtime of different inference algorithms vary signi   cantly over models with different characteristics  which can result from different data for the same query andHYBRID INFERENCE PLAN GENERATOR  Q  1 apply Qre over the base CRF models   CRF   2 apply deterministic selections in Q over base TOKENTBLs   fTig 3 apply deterministic joins in Q over fTig   T 4 apply model instantiation over T using CRF     groundCRFs 5 apply split operation to groundCRFs   linearCRFs  treeCRFs  cyclicCRFs 6 if Qinf is Marginal then 7 apply sum product to  linearCRFs   treeCRFs    res2 8 apply Gibbs to  cyclicCRFs    res3 9 else if Qinf is Top k then 10 apply Viterbi to  linearCRFs    res1 11 if Qre contains aggregate constraint but no join then 12 apply Viterbi to  cyclicCRFs   treeCRFs    res 13 apply aggregate constraint in Q over res   res1 14 apply query driven MCMC MH to  res  res1  T   res3 15 else 16 apply Gibbs to  cyclicCRFs   treeCRFs    res3 17 endif endif 18 if Qinf is Top k then 19 apply union of res1 and res3 20 else if Qinf is Marginal then 21 apply union of res2 and res3 22 endif Figure 7  Pseudo code for the hybrid inference query plan generation algorithm  model  As a result  to achieve good accuracy and runtime  we apply different inference algorithms  i e   hybrid inference  for different documents within a single query  The choice of the inference algorithm over a document is based on the characteristics of its grounded model  and rules for choosing inference algorithms we described in Section 5 3  The main steps in query processing with hybrid inference are as follows  1  Apply Query over Model  Apply the relational part of the query Qre over the underlying CRF model  2  Instantiate Model over Data  Instantiate the resulting model from the previous step over the text  and compute the important characteristics of the grounded models  3  Partition Data  Partition the data according to the properties of grounded models from the previous step  In this paper  we only partition the data according to the shape of the grounded model  More complicated partitioning techniques  such as one based on the size of the maximum clique can be considered for future work  4  Choose Inference  Choose the inference algorithms to apply according to the rules in Section 5 3 over the different data partitions based on the characteristics of the grounded models  5  Execute Inference  Execute the chosen inference algorithm over each data partition  and return the union of the results from all partitions  6 2 Query Plan Generation Algorithm We envision that the query parser takes in a    SQL IE    query and outputs  along with others non hybrid plans  a query plan which applies hybrid inference over different documents  The algorithm in Figure 7 generates a hybrid inference query plan for an input    SQL IE    query Q  consisting of the relation part Qre  and the subsequent inference operator Qinf   In Line 1  the relational operators in Qre are applied to the CRF models underlying the base TOKENTBL tables  resulting in a new CRF model CRF     In Lines 2 to 3  selection and join conditions on the deterministic attributes  e g   docID  pos  token  are applied to the base TOKENTBL tables  resulting in a set of tuples T  each of which represents a document or a document pair  In Line 4  the model instantiation is applied over T using CRF   to generate a set of    ground    models groundCRFs  In Line 5  a split operation is performed to partition the groundCRFs according to their model structures into linearCRFs  treeCRFs and cyclicCRFs  Lines 6 to 19 capture the rules for choosing inference algorithms we described in Section 5 3  Finally  a union is applied over the result sets from different inference algorithms for the same query  Lines 11 to 14 deals with a special set of queries  which compute the top k results over a simple query with aggregate conditions that induce cycles over the base linear chain CRFs  The intuition is that it is always bene   cial to apply the Viterbi algorithm over the base linear chain CRFs as a fast    ltering step before applying the MCMC methods  In Line 12  it    rst computes the top k extractions res without the aggregate constraint using Viterbi  In Line 13  it applies the constraint to the top k extractions in res  which results in a set of top k extractions that satisfy the constraints in res1  In Line 14  the query driven MCMC MH is applied to the documents in T with extractions that do not satisfy the constraint   res res1  T  An example of this special case is described in Section 6 3 3  Complexity  The complexity of generating the hybrid plan depends on the complexity of the operation on Line 5 in Figure 7  where the groundCRFs are split into subsets of linearCRFs  treeCRFs and cyclicCRFs  The split is performed by traversing the ground CRFs to determine their structural properties  which is linear to the size of the ground CRF O N   where N is the number of random variables  The complexity of choosing the appropriate inference  lines 6 to 21  is O 1   On the other hand  the complexity of Viterbi  sum product algorithms over linearCRFs and treeCRFs is O N  with a much larger constant  because of the complex computation  i e   sum  product  over jY j   jY j matrices  where jY j is the number of possible labels  The complexity of exact inference algorithm over cyclicCRFs is NP hard  Thus the cost of generating the hybrid plan is negligible from the cost of the inference algorithms  6 3 Example Query Plans In this section  we describe three example queries and show the query plans with hybrid inference generated from the algorithm in Figure 7  which take advantage of the appropriate inference algorithms for different combinations of query  text and CRF models  6 3 1 Skip Chain CRF In this query  we want to compute the marginal distribution or the top k extraction over an underlying skip chain CRF model as shown in Figure 4  The query is simply  Q4   Skip Chain Model  SELECT  Top k T1 docID    Marginal T1 pos exist   FROM TokenTbl T1 As described in  12   the MCMC methods are normally used to perform inference over the skip chain CRF model for all the documents  like the query plan in Figure 8 b   The Viterbi algorithm fails to apply because the skip chain model contains skip edges  which make the CRF model cyclic  However  the existence of such skip edges in the grounded models  instantiated from the documents  is dependant on the text  There exist documents  like the one shown in Figure 4  in which one string appears multiple times  Those documents result in cyclic CRF models  But  there also exist documents  in which only unique tokens are used except for    stop words     such as    for        a     Those documents result in linear chain CRF models  TokenTbl Skip Chain CRF model instantiation zero skip edge  linear  at least one skip edge  cyclic  Sum Product Viterbi Gibbs MCMC MH Union TokenTbl Skip Chain CRF Gibbs MCMC MH model instantiation  a   b  Figure 8  The query plan for an inference  either top k or marginal  over a skip chain CRF model  The query plan generated with hybrid inference is shown in Figure 8 a   After the model instantiation  the ground CRF model is inspected  if no skip edge exists  i e   no duplicate strings exist in a document   then the Viterbi or the sum product algorithm is applied  otherwise  the Gibbs algorithm is applied to the cyclic ground CRFs  Compared to the non hybrid query plan  the query plan with hybrid inference is more ef   cient by applying more ef     cient inference algorithms  e g   Viterbi  sum product  over the subset of the documents  where the skip chain CRF model does not induce cyclic graphs  The speedup depends on the performance of Viterbi sum product compared to Gibbs Sampling  and on the percentage of such documents that instantiate a skip chain CRF model into a grounded linear chain CRF models  6 3 2 Join over Linear chain CRF In this example  we use the join query Q1 described in Section 4 1  which computes the marginal probability of the existence of a join result  The join query is performed between each document pair on having tokens with the same strings labeled as    person     Such a join query over the underlying linear chain CRF models induces cross edges and cycles in the resulting CRF model  A typical non hybrid query plan  shown in Figure 9 with black edges  perform MCMC inference over all the documents  However  as we see in Figure 5 a  and  b   depending on the text  the joint CRF model can be instantiated into either a cyclic graph or a tree shaped graph  The red edge in Figure 9 shows the query plan with hybrid inference for the join query Q1  As we can see  instead of performing MCMC methods unilaterally across all    joinable    document pairs  i e   contain at least 1 pair of common tokens   the sum product algorithm is used over the document pairs that contain only 1 pair of common tokens  Compared to the non hybrid query plan  the hybrid inference reduces the runtime by applying the more ef   cient inference  i e   sum product  when possible  The speedup depends on the performance of sum product compared to the MCMC methods  and the percentage of the    joinable    document pairs that only share one pair of common tokens that are not    stop words     6 3 3 Aggregate Constraints In this example  we use Q2  the query with an aggregate constraint  described in Section 4 1  As shown in Figure 5 c   the aggregate constraints can induce a big clique including all the label variables in each document  In other words  regardless of the text  based on the model and the query  each document is instantiated into a cyclic graph with high tree width  Again  typically  for such a high tree width cyclic model  MCMCMH algorithms are used over all the documents to compute the topTokenTbl1 TokenTbl2 CRF1 CRF2 model instantiation CRF1 2 only 1 cross edge  tree  more than 2 cross edges  cyclic  Join token1 token2  Join token1 token2   label1 label2  Sum product Gibbs MCMC MH Union Figure 9  The query plan for the probabilistic join query followed by marginal inference  TokenTbl CRF with cliques Viterbi   Model instantiation aggregate condition MCMC Query Driven  Union  MH Linear chain CRF satisfy un satisfy aggregate condition Model instantiation TokenTbl CRF with cliques MCMC Query Driven   MH Linear chain CRF aggregate condition Model instantiation  a   b  Figure 10  The query plan for the aggregate selection query followed by a top k inference  k extractions that satisfy the constraint  Such a non hybrid query plan is shown in Figure 10 b   However  this query falls into the special case described in the query plan generation algorithm in Section 6 2 for hybrid inference  The query is to return the top k extractions over the cyclic graph induced by an aggregate constraint over a linear chain CRF model  Thus  the resulting query plan is shown in Figure 10 a   In the query plan with hybrid inference  the Viterbi algorithm runs    rst to compute the top k extraction without the constraint  Then  the results are run through the aggregate  those that satisfy the constraint are returned as part of the results  and those that do not satisfy the constraint are fed into the query driven MCMC MH algorithm  7 Evaluation So far  we have described the implementation of the MCMC algorithms  and the query plans for the hybrid inference algorithms  We now present the results of a set of experiments aimed to  1  evaluate the ef   ciency of the SQL implementation of the MCMC methods and the effectiveness of the query driven sampling techniques   2  compare the accuracy and runtime of the four inference algorithms  Viterbi  sum product  Gibbs and MCMC MH  for the two IE tasks   top k and marginal  and  3  analyze three real life text datasets to quantify the potential speedup of a query plan with hybrid inference compared to one with non hybrid inference  Setup and Dataset  We implemented the four inference algorithms  Viterbi  sum product  Gibbs and MCMC MH in PostgreSQL 8 4 1  We conducted the experiments reported here on a 2 4 GHz Intel Pentium 4 Linux system with 1GB RAM  For evaluating the ef   ciency of the in database implementation of the MCMC methods  and for comparing the accuracy and run 0 10 20 30 40 50 60 70 80 90 100 0 200000 400000 600000 800000 1000000 Runtime  sec  Number of Samples MCMC MH Runtime  SQL vs  Scala Java SQL MCMC MH SQL Gibbs Scala Java MCMC MH Figure 11  Runtime comparison of the SQL and Java Scala implementations of MCMC MH and Gibbss algorithms over DBLP  time of the inference algorithms  we use the DBLP dataset  23  and a CRF model with 10 labels and features similar to those in  18   The DBLP database contains more than 700  000 papers with attributes  such as conference  year  etc  We generate bibliography strings from DBLP by concatenating all the attribute values of each paper record  We also have similar results for the same experiments over NYTimes dataset  which we include in our technical report  For quantifying the speedup of query plans with hybrid inference  we examine the New York Times  NYTimes  dataset  and the Twitter dataset in addition to the DBLP dataset  The NYTimes dataset contains ten million tokens from 1  788 New York Times articles from the year 2004  The Twitter dataset contains around 200  000 tokens from over 40  000 tweets obtained in January 2010  We label both corpora with 9 labels  including person  location  etc  7 1 MCMC SQL Implementation In this experiment  we compare the runtime of the in database implementation of the MCMC algorithms  including Gibbs Sampling and MCMC MH  with the runtime of the Scala Java  with Scala 2 7 7 and Java 1 5  implementation of MCMC MH described in  12  over linear chain CRF models  The runtime of Scala Java implementation is measured on a different machine with better con   gurations  2 66GHz CPU Mac OSX 10 6 4 with 8G RAM   As we can see in Figure 11  the runtime of the MCMC algorithms grow linearly with the number of samples for both the SQL and the Java Scala implementations  While the Scala Java implementation of MCMC MH can generate 1 million samples in around 51 seconds  it takes about 78 seconds for the SQL implementation of the MCMC MH  and about 89 seconds for that of the Gibbs Sampling  This experiment shows that the in database implementations of the MCMC sampling algorithms achieve comparable  within a factor of 1 5  runtimes to the Java Scala implementation  7 2 Query Driven MCMC MH In this experiment  we evaluate the effectiveness of the query driven MCMC MH algorithm described in Section 4 3 with the vanilla MCMC MH in generating samples that satisfy the query constraint  The query we use is Q2 described in Section 4 1  which computes the top 1 extractions that satisfy the aggregate constraint that all title tokens are in front of the author tokens  We run the query driven MCMC MH and the vanilla MCMCMH algorithm over a randomly picked 10 documents from the DBLP dataset  Figure 12 shows the number of    quali   ed    samples that are generated by each algorithm in 1 second  As we can see  the querydriven MCMC MH generates more    quali   ed    samples  roughly 1200 for all the documents  and for half of the documents the query driven MCMC MH generates more than 10 times more quali   ed samples than vanilla MCMC MH  0 200 400 600 800 1000 1200 1400 1600 1 2 3 4 5 7 8 9 10 Number of Samples Document Number MCMC MH vs  Query driven MCMC MH MCMC MH Query driven MCMC MH Figure 12  The number of quali   ed samples generated by the querydriven MCMC MH and the vanilla MCMC MH algorithm in 1 second for different documents in DBLP  0 10000 20000 30000 40000 50000 0 50 100 150 Compuation Error  Number of Labels  Execution Time  sec  Top 1  MCMC MH  Gibbs vs  Viterbi MCMC MH Viterbi Gibbs Figure 13  Runtime Accuracy graph comparing Gibbs  MCMC MH and Viterbi over linear chain CRF for top 1 inference on DBLP  7 3 MCMC vs  Viterbi on Top k Inference This experiment compares the runtime and the accuracy of the Gibbs  the MCMC MH and the Viterbi algorithms in computing top 1 inference over linear chain CRF models  The inference is performed over 45  000 tokens in 1000 bibliography strings from the DBLP dataset  We measure the    computation error    as the number of labels different from the exact top 1 labelings according to the model 1   The Viterbi algorithm only takes 6 1 seconds to complete the exact inference over these documents  achieving zero computation error  For the MCMC algorithms  we measure the computation error and runtime for every 10k more samples  starting from 10k to 1 million samples over all documents  As we can see in Figure 13  the computation error of the Gibbs algorithm drops to 22  from 45  000 to 10  000 when 500k samples are generated  This takes around 75 seconds  more than 12 times longer than the runtime of the Viterbi algorithm  The MCMC MH converges much slower than the Gibbs Sampling  As more samples are generated  the top  1 extractions generated from the MCMC algorithms get closer and closer to the exact top 1  however very slowly  Thus  Viterbi beats the MCMC methods by far in computing top 1 extractions with linear chain CRF models  more than 10 times faster with more than 20  fewer computation errors  7 4 MCMC vs  Sum Product on Marginal Inference This experiment compares the runtime and the accuracy of the Gibbs  MCMC MH and the sum product algorithms over tree shaped graphical models induced by a join query similar to Q1  described in Section 4 1  The query computes the marginal probability of the existence of a join result for each document pair in DBLP  joining on the same    publisher     The query is performed over a set of 10  000 pairs of documents from DBLP  where the two documents in each pair have exactly one token in common  1 The top 1 extractions with zero computation error may still contain mistakes  which are caused by inaccurate models 0 0 2 0 4 0 6 0 8 1 Average Probability Difference 0 100 200 300 400 500 Execution Time  sec  MCMC MH  Gibbs vs  Sum Product Gibbs Sum Product MCMC MH Figure 14  Runtime Accuracy graph comparing Gibbs  MCMC MH and sum product over tree shaped models for marginal inference on DBLP  Data Skip chain Probabilistic Aggregate Corpora CRF Join Constraint NYTimes  5 0  4 5  10 0 Twitter  5 0  2 6 N A DBLP  1 0  1 0 N A Table 2  Speed ups achieved by hybrid inference for different queries  The sum product algorithm over these 10  000 tree shaped graphical models takes about 60 seconds  As an exact algorithm  the sum product algorithm achieves zero computation error  We measure the    computation error    as the difference between the marginal probabilities of join computed from the MCMC MH algorithms and the sum product algorithm  averaging over all document pairs  For the MCMC algorithms  we measure the computation error and runtime for every 200k more samples  starting from 200k to 2 million samples over all document pairs  As we can see in Figure 14  the probability difference between Gibbs and sum product converges to zero quickly  at 400 second  the probability difference is dropped to 0 01  The MCMC MH on the other hand  converges much slower than the Gibbs  This experiment shows that MCMC algorithms performs relatively better in computing marginal distributions than in computing top 1 extractions  However  sum product algorithm still outperforms MCMC algorithms in computing marginal probabilities over tree shaped models  more than 6 times faster with about 1  less computation error  7 5 Exploring Model Parameters In this experiment  we explore how different correlation strengths  one of the parameters we discussed in Section 5 2  affect the runtime and the accuracy of the inference algorithms  As we explained earlier  the correlation strength does not affect the accuracy or the runtime of the Viterbi algorithm  On the other hand  weaker correlation between different random variables in the CRF model leads to faster convergence for the MCMC algorithms  The setup of this experiment is the same as in Section 7 3  In Figure 15  we show the runtime accuracy graph of the Viterbi and the Gibbs algorithm to compute the top 1 extractions over models with different correlation strengths  We synthetically generated models with correlation strengths of 1  0 5  0 2 and 0 001 by dividing the original scores in the transition factors by 1  2  5 and 1000 respectively  As we can see  the weaker correlation strengths lead to faster convergence for the Gibbs algorithm  When correlation strength is 0 001 the computation error reduces to zero in less than twice that of the Viterbi runtime  The correlation strength of the CRF model depends on the dataset on which the CRF model is learned  The model we learned over NYTimes and DBLP dataset both contains strong correlation strength  0 10000 20000 30000 40000 50000 0 5 10 15 20 Computation Error   Number of Labels  Execution Time  sec  Correlation Strength  Gibbs vs  Viterbi Gibbs corr 1 Gibbs corr 0 5 Gibbs corr 0 2 Gibbs corr 0 001 Viterbi Figure 15  Runtime Accuracy graph comparing Gibbs and Viterbi over models with different correlation strengths on DBLP  7 6 Hybrid Inference for Skip chain CRF In this and the next two sections  we describe the results  in terms of</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Faerie  Ef   cient Filtering Algorithms for Approximate Dictionary based Entity Extraction ### Guoliang Li Dong Deng Jianhua Feng Department of Computer Science and Technology  Tsinghua University  Beijing 100084  China  liguoliang tsinghua edu cn  buaasoftdavid gmail com  fengjh tsinghua edu cn ABSTRACT Dictionary based entity extraction identi   es prede   ned entities  e g   person names or locations  from a document  A recent trend for improving extraction recall is to support approximate entity extraction  which    nds all substrings in the document that approximately match entities in a given dictionary  Existing methods to address this problem support either token based similarity  e g   Jaccard Similarity  or character based dissimilarity  e g   Edit Distance   It calls for a uni   ed method to support various similarity dissimilarity functions  since a uni   ed method can reduce the programming e   orts  hardware requirements  and the manpower  In addition  many substrings in the document have overlaps  and we have an opportunity to utilize the shared computation across the overlaps to avoid unnecessary redundant computation  In this paper  we propose a uni   ed framework to support many similarity dissimilarity functions  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance  We devise e   cient    ltering algorithms to utilize the shared computation and develop e   ective pruning techniques to improve the performance  The experimental results show that our method achieves high performance and outperforms state of the art studies  Categories and Subject Descriptors H 2  Database Management   Database applications  H 3 3  Information Search and Retrieval  General Terms  Algorithms  Experimentation  Performance Keywords  Approximate Entity Extraction  Uni   ed Framework  Filtering Algorithms  Pruning Techniques ### 1  INTRODUCTION Dictionary based entity extraction identi   es all the substrings from a document that match the prede   ned entities     This work is partly supported by the National Natural Science Foundation of China under Grant No  61003004  the National Grand Fundamental Research 973 Program of China under Grant No  2011CB302206  and National S T Major Project of China under Grant No  2011ZX01042 001 002  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  in a given dictionary  For example  consider a document    An E   cient Filter for Approximate Membership Checking  Venkaee shga Kamunshik kabarati  Dong Xin  Surauijt ChadhuriSIGMOD     and a dictionary with two entities    Surajit Chaudhuri    and     Dong Xin     Dictionary based entity extraction    nds the prede   ned entity    Dong Xin    from the document  This problem has many real applications in the    elds of information retrieval  molecular biology  bioinformatics  and natural language processing  However  the document may contain typographical or orthographical errors and the same entity may have di   erent representations  22   For example  the substring    Surauijt Chadhuri    in the above document has typographical errors  The traditional  exact  entity extraction cannot    nd this substring from the document  since the substring does not exactly match the prede   ned entity    Surajit Chaudhuri     Approximate entity extraction is a recent trend to address this problem  which    nds all substrings from the document that approximately match the prede   ned entities  To improve extraction recall  we study the problem of approximate dictionary based entity extraction  which  given a dictionary of entities and a document     nds all substrings from the document similar to some entities in the dictionary  Many similarity dissimilarity functions have been proposed to quantify the similarity between two strings  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance  For instance  in the above example  suppose we use edit distance and the threshold is 3  Approximate entity extraction can    nd the substring    Surauijt Chadhuri    which is similar to entity    Surajit Chaudhuri     Although there have been some recent studies on approximate entity extraction  22  4   they support either tokenbased similarity  e g   Jaccard Similarity  or character based dissimilarity  e g   Edit Distance   It calls for a uni   ed method to support di   erent similarity dissimilarity functions  since the uni   ed method can reduce not only the programming efforts  but also the hardware requirements and the manpower needed to maintain the codes for di   erent functions  In addition  we have an observation that many substrings in the document have overlaps  For example  consider the above document  many substrings  e g     Surauijt Chadhuri        urauijt ChadhuriSIG        rauijt ChadhuriSIGMOD     h ave ove r   laps  e g      Chadhuri      We have an opportunity to utilize this feature to avoid the redundant computation across overlaps of di   erent substrings  For example  we can share the computation on    Chadhuri    for di   erent substrings  To remedy the above problems  we propose a uni   ed framework to support various similarity dissimilarity functions  529To avoid redundant computation across overlaps  we develop e   cient filtering algorithms for approximate dictionary based entity extraction  called    Faerie     To summarize  we make the following contributions      We propose a uni   ed framework to support many similarity dissimilarity functions  such as jaccard similarity  cosine similarity  dice similarity  edit similarity  and edit distance      We devise e   cient    ltering algorithms  which can utilize the shared computation across the overlaps of multiple substrings of the document      We develop e   ective pruning techniques and devise ef     cient algorithms to improve the performance      We have implemented our method  and the experimental results show that our method achieves high performance and outperforms state of the art approaches  The remainder of this paper is organized as follows  We propose a uni   ed framework to support various similarity functions in Section 2  Section 3 introduces a heap based    ltering algorithm to utilize shared computation  We develop pruning techniques in Section 4 and introduce our algorithm Faerie in Section 5  We conduct extensive experimental studies in Section 6  Related works are provided in Section 7  Finally  we conclude the paper in Section 8  2  A UNIFIED FRAMEWORK We    rst formulate the problem of approximate  dictionarybased  entity extraction  Section 2 1   and then introduce a uni   ed method to support various similarity dissimilarity functions  Section 2 2   Finally we introduce a concept of    valid substrings   to prune unnecessary substrings  Section 2 3   2 1 Problem Formulation Definition 1  Approximate Entity Extraction   Given a dictionary of entities E    e1  e2          en   a document D  a similarity function  and a threshold  it    nds all     similar    pairs  s  ei  with respect to the given function and threshold  where s is a substring of D and ei     E  This paper focuses on token based similarity  characterbased similarity  and character based dissimilarity  Token based Similarity  It includes Jaccard Similarity  Cosine Similarity  and Dice Similarity  The token based similarity takes a string as a set of tokens  Let jac  cos  and dice respectively denote the jaccard similarity  cosine similarity  and dice similarity  Given two strings r and s  let  r  denote the number of tokens in r  jac r  s     r   s   r   s    cos r  s        r   s   r    s    and dice r  s    2 r   s   r   s    For example  jac    sigmod 2011 conference        sigmod 2011       2 3   cos    sigmod 2011 conference        sigmod 2011          2 6   and dice    sigmod 2011 conference        sigmod 2011       4 5   Charater based Dissimilarity  It includes Edit Distance  The charater based dissimilarity takes a string as a sequence of characters  The edit distance between two strings r and s  denoted by ed r  s   is the minimum number of singlecharacter edit operations  i e   insertion  deletion  and substitution  needed to transform string r to string s  For example  ed    surajit        surauijt       2   Table 1  A dictionary of entities and a document   a  Dictionary E ID Entity e len e   e     of q grams with q   2  1 kaushik ch 10 9 2 chakrabarti 11 10 3 chaudhuri 9 8 4 venkatesh 9 8 5 surajit ch 10 9  b  Document D an e   cient    lter for approximate membership checking  venkaee shga kamunshik kabarati  dong xin  surauijt chadhurisigmod  Charater based Similarity  It includes Edit Similarity  The edit similarity between two strings r and s is de   ned as eds r  s   1    ed r s  max   len r  len s      where len s  denotes the length of s  For instance  eds    surajit        surauijt      1    2 8   3 4   In this paper two strings are said to be similar  if their  jaccard  cosine  dice  edit  similarity is no smaller than a given similarity threshold     or their edit distance is no larger than a given edit distance threshold      For instance  consider the document D and dictionary E in Table 1  Suppose the edit distance threshold      2      venkaee sh        venkatesh          surauijt ch        surajit ch      and     chadhuri        chaudhuri     are three example results  Especially  although the substring    chadhurisigmod    in the document misses a space between    chadhuri    and     sigmod     a typographical error   our method still can    nd   chadhuri    similar to entity   chaudhuri       It has been shown that approximate entity extraction can improve recall  22   For example  the recall can increase from 65 4  to 71 4  when performing protein name recognition  In this paper  we emphasize on improving the performance  We focus on extracting textual entities  We assume the thresholds     and      are pre de   ned and take the selection of such thresholds as a future work  2 2 A Uni   ed Method In this section  we propose a uni   ed framework to support various similarity dissimilarity functions  We model both the entity and document as a set of tokens  Especially for edit distance and edit similarity  we take qgrams of an entity as tokens  A q gram of a string s is a substring of s with length q  The q gram set of s  denoted by G s   is the set of all of s   s q grams  For example  the 2 gram set of    surajit ch    is  su  ur  ra  aj  ji  it  t    c  ch   If the context is clear  we use token to denote token gram  for edit distance and edit similarity  we use e to denote G e   e   s to denote G r    G s   and  e  to denote  G e    i e    e    len e      q   1   Given an entity e and a substring s  we transform di   erent similarities dissimilarities to the overlap similarity   e     s   and use the overlap similarity as a uni   ed    ltering condition  if e and s are similar  then  e     s  must be not smaller than a threshold T   0  where T can be computed as follows      Jaccard Similarity  T      e     s          1           Cosine Similarity  T        e      s               Dice Similarity  T      e     s          2        Edit Distance  T   max  e    s              q      Edit Similarity  T    max  e    s        max  e    s    q    1        1           q   530The correctness of these thresholds is stated in Lemma 1  Lemma 1  Given an entity e and a substring s  we have 1       Jaccard Similarity   If jac e  s           e   s       e   s         1           Cosine Similarity   If cos e  s           e   s         e      s             Dice Similarity   If dice e  s           e   s       e   s         2        Edit Distance   If ed e  s            e   s    max  e    s          q      Edit Similarity   If eds e  s           e   s       max  e    s        max  e    s   q   1       1         q   In this way  we can transform various similarities dissimilarities to the overlap similarity  and develop a uni   ed    ltering condition  if  e     s    T  we prune the pair  s  e   Note that given any similarity function and a threshold  if we can deduce a lower bound for the overlap similarity of two strings  our method can apply to this function  Specially  the    ve similarity distance functions we studied are commonly used in information extraction and record linkage  22  4   2 3 Valid Substrings We have an observation that some substrings in D will not have any similar entities  For instance  consider the dictionary and document in Table 1  Suppose we use edit distance and      1  Consider substring    surauijt chadhurisigmod    with length 23  As the lengths of entities in the dictionary are between 9 and 11  the substring cannot be similar to any entity  Next we discuss how to prune such substrings  Given an entity e and a substring s  if s is similar to e  the number of tokens in s   s   should be in a range     e   e   that is    e      s       e  where    e and  e are respectively the lower and upper bound of  s   computed as below      Jaccard Similarity     e     e          and  e      e            Cosine Similarity     e     e         2   and  e      e    2        Dice Similarity     e     e         2        and  e     e      2                Edit Distance     e    e         and  e    e             Edit Similarity     e      e    q     1              q     1   and  e      e  q   1         q     1    where    is the similarity threshold and    is the edit distance threshold  The correctness of the bounds is stated in Lemma 2  Lemma 2  Given an entity e  for any substring s  we have     Jaccard Similarity   if jac e  s            e             s       e            Cosine Similarity   if cos e  s            e        2      s       e    2        Dice Similarity   if dice e  s          e        2           s      e     2                Edit Distance   if ed e  s            e              s       e             Edit Similarity   if eds e  s             e  q   1          q   1        s         e    q     1        q   1    Based on Lemma  2   given an entity e  only those substrings with token numbers between    e and  e could be similar to entity e  and others can be pruned  Especially  let    E   min    e e     E  and  E   max  e e     E   Obviously the substrings in D with token numbers between    E and  E may have a similar entity in the dictionary E  and others can be pruned  Based on this observation  we introduce the concept of    valid substring     1 In this paper  we omit the proof due to space constraints  Definition 2  Valid Substring   Given a dictionary E and a document D  a substring s in D is a valid substring for an entity e     E if    e      s       e  A substring s in D is a valid substring for dictionary E if    E      s       E  For instance  consider the dictionary and document in Table 1  Suppose we use edit similarity  and      0 8 and q   2  Consider entity e5      surajit ch        e5      e5    q     1              q     1     7  and  e5     e5  q   1         q     1    11  Only the valid substrings with token numbers between 7 and 11 could be similar to entity e5  As    E   7  and  E   12  only the valid substrings with token numbers between 7 and 12 could have similar entities in the dictionary  and all other substrings  e g      surauijt chadhurisigmod     c a n b e p r u n e d   We employ a    lter and verify framework to address the problem of approximate entity extraction  In the    lter step  we generate the candidate pairs of a valid substring in document D and an entity in dictionary E  whose overlap similarity is no smaller than a threshold T  and in the verify step  we verify the candidate pairs to get the    nal results  by computing the real similarity disimilarity  In this paper  we focus on the    lter step  3  HEAP BASED FILTERING ALGORITHMS In this section  we propose heap based    ltering algorithms to utilize the shared computation across overlaps  We    rst introduce an inverted index structure  Section 3 1   and then devise a multi heap based algorithm  Section 3 2  and a single heap based algorithm  Section 3 3   3 1 An Inverted Index Structure A valid substring is similar to an entity only if they share enough common tokens  To e   ciently count the number of their common tokens  we use the inverted index structure  We build an inverted index for all entities  where entries are tokens  for jaccard similarity  cosine similarity  and dice similarity  or q grams  for edit similarity and edit distance   and each entry has an inverted list that keeps the ids of entities that contain the corresponding token gram  sorted in ascending order  For example  Figure 1 gives the inverted list for entities in Table 1 using q grams with q   2  ka au us sh hi ik ab ba ar rt ti te es su aj ji it t  1 4 1 3 1 1 4 1 1 1 1 5 1 2 3 5 2 3 2 3 3 5 5 2 2 2 2 2 2 3 3 4 4 4 5 5 5 5 k   c ch ha ak kr dh hu ur ri ve ra 2 5 ud 3 at 4 en 4 4nk Figure 1  Inverted indexes for entities in Table 1  For each valid substring s in D  we    rst get its tokens and the corresponding inverted lists  Then for each entity in these inverted lists  we count its occurrence number in the inverted lists  i e   the number of inverted lists that contain the entity  Obviously  the occurrence number of entity e is exactly  e     s  2   For each entity e with occurrence number no smaller than T   e     s      T    s  e  is a candidate pair  For example  consider a valid substring    surauijt ch     We    rst generate its token set  su  ur  ra  au  ui  ij  jt  t    c  ch  and get the inverted lists  the italic ones in Figure 1   Suppose we use edit distance and      2   For entity e5  T   max  e5    s            q   6   As e5   s occurrence number is 6      surauijt ch     e5    surajit ch     is a candidate pair  2 In this paper  we take e and s as multisets  since there may exist duplicate tokens in entities and substrings of the document  Even if they are taken as sets  we can also use our method for extraction  531Figure 2  A heap structure for    surauijt ch    3   For simplicity  given an entity e and a valid substring s  we use e   s occurrence number in s  or s   s inverted lists  to denote e   s occurrence number in the inverted lists of tokens in s  To e   ciently count the occurrence numbers  we propose heap based    ltering algorithms in the following sections  3 2 Multi Heap based Method In this section  we propose a multi heap based method to count the occurrence numbers  We    rst enumerate the valid substrings in D  with token number between    E and  E   Then for each valid substring  we generate its tokens and construct a min heap on top of the non empty inverted lists of its tokens  Initially we use the    rst entity of every inverted list to construct the minheap  For the top entity on the heap  we count its occurrence number on the heap  i e   the number of inverted lists that contain the entity   If the number is not smaller than T  the pair of this valid substring and the entity is a candidate pair  Next  we pop the top entity  add the next entity of the inverted list from which the top entity is selected into the heap  adjust the heap  and count the occurrence number of the new top entity  Iteratively we    nd all candidate pairs  For example  consider a valid substring    surauijt ch     We    rst generate its token set and construct a min heap on top of the    rst entities of every inverted list  Figure 2   Next we iteratively adjust the heap and get the entities  1  1  1  2  2  3  3  3  5  5  5  5  5  5  in ascending order  We count the occurrence numbers of each entry  For example  the occurrence numbers of e1  e2  e3  and e5 are respectively 3  2  3  and 6  Suppose we use edit distance and      2  For entity e5  T   max  e5    s              q   6  The pair of the substring and entity e5 is a candidate pair  Finally  we verify the candidate pair and get the    nal result  Complexity  For a valid substring with l tokens  its corresponding heap contains at most l non empty inverted lists  Thus the space complexity of the heap is O l   As we can construct heaps one by one  the space complexity is the space of the maximum heap  i e   O  E   Table 2 a    The time complexity for constructing a heap of a valid substring with l tokens is O l   As there are  D     l 1 valid substrings with l tokens  the heap construction complexity for such valid substrings is O     D     l   1      l     and the total heap construction complexity is O    E l    E   D      l   1      l    Table 2 b    In addition  for each entity  we need to adjust the heap containing the entity  Consider such heap with l inverted lists  The time complexity of adjusting the heap once is O log l    There are l such heaps that contain the entity  Figure 3   Thus for each entity  the time complexity of adjusting the heaps is O    E l    E log l      l     Suppose N is the total numbers of entities in inverted lists of tokens in D  The total time complexity of adjusting the heaps is O    E l    E log l      l     N    Table 2 b    3 For ease of presentation  we use a loser tree to represent a heap structure in our examples   D  l  l tokens l D i  l         D i l    l D i  l l  l  e  i e   D i l    l      D i l tokens inverted lists D i  l l i l 1     Figure 3  A multi heap structure  Table 2  Complexity of multi heap based methods   a  Space Complexity Maximum Heap O  E   b  Time Complexity Heap Construction O    E l    E   D      l   1      l   Heap Adjustment O    E l    E log l      l     N   The multi heap based method needs to access inverted lists multiple times and does large numbers of heap adjustment operations  To address this issue  we propose a new method which accesses every inverted list only once in Section 3 3  3 3 Single Heap based Method In this section  we propose a single heap based method  We    rst tokenize the document D and get a list of tokens  For each token  we retrieve the corresponding inverted list  We use token i  to denote the i th token  and I L i  to denote the inverted list of the i th token  We construct a single minheap on top of non empty inverted lists of all tokens in D  denoted by H  and use the heap to    nd candidate pairs  For ease of presentation  we use a two dimensional array V  1           D      E           E  to count an entity   s occurrence numbers in every valid substring   s inverted lists  Formally  let D i  l  denote a valid substring of D with l tokens starting with the i th token  Given an entity e  we use V  i  l  to count e   s occurrence number in D i  l    s inverted lists  i e   V  i  l     e     D i  l    We compute V  i  l  as follows  V  i  l  is initialized as 0 for 1     i      D  and    E     l      E  For the top entity e on the heap selected from the i th inverted list  we increase the values of relevant entries in the array by 1 as follows  Without loss of generality     rstly consider the heap with l tokens  Obviously only D i   l 1  l         D i  l  contain the i th inverted list  Figure 4   thus V  i     l   1  l           V  i  l  are relevant entries  Similarly for    E     l      E  V  i     l   1  l           V  i  l  are relevant entries  We increase the value of each relevant entry by 1  If V  i  l      T   D i  l   e  is a candidate pair  Then  we pop the top entity  add the next entity in I L i  into the heap  adjust the heap and get the next entity  and count the occurrence number of the new entity  We repeat the above steps  and iteratively we can    nd all candidate pairs  Actually  for entity e  only the valid substrings with token numbers between    e and  e could be similar to entity e  4 Thus we only need to maintain array V  1           D      e           e   Next  we give a running example to walk through the single heap based method  For example  in our running example  consider a document    venkaee shga kamunshi     we construct a single heap on top of the document as shown in Figure 5  Suppose we use edit distance and      2     E   6 and  E   12  For the entity e4 selected from the    rst token  we only need to increase its occurrence numbers in valid 4 Note that  we can get entity e   s token number  e  using a hash map  which keeps the pair of an entity and its token number  thus we can get the token number of an entity in O 1  time complexity  532                                e i  e D i  l e             l       Te       tokens Inverted lists occurrence numbers occurrence numbers occurrence numbers  e D i l    l i l  Te Figure 4  A single heap structure  Table 3  Complexity of single heap based methods   a  Space Complexity Single Heap O  D   Counting Occurrence Numbers O  D         E   1    b  Time Complexity Heap Construction O  D   Heap Adjustment O   log  D       N   Counting Occurrence Numbers O   N     max    e l    e l e   E    substrings D 1  l  for    E     l      E  i e   D 1  6           D 1  12   We increase the values of V  1  6           V  1  12  by 1  For the next entity e4 selected from the second token  we increase its occurrence numbers in valid substrings D 1  l   D 2  l  for    E     l      E  Similarly  we can count all occurrence numbers  For instance  the occurrence number of entity e4     venkatesh     in D 1  9  is 5  As the occurrence number is no smaller than T   max  e4    D 1  9            q   9   2   2   5  the pair of D 1  9      venkaee sh     a n d e nt ity e4     venkatesh     is a candidate pair  Actually as    e4   6  and  e4   10  we only need to consider the entries in V  1          20  6          10   Complexity  The space complexity of the single heap is O  D    Table 3 a    To count the occurrence numbers of an entity  we do not need to maintain the array and propose an alternative method  We    rst pop all entities with the same id from the heap  with  D  space to store them   Suppose the entity is e  Then we increase e   s occurrence numbers in V  1           D     l  1  l  by varying l from    e to  e  In this way  we only need to maintain a one dimensional array  Thus the space complexity for counting the occurrence number is O max  D       e  1 e     E     O  D       E  1   Table 3 a    The time complexity of heap construction is O  D    Table 3 b    To compute the occurrence numbers of each entity  we need to adjust the heap  and the total time complexity of adjusting the heap is O log  D       N   where N is the total number of entities in every inverted list  In addition  for each entity  we need to increase its occurrence numbers  For entity e  there are   e l    e l entries needed to be increased by 1  Figure 4   and the maximum number of such entries  for any entity  is max    e l    e l e   E   Thus the total time complexity is O   N     max    e l    e l e   E     Table 3 b    Note that the single heap based method has used the shared computation across the overlaps  tokens  of di   erent valid substring  since it only needs to scan each inverted list once  It has much lower time complexity than multi heap based method  and achieves much higher performance  Section 6   4  IMPROVING THE SINGLE HEAP BASED METHOD In this section  we propose e   ective techniques to improve ve   en  nk   ka    ae    ee   e    s   sh  hg    ga   a    k    ka   am mu   un   ns   sh   hi 1 4 1 4 1    2   3   4     5    6     7   8    9   10  11  12  13   14  15  16   17  18 19  20 1 4 D 1  9  venkaee sh and entity e4 venkatesh have 5 common tokens 4    3   2   2    1    1      1   1    2    1    1    1    1    2     1                       4    3   3   2    1    1      1   2    2    1    1    1    2    2                              4    4   3   2    1    1      2   2    2    1    1    2    2                                5    4   3   2    1    2      2   2    2    1    2    2 5    4   3   2    2    2      2   2    2    2    2      5    4   4   3    2    2      2   3    3 6 7 8 9 10 11 12 Length D 5    4   3   3    2    2      2   2    3    2 1 4 1 Less than l tokens  e4   E  TE  e4  Te4  4 4 4 Occurrence numbers Figure 5  An example for the single heap based method on a document    venkaee shga kamunshi     the performance of the single heap based method  Li et al   18  have studied e   cient heap merge algorithms to    nd similar entities for a single substring  In this paper  we propose e   ective algorithms to simultaneously    nd similar entities for multiple substrings  with large number of overlaps   which are orthogonal to the heap merge algorithms  4 1 Pruning Techniques In this section  we propose several pruning techniques by estimating the lower bounds of  e     s   Lazy Count Pruning  For each top entity on the heap  we will not count its occurrence numbers for every valid string immediately  Instead  we count the numbers in a lazy manner  We    rst count its occurrence number in the heap  and if the number is small enough  we can prune the entity  Formally  given an entity e  we use a sorted position list Pe    p1             pm  to keep its occurrences in the heap  in ascending order   Each element in Pe is the position of the corresponding token whose inverted list contains entity e  We can easily get the position list using the heap structure  Then we count e   s occurrence number in the heap  i e   the number of elements in Pe   Pe    If the number is smaller than a threshold  denoted as Tl  we prune the entity  otherwise we count its occurrence number in its valid substrings with token numbers between    e and  e  Section 3 3   For example  in Figure 5  Pe1    4  9  14  19  20  and  Pe1     5  Next we discuss how to compute the threshold Tl  Recall the threshold T for the overlap similarity  Section 2 2   T depends on both  e  and  s   To derive a lower bound of T which only depends on  e   we use    e to replace  s  and the new bound Tl is computed as below      Jaccard Similarity  Tl     e               Cosine Similarity  Tl     e         2        Dice Similarity  Tl     e         2             Edit Distance  Tl    e             q      Edit Similarity  Tl     e          e    q     1       1              q      Obviously Tl     T  If  Pe    Tl     T  e cannot be similar to any substring  and thus we can prune e  Section 2 2   For instance  in Figure 5  consider e1  Suppose      1   e1    9  Tl    e1             q   9     2   7  As  Pe1     5   Tl  e1 can be pruned  The correctness is stated in Lemma 3  Lemma 3  Given an entity e on the single heap  if its occurrence number in the heap   Pe   is smaller than Tl  e will not be similar to any valid substring  533Bucket Count Pruning  Consider an entity e  If a valid substring s is similar to e  s must have at most  e tokens and shares at least Tl tokens with e  In other words  if s is similar to e  they must have no larger than  e     Tl mismatched tokens  We can use this property for e   ective pruning  Formally  given two neighbor elements in Pe  pi and pi 1  any substring containing both the two tokens  token pi  and token pi 1   has at least pi 1     pi     1 mismatched tokens  If pi 1    pi    1    e    Tl  any substrings containing both the two tokens cannot be similar to e  Thus we do not need to count e   s occurrence numbers for any substrings  To use this feature  we partition the elements in Pe into di   erent buckets  If the number of elements in a bucket is smaller than Tl  we can prune all the elements in the bucket  lazy count pruning   otherwise we use the elements in the bucket to count e   s occurrence number for valid substrings with token numbers between    e and  e  Section 3 3   Next we introduce how to do the partition  Initially  we create a bucket b1 and put the    rst element p1 into the bucket  Then we consider the next element p2  If p2     p1     1    e     Tl  we create a new bucket b2 and put p2 into bucket b2  otherwise  we put p2 into bucket b1  Iteratively we can partition all elements into di   erent buckets  We give a tighter bound for di   erent similarity functions  For example  consider edit distance  We can use pi 1     pi     1          q for pruning  since there exists at least        q   1 mismatched tokens between pi and pi 1  which need at least     1 single character edit operations to destroy these       q 1 mismatched tokens  Similarly for edit similarity  we can use pi 1     pi     1       e  q   1          1             q  for pruning  For example  in Figure 5  suppose we use edit distance and      1   Consider Pe4    1  2  3  4  9  14  19   Tl    e4           q   8     1     2   6  Obviously  e4 can pass the lazy count pruning as  Pe4       Tl  Next we check whether it can pass the bucketcount pruning  We    rst partition Pe4 into di   erent buckets  Initially  we create a bucket b1 and put p1 into the bucket  Next for p2   2   as p2     p1     1            q   2  we put p2 into bucket b1  Similarly p3   3  and p4   4 are added into b1  For p5   9   as p5     p4     1          q  we create a new bucket b2 and add p5 into bucket b2  We repeat theses steps and    nally get b1    1  2  3  4   b2    9   b3    14   and b4    19   As the size of each bucket is smaller than Tl  we can directly prune the elements in each bucket  Thus  we do not need to count the occurrence number of e4 in any valid substrings  Moreover  we can generalize this idea  Given two elements pi and pj  i   j   if pj    pi     j   i     e    Tl  any substrings containing both the two tokens  token pi  and token pj    cannot be similar to entity e  Next we will introduce how to use this property to do further pruning  Batch Count Pruning  We have an observation that we do not need to enumerate each element in the position list Pe to count the occurrence numbers for every valid substring  Instead  we check sublists of Pe and test whether the sublists can produce candidate pairs  If so  we    nd candidate pairs in the sublists  otherwise we prune the sublists  Formally  if a valid substring s is similar to entity e  they must share enough common tokens   e     s      Tl   In other words  we only need to check the sublist with no smaller than Tl elements  Consider a sublist Pe i         j  with  Pe i         j   j    i   1   Tl  Let D pi          pj   denote the substring exactly containing tokens token pi   token pi 1              token pj    Figure 6   If  D pi          pj    pj   pi 1  e  any valid substring containe                         i j  m p1 p2 pi pj pm Pe i    j if Tl Pe i   j Te Pe i     j is a  if Tl  Pe i     j Te and   e D pi     pj Te         pi pj lo max pj  Te  pi 1                                 up min pi Te  pj 1 Candidates of e are in D lo     up D pi pj Pe i  j Figure 6  Candidate window and Valid window  ing all tokens in D pi          pj   has larger than  e tokens  Thus we can prune Pe i         j   On the contrary  D pi          pj   may be similar to e if    e      D pi          pj         e  This pruning technique is much power than the mismatch based pruning  since if pj    pi    j   i     e   Tl  then pj    pi 1    e  on the contrary if pj    pi  1    e  pj    pi     j   i     e    Tl may not hold  In addition  as  Pe i         j        D pi          pj      Pe i         j   should be not larger than  e  thus Tl      Pe i         j        e  Based on this observation  we can    rst generate sublists of Pe with sizes  number of elements  between Tl and  e  i e   Tl      Pe i         j        e  Then for each such list Pe i         j   if  D pi          pj       e  we prune the sublist  otherwise if    e      D pi          pj         e  we    nd candidates of entity e  a substring s is a candidate of e if  e     s      T and    e      s       e   For each candidate s of entity e   s  e  is a candidate pair  Next we discuss how to    nd candidates of e based on Pe  For ease of presentation  we    rst introduce two concepts  Definition 3  valid window and candidate window   Consider an entity e and its position list Pe    p1          pm   A sublist Pe i         j  is called a window of Pe for 1     i     j     m  Pe i         j  is called a valid window  if Tl      Pe i         j        e  Pe i         j  is called a candidate window  if Pe i         j  is a valid window and    e      D pi          pj         e  The valid window restricts the length of a window  The candidate window restricts the number of tokens of a valid substring  If a valid substring is a candidate of entity e  it must contain a candidate window  Figure 6   For example  consider the document and entities in Table 1  Pe4    10  17  33  34  43  58  59  60  61  66  71  76  81  86   Suppose we use edit distance and      2   e4    8  Tl    e4             q   4     e4    e4           6 and  e4    e4         10  Pe4  1          4     10  17  33  34   Pe4  1          5     10  17  33  34  43   and Pe4  6          9     58  59  60  61  are three valid windows  Pe4  1          4  is not a candidate window as p4   p1 1 34   10  1    e4   The reason is that D p1          p4  contains more than  e4 tokens and any substring containing Pe4  1          4  must have more than  e4 tokens  Although p9     p6   1      e4   Pe4  6          9  is not a candidate window as p9     p6   1      e4   Notice that we can optimize the pruning condition for jaccard similarity  cosine similarity  and dice similarity  since they depend on  e     s   Given a valid window Pe i         j   let s   D pi          pj     Pe i         j        e     s  5   Take jaccard similarity as an example  If s and e are similar   e   s   e   s           D pi          pj       s       e     s       e   s         min  e   Pe i      j         Thus we give a tighter bound of  D pi          pj     For jaccard similarity     e      D pi          pj        min  e   Pe i      j         for dice similarity  5 As D pi          pj   may contain duplicate tokens   Pe i          j        e     s  and  Pe i          j   may also be larger than  e   534   e      D pi          pj        min  e    Pe i  j        2           for cosine similarity     e      D pi          pj        min  e   Pe i      j      2   Now we introduce how to    nd candidates of e from candidate windows Pe i         j   The substrings that contain all tokens in D pi          pj   may be candidates of e  We can    nd these substrings as follows  As these substrings must contain token pi   the    maximum start position    of such substrings is pi and the    maximum end position    is up pi   e    1  Similarly  as these substrings must contain token pj    the    minimum start position    is lo   pj      e   1 and the    minimum end position    is pj   Thus we only need to    nd candidates among substrings D pstart          pend  where lo     pstart     pi  pj     pend     up  Substring s   D pstart          pend  is a candidate of e if    e      s    pend    pstart   1      e and  e   s      T   Here we use threshold T as s   D pstart          pend  is known   However this method may generate duplicate candidates  For example  suppose pj      e   1   pi   1   1  D pi   1   e    D pi   1          pi   1    e     1   could be a candidate generated from Pe i         j   In this case  as    e     pj    pi   1     pj    pi   1   1      e and Tl      Pe i         j        Pe i     1          j     pj     pi   1   1      e  Pe  i     1          j  is also a candidate window  Thus Pe  i   1          j  also generates the candidate D pi   1   e   For Pe i         j   to avoid generating duplicates with Pe i   1         j  and Pe i         j 1   we will not extend Pe i         j  to positions smaller than pi   1 1 and larger than pj 1     1  and set lo   max pj      e   1  pi   1 1   up   min pi   e    1  pj 1    1   In this way  our method will not generate duplicate candidates  In summary  to    nd all candidates for an entity  we    rst get the entity   s position list  and then generate the valid windows and candidate windows  Next we identify its candidates from candidate windows  Finally the pair of each candidate and the entity is a candidate pair  The correctness and completeness of our method is formalized as below  Theorem 1  Correctness and Complexness   Our method    nds the candidate pairs completely and correctly  4 2 Finding Candidate Windows Ef   ciently Given an entity e  as there are larger numbers of valid windows     e l Tl  Pe      l   1   it could be expensive to enumerate the valid windows for    nding all candidate windows  To improve the performance  this section proposes e   cient methods to    nd candidate windows  Span and Shift based method  For ease of presentation  we    rst introduce a concept    possible candidate windows     A valid window Pe i         j  is called a possible candidate window if pj    pi   1      e  Based on this concept  we introduce two operations  span and shift  Given a current valid window Pe i         j   we use the two operations to generate new valid windows as follows  Figure 7       span  If pj     pi   1      e  for k     j  Pe i         k  may be a possible candidate window  We span it to generate all possible candidate windows starting with i  Pe i         j 1             Pe i         x   where x satis   es px     pi   1      e and px 1   pi 1  e  For j     k     x  if pk     pi   1      e  Pe i         k  is a candidate window  On the contrary  if pj     pi   1    e  for k     j  as pk    pi   1   pj    pi   1  e  Pe i         k  cannot be a candidate window  Thus we do not need to span Pe i         j       shift  We shift to a new valid window Pe  i 1          j 1    We use the two operations to    nd candidate windows as follows  Initially  we get the    rst valid window Pe 1          Tl   We do span and shift operations on Pe 1          Tl   For the new valid windows generated from the span operation  we check whether they are candidate windows  for the new valid window generated from the shift operation  we do span and shift operations on it  Iteratively we can    nd all candidate windows from Pe 1          Tl   We give an example to show how our method works  For e4    venkatesh       Pe4    10  17  33  34  43  58  59  60  61  66  71  76  81  86   Suppose    2   e4    8  Tl    e4             q   4     e4    e4           6   e4    e4         10  The    rst valid window is Pe4  1          4     10  17  33  34   As p4     p1   1     34     10   1    e4   we do not need to do span operation  We do a shift operation and get the next window Pe4  2          5   As p5     p2   1     43     17   1    e4   we do another shift operation  When we shift to valid window Pe4  6          9   as p9   p6 1 61   58 1    e4   Pe4  6          9  is not a candidate window  We do a span operation  As p10   p6 1 9    e4 and p11   p6 1 14  e4   x   10  We get a valid window Pe4  6          10   which is a candidate window  Next we shift to Pe4  7          10   Iteratively we    nd all candidate windows  Pe4  6          10  and Pe4  7          10   Figure 8   Given a valid window Pe i         j   if pj     pi   1    e  the shift operations can prune the valid windows starting with i  e g   Pe i         k  for j   k     i    e     1  However this method still scans large numbers of candidate windows  To further improve performance  we propose a binary searchbased method which can skip many more valid windows  Binary Span and Shift based method  The basic idea is as follows  Given a valid window Pe i         j   if pj    pi   1    e  we will not shift to Pe  i   1          j   1    Instead we want to directly shift to the    rst possible candidate window after i  denoted by Pe mid          mid   j     i    where mid satis   es pmid j   i     pmid   1      e and for any i     mid     mid  pmid  j   i     pmid    1    e  Similarly  if pj     pi   1      e  we will not iteratively span it to Pe i         j   1    Pe i         j   2            Pe i         x   Instead  we want to directly span to the last possible candidate window starting with i  denoted by Pe i         x   where x satis   es px     pi   1      e and for any x     x  px      pi   1    e  If the function F x    px    pi   1  for span and F    mid    pmid j   i     pmid   1  for shift are monotonic  we can use a binary search method to    nd x and mid e   ciently  For the span operation  obviously F x    px     pi   1  is monotonic as F x   1      F x    px 1     px   0  Next we give the lower bound and upper bound of the search range  Obviously x     j  In addition  as pi   j     pi j   we have px     pi    e     1     pi  e   1 and x     i    e     1  Thus we    nd x by doing a binary search between j and i    e     1  However F    mid    pmid j   i     pmid   1 is not monotonic  We have an observation that F     mid      pj    mid    i       pmid   1 is monotonic  since F     mid   1    F     mid    pmid     pmid   1     1     0  More importantly for i     mid     j  F     mid    F    mid  as   pj    mid     i        pmid j   i  Thus if F     mid     1     e  F    mid     1     e and Pe  mid     1          mid     1   j     i   cannot be a candidate window  If F     mid       e  Pe  mid          mid   j     i   could be a candidate window  In this way  we can    nd mid by doing a binary search between i and j such that F     mid     1     e and F     mid       e  If F    mid       e  we have found the last possible candidate window  otherwise  we continue to    nd mid   between mid   1  and mid   1   j     i  Iteratively  we can    nd the last possible candidate window  535i i  j j  m pi Shift Shift Pe i   j Pe i     j                 pj                 Span only if pj  pi  Te Span Pe i   j Pe i    j     Pe i   x         pi         pj                 Pe i   x  is the last possible candidate window starting with i Current valid window New valid window Current valid window New valid windows i i  j j  x m px pi 1  e and  px 1 pi 1  e Figure 7  span and shift operations  x 10 as px pi 1  Te and px 1 pi 1 Te  1  shift  2  span  3  shift   5  shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 10 17 33 34 43 58 59 60 61 66 71 76 10 17 33 34 43 58 59 60 61 66 71 76 10 17 33 34 43 58 59 60 61 66 71 76 Pe Pe Pe Pe 1 1 2 1 9876543 10 1 2 1 1 2 1 9876543 10 1 2 1 1 2 1 9876543 10 1 2 81 86 13 14 81 86 13 14 81 86 13 14 81 86 13 14  4  span Pe 10 17 33 34 43 58 59 60 61 66 71 76 1 1 2 1 9876543 10 1 2 81 86 14 13 i New valid windows New valid windows Current valid window Current valid window j i j Current valid window Current valid window Current valid window  e4   q Tl  e4  Te4  Pe 6 10  is a candidate window but Pe 6 9  is not as p9 p6 1  e4 Pe 7 10  is a candidate window Figure 8  An example for span and shift operations  Thus given a valid window Pe i         j   we use binary span and shift operations to    nd candidate windows  Figure 9       Binary span  If pj     pi   1      e  we    rst    nd x by doing a binary search between j and i    e     1  where x satis   es px     pi   1      e and px 1     pi   1    e  and then directly span to Pe i         x       Binary shift  If pj     pi   1    e  we    nd mid by doing a binary search between i and j where mid satis   es   pj    mid     i        pmid   1      e and   pj    mid     1     i        pmid   1   1    e  If pmid j   i     pmid   1    e  we iteratively do the binary shift operation between mid   1  and mid   1   j     i  On the contrary  if pj     pi   1      e  we shift to a new valid window Pe  i 1          j 1    Given a valid window Pe i         j   the binary shift can skip unnecessary valid windows  non candidate windows   such as Pe  i  1          j  1           Pe  mid   1          mid   1  j   i    as proved in Lemma 4  For example  consider the position list in Figure 10  Suppose      2  Tl   4   e4    8   e4   10  Consider the    rst valid window Pe4  1          4   The shift operation shifts it to Pe4  2          5   Pe4  3          6              Pe4  6          9   and checks whether they are candidate windows  The binary shift operation can directly shift it to Pe4  3          6  and then to Pe4  6          9   Thus it skips many valid windows  Lemma 4  Given a valid window Pe i         j  with pj    pi   1    e  if   pj  mid     i       pmid   1      e and   pj    mid     1    i      pmid   1  1    e  Pe  i 1          j 1           Pe  mid    1             mid     1    j     i     are not candidate windows  pi Binary Shift                 pj         Binary Span  if pj  pi 1 Te            pi         pj         Span to the last possible candidate window starting with i                 mid mid j i Find mid by doing a binary search between i and j such that  pj  mid i   pmid 1  e and    pj  mid 1 i   pmid 1 1 e Find x by doing a binary search between j and i  e 1 such that px  pi 1  e and  px 1  pi 1  e Current valid window the last possible candidate window the first possible candidate window pi  2   if pj  pi 1  e  Shift to the next possible candidate window                 pj                 Current valid window New valid window  1  if pj  pi 1 Te  Shift to the first possible candidate window after pi Current valid window i i  j j  m i i  j j  m i i  j j  x  m Figure 9  span and shift in a binary search way  skip lower j x  10 as  px  pi 1 e and  px 1 pi 1 e upper i Te 1 Pe 6   10  is a candidate window but Pe 6   9  is not as p9 p6 1    e  1  binary shift  2  binary span  5  binary shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 14 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 12 13 81 86 13 14 81 86 13 14  e4   q Tl  e4  Te4  Pe Pe Pe lower j Pe 7   10  is a candidate window  4  binary span  10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 13 14 Pe upper i Te 1  3  binary shift 10 17 33 34 43 58 59 60 61 66 71 76 1 11 2 9876543 10 12 81 86 13 14 Pe pj  pi 1  e Current valid window Current valid window stop pj  pi 1  e Current valid window skip i i pj  pi 1  e pj  pi 1  e pj  pi 1  e Current valid window Current valid window Figure 10  An example for binary span and shift  The binary span operation can directly span to Pe i         x  and has two advantages  Firstly  in many applications  users want to identify the best similar pairs  sharing common tokens as many as possible   and the binary span can e     ciently    nd such substrings  Secondly  we do not need to    nd candidates of e for Pe i         j   1            Pe i         x  one by one  Instead since there may be many candidates between lo   pj      e   1  and up   pi x   j    e     1  we    nd them in a batch manner  We group the candidates based on their token numbers  Entities in the same group have the same number of tokens  Consider the group with g tokens  suppose Tg is the threshold computed using  e  and g  If  Pe i         x     Tg  we prune all candidates in the group  We can use the two binary operations to replace the shift and span operations in order to skip valid windows  We give an algorithm to    nd candidate windows using the two operations as illustrated in Figure 11  It    rst initializes the    rst valid window  line 2 line 4   Then it uses the two binary operations to extend the valid window until reaching the last valid window  line 3   If its token number is no larger than  e  we do a binary span operation by calling its subroutine BinarySpan  line 6  and do a shift operation  line 7   otherwise calling its subroutine BinaryShift  line 8   BinarySpan does a binary search to    nd the last possible candidate window starting with pi  lines 3 6   Then it retrieves the 536Algorithm 1  Find Candidate Windows Input  e  An entity  Pe  Position list of e on the heap  Tl   Threshold   e  The upper bound of token numbers  1 begin 2 i   1  3 while i      Pe      Tl   1 do 4 j   i   Tl     1  5 if pj     pi   1      e then 6 BinarySpan i  j   7 i   i   1     shift to the next window    8 else i   BinaryShift i  j   9 end Procedure BinarySpan i  j  Input  i  the start point  j  the end point  1 begin 2 lower   j  upper   i    e     1   3 while lower     upper do 4 mid     upper   lower  2   5 if pmid     pi   1    e then upper   mid     1  6 else lower   mid   1  7 mid   upper   8 Find candidate windows in D i        mid   9 end Procedure BinaryShift i  j  Input  i  the start point  j  the end point Output  i  the new start point  1 begin 2 lower   i  upper   j  3 while lower     upper do 4 mid     lower   upper  2   if   pj    mid     i    5     pmid   1    e then 6 lower   mid   1  7 else upper   mid     1  8 i   lower  j   i   Tl     1  9 if pj     pi   1    e then i   BinaryShift  i  j   10 else return i  11 end Figure 11  Algorithm  Find candidate windows candidate windows  line 8   BinaryShift does a binary search to    nd the    rst possible candidate window after pi  Iteratively our method    nds all candidate windows  Figure 10 illustrates an example to walk through our algorithm  5  THE Faerie ALGORITHM In this section  we propose a single heap based algorithm  called Faerie  to e   ciently    nd all answers  We    rst construct an inverted index for all entities in the given dictionary E  Then for the document D  we get its tokens and corresponding inverted lists  Next we construct a single heap on top of inverted lists of tokens  We use the heap to generate entities in ascending order  For each entity e  we get its position list Pe  If  Pe    Tl  we prune e based on lazy count pruning  otherwise we use the two binary operations to    nd candidate windows  Then based on the candidate windows  we generate candidate pairs  Finally  we verify the candidate pairs and get the    nal results  Figure 12 gives the pseudo code of the Faerie algorithm  The Faerie algorithm    rst constructs an inverted index for prede   ned entities  line 2   and then tokenizes the document  gets inverted lists  line 3   and constructs a heap  line 4   Faerie uses  ei  pi  to denote the top element of the heap  where ei is the current minimal entity and pi is the position of the inverted list from which ei is selected  Faerie constructs a position list Pe to keep all the positions of inverted lists in the heap that contain e  line 6   Then for each top element  ei  pi  on the heap  if ei   e  we add pi Algorithm 2  Faerie Algorithm Input  A dictionary of entities E    e1  e2          en   A document D  A similarity function and a threshold  Output    s  e   s and e are similar for the function and threshold   where s is a substring of D and e     E  1 begin 2 Tokenize entities in E and construct an inverted index  3 Tokenize D and get inverted lists of tokens in D  4 Construct a heap H on top of inverted lists of D  5 e is the top element of H     keep the current entity   6 Initialize a position list Pe       while    ei  pi    H top   7        do 8 if ei    e then 9 Pe       pi      ei is the new top entity     10 else Derive the threshold Tl 11 for entity e  12 if  Pe      Tl then 13 Find candidate windows using Algorithm 1  14 Get candidates using candidate windows  15 e   ei  Pe    pi      update the current entity 16 Adjust the heap  17 Verify candidate pairs  18 end Figure 12  The Faerie Algorithm  into Pe  line 8 line 9   where e is the last popped entity from the heap  otherwise Faerie checks the position list as follows  Faerie derives a threshold Tl for entity e based on the similarity function and threshold  If  Pe      Tl  there may exist candidate pairs  Faerie generates candidate windows based on Algorithm 1  line 13   and    nds candidate pairs based on candidate windows  line 14   Faerie adjusts the heap to generate candidates for the next entity  line 16   Finally Faerie veri   es the candidates to get    nal results  line 17   Next we give a running example to walk through our algorithm  Consider the entities and document in Table 1  We    st construct a single min heap  Figure 5   Then we adjust the heap to generate the position list for each entity  Consider the position list for entity e4     venkatesh     in Fig   ure 10  Suppose      2   e4    8     E   6   E   12     e4   6   e4   10  Tl   4  We use the binary shift and span operations to get candidate windows  Pe 6          10  and Pe 7          10    and then generate candidate pairs based on the candidate windows  e g    D 58  9     venkaee sh     e4    venkatesh       Finally we verify the candidates to get the    nal answers  6  EXPERIMENTS We have implemented our proposed techniques  The objective of the experiments is to measure the performance  and in this section we report experimental results  Experimental Setting  We compared our algorithms with state of the art methods NGPP  22   the best for edit distance  and ISH  4   the best for jaccard similarity and edit similarity   We downloaded the binary codes of NGPP  22  from    Similarity Joins    project website 6 and implemented ISH by ourselves  We reported the best performance of the two existing methods  The algorithms were implemented in C   and compiled using GCC 4 2 4 with  O3    ag  All the experiments were run on a Ubuntu machine with an Intel Core 2 Quad X5450 3 0GHz processor and 4 GB memory  Datasets  We used three real datasets  DBLP 7   PubMed 8   and ACM WebPage 9   DBLP is a computer science publi  6 http   www cse unsw edu au    weiw project simjoin html 7 http   www informatik uni trier de    ley db 8 http   www ncbi nlm nih gov pubmed 9 http   portal acm org 537cation dataset  We selected 100 000 author names as entities and 10 000 papers as documents  PubMed is a medicalpublication dataset  We selected 100 000 paper titles as entities and 10 000 publication records as documents  WebPage is a set of web pages  We crawled 100 000 titles as entities and 1 000 web pages as documents  thousands of tokens   Table 4 gives the dataset statistics  len denotes the average length and token denotes the average token number   We did not consider di   erent attributes in the entities and documents  Each entity in the dictionary is just a string  Table 4  Datasets  Datasets Cardinality len tokens Details DBLP Dict 100 000 21 1 2 77 Author DBLP Docs 10 000 123 3 16 99 Papers PubMed Dict 100 000 52 96 6 98 Title PubMed Docs 10 000 235 8 33 6 Papers WebPage Dict 100 000 66 89 8 5 Title WebPage Docs 1 000 8949 1268 Web Pages 6 1 Multi Heap vs Single Heap In this section  we compared the multi heap based method with the single heap based method  without using pruning techniques in Section 4   We tested the performance of the two methods for di   erent similarity functions on the three datasets  Figure 13 shows the experimental results  We see that the single heap based method outperforms the multi heap based method by 1 2 orders of magnitude  and even 3 orders of magnitude in some cases  For example  on DBLP dataset with edit distance threshold      3  the multi heap based method took more than 10 000 seconds and the single heap based method took about 180 seconds  On PubMed dataset with eds similarity threshold      0 9  the multi heap based method took more than 14 000 seconds and the single heap based method took only 600 seconds  There are two reasons that the single heap based method is better than the multi heap based method  Firstly  the multi heap based method scans each inverted list of the document many times and the single heap based method only scans them once  Secondly the multi heap based method constructs larger numbers of heaps and does larger numbers of heap adjustment than the single heap based method  As the single heap based method outperforms the multi heapbased method  we focus on the single heap based method in the remainder of the experimental comparison  6 2 Effectiveness of Pruning Techniques In this section  we tested the e   ectiveness of our pruning techniques  We    rst evaluated the number of candidates by applying di   erent pruning techniques to our algorithm  lazycount pruning  bucket count pruning  and binary span and shift pruning  As batch count pruning is a special case of binary span and shift pruning  we only show the results for binary span and shift pruning    In the paper  the number of candidates refers to the number of non zero values in the occurrence arrays  which need to be veri   ed  Figure 14 gives the results  In the    gure  we tested edit distance on DBLP dataset  jaccard similarity on WebPage dataset  and edit similarity on PubMed dataset  Note that in the    gures  the results are in 10 x formate  For example if there are 100 million candidates  the number in the    gure is 8  10 8   100M   In the paper  our method used parameters of q   16  8  5  4  3 for      0  1  2  3  4 respectively for edit distance on DBLP and q   26  11  7  5  4 for      1  0 95  0 9  0 85  0 8 edit similarity on PubMed  We observe that our proposed pruning techniques can prune large numbers of candidates  For example  on the DBLP dataset  for      3  the method without any pruning techniques involved 11 billion candidates  and the lazy count pruning reduced the number to 860 million  The bucketcount pruning further reduced the number to 600 million  The binary span and shift pruning had only 200 million candidates  On the WebPage dataset  for      0 9  the binary span and shift pruning reduced the number of candidates from 10 billion to 35  On the PubMed dataset  for      0 85  the binary span and shift pruning reduced the number of candidates from 180 billion to 120 million  The main reason is that we compute an upper bound of the overlap of an entity and a substring  and if the bound is smaller than the overlap threshold  we prune the substring  If for any substring  the bound of an entity is smaller than the threshold  we prune the entity  This con   rms the superiority of our pruning techniques  Then we evaluated the performance bene   t of the pruning techniques  Figure 15 shows the results  We observe that the pruning techniques can improve the performance  For instance  on the DBLP dataset  for      3   the  elapsed time of the method without any pruning technique was 180 seconds  and the lazy count pruning decreased the time to 43 seconds  The binary span and shift pruning reduced the time to 25 second  On PubMed dataset  for      0 9  the pruning techniques can improve the time from 600 seconds to 8 seconds  This shows that our pruning techniques can improve the performance  In the remainder of this paper  we compared the single heap based method using the binary span and shift pruning with existing methods  6 3 Comparison with State of the art Methods In this section  we compared our algorithm Faerie with state of the art methods NGPP  22   which only supports edit distance  and ISH  4   which supports edit similarity and jaccard similarity   We tuned the parameters of NGPP and ISH  e g   pre   x length of NGPP  to make them achieve the best performance  Figure 16 shows the results  We see that Faerie achieved the highest performance  Especially Faerie outperformed ISH by 1 2 orders of magnitude for edit similarity and jaccard similarity  For example  on the PubMed with edit similarity threshold      0 9  the elapsed time of ISH was 1000 seconds  Faerie reduced the time to 8 seconds  This is because Faerie used the shared computation across overlapped tokens  In addition  our pruning techniques can prune large numbers of unnecessary valid substrings and reduce the number of candidates  Although NGPP achieved high performance for smaller edit distance thresholds  it is ine   cient for larger edit distance thresholds  The reason is that it needs to enumerate neighbors of entities and an entity has larger numbers of neighbors for larger thresholds  On jaccard similarity  as each entity has a smaller number of tokens  the average number is 8  and the thresholds Tl and  e for di   erent thresholds are nearly the same  Tl   8 for    1 and Tl   10 for    0 8   Faerie varied a little for di   erent jaccard similarity thresholds  In addition  we compared index sizes of di   erence algorithms  Note that NGPP had di   erent index sizes for different edit distance threshold      as NGPP uses    to generate neighborhoods  The larger the edit distance threshold  the larger indexes are involved for the neighborhoods of an entity  since an entity has larger numbers of neighbors for larger thresholds  On DBLP dataset  for      3  NGPP con  538 0 1  1  10  100  1000  10000  0 1 2 3 Search Time  s  Threshold    Multi Heap Single Heap  a  ed  DBLP   100  1000  10000 1 0 95 0 9 0 85 Search Time  s  Threshold    Multi Heap Single Heap  b  jac  WebPage   0 1  1  10  100  1000  10000  100000 1 0 95 0 9 0 85 Search Time  s  Threshold    Multi Heap Single Heap  c  eds  PubMed  Figure 13  Performance comparison of multi heap based methods and single heap based methods   0  2  4  6  8  10  12  14  0 1 2 3   of Candidates  10 x   Threshold    None Lazy Bucket Binary  a  ed  DBLP   0  2  4  6  8  10  12  14  16 1 0 95 0 9 0 85   of Candidates  10 x   Threshold    None Lazy Bucket Binary  b  jac  WebPage   0  2  4  6  8  10  12  14  16 1 0 95 0 9 0 85   of Candidates  10 x   Threshold    None Lazy Bucket Binary  c  eds  PubMed  Figure 14  Number of candidates with di   erent pruning techniques   0 1  1  10  1</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep3 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#siep3">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#information_extraction"/>
        <doc>Attribute Domain Discovery for Hidden Web Databases ### Xin Jin George Washington University Washington  DC 20052  USA xjin gwu edu Nan Zhang   George Washington University Washington  DC 20052  USA nzhang10 gwu edu Gautam Das y University of Texas at Arlington Arlington  TX 76019  USA gdas uta edu ABSTRACT Many web databases are hidden behind restrictive form like interfaces which may or may not provide domain information for an attribute  When attribute domains are not available  domain discovery becomes a critical challenge facing the application of a broad range of existing techniques on third party analytical and mash up applications over hidden databases  In this paper  we consider the problem of domain discovery over a hidden database through its web interface  We prove that for any database schema  an achievability guarantee on domain discovery can be made based solely upon the interface design  We also develop novel techniques which provide effective guarantees on the comprehensiveness of domain discovery  We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach  Categories and Subject Descriptors H 2 7  Database Administration   H 3 5  Online Information Services   Web based services General Terms Algorithms  Measurement  Performance Keywords Hidden Web Database  Domain Discovery ### 1  INTRODUCTION The Attribute Domain Discovery Problem  In this paper  we develop novel techniques to discover the attribute domains  i e   the set of possible values for each attribute  from hidden web databases  by external users  Hidden databases  as a large portion of the deep   Partially supported by NSF grants 0852673  0852674  0915834 and a GWU Research Enhancement Fund  y Partially supported by NSF grants 0812601  0915834  1018865  a NHARP grant from the Texas Higher Education Coordinating Board  and grants from Microsoft Research and Nokia Research  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  SIGMOD   11  June 12   16  2011  Athens  Greece  Copyright 2011 ACM 978 1 4503 0661 4 11 06     10 00  web  are hidden behind restrictive form like interfaces which allow a user to form a search query by specifying the desired values for one or a few attributes  and the system responds by returning a small number of tuples satisfying the user speci   ed search condition  A typical example of a hidden database is the award search database of the US National Science Foundation  NSF  1   which allows users to search for NSF award projects featuring userspeci   ed values on up to 20 attributes  Each attribute speci   able by users through the form like interface is represented by an input control on the interface  For certain types of controls  e g   drop down menus and radio buttons  the attribute domain can be readily retrieved by external users from source code of the interface  e g   an HTML    le   In the NSF example  attributes such as NSF organization and Award Instrument belong to this category  For other controls  especially text boxes  without features such as autocompletion   no domain information is provided  Attributes such as Program Manager belong to this category  The focus of this paper is to develop domain discovery techniques  restricted to accessing the database only via the proprietary form interface  that external users can employ to unveil possible attribute values that appear in tuples of the hidden databases  An important design goal is to develop techniques that unveil all  or most  attribute values by issuing only a small number of queries  and to provide analytical guarantees on the coverage and query cost of our methods  We emphasize that in this paper  we do not consider approaches to the domain discovery problem that rely on external knowledge sources or domain experts to provide the required domain values  We argue that while the use of external knowledge sources may have applicability in very general scenarios  e g   the use of a USPS data source to list all values of an attribute such as zipcode   these approaches will not work in more focused applications such as the Program Manager attribute on the NSF award search database  Our emphasis is to develop automated domain discovery algorithms that can power a a variety of third party applications only using the public interfaces of these databases  and without requiring any further collaborations or agreements with the database owners  Applications  Domain discovery from hidden databases belongs to the area of information extraction and deep web analytics  see tutorials in  6  11  and survey in  18    To the best of our knowledge  the only prior work that tackles our speci   c problem of discovering attribute domains of hidden databases is  20   This problem has a broad range of applications  First and foremost  it serves as a critical prerequisite for data analytics over hidden databases  because all existing aggregate estimation  8  and sampling  7  9  10  techniques for hidden databases require prior knowledge of the attribute 1 http   www nsf gov awardsearch tab do dispatch 4domains  In addition  the attribute domains being discovered can be of direct interest to third party web mashup applications  For example  such an application may use the discovered attribute domains to add autocompletion feature to text boxes in the original formlike interface  so as to improve the usability of the interface  The discovered domains may also be used to identify mapping between attributes in different hidden databases to facilitate the creation of mashup  Challenges  The main challenges to effective attribute domain discovery are the restrictions on input and output interfaces of hidden databases  Normally  input interfaces are restricted to issuing only search queries with conjunctive selection conditions   which means that queries like SELECT UNIQUE Program Manager  FROM D cannot be directly issued  eliminating the chance of directly discovering the domain of Program Manager through the interface  The output interface is usually limited by a top k constraint   i e   if more than k tuples match the user speci   ed condition  then only k of them are preferentially selected by a scoring function and returned to the user  This restriction eliminates the possibility of trivially solving the problem by issuing the single query SELECT   FROM D and then discovering all attribute domains from the returned results  The existing technique for attribute domain discovery is based on crawling the database  20   A simple instantiation of crawling is to start with SELECT   FROM D  then use domain values returned in the query answer to construct future queries  and repeat this process until all queries that can be constructed have been issued  One can see that no algorithm can beat the comprehensiveness of attribute domain discovery achieved by crawling  But crawling requires an extremely large number of queries to complete  and since most hidden databases enforce a limit on the number of queries one can issue over a period of time  e g   1  000 per IP address per day   or even charge per query  often the crawling process has to be terminated prematurely  Due to this reason  such techniques cannot provide any guarantees on the comprehensiveness of discovered attribute domains  A seemingly promising way to address the query cost problem of crawling is to instead perform sampling   i e   one    rst draws representative samples from the hidden database  and then discover attribute domains from the sample tuples to achieve statistical guarantees on the comprehensiveness of domain discovery  Sampling of hidden databases is an intensively studied problem in recent years  7  9  10   Nonetheless  a key obstacle here is a    chickenand egg    problem   i e   all existing sampling algorithms for hidden databases require the attribute domains to be known in the    rst place  preventing them from being applied to hidden databases with unknown attribute domains  Outline of Technical Results  In this paper we initiate a study of query ef   cient attribute domain discovery over hidden databases  We consider two types of output interfaces commonly offered by hidden databases  COUNT and ALERT  For a user speci   ed query  a COUNT interface returns not only the top k tuples  but also the number of tuples matching the query in the database  which can be greater than k   ALERT interface  on the other hand  only alerts the user with an over   ow    ag when more than k tuples match the query  indicating that not all tuples which match the query can be returned  The NSF award database features an ALERT interface  For COUNT interface  we start by studying the feasibility of developing deterministic algorithms for attribute domain discovery  Our main results are two achievability conditions   lower bounds on k and query cost  respectively   which a deterministic algorithm must satisfy to guarantee the discovery of all attribute domains  Unfortunately  neither condition is achievable in practice for a generic hidden database  Nonetheless  a promising lead we    nd from the study is that ef   cient and complete attribute domain discovery is indeed possible for a special type of  perhaps unrealistic  databases in which each attributes has only two domain values  For this special case  we develop B COUNT DISCOVER  a deterministic algorithm based on the key idea of constructing a series of nested search spaces for unknown domain values  and prove that the algorithm guarantees the discovery of all attribute domains with only O m2   queries  where m is the number of attributes  as long as k   2  Our main observation from B COUNT DISCOVER is an understanding of why it cannot be extended to generic hidden databases containing attributes with arbitrary sized domains   we    nd the main reason to be the large number of queries required by a deterministic algorithm to decide how to construct the nested search space  To address this  we consider the design of randomized algorithms which allows query ef   cient randomization of the nestedspace construction process  To this end  we develop RANDOMCOUNT DISCOVER  a Monte Carlo algorithm which only requires a small amount of queries to discover a large number of domain values  For ALERT interface  we similarly prove the infeasibility of developing a query ef   cient deterministic algorithm which guarantees complete attribute domain discovery  Our main result here is then an extension of RANDOM COUNT DISCOVER to RANDOMALERT DISCOVER  a Monte Carlo algorithm which achieves similar performance to RANDOM COUNT DISCOVER by estimating COUNTs of queries based on historic query answers  and using estimated COUNTs to bootstrap RANDOM COUNT DISCOVER to discover domain values  In summary  the main contributions of this paper are as follows    We initiate the study of attribute domain discovery over a hidden database through its restrictive web interface    We derive the achievability conditions for complete attribute domain discovery on k and the query cost  These conditions indicate the infeasibility of designing a query ef   cient deterministic algorithm which guarantees the discovery of all attribute domains    We propose two randomized algorithms  RANDOM COUNTDISCOVER and RANDOM ALERT DISCOVER  for COUNT and ALERT interfaces  respectively  Both algorithms require only a small amount of queries to develop a large number of domain values    We provide a thorough theoretical analysis and experimental studies that demonstrate the effectiveness of our proposed algorithms over the real world NSF award search database and a local patient discharge dataset  Paper Organization  In Section 2 we introduce preliminaries and discuss simple but ineffective crawling algorithms for attribute domain discovery over hidden databases  Sections 3 and 4 are devoted to the development of deterministic and randomized algorithms for COUNT interfaces  respectively  In Section 5 we extend the results to ALERT interfaces  Section 6 contains a detailed experimental evaluation of our proposed approaches  Section 7 discusses related work  followed by conclusion in Section 8  2  PRELIMINARIES2 1 Model of Hidden Databases Consider a hidden database table D with n tuples  Let there be m attributes A1          Am which can be speci   ed through the input interface  and Vi be the domain of Ai  We assume each tuple to be unique  and each value in Vi  i 2  1  m   to occur in at least one tuple in the database  because otherwise Vi can never be completely discovered from the database  Consider a prototypical interface which allows user to query D by specifying values for a subset of attributes   i e   to issue queries q of the form  SELECT   FROM D WHERE Ai1   vi1         Ais   vis   where vij 2 Vij   Let Sel q  be the set of tuples in D which satisfy q  Since the interface is restricted to return up to k tuples  a overly broad query  i e   jSel q j   k  will over   ow and return only the top k tuples in Sel q  selected according to a scoring function  In addition  a COUNT interface will return jSel q j  the number of tuples satisfying q  while an ALERT interface will return an over   ow    ag indicating that not all tuples which satisfy q can be returned  When a query does not over   ow  COUNT and ALERT interfaces return the exact same information to the user  In particular  if the query is too speci   c to return any tuple  we say that an under   ow occurs  If there is neither over   ow nor under   ow  i e   jSel q j 2  1  k    then Sel q  will be returned in its entirety and we have a valid query result  For each tuple returned by an over     owing or valid query  the values of all its attributes are displayed  enabling the discovery of domain values of an attribute with unknown domain  Running Example  We consider a running example of the above mentioned NSF award search database with k   50  There are 9 attributes  award amount  instrument  PI state     eld  program manager  NSF organization  PI organization  City  and ZIP code  with domain sizes 5  8  58  49  654  58  3110  1093  1995  respectively  these are the estimates we generated by running our domain discovery algorithms to be presented in the paper for an extended period of time  We do not have the ground truth for all attributes  But they can be safely considered as lower bounds on domain sizes   2 2 Problem De   nition We consider the problem of attribute domain discovery   i e   the discovery of V1          Vm through the restrictive interface  Since many hidden databases impose limits on the number of queries one can issue through their interfaces over a period of time  the performance of a domain discovery algorithm should be measured by not only the comprehensiveness of discovered domain values  but also the query cost  i e   the number of queries issued through the interface of hidden database for domain discovery  While the metric for query cost is straightforward  to measure the comprehensiveness of discovered domain values we consider the following two metrics    Coverage  i e   the total number of domain values discovered for one or a set of attributes    Recall  i e   the number of tuples in the database for which all attribute values  of A1          Am  have been discovered  Note that while the two metrics are positively correlated  e g   a recall of n indicates a coverage of jVij for each Ai   they may be preferred in different applications  For example  if the objective of domain discovery is to unveil the metadata  e g   to discover the list of program managers from the NSF award database  then coverage is a more important metric  If the objective is to enable data analytics over the hidden database  then recall is more important as it guarantees how many tuples will be covered by the subsequent aggregate estimation and sampling algorithms  Given the measures for comprehensiveness and query cost  we de   ne the problem of attribute domain discovery as follows  Problem Statement  How to maximize the coverage and recall of attribute domain discovery while minimizing the query cost through COUNT and ALERT interfaces  respectively  2 3 Crawling Based Algorithms In this subsection  we consider depth    rst search  DFS  and breadth     rst search  BFS   two simple crawling based algorithm for attribute domain discovery  and point out their problems in terms of the tradeoff between coverage recall and query cost  Both algorithms start with SELECT   FROM D to learn the    rst few domain values  but then take different methods to construct the subsequent queries  DFS  With DFS  one constructs the next query by    nding an attribute that is not speci   ed in the previous one  and then adding to the previous query a conjunctive condition de   ned by the attribute and one of its already discovered domain values  Such a process continues  with DFS learning all domain values from tuples returned by each issued query  until either the query returns under   ows valid or no discovered domain value is available for constructing the next query  at which time the algorithm backtracks by removing the last added condition and adding a new one based on another discovered domain value  We refer to the algorithm as DFS because it keeps increasing the number of predicates included in the query if possible  A main problem of DFS  however  is poor coverage and recall when a large number of attributes are strongly correlated with the    rst few attributes appended to SELECT   FROM D  For example  consider the case where A1   v1 is    rst appended to the SELECT   query  and A1   Am forms a functional dependency  Since DFS with a small query budget is likely to explore only queries with predicate A1   v1  the domain values of Am which are corresponding to the other values of A1 will not be discovered  BFS  With BFS  one    rst exhausts all 1 predicate queries that can be constructed from discovered domain values  before moving forward to construct and issue 2 predicate queries  etc  The main problem of BFS arises when a large number of domain values are strongly correlated with the scoring function used to select the top  k tuples  because BFS with a small query budget is likely to issue only over   owing queries  For example  consider a total order of values for Am and suppose that tuples with    larger    Am receives higher scores  Then  BFS will likely to only discover the larger values of Am  3  DETERMINISTIC ALGORITHMS FOR COUNT INTERFACES In this section  we analyze the achievability conditions for attribute domain discovery over a top k COUNT interface  and also develop deterministic algorithms which guarantee the discovery of all attribute domains when the achievability conditions are met  The purpose of introducing deterministic algorithms is to use them as the basis for our design of more ef   cient randomized algorithms in the next section  3 1 Discover Binary Domains We start with a simple case where each attribute has only two possible values  both unknown to the third party analyzer 2   An ex  2 The third party analyzer may or may not know the binary nature of all attributes   our results apply either way ample of such an attribute is transmission for an auto database which has two values     manual    and    automatic     While hardly any practical database consists solely of binary attributes  the results in this subsection are illustrative when considering extensions to handle arbitrary attribute domains  as we shall show in the next subsection  3 1 1 Achievability of Domain Discovery Before devising concrete attribute domain discovery algorithms  we    rst consider the problem of achievability   i e   whether it is at all possible for the third party analyzer to discover all attribute domains from the top k interface  There are two sub problems   1  whether complete attribute domain discovery is possible when the analyzer can afford an unlimited query cost  and  2  if the    rst answer is yes  then what is the minimum number of queries required by a deterministic algorithm to accomplish attribute domain discovery in the worst case  Our main results  as shown by the following three theorems  can be summarized as follows  The answer to the    rst question is a lower bound on k   i e   no algorithm without knowledge of the scoring function can guarantee complete discovery over any hidden database if k   1  while complete discovery is always possible for any combination of database and scoring function when k   2  For the second question  if k   2  a deterministic algorithm require  at least    m2   log m  queries to discover all attribute domains in the worst case  where m is the number of attributes  unless k   n  in which case SELECT   FROM D returns all tuples in the database  THEOREM 3 1  If k   1  then for any given binary database  there exists a scoring function such that no algorithm can discover the domains of all attributes  PROOF  This impossibility proof is simple   Let the tuple returned by SELECT   FROM D be t    01          0n   Assign the highest score to t  One can see that no algorithm can discover any domain value other than f01          0ng  THEOREM 3 2  For any given number of attributes m  there exists a binary database and a scoring function such that no deterministic algorithm can complete attribute domain discovery without incurring a worst case query cost of   m2   log m   even if k   2  PROOF  For the sake of simplicity  we consider the case where m is even  The case where m is odd can be proved in analogy  We construct a set of 2 m2  4 database instances D1         D 2m2 4 as follows  Each instance Di consists of 2 m 2  m 2 tuplest1          t 2m 2 m 2   We consider the    rst 2 m 2 tuples    rst   for these tuples  i e   with i 2  1  2 m 2     if j 2  1  m 2   ti Aj     0j  resp  1j   iff the j th most signi   cant bit of the binary representation of i  1 is 0  resp  1   if j 2  m 2   1  m   then there is always ti Aj     0j   For the latter m 2 tuples  their values of Am 2 1          Am are set such that ti Aj     1j iff i   2 m 2 m 2  j  and ti Aj     0j otherwise  For the values of A1          Am 2 of these tuples  they are set such that each database instance has a different combination of the m 2 tuples  Note that for each tuple  there are 2 m 2 possible choices  Thus  we can construct  2 m 2   m 2   2 m2  4 different database instances  The scoring function for each instance is designed with only one condition  each tuple which satis   es ti Aj     0j for all j 2  m 2   1  m  has a higher score than all tuples which do not satisfy this condition  There are two important observations one can make from the above construction when k   2  First  each database instance requires a different sequence of queries to unveil all domain values  The reason for this is because the only way to unveil 1j for j 2  m 2   1  m  is to issue a query that contains at least m 2 predicates corresponding to the values of A1          Am 2 for tuple t 2m 2m 2 j   respectively  Since each database instance has a different combination of values for t 2m 2 1          t 2m 2 m 2   each instance requires a different sequence of queries for complete attribute domain discovery  The second important observation is that the ability for any query to distinguish between the 2 m2  4 database instances is limited  Note that if only the returned tuples are concerned  then almost all queries  except those that unveil one of 1j for j 2  m 2  1  m   return the exact same tuples when k   2  The more powerful distinguishing factor is COUNT  Nonetheless  the COUNT of any query has only m 2 different answers for all database instances  because after all these instances differ by at most m 2 tuples  Since we focus on deterministic algorithms  it is impossible for an algorithm to receive the same answer for all previous queries but then issue a different query in the next step for two database instances  Thus  in order for the algorithm to have a different sequence of queries for each database instance  the query cost is at least logm 2 2 m2  4   i e     m2   log m   Given the achievability conditions shown in the above two theorems  we now show the existence of a deterministic algorithm that is capable of discovering all attribute domains when the achievability conditions are met  and achieves a near optimal query cost  within a factor of log m where m is the number of attributes   THEOREM 3 3  If k   2  there exists a deterministic algorithm which guarantees complete attribute domain discovery for all binary databases and all scoring functions with a query cost of O m2    The following deterministic algorithm  B COUNT DISCOVER  serves as the proof  One can see from this theorem that for databases with only binary domains  the problem of attribute domain discovery can indeed by solved with a small query cost as long as k   2  3 1 2 B COUNT DISCOVER B COUNT DISCOVER starts by issuing query q1  SELECT   FROM D  Without loss of generality  let t    01          0m  be a tuple returned by q1  Since k   2  there must be another tuple returned by q1 which differs from t on at least one attribute value  Again without loss of generality  let such an attribute be A1   i e   the analyzer learns from the answer to q1 at least the following m  1 values  f01          0m  11g  The objective of B COUNT DISCOVER then becomes to unveil 12          1m  To discover any of these values  say 1m  we essentially need to    nd a tuple with Am   1m  Initially  the search space   i e   the set of possible tuple values in the database featuring Am   1m   is very large  In particular  the initial space S1 1m  is formed by all possible value combinations for A1          Am1  and Am   1m  and thus of size 2 m1   Our main idea of B COUNT DISCOVER is to shrink the search space by leveraging COUNT information provided by the interface  In particular  we construct a series of nested spaces S1 1m   S2 1m          each half the size of the preceding one  while ensuring that every Si 1m  contains at least one tuple in the database with Am   1m  Then  even in the worst case scenario  we can unveil 1m when the search space is shrinked to size 1   by simply issuing an  m1  predicate query with A1          Am1 speci   ed according to the value left in the search space  To understand how the nested spaces are constructed  consider the following three queries  q2  SELECT   FROM D WHERE Am   0m q3  SELECT   FROM D WHERE A1   01 q4  SELECT   FROM D WHERE A1   01 AND Am   0m       Let C qi  be the COUNT returned alongside query qi  We can compute from the query answers C1   COUNT A1   01 AND Am   1m  and C2   COUNT A1   11 AND Am   1m  as C1   C q3   C q4    1  C2   C q1   C q2    C q3   C q4     2  Note that either C1 or C2  or both  must be greater than 0 if 1m occurs in the database  Suppose that C1   0  We can now shrink our search space by half to size 2 m2   by constructing S2 1m    S1 1m  with only values in S1 1m  which satisfy A1   01  One can see that  since C1   0  at least one value in S2 1m  appears in the database and has Am   1m  The shrinking process continues from here  To see how  note that since C1   0  q3 either returns a tuple with Am   1m or over   ows  If q3 over   ows without returning 1m  there must exist another attribute  in A2          Am1  which has both values appearing in the k tuples returned for q3  Without loss of generality  let the attribute be A2  We now issue the following two queries  q5  SELECT   FROM D WHERE A1   01 AND A2   02 q6  SELECT   FROM D WHERE A1   01 AND A2   02 AND Am   0m Similar to the last step  we compute C3   COUNT A1   01 AND A2   02 AND Am   1m  and C4   COUNT A1   01 AND A2   12 AND Am   1m  as C3   C q5   C q6    3  C4   C q3   C q4    C q5   C q6     4  Again  at least one of C3 and C4 must be greater than 0 because C1   0  Suppose that C4   0  We can then further shrink our search space to size 2 m3 by constructing S3 1m    S2 1m  with only values in S2 1m  which satis   es A2   12  Once again  at least one value in S3 1m  appears in the database and has Am   1m because C4   0  Note that before the next shrinking step  we might need to issue q7  SELECT   FROM D WHERE A1   01 AND A2   12 in order to discover the complete domain of another attribute  in A3          Am1   so that we can continue the shrinking process  Note that similar to the discovery of A2   s domain from q3  this discovery is guaranteed by q7 because it either returns a tuple with Am   1m  which accomplishes our task  or over   ows  in which case at least one attribute unspeci   ed in q7 must have different values appear in the k   2 tuples returned by q7  One can see that eventually  we can always discover 1m after issuing at most 3m queries  because the search space would then become Sm 1m  of size 1  Thus  B COUNT DISCOVER requires a query cost of O m2    This concludes the proof of Theorem 3 3  3 2 Discover Arbitrary Domains We now extend our results for binary domains to arbitrary domains  We start by showing that the originally practical achievability conditions for binary domains  i e   lower bounds on k and query cost  now become unrealistic for arbitrary domains  Then  we illustrate the fundamental reason for such a change which motivates our design of randomized algorithms in the next section  3 2 1 Achievability of Attribute Domain Discovery For arbitrary domains  the achievability condition for attribute domain discovery imposes a much large lower bound on k  as indicated by the following theorem  Recall that jVij is the domain size of Ai  THEOREM 3 4  If k   1   Qm i 1  jVij1   then no deterministic algorithm can guarantee the discovery of all attribute domains for all database instance scoring function combinations  PROOF  Arbitrarily pick one attribute value vi from each Vi Q i 2  1  m    respectively  Consider a database D formed by 1   m i 1  jVij  1  tuples  Qm i 1  jVij  1  of them are the Cartesian product of Vinfvig for all i 2  1  m   The one additional tuple is t    v1          vm   Suppose that t has the lowest score  One can see that no algorithm can discover any of v1          vm if t is not returned by SELECT   FROM D  Thus  no algorithm can guarantee complete attribute domain discovery when k   1  Qm i 1  jVij1   One can see from Theorem 3 4 that the binary case result  Theorem 3 1  is indeed a special case when jV1j           jVmj   2  Another observation from Theorem 3 4 is that the lower bound on k has now become hardly reachable because real world database often features a number of attributes with large domains  Running Example  The NSF award search database requires k   1   Qm i 1  jVij  1    1 93   10 19 to guarantee complete attribute domain discovery  What reinforces this impracticability result is the following theorem  Note that since the achievability condition on k is now infeasible  it makes little practical sense for us to assume k   1   Qn i 1  jVij  1   as in the binary case  when deriving the lower bound on query cost  As such  we consider the following question  for a given value of k  is it possible for a deterministic algorithm to use a small number of queries to discover all domain values that can be discovered from such a top k interface  given unlimited query cost   The following lower bound on k shows that the answer to this question is also no for hidden databases with arbitrary domains  THEOREM 3 5  For given k and m  there exists a database and a scoring function such that no deterministic algorithm can complete the discovery of all domain values that can be discovered from the top k COUNT interface without incurring a worst case query cost of     m2   jVmaxj   log jVmaxj k   log m   jVminj     5  where jVmaxj and jVminj are the maximum and minimum values in jV1j          jVmj  respectively  The proof of this theorem follows in analogy to Theorem 3 2  We do not include it due to the space limitation  One can see that the binary case lower bound on query cost  Theorem 3 2  is indeed a special case of Theorem 3 5 when jV j   2 and k   2  Another observation from Theorem 3 5 is that this arbitrary domain lower bound on query cost  just like the lower bound on k  is hardly achievable in practice  especially when the hidden database contains one or a few attributes  e g   ZIP code  with large domains  Running Example  With the setting of our NSF award search example  m2   jVmaxj   log jVmaxj  k   log m   jVminj     1 06   10 4   While the bound may differ by a constant factor  10 4 is at least an order of magnitude larger than what one would desire for attribute domain discovery  e g   in our experimental results   Although the achievability conditions for both k and query cost are unrealistic for arbitrary domains  in the following we still extend B COUNT DISCOVER to COUNT DISCOVER  a deterministic algorithm that is capable of discovering all attribute domains  Of course  the extended algorithm has to follow the achievability conditions and therefore cannot provide meaningful worst case performance  Our main purpose of studying it is to identify which part of the extension causes the signi   cant increase on query cost  Theorem 3 6 summarizes the query cost of COUNT DISCOVER                   QTHEOREM 3 6  For an m attribute database with k   1   n i 1  jVij  1   there exists an algorithm which requires at most Xm i 1    2jVdi j  1    mXi 1 j 1   jVdj j    jVdj j  1  2     6  queries to discover all attribute domains  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  Running Example  With NSF award search example  the worst case query cost of COUNT DISCOVER is 1 07   10 11   orders of magnitude larger than what one can afford  Similar to the binary case  we explain the proof of this theorem in the description of COUNT DISCOVER  Note yet again that the binary case result  Theorem 3 3  is a special case of Theorem 3 6 when jVij   2 for all i 2  1  m   Nonetheless  unlike in the binary case where B COUNT DISCOVER has a query cost within a factor of O log m  from optimal  the query cost of COUNT DISCOVER is further away from the lower bound in Theorem 3 5  In particular  observe from Theorem 3 6 the worst case query cost of COUNTDISCOVER is   jVd1 j 2   jVdmj 3    a factor of O jVd1 j   jVdmj 2   from optimal  We did not pursue to close this gap for two reasons  First  since the lower bound itself is infeasible in practice  even an optimal algorithm would not be practical anyway  Second  our purpose of introducing COUNT DISCOVER is not to promote its practical usage  but to use the comparison between B COUNTDISCOVER and COUNT DISCOVER to illustrate the main obstacle facing the discovery of arbitrary attribute domains  which motivates our design of ef   cient  and thus practical  randomized algorithms  3 2 2 COUNT DISCOVER We now extend B COUNT DISCOVER to arbitrary domains  given the condition that k   1   Qn i 1  jVij  1   Note that this lower bound on k guarantees that SELECT   FROM D must reveal the complete domain for at least one attribute  assumed to be A1 without loss of generality  In addition  it must reveal at least one domain value for the remaining m  1 attributes  assumed to be 02          0m  As such  the objective of COUNT DISCOVER is to unveil 1i          jVij i for all i 2  2  m   Like in the binary case  we consider the discovery of an arbitrary attribute domain  say Vm  by constructing a series of nested spaces that are guaranteed to contain at least one tuple in D which has its value of Am yet to be discovered  Consider the worst case scenario where the only value of Vm revealed by SELECT   FROM D is 0m  The starting search space S1 Vm  then has a size of  jVmj  1    Qm1 i 1 jVij  Since we already know the complete domain of A1  the next step is to    nd one value in V1 which can be used to construct the next nested space   which must guarantee the appearance of at least one value in 1m          jVij m  In the binary case  we only need to determine whether 01 can be used because if it cannot  then 11 can always be used because A1 has only these two possible values  In particular  the judgement for 01 can be done with just two queries   WHERE A1   01 and WHERE A1   01 and Am   0m   because we can then infer the COUNT of tuples that satisfy A1   01 and Am   1m  Note that for arbitrary domains  we can still determine whether 01 can be used with the two exact same queries  because they now reveal the COUNT of tuples that satisfy A1   01 and have their values of Am yet to be discovered  Nonetheless  since the domain of A1 is much larger  than binary   we must continue testing other values of A1 if 01 cannot be used  In the worst case scenario  we may not be able to    nd the next search space until testing jV1j  1 possible values of A1  Thus  at this step  the query cost for arbitrary domain becomes jV1j  1 times as large as that for binary domain  Let w1 2 V1 be the value we    nd and use to construct the next nested search space  We can now construct the next nested space S2 Vm  by only including value combinations in S1 Vm  which satisfy A1   w1  One can see that the size of search space now becomes  jVmj  1    Qm1 i 2 jVij  COUNT DISCOVER continues the shrinking process in the same way as the binary case   i e   by issuing query q  SELECT   FROM D WHERE A1   w1  if it has not already been issued   One can see that either q returns a yet tobe discovered value in Vm  or it over   ows  in which case we can always    nd the complete domain of an attribute in A2          Am1 from the k tuples returned by q because k   1   Qn i 1  jVij  1   As such  eventually we can always unveil one value in Vm that has not yet been discovered  Nonetheless  since we do not maintain in the search space all yet to be discovered value in Vm  the entire shrinking process may have to be repeated multiple times to unveil the entire Vm  Theorem 3 6 can be derived accordingly  A key observation from the design of COUNT DISCOVER is that the signi   cant increase on query cost  from the binary case  is not because the shrinking of search spaces becomes less powerful   indeed  ALERT DISCOVER still needs only m nested spaces S1 Vm           Sm Vm  to reach size 1  The increase on query cost is largely due to the cost of determining how to construct the next nested space  i e  by    ltering the current one    a task previously accomplished by just two queries now requires many more  In the next section  we shall show that randomizing such nested space construction process is our key idea for building ef   cient randomized algorithms for discovering arbitrary attribute domains  4  RANDOMIZED ALGORITHMS FOR COUNT INTERFACES In the last section  we developed a query ef   cient algorithm for discovering binary domains  but found its extension to arbitrary domains to be extremely inef   cient  We also found the main reason behind this obstacle to be the increased cost of determining how to construct the nested search spaces   in particular  for a given search space and an attribute Ai  how to select a domain value v of Ai such that Ai   v can be used to    lter the current search space while maintaining at least one tuple with yet to be discovered domain value s  in the    ltered space  Now that any deterministic construction of nested spaces is provably expensive due to the achievability conditions we derived in the last section  our main idea in this section is to randomize nestedspace construction  so as to spend a small number of queries for search space construction while maintaining yet to be discovered domain value s  in the constructed space with high probability  In the following  we    rst describe our randomized process of nestedspace construction and the performance  i e   recall and coverage  guarantee it is able to achieve  and then combine this process with COUNT DISCOVER to form RANDOM COUNT DISCOVER  our randomized  Monte Carlo  algorithm for attribute domain discovery which can unveil attribute domains  though not necessarily completely  no matter if the achievability conditions are met  4 1 Randomize Nested Space Construction Recall that the main obstacle for nested space construction is to    nd a predicate Ai   v that can be used to    lter the current search space  In the worst case scenario  one may have to    nd m1 such conditions to discover a domain value for an attribute say Aj   this occurs when the complete domains of all attributes but Aj have been discovered  For the purpose of discussing the randomization of nested space construction  we consider a worst case scenario               where the complete domains of A1          Am1 have been discovered  and our objective now is to discover the domain of Am  As a side note   this problem is actually interesting in its own right because it captures the scenario where Am is a textbox attribute  with unknown domain  on the hidden database interface  while a large number of other attributes  i e   A1          Am1  have domains provided by the interface  e g   through options in drop down menus   We shall shown in the experiments section that our algorithms can effectively discover the domain of Am in this scenario  RANDOM SPACE  A simple idea to randomize the construction of nested spaces is to choose v uniformly at random from the domain of Ai  With this idea  every round  i e   search space shrinking process  of our RANDOM SPACE algorithm starts with SELECT   FROM D as the    rst query  Then  if no new domain value of Am is discovered from the query answer  it randomly chooses v1 from V1 and then uses A1   v1 to    lter the search space  i e   by issuing query SELECT   FROM D WHERE A1   v1   More generally  if RANDOM SPACE fails to discover a new domain value of Am from the    rst b1 queries issued in the current round  then it constructs the b query by adding a conjunctive condition Ab   vb  where vb is chosen uniformly at random from Vb  to the selection condition of the  b  1  th query  Each round of RANDOM SPACE requires at most m queries  and may lead to two possible outcomes  One is a new domain value being discovered  The other is that RANDOM SPACE reaches a valid or under   owing query without discovering any new domain value  Note that while this outcome never occurs for COUNTDISCOVER  it might occur for RANDOM SPACE because of the random construction of nested spaces  RANDOM SPACE may be executed for multiple rounds to discover more domain values  One can see that the randomization used by RANDOM SPACE guarantees a positive probability for each domain value to be discovered  Nonetheless  the main problem of it occurs when the distribution of tuples in the database is severally skewed   e g   when almost all tuples have the same values for A1  A2  etc  In this case  a large number of queries may be wasted on a small number of tuples  rendering it unlikely for RANDOM SPACE to unveil many domain values  RANDOM COUNT SPACE  The main problem of RANDOMSPACE can be solved by leveraging the COUNT information provided by the interface  In particular  RANDOM COUNT SPACE constructs the nested spaces in a similar fashion to RANDOMSPACE  with the only exception that the value v in selection condition Ai   v is no longer drawn uniformly at random from Vi  Instead  RANDOM COUNT SPACE    rst retrieves the COUNT of all jVij possible queries that de   ne the next search space  each corresponding to a different value of v   Then  it selects v in proportional to its corresponding COUNT  The construction of nested search spaces continues until either    nding a new domain value or reaching a valid query  Note that since we now leverage COUNT information  an under   owing query will never be selected to construct the next search space  An important property of RANDOM COUNT SPACE is that if we continue the search space shrinking process until reaching a valid query  then the valid query returns each tuple in the database with roughly equal probability  precisely equal if k   1  varies at most k times otherwise   This property explains why RANDOMCOUNT SPACE solves the above mentioned problem of RANDOMSPACE  Nonetheless  RANDOM COUNT SPACE also has its own problem  In particular  if Am is strongly correlated with A1  A2  etc  e g   A1  A2   Am forms a functional dependency   and all but a few tuples have the same value of Am  then RANDOMCOUNT SPACE may spend too many queries retrieving tuples featuring the    popular    value of Am  because the spaces it constructs most likely follow the    popular    value combination of A1 and A2   but fails to retrieve the other values  Interestingly  one can see that RANDOM SPACE can handle this case pretty well  motivating our idea of mixing the two randomization approaches  HYBRID SPACE  We now consider a hybrid of RANDOM  and RANDOM COUNT SPACE to avoid the problems of both  In particular  HYBRID SPACE starts with RANDOM SPACE  It is repeated executed until none of the last r1   1 times returns any new domain value  where r1 is a small number  e g   0 to 5   the setting of which shall be discussed in the next subsection and in the experiments  At this time  we switch to RANDOM COUNT SPACE  One round of RANDOM COUNT SPACE is performed  If it unveils a new domain value  then the entire process of HYBRIDSPACE is restarted by going back to the beginning of RANDOMSPACE  If no new value is discovered  we repeat RANDOM COUNTSPACE for up to r2 other rounds or until a new domain value is discovered  whichever happens    rst  Similar to r1  we shall discuss the setting of r2 in the next subsection and in the experiments  If a new domain value is discovered  then the entire HYBRID SPACE is restarted  Otherwise  If r2 1 consecutive rounds of RANDOMCOUNT SPACE fails to return any new domain value  we terminate HYBRID SPACE  Algorithm 1 depicts the pseudocode for HYBRID SPACE  Note that the input parameter in the pseudocode is designed for integration into RANDOM COUNT DISCOVER  Algorithm 1 HYBRID SPACE 1  Input parameter  q0  set to SELECT   FROM D by default 2  repeat 3    Perform one round of RANDOM SPACE 4  q   q0 5  while q over   ows AND returns no new domain value do 6  Select Ai not speci   ed in q  Select v uniformly at random from the discovered domain of Ai 7  q    q AND Ai   v   Issue q to learn new values  8  end while 9  until the last r1   1 rounds return no new domain value 10  repeat 11    Perform one round of RANDOM COUNT SPACE 12  q   q0 13  while q over   ows AND returns no new domain value do 14  Select Ai not speci   ed in q  For each discovered v 2 Vi  Query c v    COUNT of  q AND Ai   v  15  Select v with probability proportional to c v  16  q    q AND Ai   v   Issue q to learn new values  17  end while 18  if q returns a new domain value then Goto 2  19  until the last r2   1 rounds return no new domain value 4 2 HYBRID SPACE Performance Guarantee An important feature of HYBRID SPACE is the performance guarantee it provides independent of the underlying data distribution  To illustrate such a guarantee  we consider an example where A1          Am1 each has a domain of size 10  Let there be k   50  n   1 000 000 tuples and m   10 attributes in the database  note that n is readily available from the interface   Consider a parameter setting of r1   r2   0  The following derivation shows how HYBRID SPACE guarantees either a coverage of 46 values or a median recall of 500 000 tuples with only 4 036 queries  Case 1  First consider the case where HYBRID SPACE did not terminate before 4  036 queries were issued  Let wB and wT be the number of rounds RANDOM  and RANDOM COUNT SPACE     were performed  respectively  Since r1   0  the number of discovered domain values is at least wB  1  Since A1          Am1 each has 10 domain values  an RANDOM COUNT SPACE shrinking process issues at most 9 m  1  queries  Thus  the wB   wT    nished rounds consume at least 4036 9 m1 1    40469m queries because the    nal un   nished round could have issued at most 9 m  1   1 queries  As such  c   wB   9 m  1    wT   4046  9m   7  where c is the average number of queries issued by a round of RANDOM SPACE during the execution  Since r2   0  wB   wT  Thus   c   9 m  1     wB   4046  9m  If HYBRID SPACE did not discover at least 46 values  then wB   46 and therefore c    3956 46   9 m  1    5  A key step now is to prove that c   5 indicates an absolute recall of at least 500  000  This is enabled by the following theorem which illustrates a positive correlation between the absolute recall   and the average query cost c  per round  of RANDOM SPACE  THEOREM 4 1  There is     k   10 c1 with probability close to 1 when the number of rounds is suf   ciently large  PROOF  Let there be a total of s rounds of RANDOM SPACE performed  Consider the i th round  i 2  1  s    Let  i be the absolute recall of discovered domain values before the round begins  Given the discovered values  let  i be the set of queries which may be the second to last query issued by this i th round  One can see that each query in  i must over   ow and have all returned tuples featuring the already discovered domain values  Thus  we have  i   k   j ij  where j ij is the number of queries in  i  For each q 2  i  let p q  and h q  be the probability for q to be chosen in this round of RANDOM SPACE and the number of predicates in v  respectively  Since each attribute in A1          Am1 has 10 possible values  we have p q    1 10 h q    Thus  Eq2 i  k   10 h q      X q2 i k   10 h q  10 h q    k   j ij    i  8  where the expected value is taken over the randomness of nested space construction in RANDOM SPACE  Note that k   2 x is a convex function of x  According to Jensen   s inequality  Eq2 i  k   10 h q      k   10 Eq2 i  h q     Also note that for any i   i      Thus  k   10 Eq2 i  h q      i      When s  the total number of rounds of RANDOM SPACE  is suf   ciently large  according to the central limit theorem  the probability of Eq2 i  h q     c1 for every i 2  1  s  tends to 0  Thus  the probability of     k   10 c1 tends to 1 when the number of drill downs is suf   ciently large  According to the theorem  c   5 indicates an absolute recall of at least k   10 51   500000  Case 2  Now consider the other case where HYBRID SPACE terminates before 4  036 queries were issued  This indicates that the last round of RANDOM COUNT SPACE returns no new domain value  Let the absolute recall be   n  With each round of RANDOMCOUNT SPACE  the probability for the  1       n    unrecalled    tuples to be retrieved is 1   Thus  with Bayes    theorem  the posterior probability for     50  given the empty discovery of the last round of RANDOM COUNT SPACE is Prf    50 jempty discoveryg   1 2   1 2 R 1 0 p dp   1 2   9  That is  the median absolute recall is at least 500000  In summary of both cases  HYBRID SPACE either discovers at least 46 values  or has a median absolute recall of 500000  Generic Performance Guarantee  More generally  we have the following algorithm  THEOREM 4 2  For a query budget of d over a top k interface  HYBRID SPACE achieves either an absolute coverage of m0 values  or a median absolute recall of at least min   k   Y  i 1 jVij  r2 2 s 1 2 r2   2    n    10  where     d  Pm1 i 1 jVij   1 m0    r1   1   r2   1 r1   1   mX1 i 1 jVij   11  We do not include the proof due to the space limitation  One can see from the theorem that a suggest setting for r1 and r2 is r1   r2   0  as it maximizes the lower bound derived in  10   We shall verify this suggested setting in the experiments  Running Example  To discover the domain of Instrument from the NSF award search database based on pre known domains of award amount  PI state  NSF organization  and    eld  HYBRID SPACE requires at most 1563 queries to guarantee either the complete discovery of all domain values  or an absolute recall of min 4 12   10 7   n 2   4 3 RANDOM COUNT DISCOVER We now integrate HYBRID SPACE into COUNT DISCOVER to produce RANDOM COUNT DISCOVER  our randomized  Monte Carlo  algorithm for discovering attribute domains  RANDOMCOUNT DISCOVER starts with SELECT   FROM D  and uses the domain values discovered from the returned results to bootstrap HYBRID SPACE  After that  RANDOM COUNT DISCOVER executes HYBRID SPACE for multiple rounds  using the domain values discovered from previous rounds to bootstrap the next round  In particular  at any time  RANDOM COUNT DISCOVER maintains a set of domain values that have been discovered but not used in the current round of HYBRID SPACE  Let it be V     m i 1Vi  At the start  V is empty  After every round of HYBRID SPACE  the new domain values discovered are added to V   Then  in the next round of HYBRID SPACE  we randomly select an attribute with domain values in V   say Aj and fvj1          vjhg   V   Vj   and start the search space construction process with a selection condition of Aj   v where v 2 fvj1          vjhg  We then remove v from V   RANDOM COUNT DISCOVER terminates when V is empty or the query budget is exhausted  Algorithm 2 RANDOM COUNT DISCOVER 1  q0   SELECT   FROM D  V      2  repeat 3  Execute HYBRID SPACE q0  4  V   domain values discovered by HYBRID SPACE 5  Arbitrarily select j and v such that v 2 V   Vj 6  q0   SELECT   FROM D WHERE Aj   v 7  V   V nv 8  until V     or the query budget is exhausted 5  ATTRIBUTE DOMAIN DISCOVERY FOR ALERT INTERFACES We now extend our results to ALERT interfaces which does not offer COUNT  Since the information provided by an ALERT interface is a proper subset of that provided by the corresponding                          COUNT interface  the results in   3 2 already shows the impracticability of deterministic algorithms  Our results in this section further shows that ALERT interfaces require an even higher lower bound on query cost  To ef   ciently discover attribute domains  we develop RANDOM ALERT DISCOVER  a randomized algorithm which extends RANDOM COUNT DISCOVER by augmenting a query answer returned by an ALERT interface with an approximate COUNT estimated from historic query answers  5 1 Achievability for Deterministic Algorithms For ALERT interfaces  the achievability condition for k is exactly the same as the one for COUNT interfaces   which is shown in Theorem 3 4   as one can easily verify that the proof of Theorem 3 4 does not use any COUNT returned by the interface  For the condition on query cost  however  the lower bound is signi   cantly higher for ALERT interfaces  as shown by the following theorem  THEOREM 5 1  For given k and m  there exists a database and a scoring function such that no deterministic algorithm can complete the discovery of all domain values that can be discovered from the top k ALERT interface without incurring a worst case query cost of Qm i h 1 jVdi j  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  and h is the minimum value which satis   es k   1   Qh i 1  jVdi j  1   PROOF  We prove by induction  In particular  we start with constructing a database instance D1  and then show that for any b   Qm i h 1 jVdi j  if a deterministic algorithm uses exactly b queries q1          qb to discover all attribute domains from D1  then there must exist another database instance D2 which  1  provides the exact same query answers for q1          qb1  such that the deterministic algorithm will always issue qb next  and  2  ensures that q1          qb cannot discover all attribute domains  One can see that if this is proved  then any deterministic algorithm which discovers all attribute domains must has a query cost of at least Qm i h 1 jVdi j  We construct D1 with 1  k   Qm i h 1 jVdi j tuples t1          t 2m1 in the following manner  First  for each Adi  i 2  1  h    we arbitrarily choose a domain value vi 2 Adi   Then  For each value combination of Vdh 1           Vdm  we include k other tuples with Adh 1           Adm de   ned by the value combination and Adi  6 vdi for all i 2  1  h   It is always possible to    nd such k tuples because of our assumption that k   1   Qh i 1  jVdi j  1   The one additional tuple in D1 is t which satis   es Adi   vdi for all i 2  1  h  and has an arbitrary value combination for Adh 1           Adm  The scoring function is designed such that t has the lowest score  Note that the deterministic algorithm must unveil t with the    rst b queries  Without loss of generality  we assume that the b th query returns t  An important observation here is that t can only be unveiled by a query which has no predicate for Ad1           Adh  because v1          vh could not have been discovered before t is unveiled  and one predicate for each of Adh 1           Adm  because of k   If b   Qm i h 1 jVdi j  there must exist at least one value combination of Adh 1           Adm which has not yet been  fully  speci   ed by any issued query  We then construct D2 by changing the values of Adh 1           Adm of t to this not yet speci   ed combination  One can see that the answers of q1          qb1 over D2 is exactly the same as D1  Nonetheless  qb no longer returns t  mandating a query cost of at least b   1 for attribute domain discovery over D2  Running Example  With the NSF award search example  the lower bound on worst case query cost over a top 50 ALERT interface is 9 40 10 15   orders of magnitude larger than even the upper bound for COUNT interface  THEOREM 5 2  For given k and m attribute  there exists an algorithm which requires at most h  1   Qh1 i 1 jVdi j queries to discover the domains of at least h  out of the m  attributes  where d1          dm is the permutation of 1          m which satis   es jVd1 j           jVdmj  The following description of ALERT DISCOVER serves as the proof for this theorem  5 1 1 ALERT DISCOVER The deterministic COUNT DISCOVER is capable of shrinking the search space signi   cantly at each step because it can infer the COUNT of a domain value yet to be discovered from COUNTs returned by other queries  ALERT interfaces do not provide such luxury  In particular  for the discovery of Vm  there is no direct way to check whether an unknown domain value in Vm occurs in tuples satisfying an over   owing query such as q3  SELECT   FROM D WHERE A1   01  unless q3 returns the domain value in its top k result  Despite of the lack of COUNT information  we can still shrink the search space with an ALERT interface  though the shrinking may not be as signi   cant as in COUNT DISCOVER  For example  if the above mentioned q3 turns out to be valid or under   owing without returning any unknown domain value in Vm  we can safely remove from the search space S1 Vm  all values which satisfy q3   a reduction of size Qm1 i 2 jVij   while still ensuring that the reduced search space contains at least one tuple in the database which has an unknown value of Vm  ALERT DISCOVER  our deterministic algorithm for ALERT interfaces  exactly follows this strategy to construct a series of nested spaces S1 Vm   S2 Vm          for the discovery of Vm  To decide which query to issue next  ALERT DISCOVER uses the following simple rule  Suppose that the complete domains of h attributes have been discovered  say A1          Ah without loss of generality  note that h   1 at the start of the algorithm   The next query ALERTDISCOVER issues is an h predicate query with one conjunctive condition for each attribute Ai  i 2  1  h    The value vi speci   ed for Ai in the query is determined as follows  First  the value combination hv1          vhi must lead to a query that cannot be answered based solely upon the historic query answers   i e   no previously issued query is formed by a subset of A1   v1          Ah   vh and returns valid or under   ow  Within the value combinations which satisfy this condition  we select the highest ordered one according to an arbitrary global order  The above process is repeated until all domains are discovered  To understand how such a query issuing plan leads to the shrinking of nested search spaces  consider the three possible outcomes of an above constructed query q   under   ow  valid  or over   ow  If q under   ows or is valid  then we can construct the next search space by removing from the current one all value combinations that satisfy q   a reduction of size Qm1 i h 1 jVij where h is the number of predicates in q  On the other hand  if q over   ows  then we can always discover the complete domain of at least one additional attribute because k   1   Qn i 1  jVij  1   One can see that if ALERT DISCOVER has issued at least m queries with the second outcome  then it must have discovered the domains for all m attributes  The    rst outcome  on the other hand  guarantees the reduction of search space  Theorem 5 2 follows accordingly  5 2 RANDOM ALERT DISCOVER A simple approach to enable aggregate estimation is to directly integrate RANDOM COUNT DISCOVER with HD UNBIASEDAGG  the existing aggregate estimator for hidden databases  8   Intuitively  since RANDOM COUNT DISCOVER starts with RANDOM           SPACE  one can use queries answers received by RANDOM SPACE to estimate each COUNT value required by the subsequent execution of RANDOM COUNT SPACE  The estimations will become more accurate over time after more queries are issued by RANDOM SPACE or RANDOM COUNT SPACE   Nonetheless  such a direct integration has a key problem  The existing HD UNBIASED AGG can only generate a COUNT estimation from a valid query answer  Most queries issued by RANDOMCOUNT SPACE  however  are likely to over   ow as the shrinking of search space only continues  i e   with    narrower     and possibly valid  queries being issued  if no new domain value can be discovered from the previously issued over   owing queries   an unlikely event especially at the beginning of RANDOM COUNTDISCOVER  As a result  the integration with HD UNBIASEDAGG faces a dilemma  either uses only a few valid queries and suffers a high estimation variance  or issues a large number of extra queries to    narrow down    the queries issued by RANDOMCOUNT SPACE  by adding conjunctive conditions to the queries  to valid ones  Either way  the large number of over   owing queries issued by RANDOM COUNT SPACE would be wasted  To address this problem  we propose a step by step estimation process which can generate COUNT estimations based the the results of all historic queries  including the over   owing ones  For the ease of understanding  we consider an example of estimating SELECT COUNT    FROM D WHERE A1   v1  denoted by COUNT v1   Handling of other COUNT queries immediately follows  Consider one round of RANDOM SPACE or RANDOMCOUNT SPACE during which b queries q1         qb are issued in order  Recall that q1 is SELECT   FROM D  qb may be over   owing  valid or under   owing  Without loss of generality  let A1          Ai1 be the attributes involved in the selection conditions of qi  i 2  2  b    Let pi be the transition probability from qi to qi 1   i e   the probability for qi 1 to be selected  out of all possible values of Ai  after qi is issued  For example  pi   1 jVij for RANDOM SPACE  and may differ  based on the currently estimated COUNTs  for RANDOM COUNT SPACE  We estimate COUNT v1  from all b queries with the following two steps    We    rst choose i i d  uniformly at random vi 2 Vi for each i 2  b  m  1   and then construct and issue a  conjunctive  query qF formed by appending conditions  Ab   vb  AND       AND  Am1   vm1  to qc    We then return the following estimation of COUNT v1         Xb i 1 Sv1   i  Qi1 j 1 pj     Sv1   F    Qm j b jVj j Qb1 j 1 pj  12  where  i   ftjt 2 qi  t 62  q1           qi1 g   F   ftjt 2 qF  t 62  q1           qc g  and Sv1     stands for the result of applying COUNT v1  over a set of tuples  Note that here qi Qstands for the set of tuples returned by query qi  We assume i1 j 1 pj   1 when i   1  and Sv1       0  One can see that each query in q1          qb contributes an additive component to the    nal estimation     e g   qi contributes Sv1   i   Qi1 j 1 pj   which is essentially an estimated COUNT of tuples that sastify A1   v1  can be returned by a query with predicates on A1          Ai  but cannot be returned by a query with predicates only on A1          Ai1  This is why we call the idea    stepby step estimation     In terms of query cost  step by step estimation reuses all queries issued by RANDOM SPACE or RANDOMCOUNT SPACE  In addition  it issues at most one additional query qF per round  of search space shrinking   It is easy to verify that the proof of unbiasedness for HD UNBIASED AGG  8  remains valid with step by step estimation  6  EXPERIMENTAL RESULTS 6 1 Experimental Setup 1  Hardware and Platform  All our experiments were performed on a 2 6GHz Intel Core 2 Duo machine with 2GB RAM and Windows XP OS  All algorithms were implemented in C    2  Dataset  We conducted our experiments on two real world datasets  One is the NSF award search database which has been used as a running example throughout the paper  We tested our algorithms over the real world web interface of NSF award search by issuing queries through the    searc</doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap1 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap1">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis"/>
        <doc>Data Serving in the Cloud ### Raghu Ramakrishnan Chief Scientist  Audience and Cloud Computing Brian Cooper Adam Silberstein Utkarsh Srivastava Yahoo  Research Joint work with the Sherpa team in Cloud Computing2 Outline     Clouds     Scalable serving   the new landscape     Very Large Scale Distributed systems  VLSD      Yahoo    s PNUTS Sherpa     Comparison of several systems     Preview of upcoming Y  Cloud Serving  YCS   benchmark3 Types of Cloud Services     Two kinds of cloud services      Horizontal     Platform     Cloud Services     Functionality enabling tenants to build applications or new  services on top of the cloud     Functional Cloud Services      Functionality that is useful in and of itself to tenants  E g    various SaaS instances  such as Saleforce com  Google  Analytics and Yahoo    s IndexTools  Yahoo  properties aimed  at end users and small businesses  e g   flickr  Groups  Mail   News  Shopping      Could be built on top of horizontal cloud services or from  scratch     Yahoo  has been offering these for a long while  e g   Mail for  SMB  Groups  Flickr  BOSS  Ad exchanges  YQL 5 Yahoo  Horizontal Cloud Stack Provisioning   Self serve  YCS Horizontal YCPI  Cloud ServicesBrooklyn     EDGE Monitoring Metering Security Hadoop Horizontal   Cloud Services BATCH STORAGE PNUTS SherpaHorizontal MOBStor Cloud Services   ###  OPERATIONAL STORAGE VM OS Horizontal Cloud Services     APP VM OS Horizontal Cloud Services yApache WEB Data Highway Serving Grid PHP App Engine6 Cloud Power   Yahoo  Ads  Optimization Content  Optimization Search  Index Image Video  Storage   Delivery Machine  Learning   e g  Spam  filters  Attachment Storage7 Yahoo    s Cloud   Massive Scale  Geo Footprint     Massive user base and engagement     500M  unique users per month     Hundreds of petabyte of storage      Hundreds of billions of objects     Hundred of thousands of requests sec     Global     Tens of globally distributed data centers     Serving each region at low latencies     Challenging Users     Downtime is not an option  outages cost  millions      Very variable usage patterns8 New in 2010      SIGMOD and SIGOPS are starting a new annual  conference  co located with SIGMOD in 2010   ACM Symposium on Cloud Computing  SoCC   PC Chairs  Surajit Chaudhuri   Mendel Rosenblum GC  Joe Hellerstein Treasurer  Brian Cooper      Steering committee  Phil Bernstein  Ken Birman   Joe Hellerstein  John Ousterhout  Raghu  Ramakrishnan  Doug Terry  John Wilkes9 VERY LARGE SCALE   DISTRIBUTED  VLSD   DATA SERVING ACID or BASE  Litmus tests are colorful  but the picture is cloudy10 Databases and Key Value Stores http   browsertoolkit com fault tolerance png11 Web Data Management Large data analysis  Hadoop  Structured record  storage  PNUTS Sherpa  Blob storage  MObStor     Warehousing    Scan  oriented  workloads    Focus on  sequential  disk I O      per cpu  cycle    CRUD     Point lookups  and short  scans    Index  organized  table and  random I Os      per latency    Object  retrieval and  streaming    Scalable file  storage      per GB  storage    bandwidth12 The World Has Changed     Web serving applications need      Scalability      Preferably elastic     Flexible schemas     Geographic distribution     High availability     Reliable storage     Web serving applications can do without      Complicated queries     Strong transactions     But some form of consistency is still desirable13 Typical Applications     User logins and profiles     Including changes that must not be lost      But single record    transactions    suffice     Events     Alerts  e g   news  price changes      Social network activity  e g   user goes offline      Ad clicks  article clicks     Application specific data     Postings in message board     Uploaded photos  tags     Shopping carts14 Data Serving in the Y  Cloud Simple Web Service API   s Database PNUTS   SHERPA Search Vespa Messaging Tribble Storage MObStor Foreign key photo     listing FredsList com application ALTER Listings MAKE CACHEABLE Compute Grid Batch export Caching memcached 1234323   transportation   For sale  one  bicycle  barely  used 5523442   childcare   Nanny  available in  San Jose DECLARE DATASET Listings AS   ID String PRIMARY KEY  Category String  Description Text   32138   camera   Nikon  D40  USD 30015 VLSD Data Serving Stores     Must partition data across machines     How are partitions determined      Can partitions be changed easily   Affects elasticity      How are read update requests routed      Range selections  Can requests span machines      Availability  What failures are handled      With what semantic guarantees on data access       How  Is data replicated      Sync or async  Consistency model  Local or geo      How are updates made durable      How is data stored on a single machine 16 The CAP Theorem     You have to give up one of the following in  a distributed system  Brewer  PODC 2000   Gilbert Lynch  SIGACT News 2002       Consistency of data      Think serializability     Availability     Pinging a live node should produce results     Partition tolerance     Live nodes should not be blocked by partitions17 Approaches to CAP        BASE        No ACID  use a single version of DB  reconcile later     Defer transaction commit      Until partitions fixed and distr xact can run     Eventual consistency  e g   Amazon Dynamo      Eventually  all copies of an object converge     Restrict transactions  e g   Sharded MySQL      1 M c Xacts  Objects in xact are on the same machine      1 Object Xacts   Xact can only read write 1 object     Object timelines  PNUTS  http   www julianbrowne com article viewer brewers cap theorem18 18    I want a big  virtual database       What I want is a robust  high performance virtual  relational database that runs transparently over a  cluster  nodes dropping in and out of service at will   read write replication and data migration all done  automatically  I want to be able to install a database on a server  cloud and use it like it was all running on one  machine        Greg Linden   s blog19 PNUTS   SHERPA To Help You Scale Your Mountains of Data Y  CCDI20 Yahoo  Serving Storage Problem     Small records     100KB or less     Structured records     Lots of fields  evolving     Extreme data scale   Tens of TB     Extreme request scale   Tens of thousands of requests sec     Low latency globally   20  datacenters worldwide     High Availability   Outages cost  millions     Variable usage patterns   Applications and users change 2021 E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E What is PNUTS Sherpa  E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E CREATE TABLE Parts   ID VARCHAR  StockNumber INT  Status VARCHAR       Parallel database Geographic replication Structured  flexible schema Hosted  managed infrastructure A     42342               E B     42521               W C     66354               W D     12352               E E     75656               C F     15677               E 2122 What Will It Become   E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E E     75656               C A     42342               E B     42521               W C     66354               W D     12352               E F     15677               E Indexes and views23 Scalability     Thousands of machines     Easy to add capacity     Restrict query language to avoid costly queries Geographic replication     Asynchronous replication around the globe     Low latency local access High availability and fault tolerance     Automatically recover from failures     Serve reads and writes despite failures Design Goals 23 Consistency     Per record guarantees     Timeline model      Option to relax if needed Multiple access paths     Hash table  ordered table     Primary  secondary access Hosted service     Applications plug and play     Share operational cost24 Technology Elements PNUTS      Query planning and execution     Index maintenance Distributed infrastructure for tabular data     Data partitioning      Update consistency     Replication YDOT FS     Ordered tables Applications Tribble     Pub sub messaging YDHT FS      Hash tables Zookeeper     Consistency service YCA  Authorization PNUTS API Tabular API 2425 PNUTS  Key Components     Maintains map from  database table key totablet to SU     Provides load balancing     Caches the maps from the TC     Routes client requests to  correct SU     Stores records     Services get set delete  requests 2526 Storage units Routers Tablet Controller REST API Clients Local region Remote regions Tribble Detailed Architecture 2627 DATA MODEL 2728 Data Manipulation     Per record operations     Get     Set     Delete     Multi record operations     Multiget     Scan     Getrange     Web service  RESTful  API 2829 Tablets   Hash  Table Apple Lemon Grape Orange Lime Strawberry Kiwi Avocado Tomato Banana Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  How much did you pay for this lemon  Is this a vegetable  New Zealand The perfect fruit Name Description Price  12  9  1  900  2  3  1  14  2  8 0x0000 0xFFFF 0x911F 0x2AF3 2930 Tablets   Ordered Table 30 Apple Banana Grape Orange Lime Strawberry Kiwi Avocado Tomato Lemon Grapes are good to eat Limes are green Apple is wisdom Strawberry shortcake Arrgh  Don   t get scurvy  But at what price  The perfect fruit Is this a vegetable  How much did you pay for this lemon  New Zealand  1  3  2  12  8  1  9  2  900  14 Name Description Price A Z Q H31 Flexible Schema Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Color Red Condition Good Fair32 32 Primary vs  Secondary Access Posted date Listing id Item Price 6 1 07 424252 Couch  570 6 1 07 763245 Bike  86 6 3 07 211242 Car  1123 6 5 07 421133 Lamp  15 Price Posted date Listing id 15 6 5 07 421133 86 6 1 07 763245 570 6 1 07 424252 1123 6 3 07 211242 Primary table Secondary index Planned functionality33 Index Maintenance     How to have lots of interesting indexes  and views  without killing performance      Solution  Asynchrony      Indexes views updated asynchronously when  base table updated34 PROCESSING READS   UPDATES 3435 Updates 1 Write key k 2 Write key k 7 Sequence   for key k 8 Sequence   for key k SU SU SU 3 Write key k 4 5 SUCCESS 6 Write key k Routers Message brokers 3536 Accessing Data 36 SU SU SU 1 Get key k 2 3 Record for key k Get key k 4 Record for key k37 Bulk Read 37 SU Scatter  gather  server SU SU 1  k1  k2      kn  Get k 2 1 Get k2 Get k338 Storage unit 1 Storage unit 2 Storage unit 3 Range Queries in YDOT     Clustered  ordered retrieval of records Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 1 Router Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Apple Avocado Banana Blueberry Canteloupe Grape Kiwi Lemon Lime Mango Orange Strawberry Tomato Watermelon Grapefruit   Pear  Grapefruit   Lime  Lime   Pear  Storage unit 1 Canteloupe Storage unit 3 Lime Storage unit 2 Strawberry Storage unit 139 Bulk Load in YDOT     YDOT bulk inserts can cause performance  hotspots     Solution  preallocate tablets40 ASYNCHRONOUS REPLICATION  AND CONSISTENCY 4041 Asynchronous Replication 4142 Consistency Model     If copies are asynchronously updated   what can we say about stale copies      ACID guarantees require synchronous updts     Eventual consistency  Copies can drift apart   but will eventually converge if the system is  allowed to quiesce     To what value will copies converge       Do systems ever    quiesce         Is there any middle ground 43 Example  Social Alice User Status Alice Busy West East User Status Alice Free User Status Alice     User Status Alice     User Status Alice Busy User Status Alice         Busy Free Free Record Timeline  Network fault   updt goes to East   Alice logs on 44     Goal  Make it easier for applications to reason about updates  and cope with asynchrony     What happens to a record with primary key    Alice     PNUTS Consistency Model 44 Time Record  inserted Update Update Update Update Update Delete Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Update Update As the record is updated  copies may get out of sync 45 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write Current  version Stale version Stale version PNUTS Consistency Model 45 Achieved via per record primary copy protocol  To maximize availability  record masterships automaticlly  transferred if site fails  Can be selectively weakened to eventual consistency   local writes that are reconciled using version vectors 46 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Write if   v 7 ERROR Current  version Stale version Stale version PNUTS Consistency Model 46 Te s t and set writes facilitate per record transactions47 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Current  version Stale version Stale version Read PNUTS Consistency Model 47 In general  reads are served using a local copy48 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read up to date Current  version Stale version Stale version PNUTS Consistency Model 48 But application can request and get current version49 Time v  1 v  2 v  3 v  4 v  5 v  7 Generation 1 v  6 v  8 Read     v 6 Current  version Stale version Stale version PNUTS Consistency Model 49 Or variations such as    read forward      while copies may lag the master record  every copy goes through the same sequence of changes50 OPERABILITY 5051 51 Server 1 Server 2 Server 3 Server 4 6 2 07 636353 Bike  86 6 5 07 662113 Chair  10 Distribution 6 1 07 424252 Couch  570 6 1 07 256623 Car  1123 6 7 07 121113 Lamp  19 6 9 07 887734 Bike  56 6 11 07 252111 Scooter  18 6 11 07 116458 Hammer  8000 Data shuffling for load balancing Distribution for parallelism52 Tablet Splitting and Balancing 52 Each storage unit has many tablets  horizontal partitions of the table  Overfull tablets split Tablets may grow over time Storage unit may become a hotspot Shed load by moving tablets to other servers Storage unit Tablet53 Consistency Techniques     Per record mastering     Each record is assigned a    master region        May differ between records     Updates to the record forwarded to the master region     Ensures consistent ordering of updates     Tablet level mastering     Each tablet is assigned a    master region        Inserts and deletes of records forwarded to the master region     Master region decides tablet splits     These details are hidden from the application     Except for the latency impact 54 54 Mastering A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                 E A     42342                E B     42521                E C     66354                W D     12352                E E     75656                C F     15677                 E C     66354                W B     42521                E A     42342                E D     12352                E E     75656                C F     15677                E55 55 Record vs  Tablet Master A     42342                E B     42521                W C     66354                W D     12352                E E     75656                C F     15677                E A     42342                E B     42521                W C     66354                W D  </doc>
    </owl:NamedIndividual>
    


    <!-- http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap2 -->

    <owl:NamedIndividual rdf:about="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#slsdap2">
        <rdf:type rdf:resource="http://www.semanticweb.org/ontologies/2011/3/Ontology1303310384456.owl#large_scale_data_analysis"/>
        <doc>Architectural Considerations for Distributed RFID Tracking and Monitoring ### Zhao Cao Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA caozhao cs umass edu Yanlei Diao Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA yanlei cs umass edu Prashant Shenoy Dept  of Computer Science University of Massachusetts Amherst  MA 01003  USA shenoy cs umass edu ABSTRACT In this paper we discuss architectural challenges in designing a distributed  scalable system for RFID tracking and monitoring  We argue for the need to combine inference and query processing techniques into a single system and consider several architectural choices for building such a system  Key research challenges in de  signing our system include   i  the design of inference techniques that span multiple sites   ii  distributed maintenance of inference and query state   iii  sharing of inference and query state for scalability  and  iv  the use of writeable RFID tags to transfer state information as objects move through the supply chain  We also present the status of our ongoing research and preliminary results from an early implementation ### 1  INTRODUCTION RFID is a promising electronic identi cation technology that enables a real time information infrastructure to pro  vide timely  high value content to monitoring and tracking applications  An RFID enabled information infrastructure is likely to revolutionize areas such as supply chain manage  ment  health care and pharmaceuticals  Consider  for ex  ample  a distributed supply chain environment with multiple warehouses and millions of tagged objects that move through this supply chain  Each warehouse is equipped with RFID readers that scan objects and their associated cases and pal  lets upon arrival and departure and while they are processed in the warehouse  In order to track objects and monitor the supply chain for anomalies  several types of queries may be posed on the RFID streams generated at the warehouses    Tracking queries  Report any pallet that has deviated from its intended path  List the path taken by an item through the supply chain    Containment queries  Raise an alert if a  ammable item is not packed in a  reproof case  Verify that food containing peanuts is never exposed to other food cases for more than an hour    Hybrid queries  Report if a drug has been exposed to a temperature of more than 80 degrees for 12 hours  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro   t or commercial advantage and that copies bear this notice and the full citation on the    rst page  To copy otherwise  to republish  to post on servers or to redistribute to lists  requires prior speci   c permission and or a fee  NetDB    09 Big sky  MT USA Copyright 200X ACM X XXXXX XX X XX XX     10 00  The  rst class of queries are location queries that require object locations or location history  The second class in  volves containment  i e   relationships between objects  cases and pallets  The third kind involves processing of sensor streams  e g   temperature readings  in conjunction with RFID streams to detect certain conditions  Typically raw RFID streams contain noisy data that lacks any location or containment information  Hence  such continuous queries require derivation of location and containment information from raw RFID data as well as processing of heterogeneous sensor streams along with RFID data streams  In this paper  we discuss the architectural challenges in designing a scalable  distributed stream processing system for RFID tracking and monitoring  We propose to com  bine location and containment inference with scalable query processing into a single architecture  in contrast to prior ap  proaches that dealt with these two problems separately  We present three architectural choices in instantiating such a system over large supply chains and present an analysis of their communication overheads  By doing so  we show that the choice between centralized and distributed approaches mainly depends on the read frequency of RFID readers and the number of active queries in the system  Furthermore  utilizing local storage of writeable RFID tags for inference and query processing makes the distributed approach a bet  ter solution with signi cantly reduced communication cost  In this paper  we also describe key technical challenges in designing a distributed architecture  which include  i  the design of novel inference techniques that span multiple warehouses of the supply chain   ii  distributed  consistent maintenance of inference and query state as objects move through the supply chain  and  iii  sharing of inference and query state for scalability  A novel aspect of our system is its ability to exploit writeable RFID tags  when available  and use the onboard tag storage to transfer query and infer  ence state as the object moves from one location to another  We  nally present the status of our ongoing research and preliminary results from an early implementation  2  RELATED WORK RFID stream processing  The HiFi system  9  12  o ers a declarative framework for RFID data cleaning and process  ing  It focuses on per tag smoothing and multi tag aggre  gation  but does not capture relationships between objects such as containment or estimate object locations via con  tainment  Our system can produce a rich event stream with object location and containment information  It further of  fers distributed inference and event processing methods Central Server Warehouse Warehouse Warehouse Warehouse Local Server Local Server Local Server Local Server Objects  Tags  Figure 1  A distributed RFID data management system  RFID databases  Siemens RFID middleware 20  uses ap  plication rules to archive RFID data streams into databases  Cascadia  21  supports RFID based pervasive computing with event speci cation  extraction and archival  Techniques are also available to integrate data cleansing with query pro  cessing  16   encode  ow information  13  and recover high  level information from incomplete  noisy data by exploiting known constraints  23   These techniques  however  are not designed for fast RFID stream processing and only for cen  tralized processing  Event query processing  Most event processing methods  1  14  22  use centralized processing which requires all RFID data to be transferred to a single site  incurring high com  munication cost  The system in  2  uses multi step event acquisition and processing to minimize event transmission cost  In contrast  our system performs both inference and query processing  does so at a local site whenever possible  and then transmits computation state across sites for dis  tributed inference and event pattern detection  Probabilistic inference in sensor networks  There have been several recent techniques  15  5  18  11  for inferring the true value of a phenomenon  such as temperature  light  or an object s position  that a sensor network is deployed to measure  Our inference problem di ers because we aim to infer inter object relationships  such as containment  that sensors cannot directly measure  hence the di erent design of our graph model for inference  Further  our system combines inference with query processing into a single architecture and explores advanced techniques to reduce the combined cost of inference query state migration in a distributed system  3  OVERVIEW OF THE SPIRE SYSTEM In this section  we provide an architectural overview of a distributed RFID data management system  which we call SPIRE  This system is designed to support RFID based tracking and monitoring in large scale supply chains with multiple warehouses and millions of objects  In a typical environment as depicted in Figure 1  each ob  ject is a xed with a tag that has a unique identity under the EPC standard  7   Most tags are passive tags that have no individual power systems but small amounts of memory  e g   1 4 KB in the current generation of EPC tags  17  and up to 64KB  10  in the next generation  Such tag memory can be used to store object information and facilitate query processing as we describe in the next section  An RFID reader periodically sends a radio signal to the tags in its read range  the tags use the radio energy to send back their tag ids  The reader immediately returns the sensed data in the form of  tag id  reader id  time   The local servers of a warehouse collect raw RFID data streams from all of the readers  and  lter  aggregate  and process these streams  The data streams from di erent warehouses are further ag  gregated to support global tracking and monitoring  We next illustrate two tracking and monitoring queries using an extension of the Continuous Query Language  3  with additional constructs for event pattern matching  1  22   These queries assume that events in the input stream contain attributes  tag id  time  location  container  and optional attributes describing object properties  such as the type of food and expiration date  which can be obtained from the manufacturer s database 1   It is worth noting the di erence in schema between raw RFID readings and events required for query processing  Such di erences motivate our work on inference  which is discussed shortly  Query 1 below is an example of containment queries  It sends an alert when peanut free food has been contained in the same case as food containing peanuts for more than 1 hour  The inner  nested  query block performs a self join over the input stream  where one join input retains only the events for peanut free food  the other input retains only those for food containing peanuts  and the join is based on equality on container  Each join result represents an event that a container contains both food with peanuts and foot without peanuts  This event is published immediately into a new stream  The outer query block detects a sequence pattern over the new stream  each match of the pattern contains a sequence of events that refer to the same tag id of the peanut free food and span a time period of more than 1 hour  For each pattern match  the query returns the tag id of the peanut free food and the length of the period such information can assist a retail store in deciding whether to dispose of the food  Query 1  Select tag id  A A len  time   A 1  time From   Select Rstream R1 tag id  R1 loc  From Food  Range 3 minutes  As R1  Food  Range 3 minutes  As R2 Where R1 type    peanut free  and R2 type    peanut  and R1 container   R2 container   As S   Pattern SEQ A   Where A i  tag id   A 1  tag id and A A len  time   A 1  time   1 hr  Query 2 combines RFID readings with temperature sen  sor readings and alerts if an object has been exposed to a temperature of more than 80    for 12 hours  It has a similar structure as Query 1  but returns all the sensor readings in the period when the temperature regulation was violated  Query 2  Select tag id  A   temp From   Select Rstream R tag id  R loc  T temp  From Object  Now  as R  Temperature  Rows 1 minute  as T Where R type    drug  and T temp   80     and R loc   T loc   As S   Pattern SEQ A   Where A i  tag id   A 1  tag id and A A len  time   A 1  time   12 hrs  The SPIRE system we design has two main functionali  ties  inference and query processing  Both of them can be 1 How to obtain the object properties to the site of query process  ing is an architectural issue  which we discuss in the next section  b  Inference   Inference State Event Stream      tag id  time  location  container        Raw RFID Stream    tag id  reader id  time  Local   Distributed inference  with state migration Query 2 Query m         c  Query Processing location and  containment 1 4 5 7 8 9 10 2 6 11 12 3 Query 1 Distributed  processing  w  state  migration O2 Window Window processing Local           On a 1  a i  Query  State O1 O2 Local Distributed Local Distributed  a  RFID Sensing   F Figure 2  Architectural overview of the SPIRE system  implemented in a centralized or distributed fashion  The tradeo s between these implementation choices are the fo  cus of the next section  Inference  While data stream processing has been a topic of intensive recent research  RFID data stream processing presents several new challenges  I Insu cient information  As the above examples show  query processing often requires information about ob  ject locations and inter object relationships such as containment  However  raw RFID data contains only the observed tag id and its reader id due to the limi  tations of being an identi cation technology  I Incomplete  noisy data  The problem of deriving lo  cation and containment information from raw data is compounded by the fact that RFID readings are in  herently noisy  with read rates in actual deployments often in the 60  70  range  12   This is largely due to the sensitivity of radio frequencies to environmental factors such as occluding metal objects and interfer  ence  8   Mobile RFID readers may read objects from arbitrary angles and distances  hence more susceptible to variable read rates  In SPIRE  we design an inference module that derives ob  ject locations and containment relationships despite missed readings  This module resides between the RFID sensing module and the query processing module  as shown in Fig  ure 2  We focus on containment in the following discussion  inference for object locations alone is detailed in our recent publication  19    First  an RFID reader can read several containers and all of their contained items simultaneously  which makes it di cult to infer the exact container of each item  In SPIRE  we explore the correlations of observations obtained at di erent locations at di erent times to infer con  tainment  For instance  while the containment information at the loading dock of a warehouse is ambiguous due to the readings of two containers simultaneously  true containment may be revealed at the receiving belt where containers are read one at a time  Such containment remains unchanged as containers are placed on shelves but may change later in the repackaging area  The inference algorithm needs to adapt to such changes in a timely fashion  However  missed readings signi cantly complicate the in  ference problem  Consider a scenario that an item was last seen in location A with its container  Now its container is observed in B but the item is not observed in any location  There are a few possible locations for the item  it was left behind in location A but the reading in A was missed  it moved to location B with its container and its reading in B was missed  it disappeared unexpectedly  e g   stolen   The inference algorithm needs to account for all these possibili  ties when deriving containment and location information  In SPIRE  we employ a time varying graph model to infer object location and containment information  as depicted in Figure 3  the example is taken from our ICDE poster paper  6    Our graph model G    V  E  encodes the current view of the objects in the physical world  including their reported locations and  unreported  possible containment relationships  In addition  the model incorporates statistical history about co occurrences of objects  The node set V of the graph denotes all RFID tagged objects in the physical world  These nodes are arranged into layers  with one layer for each packaging level  e g   an item  case or a pallet  which is encoded in each tag id  Nodes are assigned colors to denote their locations  The node colors are updated from the RFID readings in each epoch using the color of the location where each tag is observed  If an object is not read in an epoch  its node becomes uncolored but retains memory of the most recent observation  The directed edge set E encodes possible containment re  lationships between objects  We allow multiple outgoing and incoming edges to and from each node  indicating an object such as a case may contain multiple items  and conversely  an item may have multiple potential cases  our probabilistic inference will subsequently chose only one of these possibili  ties   To enable inference  the graph also encodes additional statistics  Each edge maintains a bit vector to record recent positive and negative evidence for the co location of the two objects  For example  the recent co location bit vector for nodes 3 and 7 in Figure 3 is 01 at time t 2 and and 011 at t 3  Further  each node remembers the last con rmed containment by a special reader such as a belt reader that scans cases one at a time  For example  at time t 2 in the  gure  the belt reader con rms that the edge from node 3 to node 7 represents the true containment  This informa  tion stays valid for a while but then becomes obsolete when containment changes  In summary  the graph model encodes the following infor  mation about each object for inference  which we call the inference state   i  the most recent observation of the object   ii  all of its possible containers   iii  its recent co  location history with each of the containers  and  iv  its con rmed container in the past by a special reader  After the graph is updated from the RFID readings in each epoch  an inference algorithm runs on the graph to es  timate the most likely container and location of each object  Our algorithm combines node inference  which derives the most likely location of an object  with edge inference  which derives the most likely container of an object  in an itera 1 2 3 4 5 6 Level 1 Locations A  loading dock Time C  packaging area t   1 t   2 t   3 Level 2 Level 3 2 3 4 5 6 7 8 2 3 4 5 6 7 10 11 9  A 1   A 1   A 1   B  2   B 2   B 2   A 1   A 1   C 3   C 3   C 3   A 1   A 1   B 3   C 3   C 2   C 3   A 1   A 1   A 1   A 1   A 1   A 1  1  A 1  B  belt B  belt 10 11 9 C  packaging area  C 2   C 2   C 2  1 Figure 3  Examples of the time varying colored graph model for containment and location inference  tive fashion through the graph  In particular  the algorithm runs  1  from the colored nodes  with known locations    2  through the edges linked to the colored nodes  where edge inference determines the container of a color node   3  to the uncolored nodes incident to these edges  where node in  ference determines the location of an uncolored node given its recent color and the colors of the processed neighboring nodes   4  to the edges linked to these nodes  and so on  As such  inference sweeps through the graph in increasing distance from the colored nodes  The above description assumes that all the data is avail  able at a central server for inference  If inference is to be made in a distributed fashion  as objects move from site to site we need to transfer inference state with the objects so that information is available for subsequent inference this process is called inference state migration  Revisit the example in Figure 3  Suppose that after time t 3  cases 2  3 and items 4  5  7 move to a new warehouse and are observed at the loading dock of the new warehouse  Now we want to infer the containers of items 4  5  7  The information in the previous inference state  such as case 3 being the con rmed container of item 7 and the co location history of items 4  5 with case 2  3  will be very useful to the inference in the new location  This indicates that inference state needs to be maintained across sites on a global scale  We detail several architectural choices for doing so in the next section  Query Processing  As the inference module streams out events with inferred location and containment information  the query processor  as shown in Figure 2 c   processes these events to answer continuous monitoring queries like the two examples above  As the  gure shows  part of query process  ing can be performed at each warehouse  such as  ltering of events based on object properties  Queries 1 and 2   a self  join over the input stream  Query 1   and a join between an object stream and a sensor stream  Query 2   A more challenging issue is with the part of query pro  cessing that spans sites  such as the detection of a complex pattern over a large period of time  see the pattern clause in Queries 1 and 2   Our discussion  rst assumes that queries run at a central server with all the information needed for query processing transfered to the server  Our query pro  cessing module employs a new type of automaton to gov  ern the pattern matching process  Each query automaton comprises a nondeterministic  nite automaton and a match bu er that stores each match of the pattern  The automa  ton for Query 1 is depicted in Figure 2 c   The start state  a 1   is where the Kleene plus operator starts to select the  rst relevant event into the match bu er  At the next state a i   it attempts to select zero  one  or more events into the bu er  The  nal state  F  represents the completion of the matching process  Each state is associated with a number of edges  representing the possible actions  Each edge has a formula expressing the condition on taking the edge  Edge formulas are evaluated using the values from the current event as well as the previous events   Details of this query automaton model are reported in our recent publication  1    In summary  the following information  called the query state  is maintained to evaluate a pattern query using our automaton model   i  the current automaton state   ii  the minimum set of values extracted from the input events that future automaton evaluation requires  e g   A 1  tag id and A 1  time for Query 1  details on extracting these values are available in  1    and  iii  the set of values that the query intends to return  which can be simple values as in Query 1  or a long sequence of readings as in Query 2  If the pattern query is de ned on a per object basis  as in both of our example queries  the system needs to maintain a copy of query state for each object  as depicted by the copies labeled O1          On in Figure 2 c   Finally  if a monitoring system supports multiple queries  the size of query state is further multiplied by the number of concurrent queries  Similar to inference  if we evaluate pattern queries in a distributed fashion  we need to perform query state mi  gration across sites so that we can resume the automaton execution in a new location and continue to expand the set of values that the query intends to return  4  ARCHITECTURAL CHOICES  BENEFITS AND DRAWBACKS There are three possible choices for instantiating the sys  tem architecture presented in the previous section  Centralized warehouse  This simplest approach is to employ a centralized architecture similar to a centralized warehouse where all RFID data is sent to a central location for stream processing  and possibly archival  The advantage of such a centralized approach is that the system has a global view of the entire supply chain  which simpli es stream pro  cessing  Inference is also simpler since all of the object state is maintained at a single location  In this case  the local servers depicted in Figure 1 only need to perform simple processing tasks such as cleaning and or compression  The primary disadvantage of the approach is the high commu  nication cost of transmitting RFID streams to the central location  since RFID data can be voluminous  the network bandwidth costs can be substantial  Analysis  Consider a supply chain with 1000 warehouses  where each warehouse stores 10 000 cases  and each case contains 10 items  Both the number of readers in a ware  house and the read frequency of each reader will vary in di erent supply chains  More readers and a higher read fre  quency yield greater monitoring accuracy  but can also lead to higher deployment and data processing costs  The choice of these parameters depends on the system budget and the monitoring requirement  Assume that  on average  there arebetween 500 to 5000 RFID readings per object every day depending on the actual deployment  Further  assume that the temperature sensors report temperature readings every 5 minutes and that there are 100 temperature sensors in each warehouse  Let each RFID reading tuple be 20 bytes  and temperature reading tuple be 9 bytes  A simple calcu  lation shows that  in this scenario  approximately 1 1 to 11 TB of data will be sent to the central location every day  Even if a compression scheme o ers a factor of 20 reduction in data volume  a  gure that we have observed in our recent research  this will still</doc>
    </owl:NamedIndividual>
</rdf:RDF>



<!-- Generated by the OWL API (version 3.2.5.1912) http://owlapi.sourceforge.net -->

